[
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-10-01T04:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8865,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a9114f80-750d-4982-9826-f1a1e31cffd4",
        "siteSettingsId": 8865,
        "fieldLayoutId": 4,
        "contentId": 3032,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Core FQL concepts part 3: Data aggregation",
        "slug": "core-fql-concepts-part-3-data-aggregation",
        "uri": "blog/core-fql-concepts-part-3-data-aggregation",
        "dateCreated": "2020-09-29T10:33:38-07:00",
        "dateUpdated": "2020-10-07T08:13:00-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/core-fql-concepts-part-3-data-aggregation",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/core-fql-concepts-part-3-data-aggregation",
        "isCommunityPost": true,
        "blogBodyText": "<p>Today we're going to explore some of the aggregate functions of FQL and a number of techniques for data aggregation.</p>\r\n<ul><li><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\">Part 1:  Working with dates and times</a></li><li><a href=\"https://fauna.com/blog/core-fql-concepts-part-2-temporality-in-faunadb\">Part 2: Temporality in FaunaDB</a></li><li>Part 3: Data Aggregation</li></ul>\r\n<p>This series assumes you have a grasp on the basics. If you're new to FaunaDB and/or FQL here's <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">my introductory series on FQL</a>.<br></p>\r\n<ul></ul>\r\n<h2>In this article:</h2>\r\n<ul><li>Introduction</li><li>Basic aggregate functions</li><li>Retrieving data for aggregation</li><li>Grouping results</li><li>Ultimate robots report</li></ul>\r\n<h1>Introduction</h1>\r\n<p>The basic idea of data aggregation is to perform one or more calculations over a set of values. Common aggregation tasks include computing the total, average, range, standard deviation, etc. In other words, you can answer questions such as \"How many spaceships are currently stationed in our Moon base?\" or \"What's the average duration of spaceship repairs?\".</p>\r\n<p>Before we get into the nuts and bolts, let's prepare some data that we can aggregate.</p>\r\n<p>First, let's create a collection and a simple index to retrieve all of the references of its documents:</p>\r\n<pre>&gt; CreateCollection({\r\n  name: \"Robots\"\r\n})\r\n\r\n&gt; CreateIndex({\r\n  name: \"all_Robots\",\r\n  source: Collection(\"Robots\")\r\n})</pre>\r\n<p>Also, let’s create some documents:</p>\r\n<pre>&gt; Create(\r\n  Collection(\"Robots\"),\r\n  {\r\n    data: {\r\n      name: \"R3-D3\",\r\n      type: \"ASTROMECH\",\r\n      weightKg: 20\r\n    }\r\n  }\r\n)\r\n\r\n&gt; Create(\r\n  Collection(\"Robots\"),\r\n  {\r\n    data: {\r\n      name: \"BB-9\",\r\n      type: \"ASTROMECH\",\r\n      weightKg: 10\r\n    }\r\n  }\r\n)\r\n\r\n&gt; Create(\r\n  Collection(\"Robots\"),\r\n  {\r\n    data: {\r\n      name: \"David\",\r\n      type: \"ANDROID\",\r\n      weightKg: 90\r\n    }\r\n  }\r\n)\r\n\r\n&gt; Create(\r\n  Collection(\"Robots\"),\r\n  {\r\n    data: {\r\n      name: \"T1000\",\r\n      type: \"ANDROID\",\r\n      weightKg: 150\r\n    }\r\n  }\r\n)\r\n\r\n&gt; Create(\r\n  Collection(\"Robots\"),\r\n  {\r\n    data: {\r\n      name: \"ASIMO\",\r\n      type: \"HUMANOID\",\r\n      weightKg: 48\r\n    }\r\n  }\r\n)</pre>\r\n<h1>Basic aggregate functions</h1>\r\n<h2>Count</h2>\r\n<p>To be able to count the elements in an array, we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/count?lang=javascript\">Count()</a> function:</p>\r\n<pre>&gt; Count([\"A\", \"B\", \"C\"])\r\n \r\n3</pre>\r\n<p>Many functions in FQL that accept arrays as input also work with other types. For example, <strong>Count()</strong> also works with a <strong>SetRef</strong> returned by <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/documents?lang=javascript\">Documents()</a> or <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/match?lang=javascript\">Match()</a> :</p>\r\n<pre>&gt; Count(Documents(Collection(\"Robots\")))\r\n\r\n5\r\n\r\n&gt; Count(Match(Index(\"all_Robots\")))\r\n \r\n5</pre>\r\n<p><strong>Documents()</strong> is a helper function that performs exactly the same function as the <strong>all_Robots</strong> index, and returns the set of references for all documents in a collection.</p>\r\n<p>We can also count how many items are returned in a page by the <strong>Paginate()</strong> function:</p>\r\n<pre>&gt; Count(Paginate(Match(Index(\"all_Robots\"))))\r\n \r\n{\r\n  data: [5]\r\n}</pre>\r\n<p>You might be wondering why the result is not exactly a number but an object with a <strong>data</strong> property. FaunaDB is actually returning a <strong>Page</strong> because we used the <strong>Page</strong> returned by <strong>Paginate()</strong> instead of an array.</p>\r\n<p>Many FQL functions that use arrays as inputs behave this way when receiving a page. Consult <a href=\"https://docs.fauna.com/fauna/current/api/fql/types?lang=javascript\">the docs</a> for more information on this.</p>\r\n<h2>Min/max values</h2>\r\n<p>To determine the max value in an array we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/max?lang=javascript\">Max()</a> function:</p>\r\n<pre>&gt; Max([22, 4, 63])\r\n \r\n63</pre>\r\n<p>It also works with strings or dates:</p>\r\n<pre>&gt; Max([\"A\", \"B\", \"C\"])\r\n \r\nC\r\n\r\n&gt; Max([\r\n  Date(\"2000-01-01\"),\r\n  Date(\"2010-01-01\"),\r\n  Date(\"2020-01-01\")\r\n])\r\n \r\nDate(\"2020-01-01\")</pre>\r\n<p>As you probably have guessed, we use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/min?lang=javascript\">Min()</a> to determine the minimum value and it works in the exact same fashion:</p>\r\n<pre>&gt; Min([22, 4, 63])\r\n \r\n4</pre>\r\n<p>On top of numbers, strings, and dates, both <strong>Min()</strong> and <strong>Max()</strong> work with other FaunaDB types. Check <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/max?lang=javascript\">the documentation</a> for more information.</p>\r\n<h2>Summing values</h2>\r\n<p>To sum a list of values we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/sum?lang=javascript\">Sum()</a> function:</p>\r\n<pre>&gt; Sum([1,2,3,4,5])\r\n \r\n15</pre>\r\n<p>Obviously, it only works with numbers. If you want to concatenate strings, you would use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/concat?lang=javascript\">Concat()</a> function.</p>\r\n<h2>Unique values</h2>\r\n<p>Another useful FQL function for aggregation queries is <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/distinct?lang=javascript\">Distinct()</a> which returns the unique values in an array:</p>\r\n<pre>&gt; Distinct([\"A\", \"B\", \"A\", \"C\", \"C\"])\r\n \r\n[\"A\", \"B\", \"C\"]</pre>\r\n<p>As we'll see later, this becomes quite handy for grouping results.</p>\r\n<h1>Retrieving data for aggregation</h1>\r\n<p>In relational databases, aggregate functions are used on a column of data (either from a table or the results of a query). FaunaDB uses documents as its underlying storage, so these calculations are done over arrays. The approach is very similar to what you'd do in regular programming using functional style techniques, which FQL naturally lends itself to.</p>\r\n<p>Let's answer the following question: what's the total weight of all our robots?</p>\r\n<h2>Using indexes</h2>\r\n<p>The first approach to getting data for aggregation is using indexes. This is the most cost-effective approach as it consumes a single read operation per page returned. Indexes do not read every document they index on every query. Instead, they return data that has been pre-written, so to speak.</p>\r\n<p>You can check how many operations a query has performed by hovering over the little icon on the sidebar of the shell results:</p>\r\n<figure><img src=\"{asset:8864:url||https://fauna.com/assets/site/bio-photos/Core-FQL-Part3-1.png}\" data-image=\"8864\" style=\"opacity: 1;\"></figure>\r\n<p>First, let's create an index that returns all of the weights of all our robots:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"Robots_weights\",\r\n  source: Collection(\"Robots\"),\r\n  values: [\r\n    {field: [\"data\", \"weightKg\"]}\r\n  ]\r\n})</pre>\r\n<p>By default, this is what our index returns:</p>\r\n<pre>&gt; Paginate(Match(Index(\"Robots_weights\")))\r\n \r\n{\r\n  data: [10, 20, 48, 90, 150]\r\n}</pre>\r\n<p>So now can just use <strong>Sum()</strong> with the results of the index:</p>\r\n<pre>&gt; Sum(Paginate(Match(Index(\"Robots_weights\"))))\r\n \r\n{\r\n  data: [318]\r\n}\r\n</pre>\r\n<h2>Aggregating the result of Map/Get</h2>\r\n<p>The second approach is to iterate over a number of documents and extract the relevant data into an array.</p>\r\n<p>This approach is certainly more flexible, as you won't need to create a new index for every value that you want to aggregate, but it consumes a read operation every time you <strong>Get()</strong> a document. You should evaluate which approach fits your use case better and choose between efficiency and flexibility.</p>\r\n<p>Before we actually sum all of the weights, let's see how to collect those values into an array:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    docs: Map(\r\n      Paginate(Documents(Collection(\"Robots\"))),\r\n      Lambda(\"ref\", Get(Var(\"ref\")))\r\n    ),\r\n    weights: Map(\r\n      Var(\"docs\"),\r\n      Lambda(\"doc\", Select([\"data\", \"weightKg\"], Var(\"doc\")))\r\n    )\r\n  },\r\n  Var(\"weights\")\r\n)\r\n\r\n{\r\n  data: [20, 10, 90, 150, 48]\r\n}</pre>\r\n<ol><li>First we get all of the documents and put them into the <strong>docs</strong> binding. Again, we could have used our <strong>all_Robots</strong> index instead of the <strong>Documents()</strong> helper.</li><li>Then we create a <strong>weights</strong> array by iterating over those documents with <strong>Map()</strong> and selecting the <strong>weightKg</strong> value.</li></ol>\r\n<p>We can now just use <strong>Sum()</strong> to calculate the total weight:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    docs: Map(\r\n      Paginate(Documents(Collection(\"Robots\"))),\r\n      Lambda(\"ref\", Get(Var(\"ref\")))\r\n    ),\r\n    weights: Map(\r\n      Var(\"docs\"),\r\n      Lambda(\"doc\", Select([\"data\", \"weightKg\"], Var(\"doc\")))\r\n    )\r\n  },\r\n  Sum(Var(\"weights\"))\r\n)\r\n \r\n{\r\n  data: [318]\r\n}</pre>\r\n<h1>Grouping results</h1>\r\n<h2>How many robots of each type are there?</h2>\r\n<p>To be able to answer that question, we first need to find out which robot types there are in our collection. To avoid reading all of the documents, we create an index that returns all of the types:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"Robots_types\",\r\n  source: Collection(\"Robots\"),\r\n  values: [\r\n    {field: [\"data\", \"type\"]}\r\n  ]\r\n})</pre>\r\n<p>This is what this index returns:</p>\r\n<pre>&gt; Paginate(Match(Index(\"Robots_types\")))\r\n \r\n{\r\n  data: [\"ANDROID\", \"ANDROID\", \"ASTROMECH\", \"ASTROMECH\", \"HUMANOID\"]\r\n}</pre>\r\n<p>We can now use <strong>Distinct()</strong> to get only the unique types:</p>\r\n<pre>&gt; Distinct(Paginate(Match(Index(\"Robots_types\"))))\r\n\r\n{\r\n  data: [\"ANDROID\", \"ASTROMECH\", \"HUMANOID\"]\r\n}</pre>\r\n<p>Great. Now we need to find a way to be able to count how many robots there are for each of those types.</p>\r\n<p>One way to do that could be using another index that filters robots per type:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"Robots_by_type\",\r\n  source: Collection(\"Robots\"),\r\n  terms: [\r\n    {field: [\"data\", \"type\"]}\r\n  ]\r\n})</pre>\r\n<p>If we now combine everything into a single query, we get this:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    types: Distinct(Paginate(Match(Index(\"Robots_types\"))))\r\n  },\r\n  Map(\r\n    Var(\"types\"),\r\n    Lambda(\r\n       \"type\",\r\n      Let(\r\n        {\r\n          refs: Match(Index(\"Robots_by_type\"), Var(\"type\"))\r\n        },\r\n        {\r\n          type: Var(\"type\"),\r\n          total: Count(Var(\"refs\"))\r\n        }\r\n      )\r\n    )\r\n  )\r\n)\r\n\r\n{\r\n  data: [\r\n    { type: 'ANDROID', total: 2 },\r\n    { type: 'ASTROMECH', total: 2 },\r\n    { type: 'HUMANOID', total: 1 }\r\n  ]\r\n}</pre>\r\n<p>We got our answer and used four read operations, one for each index that we queried.</p>\r\n<h2>What is the average weight of each robot type?</h2>\r\n<p>To answer this question, we won't be able to get away by counting elements on an array. We'll need a new index to be able to get the weights for each robot type:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"Robots_weights_by_type\",\r\n  source: Collection(\"Robots\"),\r\n  terms: [\r\n    {field: [\"data\", \"type\"]}\r\n  ],\r\n  values: [\r\n    {field: [\"data\", \"weightKg\"]}\r\n  ]\r\n})</pre>\r\n<p>Which returns this:</p>\r\n<pre>&gt; Paginate(Match(Index(\"Robots_weights_by_type\"), \"ASTROMECH\"))\r\n \r\n{\r\n  data: [10, 20]\r\n}</pre>\r\n<p>We can now compose the query to answer our question:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    types: Distinct(Paginate(Match(Index(\"Robots_types\"))))\r\n  },\r\n  Map(\r\n    Var(\"types\"),\r\n    Lambda(\r\n      \"type\",\r\n      {\r\n        type: Var(\"type\"),\r\n        averageWeight: Mean(\r\n          Match(Index(\"Robots_weights_by_type\"), Var(\"type\"))\r\n        )\r\n      }\r\n    )\r\n  )\r\n)\r\n\r\n{\r\n  data: [\r\n    { type: 'ANDROID', averageWeight: 120 },\r\n    { type: 'ASTROMECH', averageWeight: 15 },\r\n    { type: 'HUMANOID', averageWeight: 48 }\r\n  ]\r\n}\r\n</pre>\r\n<p>In this query, we're returning an array of weights for each type using the <strong>Robots_weights_by_type</strong> index, and then using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/mean?lang=javascript\">Mean()</a> to calculate the average weight.</p>\r\n<p>Just as before, we are only using 4 read operations (one for each index), which works for any number of documents in the collection.</p>\r\n<h1>Ultimate robots report</h1>\r\n<p>So let's combine everything that we've learned so far to create the ultimate robots report:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    types: Paginate(Match(Index(\"Robots_types\"))),\r\n    uniqueTypes: Distinct(Select([\"data\"], Var(\"types\"))),\r\n    weights: Paginate(Match(Index(\"Robots_weights\"))),\r\n  },\r\n  {\r\n    totalRobots: Count(Select([\"data\"], Var(\"types\"))),\r\n    totalWeight: Sum(Select([\"data\"], Var(\"weights\"))),\r\n    types: Map(\r\n      Var(\"uniqueTypes\"),\r\n      Lambda(\r\n        \"type\",\r\n        Let(\r\n          {\r\n            typeWeights: Paginate(\r\n              Match(Index(\"Robots_weights_by_type\"), Var(\"type\"))\r\n            ),\r\n          },\r\n          {\r\n            type: Var(\"type\"),\r\n            total: Count(Select([\"data\"], Var(\"typeWeights\"))),\r\n            totalWeight: Sum(Select([\"data\"], Var(\"typeWeights\"))),\r\n            averageWeight: Mean(Select([\"data\"], Var(\"typeWeights\")))\r\n          }\r\n        )\r\n      )\r\n    )\r\n  }\r\n)\r\n\r\n{\r\n  totalRobots: 5,\r\n  totalWeight: 318,\r\n  types: [\r\n    { type: 'ANDROID', total: 2, totalWeight: 240, averageWeight: 120 },\r\n    { type: 'ASTROMECH', total: 2, totalWeight: 30, averageWeight: 15 },\r\n    { type: 'HUMANOID', total: 1, totalWeight: 48, averageWeight: 48 }\r\n  ]\r\n}</pre>\r\n<h1>Conclusion</h1>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In the following article we will do a deep dive into advanced index queries.</p>\r\n<p>If you have any questions don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8869"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "603",
        "postDate": "2020-09-30T04:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8868,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "05f0f6fe-43a4-44c9-a721-b402bb528f6c",
        "siteSettingsId": 8868,
        "fieldLayoutId": 4,
        "contentId": 3035,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Build Fearlessly Podcast - Episode 3: Learning how to program (from a 12 year old)",
        "slug": "build-fearlessly-podcast-episode-3-learning-how-to-program-from-a-12-year-old",
        "uri": "blog/build-fearlessly-podcast-episode-3-learning-how-to-program-from-a-12-year-old",
        "dateCreated": "2020-09-29T11:04:49-07:00",
        "dateUpdated": "2020-09-30T08:38:30-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/build-fearlessly-podcast-episode-3-learning-how-to-program-from-a-12-year-old",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/build-fearlessly-podcast-episode-3-learning-how-to-program-from-a-12-year-old",
        "isCommunityPost": false,
        "blogBodyText": "<p>In this episode of the Build Fearlessly podcast, Fauna’s frontend developer, Ryan Harris, talks to the very talented 12 year old, Rishab Kattimani and his father Sanjay. Rishab has not only used his time in quarantine the past few months to get more acquainted with programming, but has also given back to the developer community through his YouTube channel, <a href=\"https://www.youtube.com/channel/UC6OrQk8WsnCOR1OezlUU9zQ/featured\">Rishab Teaches Tech</a>, where he’s created video tutorials for other beginner programmers.</p>\r\n<p></p>\r\n<figure><iframe src=\"https://player.blubrry.com/id/68201737/\" scrolling=\"no\" width=\"100%\" height=\"138px\" frameborder=\"0\"></iframe></figure>\r\n\r\n<h1><br>Show Notes</h1>\r\n<p><strong>Tools discussed:</strong></p>\r\n<ul><li><a href=\"https://fauna.com/\">FaunaDB</a></li><li><a href=\"https://www.python.org/\">Python</a></li><li><a href=\"https://opencv-python-tutroals.readthedocs.io/en/latest/index.html\">OpenCV</a></li><li><a href=\"https://github.com/fauna/faunadb-python\">Fauna’s Python Driver</a></li><li><a href=\"https://pypi.org/project/face-recognition/\">Face_recognition</a></li><li><a href=\"https://www.raspberrypi.org/\">Raspberry Pi</a></li><li><a href=\"https://aws.amazon.com/\">Amazon Web Services</a></li></ul>\r\n<p><strong>Other resources:</strong><br></p>\r\n<ul><li><a href=\"https://www.youtube.com/channel/UC6OrQk8WsnCOR1OezlUU9zQ/featured\">Rishab Teaches Tech (YouTube Channel)</a> </li><li><a href=\"https://scratch.mit.edu/\">Scratch programming</a></li><li><a href=\"https://www.youtube.com/watch?v=1tYCK4Yh8rQ&t=5s\">Facial recognition app (video)</a></li><li>Heroes:<ul><li><a href=\"https://twitter.com/elonmusk\">Elon Musk</a></li><li><a href=\"https://www.linkedin.com/in/tanmay-bakshi-b15012a1/\">Tanmay Bakshi</a></li></ul></li></ul>",
        "blogCategory": [
            "6957"
        ],
        "mainBlogImage": [
            "8867"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "8835",
        "postDate": "2020-09-22T09:51:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8834,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "2b407401-fed4-43f4-8f20-47a1c1551b39",
        "siteSettingsId": 8834,
        "fieldLayoutId": 4,
        "contentId": 3019,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "New Engineering and Product Leadership @ Fauna",
        "slug": "new-engineering-and-product-leadership-fauna",
        "uri": "blog/new-engineering-and-product-leadership-fauna",
        "dateCreated": "2020-09-22T09:53:31-07:00",
        "dateUpdated": "2020-09-22T11:04:52-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/new-engineering-and-product-leadership-fauna",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/new-engineering-and-product-leadership-fauna",
        "isCommunityPost": false,
        "blogBodyText": "<p>I am super excited today to welcome two new leaders to the Fauna executive team who bring a deep excellence in their respective functions and successful track records defining, operating, and scaling enterprise-class services that both delight developers and make enterprise customers wildly successful.</p>\r\n<p>Hassen Karaa joins as our Vice President of Product.  Hassen and I worked together for several years at Okta where he played a critical role in scaling the products and company as Okta went from an early stage startup to a public company with $650M in revenue. Prior to that, he spent several years as a Senior Program Manager at Microsoft working on a variety of enterprise software products.</p>\r\n<p>Tyson Trautmann joins as our Vice President of Engineering.  Tyson has a great background for Fauna with experience both building large scale platforms for internal developers during his time at Riot Games and most recently was a GM at AWS where he was responsible for the enterprise developer offerings AWS CodePipeline and AWS CodeDeploy.  He also spent some formative years early on in his career working on large scale distributed systems at Microsoft.</p>\r\n<p>Enterprise developers are rapidly adopting serverless technologies to underpin their next generation of applications and very much expect their database to be serverless as well.  Fauna has a huge opportunity ahead of us as we continue to deliver on our vision of the <a href=\"https://fauna.com/company\">data API</a> that enables developers to simplify code, reduce costs and ship faster.</p>\r\n<p>One of my highest priorities after joining Fauna was to recruit world class leaders for our product and engineering functions to compliment the team that was already on the ground.  The engineering organization has built a very impressive technology platform and as we have seen a tremendous amount of interest in Fauna from developers and enterprise organizations.  The addition of Hassen and Tyson will really help us accelerate our innovation engine as we move forward.</p>\r\n<p>So please join me in welcoming Hassen and Tyson to the Fauna community and please check out our <a href=\"https://www.businesswire.com/news/home/20200922005250/en/Fauna-Expands-Executive-Team-With-Enterprise-Talent-From-AWS-and-Okta\">press</a> release if you want to read more and hear directly from them as to why they are also so excited about the road ahead.</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [
            "8837"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-09-16T06:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8828,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "98a6fff0-32fc-4b23-ac43-098e265c7132",
        "siteSettingsId": 8828,
        "fieldLayoutId": 4,
        "contentId": 3013,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Core FQL concepts part 2: Temporality in FaunaDB",
        "slug": "core-fql-concepts-part-2-temporality-in-faunadb",
        "uri": "blog/core-fql-concepts-part-2-temporality-in-faunadb",
        "dateCreated": "2020-09-15T10:20:01-07:00",
        "dateUpdated": "2020-09-29T10:37:29-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/core-fql-concepts-part-2-temporality-in-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/core-fql-concepts-part-2-temporality-in-faunadb",
        "isCommunityPost": true,
        "blogBodyText": "<p>Today we're going to explore one of FaunaDB's most unique features: its temporal capabilities.</p>\r\n<p>This series assumes you have a grasp on the basics. If you're new to FaunaDB and/or FQL here's <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">my introductory series on FQL</a>.</p>\r\n<p><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a></p><ul><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a><li><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\">Part 1: Working with dates and times</a></li><li>Part 2: Temporality in FaunaDB</li><li>Part 3: Data Aggregation</li></ul><ul></ul>\r\n<h2>In this article:</h2>\r\n<ul><li>Document history</li><li>Querying document history</li><li>Filtering history events</li><li>Travelling back in time</li><li>Recovering documents</li><li>Careful with the spacetime continuum</li><li>Use cases and considerations</li></ul>\r\n<h1>Document history</h1>\r\n<p>One of the main temporal features of FaunaDB is being able to record every change you make to a document. When this is configured, new document snapshots are created for every change and added to the document's history.</p>\r\n<p>By default, when creating a collection we get 30 days of minimum document history:</p>\r\n<pre>CreateCollection({name: \"MyCollection\"})\r\n\r\n{\r\nref: Collection(\"MyCollection\"),\r\nts: 1598891726415000,\r\nhistory_days: 30,\r\nname: \"MyCollection\"\r\n}</pre>\r\n<p>History will last for at least 30 days with the default <strong>history_days</strong> value, but there are no guarantees when the deletion occurs as FaunaDB periodically reclaims expired snapshots.</p>\r\n<p>We can change the minimum default to any number of days that fits our use case:</p>\r\n<pre>CreateCollection({\r\n  name: \"CollectionWithAYearOfHistory\",\r\n  history_days: 365\r\n})</pre>\r\n<p>If we wanted to retain all history until the end of times we'd just use a <strong>null</strong> value:</p>\r\n<pre>CreateCollection({\r\n  name: \"CollectionForever\",\r\n  history_days: null\r\n})\r\n</pre>\r\n<p>It's also possible to expire all history by default by passing a <strong>0</strong> value:</p>\r\n<pre>CreateCollection({\r\n  name: \"CollectionNoHistory\",\r\n  history_days: 0\r\n})\r\n</pre>\r\n<h2>Difference with ttl_days</h2>\r\n<p>When creating a new collection it's also possible to configure <strong>ttl_days</strong> (time-to-live days). This setting is not related to its history. Instead, it refers to how long you'd like to keep the current or active version of a document alive. This is most useful for ephemeral data such as sessions, caches, and so on.</p>\r\n<p>Check <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/createcollection?lang=javascript\">the documentation</a> for more details.</p>\r\n<h2>Security implications</h2>\r\n<p>It should be noted that having permissions to read a document doesn't automatically grant permission to read or modify its history. Your users will need dedicated permissions to be able to interact with the history of a document.</p>\r\n<p>For more info on authorization in FaunaDB <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5\">check my previous article</a>.</p>\r\n<h1>Querying document history</h1>\r\n<p>Before we go on, let's create some data:</p>\r\n<pre>&gt; CreateCollection({name: \"Lasers\"})\r\n\r\n&gt; Create(\r\n  Collection('Lasers'),\r\n  {\r\n    data:{\r\n      color:\"BLUE\",\r\n      crystal: \"KYBER\"\r\n    }\r\n  }\r\n)\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n  ts: 1599063943620000,\r\n  data: {\r\n    color: \"BLUE\",\r\n    crystal: \"KYBER\"\r\n  }\r\n}\r\n</pre>\r\n<p>Let's also modify the document we just created to get some history:</p>\r\n<pre>&gt; Update(\r\n  Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n  {data:{color:\"GREEN\"}}\r\n)\r\n\r\n&gt; Delete(\r\n  Ref(Collection(\"Lasers\"), \"275571113732342290\")\r\n)\r\n</pre>\r\n<p>We can query a document's history by using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/events?lang=javascript\">Events()</a> function. Here's an example of what this history looks like:</p>\r\n<pre>&gt; Paginate(\r\n  Events(Ref(Collection(\"Lasers\"), \"275571113732342290\"))\r\n)\r\n \r\n{\r\n  data: [\r\n    {\r\n      ts: 1599063943620000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n      data: {\r\n        color: \"BLUE\",\r\n        crystal: \"KYBER\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599064033700000,\r\n      action: \"update\",\r\n      document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n      data: {\r\n        color: \"GREEN\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599064043550000,\r\n      action: \"delete\",\r\n      document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n      data: null\r\n    }\r\n  ]\r\n}</pre>\r\n<p>As you can see, <strong>Events()</strong> takes a document reference and returns a list of all the changes we've performed on that document. An important point is that this history is available even after the document has been deleted.</p>\r\n<p>Instead of using <strong>Events(),</strong> we could also use the options of <strong>Paginate()</strong> to retrieve the history of multiple documents at once:</p>\r\n<pre>&gt; Paginate(\r\n  Documents(Collection(\"Lasers\")),\r\n  {events: true}\r\n)\r\n\r\n{\r\n  data: [\r\n    {\r\n      ts: 1599063943620000,\r\n      action: \"add\",\r\n      document: Ref(Collection(\"Lasers\"), \"275571113732342290\")\r\n    },\r\n    {\r\n      ts: 1599064043550000,\r\n      action: \"remove\",\r\n      document: Ref(Collection(\"Lasers\"), \"275571113732342290\")\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>In this example, we are getting all the history of all the documents in the <strong>Lasers</strong> collection by combining <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/documents?lang=javascript\">Documents()</a> with <strong>{events: true}</strong>.</p>\r\n<p>This approach is extremely powerful as you could, for instance, combine multiple indexes to fetch the history of documents based on complex filters, or even from documents from multiple collections. Definitely check out the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/paginate?lang=javascript\">Paginate()</a> documentation for more info on this.</p>\r\n<h1>Filtering history events</h1>\r\n<p>When querying the history of a document by using <strong>Events()</strong>, we get an array which can be manipulated just like any other array in FaunaDB.<br><br>For example here's how we could just get the <strong>update</strong> events:</p>\r\n<pre>&gt; Filter(\r\n  Select(\r\n    [\"data\"],\r\n    Paginate(Events(Ref(Collection(\"Lasers\"), \"275571113732342290\")))\r\n  ),\r\n  Lambda(\r\n    \"event\",\r\n    Equals(\r\n      Select([\"action\"], Var(\"event\")),\r\n      \"update\"\r\n    )\r\n  )\r\n)\r\n \r\n[\r\n  {\r\n    ts: 1599064033700000,\r\n    action: \"update\",\r\n    document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n    data: {\r\n      color: \"GREEN\"\r\n    }\r\n  }\r\n]\r\n</pre>\r\n<p>We could also filter events by date:</p>\r\n<pre>&gt; Filter(\r\n  Select(\r\n    [\"data\"],\r\n    Paginate(Events(Ref(Collection(\"Lasers\"), \"275571113732342290\")))\r\n  ),\r\n  Lambda(\r\n    \"event\",\r\n    Equals(\r\n      ToDate(\r\n        Epoch(\r\n          Select([\"ts\"], Var(\"event\")),\r\n          \"microseconds\"\r\n        )\r\n      ),\r\n      Date('2020-09-02')\r\n    )\r\n  )\r\n)\r\n \r\n[\r\n  {\r\n    ts: 1599063943620000,\r\n    action: \"create\",\r\n    document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n    data: {\r\n      color: \"BLUE\",\r\n      crystal: \"KYBER\"\r\n    }\r\n  },\r\n  {\r\n    ts: 1599064033700000,\r\n    action: \"update\",\r\n    document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n    data: {\r\n      color: \"GREEN\"\r\n    }\r\n  },\r\n  {\r\n    ts: 1599064043550000,\r\n    action: \"delete\",\r\n    document: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n    data: null\r\n  }\r\n]\r\n</pre>\r\n<p><strong>Quick note:</strong> We explored working with dates and times <a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\">in the previous article</a>. As a quick reminder, we're converting an integer to a <strong>Timestamp</strong> using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/epoch?lang=javascript\">Epoch()</a>, and then that timestamp to a <strong>Date,</strong> to be able to compare it with <strong>Date('2020-09-02')</strong>.</p>\r\n<p>We could also use the <strong>Paginate()</strong> options to get the events after or before a certain date:</p>\r\n<pre>&gt; Paginate(\r\n  Documents(Collection(\"Lasers\")),\r\n  {\r\n    events: true,\r\n    before: Date(\"2020-09-03\")\r\n  }\r\n)\r\n\r\n&gt; Paginate(\r\n  Documents(Collection(\"Lasers\")),\r\n  {\r\n    events: true,\r\n    after: Date(\"2020-09-03\")\r\n  }\r\n)\r\n</pre>\r\n<h1>Travelling back in time</h1>\r\n<p>The document history tells us that something happened at some point in time. What if we wanted to know the complete state of a document in the past? For example, after an update event or even in between history events?</p>\r\n<p>We can use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/at?lang=javascript\">At()</a> function to do exactly that:</p>\r\n<pre>&gt; At(\r\n  1599064033700000,\r\n  Get(Ref(Collection(\"Lasers\"), \"275571113732342290\"))\r\n)\r\n \r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275571113732342290\"),\r\n  ts: 1599064033700000,\r\n  data: {\r\n    color: \"GREEN\",\r\n    crystal: \"KYBER\"\r\n  }\r\n}</pre>\r\n<p>Much like a time machine, <strong>At()</strong> allows us to execute FQL queries in the past. Here we're using the <strong>1599064033700000</strong> timestamp from the <strong>update</strong> event to get the full document when we updated the color to <strong>GREEN</strong>.<strong><br><br>At()</strong> sets the point in time for all  reads in FQL for that query. We can use indexes, filter data, etc. It's exactly like querying FaunaDB at that time.<br><br>It doesn't work exactly like a time machine though. We can't really write changes in the past with it. If we try to use <strong>At()</strong> to make any changes at any other time than <strong>Now()</strong> we will get an error.<br><br>Let's create a new document to try this out:</p>\r\n<pre>&gt; Create(\r\n  Collection('Lasers'),\r\n  {data:{color:\"RED\"}}\r\n)\r\n\r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275572558214988288\"),\r\n  ts: 1599065321160000,\r\n  data: {\r\n    color: \"RED\"\r\n  }\r\n}</pre>\r\n<p>And now let's add a couple of milliseconds to the creation timestamp and try to update the document in the past:</p>\r\n<pre>&gt; At(\r\n  1599065321170000,\r\n  Update(\r\n    Ref(Collection(\"Lasers\"), \"275572558214988288\"),\r\n    {data: {color: \"PURPLE\"}}\r\n  )\r\n)\r\n\r\nError: [\r\n  {\r\n    \"position\": [\r\n      \"expr\"\r\n    ],\r\n    \"code\": \"invalid write time\",\r\n    \"description\": \"Cannot write outside of snapshot time.\"\r\n  }\r\n]</pre>\r\n<h1>Altering the spacetime continuum</h1>\r\n<p>While we cannot use <strong>At()</strong> to alter the past, it is possible to use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/insert?lang=javascript\">Insert()</a> and <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/remove?lang=javascript\">Remove()</a> to modify the events in a document's history.</p>\r\n<p>To demonstrate this, let's create a new document first:</p>\r\n<pre>&gt; Create(\r\n  Collection('Lasers'),\r\n  {\r\n    data:{\r\n      color:\"ORANGE\",\r\n      crystal: \"QUARTZIUM\"\r\n    }\r\n  }\r\n)\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  ts: 1599166374745000,\r\n  data: {\r\n    color: \"ORANGE\",\r\n    crystal: \"QUARTZIUM\"\r\n  }\r\n}\r\n</pre>\r\n<p>Now let's see what happens if we insert a create event 1 second before we create that document:</p>\r\n<pre>&gt; Insert(\r\n  Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  TimeSubtract(Epoch(1599166374745000, \"microseconds\"), 1, \"second\"),\r\n  \"create\",\r\n  {\r\n    data: {\r\n      color: \"YELLOW\",\r\n      crystal: \"QUARTZIUM\"\r\n    }\r\n  }\r\n)\r\n \r\n{\r\n  ts: 1599166373745000,\r\n  action: \"create\",\r\n  document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  data: {\r\n    color: \"YELLOW\",\r\n    crystal: \"QUARTZIUM\"\r\n  }\r\n}</pre>\r\n<p>Keep in mind that the history is related to a document reference. Be careful not to use the wrong reference with <strong>Insert()</strong> and <strong>Remove()</strong>, otherwise these changes will have unintended consequences.</p>\r\n<p>Let's see how the insertion of the <strong>create</strong> event affected the history of the document:</p>\r\n<pre>&gt; Paginate(Events(Ref(Collection(\"Lasers\"), \"275678520646042112\")))\r\n \r\n{\r\n  data: [\r\n    {\r\n      ts: 1599166373745000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n      data: {\r\n        color: \"YELLOW\",\r\n        crystal: \"QUARTZIUM\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599166374745000,\r\n      action: \"update\",\r\n      document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n      data: {\r\n        color: \"ORANGE\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<p>The new <strong>create</strong> event we just inserted is where we expect it to be, one second before the initial  document creation, but look what happened with the last event in the array. It used to be a <strong>create</strong> event but it has been now converted to an <strong>update</strong> event.</p>\r\n<p>In any case, if we now get the latest version of the document we can see it is unchanged:</p>\r\n<pre>&gt; Get(Ref(Collection(\"Lasers\"), \"275678520646042112\"))\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  ts: 1599166374745000,\r\n  data: {\r\n    color: \"ORANGE\",\r\n    crystal: \"QUARTZIUM\"\r\n  }\r\n}</pre>\r\n<h1>Recovering deleted documents</h1>\r\n<p>I'm sure I'm not the only one that has wanted to go back in time to restore something that was deleted by accident. FaunaDB offers a couple of ways to do this.</p>\r\n<h2>Removing the delete event</h2>\r\n<p>We've seen that deleting a document creates a <strong>delete</strong> event. Could we use <strong>Remove()</strong> to undo that?</p>\r\n<p>First let's delete our orange laser:</p>\r\n<pre>&gt; Delete(Ref(Collection(\"Lasers\"), \"275678520646042112\"))</pre>\r\n<p>Now let's get all the delete events for this document:</p>\r\n<pre>&gt; Filter(\r\n  Select(\r\n    [\"data\"],\r\n    Paginate(Events(Ref(Collection(\"Lasers\"), \"275678520646042112\")))\r\n  ),\r\n  Lambda(\r\n    \"event\",\r\n    Equals(Select([\"action\"], Var(\"event\")),\"delete\")\r\n  )\r\n)\r\n\r\n[\r\n  {\r\n    ts: 1599177868380000,\r\n    action: \"delete\",\r\n    document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n    data: null\r\n  }\r\n]</pre>\r\n<p>Cool, so let's use <strong>Remove() </strong>to remove the delete event:</p>\r\n<pre>&gt; Remove(\r\n  Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  1599178388870000,\r\n  \"delete\"\r\n)</pre>\r\n<p>We can confirm the removal of the <strong>delete</strong> event by checking its history:</p>\r\n<pre>&gt; Paginate(Events(Ref(Collection(\"Lasers\"), \"275678520646042112\")))\r\n \r\n{\r\n  data: [\r\n    {\r\n      ts: 1599166373745000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n      data: {\r\n        color: \"YELLOW\",\r\n        crystal: \"QUARTZIUM\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599166374745000,\r\n      action: \"update\",\r\n      document: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n      data: {\r\n        color: \"ORANGE\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<p>We now can even <strong>Get()</strong> the document as usual:</p>\r\n<pre>Get(Ref(Collection(\"Lasers\"), \"275678520646042112\"))\r\n\r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275678520646042112\"),\r\n  ts: 1599166374745000,\r\n  data: {\r\n    color: \"ORANGE\",\r\n    crystal: \"QUARTZIUM\"\r\n  }\r\n}</pre>\r\n<h2>Bringing data back from the past</h2>\r\n<p>Another option to recover data is basically reading it from the past using <strong>At()</strong> and inserting it again back into the present.</p>\r\n<p>Let's create a new laser to start with a fresh document:</p>\r\n<pre>&gt; Create(\r\n  Collection('Lasers'),\r\n  {\r\n    data:{\r\n      color:\"BLUE\"\r\n    }\r\n  }\r\n)\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n  ts: 1599236727185000,\r\n  data: {\r\n    color: \"BLUE\"\r\n  }\r\n}</pre>\r\n<p>Let's delete our document and check its history:</p>\r\n<pre>&gt; Delete(Ref(Collection(\"Lasers\"), \"275752290343715347\"))\r\n\r\n&gt; Paginate(Events(Ref(Collection(\"Lasers\"), \"275752290343715347\")))\r\n \r\n{\r\n  data: [\r\n    {\r\n      ts: 1599236727185000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n      data: {\r\n        color: \"BLUE\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599236793325000,\r\n      action: \"delete\",\r\n      document: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n      data: null\r\n    }\r\n  ]\r\n}</pre>\r\n<p>We could now use the timestamp of the <strong>create</strong> event with <strong>At()</strong> to read it:</p>\r\n<pre>&gt; At(\r\n  1599236727185000,\r\n  Get(Ref(Collection(\"Lasers\"), \"275752290343715347\"))\r\n)\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n  ts: 1599236727185000,\r\n  data: {\r\n    color: \"BLUE\"\r\n  }\r\n}</pre>\r\n<p>And finally restore the data before deletion, by simply creating a document with the same reference and data:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    deletedDoc: At(\r\n      1599236727185000,\r\n      Get(Ref(Collection(\"Lasers\"), \"275752290343715347\"))\r\n    ),\r\n    ref: Select([\"ref\"], Var(\"deletedDoc\")),\r\n    data: Select([\"data\"], Var(\"deletedDoc\"))\r\n  },\r\n  Create(\r\n    Var(\"ref\"),\r\n    { data: Var(\"data\") }\r\n  )\r\n)\r\n \r\n{\r\n  ref: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n  ts: 1599237181566000,\r\n  data: {\r\n    color: \"BLUE\"\r\n  }\r\n}</pre>\r\n<p><strong>Quick note:</strong> <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/let?lang=javascript\">Let()</a> not only allows us to format output objects but also execute any output query like we're doing here after having collected the necessary data.</p>\r\n<p>Let's check what happened to the document's history:</p>\r\n<pre>Paginate(Events(Ref(Collection(\"Lasers\"), \"275752290343715347\")))\r\n \r\n{\r\n  data: [\r\n    {\r\n      ts: 1599236727185000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n      data: {\r\n        color: \"BLUE\"\r\n      }\r\n    },\r\n    {\r\n      ts: 1599236793325000,\r\n      action: \"delete\",\r\n      document: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n      data: null\r\n    },\r\n    {\r\n      ts: 1599237181566000,\r\n      action: \"create\",\r\n      document: Ref(Collection(\"Lasers\"), \"275752290343715347\"),\r\n      data: {\r\n        color: \"BLUE\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n</pre>\r\n<p>As you can see, even though we created a new document, the new <strong>create</strong> event has been appended to the previous history for that reference. This is happening because the history is actually associated with a document reference.</p>\r\n<h1>Use cases and considerations</h1>\r\n<p>Now that we've seen an overview of the temporal features in FaunaDB let's examine a couple of common use cases in more detail.</p>\r\n<h2>Safe delete</h2>\r\n<p>It's very common in databases without history or versioning to follow a couple of patterns to prevent data loss. One of those patterns marks data as deleted but keeps it around in its original place. This practice is usually called \"soft delete\". Another alternative is to have a \"recycle bin\", that's a table or a collection where you move the data to be deleted instead of actually deleting it.</p>\r\n<p>Both of these patterns have pros and cons, and you can certainly implement them with FaunaDB. Although, unless you need specific features, the simplest option is certainly to delete documents as usual and use the temporal features we just saw to keep deleted data around for as long as you need it.</p>\r\n<p>There are two major points to consider though:<br></p>\r\n<ol><li>Fauna stores every single document change in its history, which occupies storage space. It might be overkill for simple safe delete at scale if you don't need document versioning.</li><li>Since querying the history returns an array of events, you won't have as much flexibility as you would have by storing the deleted documents in a dedicated collection. For example, by using a \"recycle bin\" collection approach, you could use indexes and filter deleted documents by user, deletion reason, etc.</li></ol>\r\n<h2>Version control</h2>\r\n<p>Another major use case for the temporal features of FaunaDB is storing and keeping track of content changes. Let's say you're working on SpaceDocs, the next app for collaborative writing of spacey documents. You obviously need a version history feature to keep track of changes.</p>\r\n<p>Storage shouldn't be a concern here, since you would be storing all of the versions anyway, say in a <strong>DocVersions</strong> collection. But, again, if you used the history to keep older versions around you wouldn't be able to use common FQL features such as indexes. Maybe this would limit the kind of features you can build in the future.</p>\r\n<p>Again, it all depends on your use case.</p>\r\n<h1>Conclusion</h1>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In the following article of the series, we will continue our space adventure by checking on aggregation queries and other features.</p>\r\n<p><br>If you have any questions don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a><br></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8829"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "8698",
        "postDate": "2020-09-01T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8697,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "68c9dea2-2f09-44f6-a0a3-d7958b4e05ec",
        "siteSettingsId": 8697,
        "fieldLayoutId": 4,
        "contentId": 2948,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Building a minimum viable full-stack with RedwoodJS and FaunaDB",
        "slug": "building-a-minimum-viable-stack-with-redwoodjs-and-faunadb",
        "uri": "blog/building-a-minimum-viable-stack-with-redwoodjs-and-faunadb",
        "dateCreated": "2020-08-28T13:08:00-07:00",
        "dateUpdated": "2020-09-03T18:47:02-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/building-a-minimum-viable-stack-with-redwoodjs-and-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/building-a-minimum-viable-stack-with-redwoodjs-and-faunadb",
        "isCommunityPost": true,
        "blogBodyText": "<h2>Introduction</h2>\r\n<h3>RedwoodJS</h3>\r\n<p>RedwoodJS is an opinionated, full-stack, serverless web application framework for building and deploying JAMstack\r\n    applications. It was created by Tom-Preston Werner and combines a variety of technologies and techniques that have\r\n    been influencing the industry over the last 5-10 years. This includes:</p>\r\n<ul>\r\n    <li>React frontend</li>\r\n    <li>Statically delivered files served from a CDN</li>\r\n    <li>GraphQL to connect your frontend and backend</li>\r\n    <li>AWS Lambdas for serverless deployment</li>\r\n    <li>Deployable with a single git push command</li>\r\n</ul>\r\n<blockquote>My dream of a future is for something I call a universal deployment machine, which means I write my code. It's\r\n        all text, I just write text. Then I commit to GitHub. Then it's picked up and it's deployed into reality. That's\r\n        it, that's the whole thing. -Tom Preston-Warner</blockquote>\r\n<p><a href=\"https://shoptalkshow.com/412/\">RedwoodJS Shoptalk (May 11, 2020)</a></p>\r\n<h3>FaunaDB</h3>\r\n<p>\r\n    FaunaDB is a serverless global database designed for low latency and developer productivity. It has proven to be\r\n        particularly appealing to Jamstack developers for its global scalability, native GraphQL API, and FQL query\r\n        language.\r\n</p>\r\n<blockquote>Being a serverless distributed database, the JAMstack world is a natural fit for our system, but long before we\r\n        were chasing JAMstack developers, we were using the stack ourselves. -Matt Attaway</blockquote>\r\n<p><a href=\"https://fauna.com/blog/lessons-learned-livin-la-vida-jamstack\">Lessons Learned Livin' La Vida JAMstack\r\n            (January 24, 2020)</a></p>\r\n<p>In this post, we will walk through how to create an application with RedwoodJS and FaunaDB.</p>\r\n<h2>Redwood Monorepo</h2>\r\n<h3>Create Redwood App</h3>\r\n<p>To start we'll create a new Redwood app from scratch with the Redwood CLI. If you don't have yarn installed enter the\r\n    following command:</p>\r\n<pre>yarn install</pre>\r\n<p> </p>\r\n<p>Now we'll use yarn create redwood-app to generate the basic structure of our app.</p>\r\n<pre>yarn create redwood-app ./redwood-fauna</pre>\r\n<p>I've called my project redwood-fauna but feel free to select whatever name you want for your application. We'll now\r\n    cd into our new project and use yarn rw dev to start our development server.</p>\r\n<pre>cd redwood-fauna\r\nyarn rw dev</pre>\r\n<p>Our project's frontend is running on localhost:8910 and our backend is running on localhost:8911 ready to receive\r\n    GraphQL queries.</p>\r\n<figure><img src=\"{asset:8785:url||https://fauna.com/assets/site/icons-features/ycisyl2bbxasa9j6zr9b.png}\" data-image=\"8785\" alt=\"Redwood.js\" style=\"opacity: 1;\"></figure>\r\n<h3>Redwood Directory Structure</h3>\r\n<p>One of Redwood's guiding philosophies is that there is power in standards, so it makes decisions for you about which\r\n    technologies to use, how to organize your code into files, and how to name things.</p>\r\n<p>It can be a little overwhelming to look at everything that's already been generated for us. The first thing to pay\r\n    attention to is that Redwood apps are separated into two directories:</p>\r\n<ul>\r\n    <li>api for backend</li>\r\n    <li>web for frontend</li>\r\n</ul>\r\n<pre>├── api\r\n│ ├── prisma\r\n│ │ ├── schema.prisma\r\n│ │ └── seeds.js\r\n│ └── src\r\n│ ├──functions\r\n│ │ └── graphql.js\r\n│ ├── graphql\r\n│ ├── lib\r\n│ │ └── db.js\r\n│ └──services\r\n└── web\r\n├── public\r\n│ ├── favicon.png\r\n│ ├── README.md\r\n│ └──robots.txt\r\n└── src\r\n├── components\r\n├── layouts\r\n├── pages\r\n├── FatalErrorPage\r\n│\r\n└── FatalErrorPage.js\r\n└── NotFoundPage\r\n└── NotFoundPage.js\r\n├── index.css\r\n├──index.html\r\n├── index.js\r\n└── Routes.js\r\n                </pre>\r\n<p>Each side has their own path in the codebase. These are managed by Yarn <a href=\"https://classic.yarnpkg.com/en/docs/workspaces/\">workspaces</a>. We will be talking to the Fauna client\r\n    directly so we can delete the prisma directory along with the files inside it and we can delete all the code in\r\n    db.js.</p>\r\n<h3>Pages</h3>\r\n<p>With our application now set up we can start creating pages. We'll use the generate page command to create a home\r\n    page and a folder to hold that page. Instead of generate we can use g to save some typing.</p>\r\n<pre>yarn rw g page home /</pre>\r\n<figure><img src=\"{asset:8786:url||https://fauna.com/assets/site/icons-features/bi7y7f1qd5o1fdi8jhfh.jpg}\" data-image=\"8786\" style=\"opacity: 1;\"></figure>\r\n<p>If we go to our web/src/pages directory we'll see a HomePage directory containing this HomePage.js file:</p>\r\n<pre>// web/src/pages/HomePage/HomePage.js\r\n\r\nimport { Link } from '@redwoodjs/router'\r\n\r\nconst HomePage = () =&gt; {\r\n  return (\r\n    &lt;&gt;\r\n      &lt;h1&gt;HomePage&lt;/h1&gt;\r\n      &lt;p&gt;Find me in \"./web/src/pages/HomePage/HomePage.js\"&lt;/p&gt;\r\n      &lt;p&gt;\r\n        My default route is named \"home\", link to me with `\r\n        &lt;Link to=\"home\"&gt;routes.home()&lt;/Link&gt;`\r\n      &lt;/p&gt;\r\n    &lt;/&gt;\r\n  )\r\n}\r\n\r\nexport default HomePage\r\n</pre>\r\n<p>Let's clean up our component. We'll only have a single route for now so we can delete the Link import and\r\n    routes.home(), and we'll delete everything except a single &lt;h1&gt; tag.</p>\r\n<pre>// web/src/pages/HomePage/HomePage.js\r\n\r\nimport { Link } from '@redwoodjs/router'\r\n\r\nconst HomePage = () =&gt; {\r\n  return (\r\n    &lt;&gt;\r\n      &lt;h1&gt;HomePage&lt;/h1&gt;\r\n      &lt;p&gt;Find me in \"./web/src/pages/HomePage/HomePage.js\"&lt;/p&gt;\r\n      &lt;p&gt;\r\n        My default route is named \"home\", link to me with `\r\n        &lt;Link to=\"home\"&gt;routes.home()&lt;/Link&gt;`\r\n      &lt;/p&gt;\r\n    &lt;/&gt;\r\n  )\r\n}\r\n\r\nexport default HomePage\r\n</pre>\r\n<figure><img src=\"{asset:8787:url||https://fauna.com/assets/site/icons-features/yujrc0ic7tewsr1uszic.jpg}\" data-image=\"8787\" style=\"opacity: 1;\"></figure>\r\n<h3>Cells</h3>\r\n<p>Cells provide a simpler and more declarative approach to data fetching. They contain the GraphQL query, loading,\r\n    empty, error, and success states, each one rendering itself automatically depending on what state your cell is in.\r\n</p>\r\n<p>Create a folder in web/src/components called PostsCell and inside that folder create a file called PostsCell.js with\r\n    the following code:</p>\r\n<pre>// web/src/components/PostsCell/PostsCell.js\r\n\r\nexport const QUERY = gql`\r\n  query POSTS {\r\n    posts {\r\n      data {\r\n        title\r\n      }\r\n    }\r\n  }\r\n`\r\n\r\nexport const Loading = () =&gt; &lt;div&gt;Loading posts...&lt;/div&gt;\r\nexport const Empty = () =&gt; &lt;div&gt;No posts yet!&lt;/div&gt;\r\nexport const Failure = ({ error }) =&gt; &lt;div&gt;Error: {error.message}&lt;/div&gt;\r\n\r\nexport const Success = ({ posts }) =&gt; {\r\n  const {data} = posts\r\n  return (\r\n    &lt;ul&gt;\r\n      {data.map(post =&gt; (\r\n        &lt;li&gt;{post.title}&lt;/li&gt;\r\n      ))}\r\n    &lt;/ul&gt;\r\n  )\r\n}\r\n</pre>\r\n<p>We’re exporting a GraphQL query that will fetch the posts in the database. We use object destructuring to access the\r\n    data object and then we map over that response data to display a list of our posts. To render our list of posts we\r\n    need to import PostsCell in our HomePage.js file and return the component.</p>\r\n<pre>// web/src/pages/HomePage/HomePage.js\r\n\r\nimport PostsCell from 'src/components/PostsCell'\r\n\r\nconst HomePage = () =&gt; {\r\n  return (\r\n    &lt;&gt;\r\n      &lt;h1&gt;RedwoodJS+Fauna&lt;/h1&gt;\r\n      &lt;PostsCell /&gt;\r\n    &lt;/&gt;\r\n  )\r\n}\r\n\r\nexport default HomePage\r\n</pre>\r\n<figure><img src=\"{asset:8788:url||https://fauna.com/assets/site/icons-features/Screen-Shot-2020-08-24-at-6.13.06-AM.jpg}\" data-image=\"8788\" style=\"opacity: 1;\"></figure>\r\n<h3>Schema Definition Language</h3>\r\n<p>In our graphql directory we'll create a file called posts.sdl.js containing our GraphQL schema. In this file we'll export a schema object containing our GraphQL schema definition language. It is defining a Post type which has a title that is the type of String.</p>\r\n<p>Fauna automatically creates a PostPage type for pagination which has a data type that'll contain an array with every Post. When we create our database you will need to import a slightly modified schema so Fauna knows how to respond to our GraphQL queries. We will explain how to do this in the Import Schema section after creating the database.</p>\r\n<pre>// api/src/graphql/posts.sdl.js\r\n\r\nimport gql from 'graphql-tag'\r\n\r\nexport const schema = gql`\r\n  type Post {\r\n    title: String\r\n  }\r\n\r\n  type PostPage {\r\n    data: [Post]\r\n  }\r\n\r\n  type Query {\r\n    posts: PostPage\r\n  }\r\n`\r\n</pre>\r\n<h3>DB</h3>\r\n<p>When we generated our project, db defaulted to an instance of PrismaClient. Since Prisma does not support Fauna at\r\n    this time we will be using the graphql-request library to query Fauna's GraphQL API. First make sure to add the\r\n    library to your project.</p>\r\n<pre>npm add graphql-request graphql</pre>\r\n<p>To access our FaunaDB database through the GraphQL endpoint we’ll need to set a request header containing our\r\n    database key. We’ll see how to create our database key later in this tutorial.</p>\r\n<pre>// api/src/lib/db.js\r\n\r\nimport { GraphQLClient } from 'graphql-request'\r\n\r\nexport const request = async (query = {}) =&gt; {\r\n  const endpoint = 'https://graphql.fauna.com/graphql'\r\n\r\n  const graphQLClient = new GraphQLClient(endpoint, {\r\n    headers: {\r\n      authorization: 'Bearer &lt;FAUNADB_KEY&gt;'\r\n    },\r\n  })\r\n  try {\r\n    return await graphQLClient.request(query)\r\n  } catch (error) {\r\n    console.log(error)\r\n    return error\r\n  }\r\n}\r\n</pre>\r\n<h3>Services</h3>\r\n<p>In our services directory we'll create a posts directory with a file called posts.js. Services are where Redwood\r\n    centralizes all business logic. These can be used by your GraphQL API or any other place in your backend code. The\r\n    posts function is querying the Fauna GraphQL endpoint and returning our posts data so it can be consumed by our\r\n    PostsCell.</p>\r\n<pre>// api/src/services/posts/posts.js\r\n\r\nimport { request } from 'src/lib/db'\r\nimport { gql } from 'graphql-request'\r\n\r\nexport const posts = async () =&gt; {\r\n  const query = gql`\r\n  {\r\n    posts {\r\n      data {\r\n        title\r\n      }\r\n    }\r\n  }\r\n  `\r\n\r\n  const data = await request(query, 'https://graphql.fauna.com/graphql')\r\n\r\n  return data['posts']\r\n}\r\n</pre>\r\n<h3>GraphQL Serverless Function</h3>\r\n<p>Files in api/src/functions are serverless functions. Most of @redwoodjs/api is for setting up the GraphQL API Redwood\r\n    Apps come with by default. It happens in essentially four steps:</p>\r\n<ol>\r\n    <li>Everything (i.e. sdl and services) is imported</li>\r\n    <li>The services are wrapped into resolvers</li>\r\n    <li>The sdl and resolvers are merged/stitched into a schema</li>\r\n    <li>The ApolloServer is instantiated with said merged/stitched schema and context</li>\r\n</ol>\r\n<pre>// api/src/functions/graphql.js\r\n\r\nimport {\r\n  createGraphQLHandler,\r\n  makeMergedSchema,\r\n  makeServices,\r\n} from '@redwoodjs/api'\r\n\r\nimport schemas from 'src/graphql/**/*.{js,ts}'\r\nimport services from 'src/services/**/*.{js,ts}'\r\n\r\nimport { db } from 'src/lib/db'\r\n\r\nexport const handler = createGraphQLHandler({\r\n  schema: makeMergedSchema({\r\n    schemas,\r\n    services: makeServices({ services }),\r\n  }),\r\n  db,\r\n})\r\n</pre>\r\n<p>Let's take one more look at our entire directory structure before moving on to the Fauna Shell.</p>\r\n<pre>├── api\r\n│   └── src\r\n│       ├── functions\r\n│       │   └── graphql.js\r\n│       ├── graphql\r\n│       │   └── posts.sdl.js\r\n│       ├── lib\r\n│       │   └── db.js\r\n│       └── services\r\n│           └── posts\r\n│               └── posts.js\r\n└── web\r\n    ├── public\r\n    │   ├── favicon.png\r\n    │   ├── README.md\r\n    │   └── robots.txt\r\n    └── src\r\n        ├── components\r\n        │   └── PostsCell\r\n        │       └── PostsCell.js\r\n        ├── layouts\r\n        ├── pages\r\n            ├── FatalErrorPage\r\n            ├── HomePage\r\n            │   └── HomePage.js\r\n            └── NotFoundPage\r\n        ├── index.css\r\n        ├── index.html\r\n        ├── index.js\r\n        └── Routes.js\r\n</pre>\r\n<h2>Fauna Database</h2>\r\n<h3>Create FaunaDB account</h3>\r\n<p>You'll need a FaunaDB account to follow along but it's free for creating simple low traffic databases. You can use\r\n    your email to create an account or you can use your Github or Netlify account. FaunaDB Shell does not currently\r\n    support GitHub or Netlify logins so using those will add a couple extra steps when we want to authenticate with the\r\n    fauna-shell.</p>\r\n<p>First we will install the fauna-shell which will let us easily work with our database from the terminal. You can also\r\n    go to your dashboard and use Fauna's Web Shell.</p>\r\n<pre>npm install -g fauna-shell</pre>\r\n<p>Now we'll login to our Fauna account so we can access a database with the shell.</p>\r\n<pre>fauna cloud-login</pre>\r\n<p>You'll be asked to verify your email and password. If you signed up for FaunaDB using your GitHub or Netlify\r\n    credentials, follow <a href=\"https://docs.fauna.com/fauna/current/start/cloud-github\">these steps</a>, then skip the\r\n    Create new Database section and continue this tutorial at the beginning of the Collections section.</p>\r\n<h3>Create new Database</h3>\r\n<p>To create your database enter the fauna create-database command and give your database a name.</p>\r\n<pre>fauna create-database my_db</pre>\r\n<p>To start the fauna shell with our new database we'll enter the fauna shell command followed by the name of the\r\n    database.</p>\r\n<pre>fauna shell my_db</pre>\r\n<h3>Import Schema</h3>\r\n<p>Save the following code into a file called sdl.gql and import it to your database:\r\n</p>\r\n<pre>type Post {\r\n  title: String\r\n}\r\ntype Query {\r\n  posts: [Post]\r\n}\r\n</pre>\r\n<figure><img src=\"{asset:8791:url||https://fauna.com/assets/site/icons-features/unnamed.jpg}\" data-image=\"8791\" style=\"opacity: 1;\"></figure>\r\n<h3>Collections</h3>\r\n<p>To test out our database we'll create a collection with the name Post. A database’s schema is defined by its\r\n    collections, which are similar to tables in other databases. After entering the command fauna shell will respond\r\n    with the newly created Collection.</p>\r\n<pre>CreateCollection({ name: \"Post\" })\r\n</pre>\r\n<pre>{\r\n  ref: Collection(\"Post\"),\r\n  ts: 1597718505570000,\r\n  history_days: 30,\r\n  name: 'Post'\r\n}\r\n</pre>\r\n<h3>Create</h3>\r\n<p>The Create function adds a new document to a collection. Let's create our first blog post:</p>\r\n<pre>Create(\r\n Collection(\"Post\"),\r\n {\r\n   data: {\r\n     title: \"Deno is a secure runtime for JavaScript and TypeScript\"\r\n   }\r\n }\r\n)\r\n</pre>\r\n<pre>{\r\n ref: Ref(Collection(\"Post\"), \"274160525025214989\"),\r\n ts: 1597718701303000,\r\n data: {\r\n   title: \"Deno is a secure runtime for JavaScript and TypeScript\"\r\n }\r\n}\r\n</pre>\r\n<h3>Map</h3>\r\n<p>We can create multiple blog posts with the Map function. We are calling Map with an array of posts and a Lambda that\r\n    takes post_title as it's only parameter. post_title is then used inside the Lambda to provide the title field for\r\n    each new post.</p>\r\n<pre>Map(\r\n [\r\n   \"Vue.js is an open-source model–view–viewmodel JavaScript framework for building user interfaces and single-page applications\",\r\n   \"NextJS is a React framework for building production grade applications that scale\"\r\n ],\r\n Lambda(\"post_title\",\r\n   Create(\r\n     Collection(\"Post\"),\r\n     {\r\n       data: {\r\n         title: Var(\"post_title\")\r\n       }\r\n     }\r\n   )\r\n )\r\n)\r\n</pre>\r\n<pre>[\r\n {\r\n   ref: Ref(Collection(\"Post\"), \"274160642247624200\"),\r\n   ts: 1597718813080000,\r\n   data: {\r\n     title:\r\n       \"Vue.js is an open-source model–view–viewmodel JavaScript framework for building user interfaces and single-page applications\"\r\n   }\r\n },\r\n {\r\n   ref: Ref(Collection(\"Post\"), \"274160642247623176\"),\r\n   ts: 1597718813080000,\r\n   data: {\r\n     title:\r\n       \"NextJS is a React framework for building production grade applications that scale\"\r\n   }\r\n }\r\n]</pre>\r\n<h3>Get</h3>\r\n<p>The Get function retrieves a single document identified by ref. We can query for a specific post by using its ID.</p>\r\n<pre>Get(\r\n Ref(\r\n   Collection(\"Post\"), \"274160642247623176\"\r\n )\r\n)\r\n</pre>\r\n<pre>    \r\n{\r\n ref: Ref(Collection(\"Post\"), \"274160642247623176\"),\r\n ts: 1597718813080000,\r\n data: {\r\n   title:\r\n     \"NextJS is a React framework for building production grade applications that scale\"\r\n }\r\n}\r\n</pre>\r\n<h3>Indexes</h3>\r\n<p>Now we'll create an index for retrieving all the posts in our collection.</p>\r\n<pre>   \r\n CreateIndex({\r\n name: \"posts\",\r\n source: Collection(\"Post\")\r\n})\r\n</pre>\r\n<pre>{\r\n ref: Index(\"posts\"),\r\n ts: 1597719006320000,\r\n active: true,\r\n serialized: true,\r\n name: \"posts\",\r\n source: Collection(\"Post\"),\r\n partitions: 8\r\n}\r\n</pre>\r\n<h3>Match</h3>\r\n<p>Index returns a reference to an index which Match accepts and uses to construct a set. Paginate takes the output from\r\n    Match and returns a Page of results fetched from Fauna. Here we are returning an array of references.</p>\r\n<pre>Paginate(\r\n Match(\r\n   Index(\"posts\")\r\n )\r\n)\r\n</pre>\r\n<pre>{\r\n data: [\r\n   Ref(Collection(\"Post\"), \"274160525025214989\"),\r\n   Ref(Collection(\"Post\"), \"274160642247623176\"),\r\n   Ref(Collection(\"Post\"), \"274160642247624200\")\r\n ]\r\n}</pre>\r\n<h3>Lambda</h3>\r\n<p>We can get an array of references to our posts, but what if we wanted an array of the actual data contained in the\r\n    reference? We can Map over the array just like we would in any other programming language.</p>\r\n<pre>Map(\r\n Paginate(\r\n   Match(\r\n     Index(\"posts\")\r\n   )\r\n ),\r\n Lambda(\r\n   'postRef', Get(Var('postRef'))\r\n )\r\n)\r\n</pre>\r\n<pre>{\r\n data: [\r\n   {\r\n     ref: Ref(Collection(\"Post\"), \"274160525025214989\"),\r\n     ts: 1597718701303000,\r\n     data: {\r\n       title: \"Deno is a secure runtime for JavaScript and TypeScript\"\r\n     }\r\n   },\r\n   {\r\n     ref: Ref(Collection(\"Post\"), \"274160642247623176\"),\r\n     ts: 1597718813080000,\r\n     data: {\r\n       title:\r\n         \"NextJS is a React framework for building production grade applications that scale\"\r\n     }\r\n   },\r\n   {\r\n     ref: Ref(Collection(\"Post\"), \"274160642247624200\"),\r\n     ts: 1597718813080000,\r\n     data: {\r\n       title:\r\n         \"Vue.js is an open-source model–view–viewmodel JavaScript framework for building user interfaces and single-page applications\"\r\n     }\r\n   }\r\n ]\r\n}\r\n</pre>\r\n<p>So at this point we have our Redwood app set up with just a single:</p>\r\n<ul>\r\n    <li><strong>Page</strong> - HomePage.js</li>\r\n    <li><strong>Cell</strong> - PostsCell.js</li>\r\n    <li><strong>Function</strong> - graphql.js</li>\r\n    <li><strong>SDL</strong> - posts.sdl.js</li>\r\n    <li><strong>Lib</strong> - db.js</li>\r\n    <li><strong>Service</strong> - posts.js</li>\r\n</ul>\r\n<p>We used FQL functions in the Fauna Shell to create a database and seed it with data. FQL functions included:</p>\r\n<ul>\r\n    <li><strong>CreateCollection</strong> - Create a collection</li>\r\n    <li><strong>Create</strong> - Create a document in a collection</li>\r\n    <li><strong>Map</strong> - Applies a function to all array items</li>\r\n    <li><strong>Lambda</strong> - Executes an anonymous function</li>\r\n    <li><strong>Get</strong> - Retrieves the document for the specified reference</li>\r\n    <li><strong>CreateIndex</strong> - Create an index</li>\r\n    <li><strong>Match</strong> - Returns the set of items that match search terms</li>\r\n    <li><strong>Paginate</strong> - Takes a Set or Ref, and returns a page of results</li>\r\n</ul>\r\n<p>If we return to the home page we'll see our PostsCell is fetching the list of posts from our database.</p>\r\n<figure><img src=\"{asset:8789:url||https://fauna.com/assets/site/icons-features/Screen-Shot-2020-09-03-at-4.52.15-PM.jpg}\" data-image=\"8789\" style=\"opacity: 1;\"></figure>\r\n<p>And we can also go to our GraphiQL playground on localhost:8911/graphql.</p>\r\n<figure><img src=\"{asset:8790:url||https://fauna.com/assets/site/icons-features/Screen-Shot-2020-09-03-at-4.52.37-PM.jpg}\" data-image=\"8790\" style=\"opacity: 1;\"></figure>\r\n<p>RedwoodJS is querying the FaunaDB GraphQL API with our posts service on the backend and fetching that data with our\r\n    PostsCell on the frontend. If we wanted to extend this further we could add mutations to our schema definition\r\n    language and implement full CRUD capabilities through our GraphQL client.</p>\r\n<p>If you want to learn more about RedwoodJS you can check out the <a href=\"https://redwoodjs.com/docs/introduction\">documentation</a> or visit the RedwoodJS <a href=\"https://community.redwoodjs.com/\">community forum</a>. We would love to see what you’re building and we’re\r\n    happy to answer any questions you have!</p>",
        "blogCategory": [
            "10",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "8773"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-08-31T16:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8700,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "fea20825-fafa-4aad-8448-382287d6d054",
        "siteSettingsId": 8700,
        "fieldLayoutId": 4,
        "contentId": 2951,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Core FQL concepts, part 1: Working with dates and times",
        "slug": "core-fql-concepts-part-1-working-with-dates-and-times",
        "uri": "blog/core-fql-concepts-part-1-working-with-dates-and-times",
        "dateCreated": "2020-08-28T13:24:34-07:00",
        "dateUpdated": "2020-09-29T10:37:46-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/core-fql-concepts-part-1-working-with-dates-and-times",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times",
        "isCommunityPost": true,
        "blogBodyText": "<p>Welcome! This is the first article in a new series exploring some core concepts of FQL, the native query language of FaunaDB.</p>\r\n<p>This series will assume you have a grasp on the basics. If you're new to FaunaDB and/or FQL here's <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">my introductory series on FQL</a>.</p>\r\n<p>Today we'll explore how to work with dates and timestamps.</p>\r\n<p><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a></p><ul><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a><li><a href=\"https://fauna.com/blog/core-fql-concepts-part-1-working-with-dates-and-times\"></a>Part 1: Working with dates and times</li><li><a href=\"https://fauna.com/blog/core-fql-concepts-part-2-temporality-in-faunadb\">Part 2: Temporality in FaunaDB</a></li><li>Part 3: Data Aggregation</li></ul>\r\n<h2>In this article:</h2>\r\n<ul><li>Date and time basics</li><li>Printing date and time</li><li>Time operations</li><li>Sorting time results</li></ul>\r\n<h1>Date and time basics</h1>\r\n<p>FaunaDB has two native date types:<br></p>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/types#date\">Date</a> to store a calendar date</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/types#timestamp\">Timestamp</a> to store a UTC date and time, with nanosecond precision</li></ul>\r\n<p>The most common way to create a new <strong>Date</strong> is by simply passing an ISO date string to the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/date\">Date()</a> function:</p>\r\n<pre>Date(\"2020-08-15\")</pre>\r\n<p>Similarly, we can create a new <strong>Timestamp</strong> value by passing an UTC ISO time and date string to the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/time\">Time()</a> function:</p>\r\n<pre>Time(\"2020-08-15T18:39:45Z\")</pre>\r\n<p>Timestamps in FaunaDB are always stored in UTC time. If we pass an ISO string with a time zone offset it is automatically converted:</p>\r\n<pre>&gt; Time('2020-08-15T00:00:00+04:00')\r\nTime(\"2020-08-14T20:00:00Z\")</pre>\r\n<p>As you can see, the day of the date has changed because the string we passed was 4 hours ahead of UTC time.</p>\r\n<p>It's also possible to use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/epoch\">Epoch()</a> to create a timestamp based on a UNIX time:</p>\r\n<pre>&gt; Epoch(1597533145964, \"millisecond\")\r\nTime(\"2020-08-15T23:12:25.964Z\")</pre>\r\n<p>Since FaunaFB timestamps can store up to nanosecond precision, the <strong>Epoch()</strong> function requires a second argument to determine the unit of the created <strong>Timestamp</strong>.</p>\r\n<p>We can also use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/now\">Now()</a> to create a new <strong>Timestamp</strong> (the FaunaDB type, not the UNIX time) at the current moment in microseconds (1 millisecond = 1000 microseconds):</p>\r\n<pre>&gt; Now()\r\nTime(\"2020-08-15T23:04:14.455004Z\")</pre>\r\n<p><strong>Quick note: </strong>The <strong>Timestamp</strong> produced by <strong>Now()</strong> is based on the transaction time. If you call it multiple times in the same transaction the result will always be the same:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    now1: Now(),\r\n    now2: Now()\r\n  },\r\n  {\r\n    ts1: Var(\"now1\"),\r\n    ts2: Var(\"now2\")\r\n  }\r\n)\r\n{\r\n  ts1: Time(\"2020-08-15T23:27:41.885588Z\"),\r\n  ts2: Time(\"2020-08-15T23:27:41.885588Z\")\r\n}</pre>\r\n<h2>Converting between Date and Timestamp types</h2>\r\n<p>To convert a <strong>Timestamp</strong> to a <strong>Date</strong> we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/todate\">ToDate()</a> function. All of these examples produce the same result:</p>\r\n<pre>&gt; ToDate(Time(\"2020-08-15T23:12:25Z\"))\r\nDate(\"2020-08-15\")\r\n\r\n\r\n&gt; ToDate(Epoch(1597533145964, \"millisecond\"))\r\nDate(\"2020-08-15\")\r\n\r\n\r\n&gt; ToDate(Now())\r\nDate(\"2020-08-15\")</pre>\r\n<p>Likewise, to convert a <strong>Date</strong> to a <strong>Timestamp</strong> we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/totime\">ToTime()</a> function:</p>\r\n<pre>&gt; ToTime(Date(\"2020-08-15\"))\r\nTime(\"2020-08-15T00:00:00Z\")</pre>\r\n<h2>Comparing dates</h2>\r\n<p>As expected, we can use <strong>Date</strong> and <strong>Timestamp</strong> types with all of the comparison functions available to us in FQL:</p>\r\n<pre>&gt;Equals(Now(), Now())\r\ntrue</pre>\r\n<p>We can even use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lt\">LT()</a> and <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lte\">LTE()</a> to know which date is \"larger\", or more recent:</p>\r\n<pre>&gt; LT(Now(), Date(\"1970-11-05\"))\r\ntrue</pre>\r\n<h1>Printing dates and times</h1>\r\n<p>Once you have your <strong>Date</strong> or <strong>Timestamp</strong> values stored in FaunaDB, you probably need to read those in your programming language, or at least present those values directly to your users in a more human way.</p>\r\n<p>The <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/format\">Format()</a> function is your bread and butter tool to do that. It's extremely powerful, so we're only going to scratch the surface here.</p>\r\n<p>For example, you might want to get the number of milliseconds since 1970 (UNIX time) which is the type of value you get when doing <strong>Date.now()</strong> in JavaScript:</p>\r\n<pre>&gt; Format('%tQ', Now())\r\n1597939216796</pre>\r\n<p>It's also very common to use ISO date strings:</p>\r\n<pre>&gt; Format('%t', Now())\r\n2020-08-20T16:13:13.654455Z</pre>\r\n<p>You might also want to print the date in some other format than <strong>YYYY-MM-DD</strong>:</p>\r\n<pre>&gt; Format('%tD', Now())\r\n08/20/20</pre>\r\n<p>And again, in the European format:</p>\r\n<pre>&gt; Format('%td/%tm/%ty', Now(), Now(), Now())\r\n20/08/20</pre>\r\n<p>As I said, <strong>Format()</strong> is extremely powerful. <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/format\">Check the docs</a> for all of the available options to format strings.</p>\r\n<h1>Time operations</h1>\r\n<p>FaunaDB has many powerful capabilities to work with dates. Let's see a couple of practical examples.</p>\r\n<p>For the rest of the article we'll just assume that all planets across the galaxy follow Earth's time. My apologies to any Vulkans reading this article.</p>\r\n<h2>Addition</h2>\r\n<p>A couple of weeks ago, the fleet installed a teleporter to beam personnel from the home base to spaceships. Obviously people started abusing it since it was too much fun, so the admiral has tasked us to build a little system to make appointments and control the teleporting traffic.</p>\r\n<p>First, let's create a new collection to track the teleportations:</p>\r\n<pre>&gt; CreateCollection({name:\"Teleportations\"})\r\n{\r\n  ref: Collection(\"Teleportations\"),\r\n  ts: 1597715786192000,\r\n  history_days: 30,\r\n  name: 'Teleportations'\r\n}</pre>\r\n<p>Let’s also create a new collection to track the pilots:</p>\r\n<pre>&gt; CreateCollection({name:\"Pilots\"})\r\n{\r\n  ref: Collection(\"Pilots\"),\r\n  ts: 1597715790726000,\r\n  history_days: 30,\r\n  name: 'Pilots'\r\n}</pre>\r\n<p>Let's schedule a teleportation 10 days from now:</p>\r\n<pre>&gt; Create(\r\n  Collection(\"Teleportations\"),\r\n  {\r\n    data: {\r\n      personRef: Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n      status: 'SCHEDULED',\r\n      ts: TimeAdd(Now(), 10, \"days\")\r\n    }\r\n  }\r\n)\r\n{\r\n  ref: Ref(Collection(\"Teleportations\"), \"274157476972069395\"),\r\n  ts: 1597715794460000,\r\n  data: {\r\n    personRef: Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    status: \"SCHEDULED\",\r\n    ts: Time(\"2020-08-28T01:56:34.304009Z\")\r\n  }\r\n}</pre>\r\n<p>This is the interesting part:</p>\r\n<pre>TimeAdd(Now(), 10, \"days\")</pre>\r\n<p>The <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/timeadd\">TimeAdd()</a> function takes a <strong>Date</strong> or a <strong>Timestamp</strong> and adds a number of units to it. If you're using dates you have to use days, but with timestamps you can use any unit down to nanoseconds. Check <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/timeadd\">the documentation</a> to see all of the available units.</p>\r\n<p>If the teleporter were to undergo some maintenance, we could just reschedule the pending teleportations by adding the duration of the repairs.</p>\r\n<p>For example, here's how we could add 8 hours to the scheduled timestamp:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    ref: Ref(Collection(\"Teleportations\"), \"274157476972069395\"),\r\n    ts: Select([\"data\", \"ts\"], Get(Var(\"ref\")))\r\n  },\r\n  Update(\r\n    Var(\"ref\"),\r\n    {\r\n      data: {\r\n        ts: TimeAdd(Var(\"ts\"), 8, \"hours\")\r\n      }\r\n    }\r\n  )\r\n)\r\n{\r\n  ref: Ref(Collection(\"Teleportations\"), \"274157476972069395\"),\r\n  ts: 1597716265635000,\r\n  data: {\r\n    personRef: Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    status: \"SCHEDULED\",\r\n    ts: Time(\"2020-08-28T09:56:34.304009Z\")\r\n  }\r\n}</pre>\r\n<p><strong>Quick tip:</strong> The <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/let\">Let()</a> function is commonly used to return a custom object, but it actually executes any FQL expression in its second parameter. Here we're using it to execute the update after having collected the necessary data first.</p>\r\n<h2>Subtraction</h2>\r\n<p>We can just as easily subtract units of time by using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/timesubtract\">TimeSubtract()</a> which works exactly like<strong> TimeAdd()</strong>:</p>\r\n<pre>TimeSubtract(Now(), 4, \"hours\")\r\n</pre>\r\n<h2>Calculating time offsets</h2>\r\n<p>Sometimes pilots want to use the teleporter before their scheduled time, and we can't really allow that. We could at least show them how much time is left before they can use it.</p>\r\n<p>As its name implies, we use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/timediff\">TimeDiff()</a> to calculate differences between two times:</p>\r\n<pre>&gt; TimeDiff(\r\n  Date(\"2020-08-15\"),\r\n  Date(\"2020-08-20\"),\r\n  \"days\"\r\n)\r\n5</pre>\r\n<p>Again, if you're using dates you must use day units. You can calculate the offset in any another unit, like say hours, by converting dates to timestamps using <strong>ToTime()</strong>:</p>\r\n<pre>&gt; TimeDiff(\r\n  ToTime(Date(\"2020-08-15\")),\r\n  Now(),\r\n  \"hours\"\r\n)\r\n74</pre>\r\n<p>Another interesting thing to know about <strong>TimeDiff()</strong> is that it always rounds to the lowest whole value:<br></p>\r\n<pre>&gt;\r\nLet(\r\n  {\r\n    ts1: Now(),\r\n    ts2: TimeAdd(Now(), 119, \"minutes\")\r\n  },\r\n  TimeDiff(Var(\"ts1\"), Var(\"ts2\"), \"hours\")\r\n)\r\n1</pre>\r\n<p>119 minutes is almost two hours, and yet FaunaDB returns one hour.</p>\r\n<p>Ok, with this in mind let's calculate how much time a pilot has to wait before teleporting:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    ref: Ref(Collection(\"Teleportations\"), \"274157476972069395\"),\r\n    ts: Select([\"data\", \"ts\"], Get(Var(\"ref\")))\r\n  },\r\n  TimeDiff(Now(), Var(\"ts\"), \"minutes\")\r\n)\r\n14845</pre>\r\n<p>Phew! 14,845 minutes is a lot of waiting!</p>\r\n<p>Obviously, we would need to massage that data into hours and minutes before showing it to our user. We could also create a simple user-defined-function (or UDF) like we saw <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">in a previous article</a> to do that for us.</p>\r\n<p>Or, we could just do it right in our FQL query:</p>\r\n<pre>&gt; Let(\r\n  {\r\n    ref: Ref(Collection(\"Teleportations\"), \"274157476972069395\"),\r\n    ts: Select([\"data\", \"ts\"], Get(Var(\"ref\"))),\r\n    timeDiffHours: TimeDiff(Now(), Var(\"ts\"), \"hours\"),\r\n    hoursInMinutes: Multiply(Var(\"timeDiffHours\"), 60),\r\n    timeDiffMinutes: TimeDiff(Now(), Var(\"ts\"), \"minutes\"),\r\n    remainingMinutes: Subtract(Var(\"timeDiffMinutes\"), Var(\"hoursInMinutes\"))\r\n  },\r\n  Format(\r\n    \"You have to wait %s hours and %s minutes\",\r\n    Var(\"timeDiffHours\"),\r\n    Var(\"remainingMinutes\")\r\n  )\r\n)\r\nYou have to wait 246 hours and 45 minutes</pre>\r\n<p>Since we know <strong>timeDiffHours</strong> is rounded to the lowest value, we just need to find a way to calculate the remaining minutes. To do that we simply convert these hours into minutes (by multiplying the hours to 60) and subtract that to the total time in minutes.</p>\r\n<h1>Sorting time results</h1>\r\n<p>Sorting index results by date or timestamps is no different than sorting by any other type of value. Let's create an index that sorts all the documents in the <strong>Teleportations</strong> collection by the <strong>ts</strong> property:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"all_Teleportations_by_ts\",\r\n  source: Collection(\"Teleportations\"),\r\n  values: [\r\n    { field: [\"data\", \"ts\"] },\r\n    { field: [\"ref\"] }\r\n  ]\r\n})</pre>\r\n<p>We already saw <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">in a previous article</a> how indexes and sorting works, so I won't go into much detail here.</p>\r\n<p>Here's a possible query to get the results from that index:</p>\r\n<pre>&gt; Paginate(Match(Index(\"all_Teleportations_by_ts\")))\r\n{\r\n  data: [\r\n    [\r\n      Time(\"2020-08-19T12:56:31.102631Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274138599280083456\")\r\n    ],\r\n    [\r\n      Time(\"2020-08-28T09:56:34.304009Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274157476972069395\")\r\n    ]\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>By default, FaunaDB sorts values in ascending order, and it's no different here as we're getting older results first.</p>\r\n<p>If we wanted to get the most recent results first, we'd need an index with a reverse order:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"all_Teleportations_by_ts_desc\",\r\n  source: Collection(\"Teleportations\"),\r\n  values: [\r\n    { field: [\"data\", \"ts\"], reverse: true },\r\n    { field: [\"ref\"] }\r\n  ]\r\n})</pre>\r\n<h1>Filtering time values</h1>\r\n<p>There are a couple of strategies we can follow depending on our use case. Obviously, all of these techniques involve using indexes. If you're new to FQL <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">here's my introductory article on indexes</a>.</p>\r\n<h2>Filter by exact date</h2>\r\n<p>The easiest use case is to filter by an exact date. Since our <strong>Teleportations</strong> documents use a timestamp we would need to add a date before we can do that.</p>\r\n<p>So, let's create a simple query that updates all the documents with a date property:</p>\r\n<pre>&gt; Map(\r\n  Paginate(Match(Index(\"all_Teleportations_by_ts\"))),\r\n  Lambda(\r\n    [\"ts\",\"ref\"],\r\n    Update(\r\n      Var(\"ref\"),\r\n      {data: {date:ToDate(Var(\"ts\"))}}\r\n    )\r\n  )\r\n)\r\n{\r\n  data: [\r\n    {\r\n      ref: Ref(Collection(\"Teleportations\"), \"274138599280083456\"),\r\n      ts: 1597854857396000,\r\n      data: {\r\n        personRef: Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n        status: \"SCHEDULED\",\r\n        ts: Time(\"2020-08-19T12:56:31.102631Z\"),\r\n        date: Date(\"2020-08-19\")\r\n      }\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>Great, so now we can create a simple index to filter by date</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"all_Teleporations_by_date\",\r\n  source: Collection(\"Teleportations\"),\r\n  terms: [\r\n    { field: [“data”, \"date\"]}\r\n  ]\r\n})</pre>\r\n<p>And here's how we'd get all the teleportations for a given date:</p>\r\n<pre>&gt; Paginate(Match(Index(\"all_Teleporations_by_date\"), Date(\"2020-08-19\")))\r\n{\r\n  data: [Ref(Collection(\"Teleportations\"), \"274138599280083456\")]\r\n}</pre>\r\n<h2>Filter by day of the week</h2>\r\n<p>What if we wanted to know which teleportations happened on, say, a Wednesday?</p>\r\n<p>Again, we could just add the day of the week to our documents and filter using that:</p>\r\n<pre>&gt; Map(\r\n  Paginate(Match(Index(\"all_Teleportations_by_ts\"))),\r\n  Lambda(\r\n    [\"ts\",\"ref\"],\r\n    Update(\r\n      Var(\"ref\"),\r\n      {data: {weekday: DayOfWeek(Var(\"ts\"))}}\r\n    )\r\n  )\r\n)\r\n{\r\n  data: [\r\n    {\r\n      ref: Ref(Collection(\"Teleportations\"), \"274138599280083456\"),\r\n      ts: 1597855390458000,\r\n      data: {\r\n        personRef: Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n        status: \"SCHEDULED\",\r\n        ts: Time(\"2020-08-19T12:56:31.102631Z\"),\r\n        date: Date(\"2020-08-19\"),\r\n        weekday: 3\r\n      }\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/dayofweek\">DayOfWeek()</a> takes a timestamp and returns an integer from 1 to 7. Monday would be 1, Tuesday 2, and so on.</p>\r\n<p>Now we just need to create a new index::</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"all_Teleportations_by_weekday\",\r\n  source: Collection(\"Teleportations\"),\r\n  terms: [\r\n    {field: [\"data\", \"weekday\"]}\r\n  ]\r\n})</pre>\r\n<p>And here's how we would find all the teleportations that happened on a Wednesday:</p>\r\n<pre>&gt; Paginate(Match(Index(\"all_Teleportations_by_weekday\"), 3))\r\n{\r\n  data: [Ref(Collection(\"Teleportations\"), \"274138599280083456\")]\r\n}</pre>\r\n<h2>Filter by a time range</h2>\r\n<h4><strong>Introduction to ranges queries in FaunaDB</strong></h4>\r\n<p>Before being able to filter by a time range, we need to take a little detour to explain how range queries work in FaunaDB.</p>\r\n<p>Let's create a quick collection and fill it with some documents first:</p>\r\n<pre>&gt; CreateCollection({name: \"Numbers\"})\r\n\r\n&gt; Create(Collection(\"Numbers\"), {data: {value: 1}})\r\n&gt; Create(Collection(\"Numbers\"), {data: {value: 2}})\r\n&gt; Create(Collection(\"Numbers\"), {data: {value: 3}})\r\n// etc...</pre>\r\n<p>Let's now create an index that sorts and returns the <strong>value</strong> property of those documents:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"Numbers_by_value\",\r\n  source: Collection(\"Numbers\"),\r\n  values:[\r\n    {field: [\"data\", \"value\"]}\r\n  ]\r\n})</pre>\r\n<p>This index is simply returning the <strong>value</strong> of all the documents in the collection:</p>\r\n<pre>&gt; Paginate(Match(Index(\"Numbers_by_value\")))\r\n{\r\n  data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n}</pre>\r\n<p>This is what happens when we use<a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/range\">Range()</a> with this index:</p>\r\n<pre>&gt; Paginate(\r\n  Range(Match(Index(\"Numbers_by_value\")), 2, 6)\r\n)\r\n{\r\n  data: [2, 3, 4, 5, 6]\r\n}</pre>\r\n<p>Makes sense, right? <strong>Range()</strong> filters those index results within the bounds of 2 and 6.</p>\r\n<p>There are a couple of nuances lost in this simple example though. What happens when the index returns an array instead of a single value?</p>\r\n<p>Let's create some test data and an index to test this out:</p>\r\n<pre>&gt; CreateCollection({name: \"NumbersMultiple\"})\r\n\r\n&gt; Create(Collection(\"NumbersMultiple\"), {data: {a: 1, b: 8}})\r\n&gt; Create(Collection(\"NumbersMultiple\"), {data: {a: 2, b: 4}})\r\n// etc...\r\n\r\n&gt; CreateIndex({\r\n  name: \"NumbersMultiple_by_a_and_b\",\r\n  source: Collection(\"NumbersMultiple\"),\r\n  values: [\r\n    {field: [\"data\", \"a\"]},\r\n    {field: [\"data\", \"b\"]}\r\n  ]\r\n})\r\n</pre>\r\n<p>Check what happens when using <strong>Range()</strong> now:</p>\r\n<pre>&gt; Paginate(\r\n  Range(Match(Index(\"NumbersMultiple_by_a_and_b\")), 2, 6)\r\n)\r\n{\r\n  data: [\r\n    [2, 4],\r\n    [3, 50],\r\n    [4, 0],\r\n    [5, 6],\r\n    [6, 9]\r\n  ]\r\n}\r\n</pre>\r\n<p>As you can see, <strong>Range()</strong> is pretty much ignoring the second value. When receiving an array, <strong>Range()</strong> only takes into account the first value that can be fit into the bounds and ignores the rest.</p>\r\n<p>We will go into more detail on how range queries work in a future article, but we're now ready to tackle our time filtering problem.</p>\r\n<h4><strong>Filtering by a date range</strong></h4>\r\n<p>We could maybe reuse the previous <strong>all_Teleporations_by_date</strong> index like this:</p>\r\n<pre>&gt; Paginate(\r\n  Range(\r\n    Match(Index(\"all_Teleporations_by_date\")),\r\n    Date(\"2020-01-01\"),\r\n    Date(\"2021-01-01\")\r\n  )\r\n)\r\n{\r\n  data: []\r\n}\r\n</pre>\r\n<p>Huh? How come that doesn't work?</p>\r\n<p>If you scroll up a bit, you can see that the <strong>all_Teleporations_by_date</strong> index doesn't have a <strong>values</strong> object, so it just returns an array with references. <strong>Range()</strong> didn't return anything simply because it couldn't compare dates with references.</p>\r\n<p>To fix this we need to create a new index with a <strong>values</strong> object, that returns values that <strong>Range()</strong> can use to filter:</p>\r\n<pre>&gt; CreateIndex({\r\n  name: \"all_Teleporations_by_ts_range\",\r\n  source: Collection(\"Teleportations\"),\r\n  values: [\r\n    { field: [\"data\", \"ts\"]},\r\n    { field: \"ref\"}\r\n  ]\r\n})</pre>\r\n<p>And then query it:</p>\r\n<pre>&gt; Paginate(\r\n  Range(\r\n    Match(Index(\"all_Teleporations_by_ts_range\")),\r\n    Now(),\r\n    TimeAdd(Now(), 100, \"days\")\r\n  )\r\n)\r\n{\r\n  data: [\r\n    [\r\n      Time(\"2020-08-19T12:56:31.102631Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274138599280083456\")\r\n    ],\r\n    [\r\n      Time(\"2020-08-28T09:56:34.304009Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274157476972069395\")\r\n    ]\r\n  ]\r\n}</pre>\r\n<p>Take note that we need to pass timestamps for this to work. Otherwise, we won't get any results since the comparison wouldn't be possible:</p>\r\n<pre>&gt; Paginate(\r\n  Range(\r\n    Match(Index(\"all_Teleporations_by_ts_range\")),\r\n    Date(\"2020-01-01\"),\r\n    Date(\"2021-01-01\")\r\n  )\r\n)\r\n{\r\n  data: []\r\n}</pre>\r\n<p>We could, of course, simply cast those dates to timestamps, so that <strong>Range()</strong> is comparing timestamps with timestamps:</p>\r\n<pre>&gt; Paginate(\r\n  Range(\r\n    Match(Index(\"all_Teleporations_by_ts_range\")),\r\n    ToTime(Date(\"2020-01-01\")),\r\n    ToTime(Date(\"2021-01-01\"))\r\n  )\r\n)\r\n{\r\n  data: [\r\n    [\r\n      Time(\"2020-08-19T12:56:31.102631Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274138599280083456\")\r\n    ],\r\n    [\r\n      Time(\"2020-08-28T09:56:34.304009Z\"),\r\n      Ref(Collection(\"Teleportations\"), \"274157476972069395\")\r\n    ]\r\n  ]\r\n}</pre>\r\n<h1>Conclusion</h1>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In the following article of the series, we will continue our space adventure by checking out all the temporality features in FaunaDB.</p>\r\n<p>If you have any questions don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>",
        "blogCategory": [
            "10",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "8772"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "602",
        "postDate": "2020-08-18T05:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8673,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "8f9ec734-dc0d-4f31-9a0d-1bcdda348baf",
        "siteSettingsId": 8673,
        "fieldLayoutId": 4,
        "contentId": 2933,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing FaunaDB API v3",
        "slug": "announcing-faunadb-api-v3",
        "uri": "blog/announcing-faunadb-api-v3",
        "dateCreated": "2020-08-17T12:18:24-07:00",
        "dateUpdated": "2020-08-18T14:11:48-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-faunadb-api-v3",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-faunadb-api-v3",
        "isCommunityPost": false,
        "blogBodyText": "<p>We are happy to announce the availability of FaunaDB API v3. This release adds four new functions and deprecates one function. In this release, we also introduced a substantial change to our API versioning strategy--a change that will allow us to ship new features faster without introducing breaking changes.</p>\r\n<p>To learn more about any of the changes discussed here, please read our official documentation on <a href=\"http://docs.fauna.com/fauna/current/release_notes/versioning\">API versioning</a>, <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/ContainsPath.html\">ContainsPath()</a>, <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/ContainsField.html\">ContainsField()</a>, <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/ContainsValue.html\">ContainsValue()</a>, and <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/Reverse.html\">Reverse()</a>.</p>\r\n<h2>New API versioning strategy</h2>\r\n<p>As of today, you might notice that the v3 drivers send an api_version field in the header of every query. This allows the server to interpret queries from multiple driver versions, and run their semantics accordingly, without causing breaking changes to apps running on older drivers. </p>\r\n<p>The possibility of introducing breaking changes has slowed us down in the past. We're excited about this new versioning strategy, because it will speed up our release cadence and allow for more substantial improvements to the API.</p>\r\n<p>This new versioning strategy also improves the way in which we communicate and share \"beta\" functionality with our users. In the past, we tried using a \"preview\" badge to denote unstable features, but included those features in our (otherwise) production-ready drivers. Moving forward, we will make \"beta\" functionality more inescapably obvious by only releasing it as part of a \"snapshot\" version of the driver. </p>\r\n<h2>Updates to the Contains function family</h2>\r\n<p>API v3 introduces three new functions to the Contains function family: ContainsValue(), ContainsField(), and ContainsPath(). However, ContainsPath() does not actually introduce new functionality and works exactly like the now deprecated Contains() function. We decided to rename Contains() to ContainsPath() to help avoid user confusion. </p>\r\n<p>With respect to our new API versioning strategy, this means that the server will continue to run requests with the Contains() function for all supported driver versions. Future driver versions will remove Contains() in favor of ContainsPath(), but your app will remain unaffected until we remove support for the last driver that included Contains(). The Contains() function will only become truly unavailable once the last API version that supports it becomes unsupported by the service. </p>\r\n<h3>ContainsPath()</h3>\r\n<p>Exactly like Contains(), ContainsPath() checks to see if a path exists in an object. This is useful when you need to distinguish between objects, arrays, or documents that contain a nested path and those that do not. For example, the following query returns `true` because the path `['inventory', 'apples']` exists in the provided object.</p>\r\n<pre>ContainsPath(\r\n  ['inventory', 'apples'],\r\n  {\r\n    store_id: 623,\r\n    inventory: { apples: 0 },\r\n    employees: 7\r\n  }\r\n)</pre>\r\n<h3>ContainsField()</h3>\r\n<p>The new ContainsField() function behaves a little like the above ContainsPath(), but only takes one field for the path. For example, the following query returns `true` because the field 'inventory' exists in the provided object:</p>\r\n<pre>ContainsField(\r\n  'inventory',\r\n  {\r\n    store_id: 623,\r\n    inventory: { apples: 0 },\r\n    employees: 7\r\n  }\r\n)</pre>\r\n<h3>ContainsValue()</h3>\r\n<p>Finally, the most exciting addition to this function family is ContainsValue(). This new function checks for the existence of a particular value in an array, set, or object. For example, the following query returns `true` because the value 7 exists in the provided object:</p>\r\n<pre>ContainsValue(\r\n  7,\r\n  {\r\n    store_id: 623,\r\n    inventory: { apples: 0 },\r\n    employees: 7\r\n  },\r\n)</pre>\r\n<p>Please note that ContainsValue() only checks the top-level of objects, so the below would return `false`:<br></p>\r\n<pre>ContainsValue(\r\n  0,\r\n  {\r\n    store_id: 623,\r\n    inventory: { apples: 0 },\r\n    employees: 7\r\n  },\r\n)</pre>\r\n<p>In this case, you would use a <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/Select.html\">Select()</a> to target what you need. See the  <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/ContainsValue.html\">documentation</a> for more examples. </p>\r\n<h2>New Reverse() function</h2>\r\n<p>The new Reverse() function was inspired by community requests and <a href=\"https://github.com/shiftx/faunadb-fql-lib\">this open source FQL library</a> from Eigil Sagafos of <a href=\"https://shiftx.com/\">ShiftX</a>. Reverse() simply takes an array, page, or set, and returns the reversed version. Here is a simple example, using the data model above:</p>\r\n<pre>Reverse([\r\n    {store_id: 623},\r\n    {inventory: { apples: 0 }},\r\n    {employees: 7}\r\n])</pre>\r\n<p></p>\r\n<pre>[\r\n    {employees: 7},\r\n    {inventory: { apples: 0 }},\r\n    {store_id: 623}\r\n]</pre>\r\n<p>See the <a href=\"https://docs.fauna.com/fauna/current/API/FQL/functions/Reverse.html\">documentation</a> for more complex examples. </p>\r\n<h2>Conclusion</h2>\r\n<p>While API v3 introduces only a few new functions, we feel that our new API versioning strategy heralds broader changes in how we work here at Fauna, and how we interact with our community. </p>\r\n<p>For a taste of what's to come in future releases, our engineers are hard at work getting our streaming feature ready for beta testing, improving our GraphQL schema import experience, providing better support for third-party authentication providers, and lots of other exciting new features and improvements.</p>\r\n<p>We also take user feedback very seriously in prioritizing our product roadmap, and are always eager to hear from you about how we can improve. Please post comments/suggestions in the \"Feedback\" category on <a href=\"http://forums.fauna.com/\">our forums</a>.<br></p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "8674"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "603",
        "postDate": "2020-08-14T05:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8650,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "1ebfb554-503d-40da-bfbf-210a7b3967ff",
        "siteSettingsId": 8650,
        "fieldLayoutId": 4,
        "contentId": 2923,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Build Fearlessly Podcast - Episode 2: The ins and outs of Quid Sentio",
        "slug": "build-fearlessly-podcast-episode-2-the-ins-and-outs-of-quid-sentio",
        "uri": "blog/build-fearlessly-podcast-episode-2-the-ins-and-outs-of-quid-sentio",
        "dateCreated": "2020-08-12T13:21:52-07:00",
        "dateUpdated": "2020-08-14T08:42:25-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/build-fearlessly-podcast-episode-2-the-ins-and-outs-of-quid-sentio",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/build-fearlessly-podcast-episode-2-the-ins-and-outs-of-quid-sentio",
        "isCommunityPost": false,
        "blogBodyText": "<p>Hear frontend developer Rodrigo Pontes, discuss his project <a href=\"https://www.quidsentio.com/\">Quid Sentio</a> - a shareable personal journal - with Fauna’s developer advocate, Brecht De Rooms. Rodrigo talks about where Quid Sentio is currently, his thought process behind choosing his serverless stack, and what he hopes to accomplish in the future.</p>\r\n<p></p>\r\n<figure><iframe src=\"https://player.blubrry.com/id/65837434/\" scrolling=\"no\" width=\"100%\" height=\"138px\" frameborder=\"0\"></iframe></figure>\r\n<p></p>\r\n<h2>Show notes</h2>\r\n<p><strong>Quid Sentio’s Serverless Architecture </strong>(discussion starting at minute 7:20):</p>\r\n<ul><li><a href=\"https://www.netlify.com/\">Netlify</a>: hosting and serverless functions</li><li><a href=\"https://fauna.com/\">FaunaDB</a>: serverless database&nbsp;</li><li><a href=\"https://userbase.com/\">Userbase</a>: encryption&nbsp;</li><li><a href=\"https://stripe.com/\">Stripe</a>: payment service&nbsp;</li><li><a href=\"https://reactjs.org/\">React</a>: javascript library&nbsp;</li></ul>\r\n<p><strong>Other Resources:</strong></p>\r\n<ul><li><a href=\"https://www.quidsentio.com/\">Quid Sentio</a>&nbsp;</li><li><a href=\"https://twitter.com/quidsentio\">Follow Quid Sentio</a> on Twitter&nbsp;</li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">Fauna Query Language (FQL) tutorial&nbsp;</a></li><li><a href=\"https://community.fauna.com/\">Fauna’s Community</a></li></ul>\r\n<p>-------------------</p>\r\n<p>To learn more about this podcast and Fauna, check out these episodes on iTunes</p>\r\n<ul><li><a href=\"https://podcasts.apple.com/us/podcast/episode-0-jamstack-faunadb-and-faunas-ecosystem/id1502140620?i=1000467897112\">Episode 0: JAMstack, FaunaDB and Fauna’s ecosystem</a> </li><li><a href=\"https://fauna.com/blog/build-fearlessly-podcast-static-fun\">Episode 1: Dissecting Static.fun</a></li></ul>\r\n<p><strong><em>Follow the Build Fearlessly Podcast on<a href=\"https://podcasts.apple.com/us/podcast/build-fearlessly/id1502140620\"> iTunes</a> to hear more stories as they develop from the developer community.</em></strong></p>",
        "blogCategory": [
            "6957"
        ],
        "mainBlogImage": [
            "8651"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-08-06T04:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8606,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "7a1723f2-83e8-4b29-b3b8-da53e29394eb",
        "siteSettingsId": 8606,
        "fieldLayoutId": 4,
        "contentId": 2908,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting started with FQL, FaunaDB’s native query language - part 5",
        "slug": "getting-started-with-fql-faunadbs-native-query-language-part-5",
        "uri": "blog/getting-started-with-fql-faunadbs-native-query-language-part-5",
        "dateCreated": "2020-08-04T12:11:59-07:00",
        "dateUpdated": "2020-09-02T09:09:52-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-fql-faunadbs-native-query-language-part-5",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5",
        "isCommunityPost": true,
        "blogBodyText": "<p>Welcome back, fellow space developer!</p>\r\n<ul><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">Part 1: a look at FQL and fundamental FaunaDB concepts</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">Part 2: a deep dive into indexes with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">Part 3: a look into the principles of modeling data with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">Part 4:&nbsp;a look at how to create custom functions that run straight in FaunaDB</a></li><li>Part 5: a look at authentication and authorization in FaunaDB</li></ul>\r\n<p>Today, in the final article of this series, we're going to take a look at authentication and authorization in FaunaDB.</p>\r\n<h2>In this article:</h2>\r\n<ul><li>Introduction</li><li>About tokens, keys, and secrets</li><li>Introduction to roles and privileges</li><li>Creating a server key</li><li>Basics of authentication</li><li>Authentication in your application</li><li>Your first custom role</li><li>Fine-grained privileges</li><li>Fine-grained memberships</li><li>Privileges over UDFs</li></ul>\r\n<h2>Introduction</h2>\r\n<p>Authentication and authorization are commonly implemented in the application layer. FaunaDB follows a different approach by centralizing those right at the database.</p>\r\n<p>This means that any piece of code can now become a client of your database without having to reimplement authentication or authorization:</p>\r\n<ul><li>Mobile apps</li><li>Server applications</li><li>Cloud functions</li><li>Microservices</li><li>Frontend web apps</li><li>Desktop apps</li><li>Etc.</li></ul>\r\n<p>Before we get to the code, let me introduce a couple of core concepts.</p>\r\n<h2>About tokens, keys, and secrets</h2>\r\n<p>FaunaDB is secure by default. To execute queries, you will always need to pass a secret which is associated either with an application key or an access token.</p>\r\n<h4><strong>Secrets</strong></h4>\r\n<p>Whenever you instantiate a FaunaDB client in your application, you will need to use a secret. A secret looks very much like a password:</p>\r\n<pre>fnADviINFNACBaG5LTgmxtf2fwpdqohworOfFGJ_</pre>\r\n<h4><strong>Application keys</strong></h4>\r\n<p>Like their name implies, application keys are used by your applications. Each key has its own secret and can be used any number of times on multiple applications.</p>\r\n<p>You create keys manually using FQL, or via FaunaDB's dashboard. Keys do not expire until you manually delete them.</p>\r\n<h4><strong>Access tokens</strong></h4>\r\n<p>Tokens are somewhat similar to keys, but are used by users instead of applications. Tokens and their secrets are usually generated for you when authenticating successfully with FaunaDB, so a single user could use multiple secrets in different devices simultaneously.</p>\r\n<p>Tokens can be deleted manually, or upon logging a user out. It's also possible to define an optional time-to-live setting to determine how long a token will be valid.</p>\r\n<h2>Introduction to roles and privileges</h2>\r\n<p>FaunaDB features a fine-grained authorization system based on attributes, also known as <a href=\"https://docs.fauna.com/fauna/current/security/abac\">ABAC</a>.</p>\r\n<h4><strong>Custom roles and privileges</strong></h4>\r\n<p>Roles grant privileges to keys and tokens to access resources in the database. The most important types of resources that you can grant access to are:</p>\r\n<ul><li>Collections</li><li>Indexes</li><li>User-defined functions (UDFs in short)</li></ul>\r\n<p>These privileges can range from <em>\"this role can read and delete any document of this collection\"</em> to more sophisticated behaviors such as <em>\"this role can modify this document if the logged in user is its author\"</em>.</p>\r\n<h4><strong>Server role</strong></h4>\r\n<p>All FaunaDB databases include a special server role that can access all resources. Beware: if you're using a key with this role, you should store its secret safely and never commit it to your GIT repository.</p>\r\n<figure><img src=\"{asset:8608:url}\" data-image=\"8608\"></figure>\r\n<h2>Creating a server key</h2>\r\n<p>As explained before, if you're accessing FaunaDB from a server-side environment, you will need an application key and its secret.</p>\r\n<p>The easiest way to create keys is from the security tab in FaunaDB's dashboard:</p>\r\n<p></p>\r\n<p>Select the <strong>Server</strong> role in the dropdown which will grant this key access to everything in the database.</p>\r\n<p>Finally, click <strong>SAVE</strong>:</p>\r\n<figure><img src=\"{asset:8609:url}\" data-image=\"8609\"></figure>\r\n<p>After creating your key, FaunaDB will show you its secret, which you'll use in your code. Don't forget to store it somewhere safe. It will never be displayed again.</p>\r\n<figure><img src=\"{asset:8610:url}\" data-image=\"8610\"></figure>\r\n<p>You can also create keys with FQL using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/createkey\">CreateKey</a> function:</p>\r\n<pre>CreateKey({\r\n  role: \"server\"\r\n})\r\n\r\n// Result:\r\n \r\n{\r\n  ref: Key(\"269237655682679301\"),\r\n  ts: 1593023887265000,\r\n  role: \"server\",\r\n  secret: \"fnADvIY4qyACBa1k0EGH_N4YGXiFTLuj7q_7aBjP\",\r\n  hashed_secret: \"$2a$05$MPFpLVrMFV5Oszfe9lqwG.FvH.LvVeryNOH4DEd4qbOiZ5N7uzk82\"\r\n}</pre>\r\n<h4><strong>About client-side keys</strong></h4>\r\n<p>If you're accessing FaunaDB from a client-side environment (e.g., frontend web app), you should never use a key with a server role. Anyone would be able to read the key from your JavaScript code and gain full access to the database.</p>\r\n<p>Again, <strong>don't use server keys in your frontend web app</strong>. </p>\r\n<p>That said, it's certainly possible to query FaunaDB directly from your frontend apps or mobile apps by creating keys with custom roles. Depending on the security features you desire, you could go with a frontend-only approach and move the authentication flow server-side. The FaunaDB team is currently working on guidance on the best security practices regarding different authentication scenarios.</p>\r\n<p>For simplicity's sake, from now on we're just going to assume short-lived access tokens are generated server-side. These tokens could then be used from any type of application.</p>\r\n<h2>Basics of authentication</h2>\r\n<p>Let's see how to solve one of the most common authentication scenarios: logging in a user with an email and a password.</p>\r\n<p>Before we get into the details, let's create a new collection for our users:</p>\r\n<pre>CreateCollection({name: \"SpaceUsers\"})</pre>\r\n<h4><strong>Where to store the password?</strong></h4>\r\n<p>You might be tempted to store a hashed password in the user document like you've probably done with other databases:</p>\r\n<pre>// Don't do this!\r\n\r\nCreate(\r\n  Collection(\"SpaceUsers\"),\r\n  {\r\n    data: {\r\n      email: \"darth@empire.com\",\r\n      password: \"$2y$12$XUxxWc.81aq4CKsV/...\"\r\n    }\r\n  }\r\n)</pre>\r\n<p>You could certainly do that if you wanted to roll your own authentication system, but FaunaDB already includes a better way which is easier to use and more secure.</p>\r\n<p>Ok, so where do we store the password, and how do we use it?</p>\r\n<p>I mentioned earlier that access tokens are used by users. The way to tell FaunaDB that an entity (such as a user document) <em>\"has a password\"</em> is by adding a credentials object to the metadata of a document.</p>\r\n<p>With this in mind, let's create our first user:</p>\r\n<pre>Create(\r\n  Collection(\"SpaceUsers\"),\r\n  {\r\n    data: {\r\n      email: \"darth@empire.com\"\r\n    },\r\n    credentials: {\r\n      password: \"iamyourfather\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n \r\n{\r\n  ref: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  ts: 1592960287940000,\r\n  data: {\r\n    email: \"darth@empire.com\"\r\n  }\r\n}</pre>\r\n<p>As you can see, the <strong>credentials</strong> object is not part of the document's data and it's never returned when accessing the document. Because of that, you won't be able to expose the hashed credentials by mistake.</p>\r\n<p>It really doesn't matter <em>where</em> these credentials are stored. All the encryption and verification of passwords is solved for you when using FaunaDB's authentication system.</p>\r\n<h4><strong>Logging in</strong></h4>\r\n<p>Since credentials are associated with documents, we will need to find a user's document in the <strong>SpaceUsers</strong> collection to be able to log them in.</p>\r\n<p>Let's create an index to do just that, and make sure there can only be one user for each email address by setting <strong>unique</strong> to <strong>true</strong>:</p>\r\n<pre>CreateIndex({\r\n  name: \"SpaceUsers_by_email\",\r\n  source: Collection(\"SpaceUsers\"),\r\n  terms: [{field: [\"data\", \"email\"]}],\r\n  unique: true\r\n})</pre>\r\n<p>Now, we can use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/login\">Login</a> function in combination with the <strong>SpaceUsers_by_email</strong> index.</p>\r\n<pre>Login(\r\n  Match(Index(\"SpaceUsers_by_email\"), \"darth@empire.com\"),\r\n  {\r\n    password: \"iamyourfather\",\r\n    ttl: TimeAdd(Now(), 3, 'hour')\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Ref(\"tokens\"), \"269770764488540678\"),\r\n  ts: 1593532299503000,\r\n  ttl: Time(\"2020-06-30T18:51:39.033543Z\"),\r\n  instance: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  secret: \"fnEDvmsUvCACBgOyQyF20AITzhwm2fKyWe7M8Qwz11CFnU1sgQ0\"\r\n}</pre>\r\n<p>The <strong>Login</strong> function first takes a reference to a document or a set produced by Match. Its second argument is an object with the password and an optional time-to-live.</p>\r\n<p>If everything is ok, a new access token will be created and returned to us with a secret we can use in our application.</p>\r\n<p>Obviously, if the credentials are wrong, FaunaDB will return an error:</p>\r\n<pre>Login(\r\n  Match(Index(\"SpaceUsers_by_email\"), \"darth@empire.com\"),\r\n  {password: \"darksidemaster\"}\r\n)\r\n\r\n// Result:\r\n\r\nerror: authentication failed\r\nThe document was not found or provided password was incorrect.</pre>\r\n<h2>Authentication in your application</h2>\r\n<p>Let's see how we'd actually authenticate our users in a server-side JavaScript application. The approach should be very similar if you're using FaunaDB with <a href=\"https://docs.fauna.com/fauna/current/drivers/\">other programming languages</a>.</p>\r\n<p>First, we'd need to import FaunaDB's driver and define a couple of constants:</p>\r\n<pre>const faunadb = require('faunadb');\r\nconst q = faunadb.query;\r\nconst SERVER_SECRET = \"BQOyQyF20AITt7nMIqW1XzW...\";</pre>\r\n<p>We're hardcoding the secret here for simplicity's sake. Even in a server-side project, you should get the secret from an environment config and avoid committing it to Git with your code.</p>\r\n<p>Then, we instantiate our client using the secret from our server key:</p>\r\n<pre>const client = new faunadb.Client({\r\n  secret: SERVER_SECRET\r\n});</pre>\r\n<p>Finally, here's an example of an authentication function:</p>\r\n<pre>async function authenticate (email, password) {\r\n  return await client.query(\r\n    q.Login(\r\n      q.Match(q.Index('SpaceUsers_by_email'), email),\r\n      {password: password}\r\n    )\r\n  );\r\n}</pre>\r\n<p>After a successful login, we'll get an access token document with its secret like we saw previously:</p>\r\n<pre>{\r\n  ref: Ref(Ref(\"tokens\"), \"269174603208720901\"),\r\n  ts: 1592963755720000,\r\n  instance: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  secret: \"fnEDvEzgHtACBQOyQyF20AITt7nMIqW1XzWCqykziZa53WyVm8E\"\r\n}</pre>\r\n<p>Now that we have a token for our user, we should be using its secret for any subsequent queries to FaunaDB on behalf of our user.</p>\r\n<p>You have many options for storing the secret. Here are some examples:</p>\r\n<ul><li><strong>Pure client-side: </strong>If you intend on accessing FaunaDB client-side you could send the secret back to the client and store it in memory.</li><li><strong>Partial backend with cookie: </strong>If you're working on a server API you could store the secret in a session and send it back to the client using a secure cookie.</li><li><strong>Partial backend with httpOnly cookie: </strong>You could also combine the above two approaches by creating two types of tokens in FaunaDB. One that could be used as a refresh token and stored in an httpOnly cookie, and another short-lived one that could be used and stored in the frontend.</li><li><strong>Full blown backend: </strong>You could also decide you never want your clients receiving the secret and store the session in some cache and send back just a session id.</li></ul>\r\n<p>These examples have very different security implications which are far too vast and complex to discuss in this introductory article. You will have to decide carefully how you want to manage secrets for your particular use case.</p>\r\n<h4><strong>Logging out</strong></h4>\r\n<p>To log out, we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/logout\">Logout</a> function which will destroy the token we created when logging in.</p>\r\n<p>This could be our logout function:</p>\r\n<pre>async function logout (deleteAllTokens = false) {\r\n  return await client.query(q.Logout(deleteAllTokens));\r\n}</pre>\r\n<p>Note that we don't need to pass any reference to the token since we instantiated the client with a token's secret.</p>\r\n<p><strong>Logout</strong> takes a single boolean parameter to determine if all the tokens associated with a user should be deleted or only the one being used with the current secret. If we had used <strong>q.Logout(true) </strong>our user Darth would now be logged out from all his devices. Take that, evil Sith lord!</p>\r\n<p>Also note that <strong>Logout</strong> is actually a convenience function. You could also delete tokens manually with a reference to the token's document:</p>\r\n<pre>Delete(Ref(Ref(\"tokens\"), \"269174603208720901\"))</pre>\r\n<h4><strong>Advanced authentication</strong></h4>\r\n<p>You can keep using FaunaDB's authentication system even for custom scenarios without having to roll your own system from scratch.</p>\r\n<p>For example, you can create your own tokens with:</p>\r\n<pre>Create(Tokens(), {\r\n  instance: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  ttl: TimeAdd(Now(), 3, 'hour')\r\n})\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Ref(\"tokens\"), \"269776756060193286\"),\r\n  ts: 1593538013760000,\r\n  instance: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  ttl: Time(\"2020-06-30T20:26:53.134950Z\"),\r\n  secret: \"fnEDvnCHwaACBgOyQyF20AITOp_00s0UzldXKztxeEdM0Z48bxw\"\r\n}</pre>\r\n<p>And, then, use these other FQL functions to customize your authentication logic:</p>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/identify\">Identify</a> to check if a password is valid against a document's credentials.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/hasidentity\">HasIdentity</a> to check if a current FaunaDB client is associated with a document or not.</li></ul>\r\n<h2>Your first custom role</h2>\r\n<p>Our users can now log in, but they can't access any resource in our database. We need to create a role to give them access to collections, indexes, etc.</p>\r\n<p>Keep in mind that you can also manage roles from the dashboard. If you go to the security tab and click on <strong>Manage Roles</strong>, you'll find the roles section:</p>\r\n<figure><img src=\"{asset:8611:url}\" data-image=\"8611\"></figure>\r\n<p></p>\r\n<p>Let's start with something simple. We'll just create a <strong>User</strong> role with a single privilege:</p>\r\n<pre>CreateRole({\r\n  name: \"User\",\r\n  membership: {\r\n    resource: Collection(\"SpaceUsers\")\r\n  },\r\n  privileges: [\r\n    {resource: Collection(\"Spaceships\"), actions: {read: true}}\r\n  ]\r\n})</pre>\r\n<p>The <strong>SpaceUsers</strong> collection is now a member of the <strong>User</strong> role. Any token associated with a document from that collection will inherit the role's privileges, including previously created tokens.</p>\r\n<p>We've also granted a single read-only privilege on any document from the <strong>Spaceships</strong> collection. Check the docs for <a href=\"https://docs.fauna.com/fauna/current/security/roles#actions\">the complete list of actions</a> we can use to define privileges.</p>\r\n<p>Darth will now be able to retrieve any <strong>Spaceships</strong> document, but he won't be able to create new documents in that collection or modify existing ones.</p>\r\n<p>He won't be able to use any indexes either, but he will be able to use <strong>Get </strong>to retrieve a specific spaceship document and also list all spaceship documents using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/documents\">Documents</a> function we saw in previous articles:</p>\r\n<pre>Map(\r\n  Paginate(Documents(Collection(\"Spaceships\"))),\r\n  Lambda(\"ref\", Get(Var(\"ref\")))\r\n)</pre>\r\n<h4><strong>Updating roles</strong></h4>\r\n<p>Let's update the role with another privilege by using <strong>Update</strong>. Remember that we need to pass all privileges, including the ones we had previously set, because <strong>Update</strong> will replace the entire array.</p>\r\n<pre>Update(\r\n  Role(\"User\"),\r\n  {\r\n    privileges: [\r\n      { resource: Collection(\"Spaceships\"), actions: { read: true } },\r\n      { resource: Collection(\"Planets\"), actions: { read: true } }\r\n    ]\r\n  }\r\n)</pre>\r\n<p>Note that existing keys and tokens belonging to a role will be affected by the updated privileges.</p>\r\n<h2>Fine-grained privileges</h2>\r\n<p>It's also possible to create custom behaviors for privileges instead of simply using <strong>true</strong> or <strong>false</strong>. </p>\r\n<p>For example, we might want Darth to be able to access his own <strong>SpaceUsers</strong> document, but we certainly don't want him poking around all users' documents to obtain their email addresses and spam them to join his empire.</p>\r\n<p>We do that by using a <strong>Lambda</strong> to define any type of behavior we might need:</p>\r\n<pre>Update(\r\n  Role(\"User\"),\r\n  {\r\n    privileges: [\r\n      { resource: Collection(\"Spaceships\"), actions: { read: true } },\r\n      { resource: Collection(\"Planets\"), actions: { read: true } },\r\n      {\r\n        resource: Collection(\"SpaceUsers\"),\r\n        actions: {\r\n          read: Query(\r\n            Lambda(\"ref\",\r\n              Equals(\r\n                Identity(),\r\n                Var(\"ref\")\r\n              )\r\n            )\r\n          )\r\n        }\r\n      }\r\n    ]\r\n  }\r\n)</pre>\r\n<p>In this case, we've used this <strong>Lambda</strong> function on the read action for the <strong>SpaceUsers</strong> collection:</p>\r\n<pre>Query(\r\n  Lambda(\"ref\",\r\n    Equals(\r\n      Identity(),\r\n      Var(\"ref\")\r\n    )\r\n  )\r\n)</pre>\r\n<p></p>\r\n<ul><li>We need to use <strong>Query</strong> because we don't want <strong>Lambda</strong> to execute when we're only updating the role itself.</li><li>Whenever a <strong>SpaceUsers</strong> document is accessed, FaunaDB will trigger the <strong>Lambda</strong> and pass a reference of the document it's checking. Access will be granted only if that <strong>Lambda</strong> returns <strong>true</strong>.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/identity\">Identity</a> will return a reference to the document associated with the current token in use. In our example, it would return the document in the <strong>SpaceUsers</strong> collection for the current logged in user.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/equals\">Equals</a> will return <strong>true</strong> or <strong>false</strong> when comparing the reference returned by <strong>Identity</strong> to the reference of the document we're trying to read.</li></ul>\r\n<p>In plain English: <em>\"if the document in </em><strong><em>SpaceUsers</em></strong><em> is the same as the document we've logged in with, return true, otherwise return false\"</em>.</p>\r\n<p>To test this, let's create a new user:</p>\r\n<pre>Create(\r\n  Collection(\"SpaceUsers\"),\r\n  {\r\n    data: {\r\n      email: \"yoda@jedi.com\"\r\n    },\r\n    credentials: {\r\n      password: \"thereisnotry\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"SpaceUsers\"), \"269412903498547719\"),\r\n  ts: 1593191016630000,\r\n  data: {\r\n    email: \"yoda@jedi.com\"\r\n  }\r\n}</pre>\r\n<p>So now, if we try to access Yoda's document using Darth's token in our application, we will get an error:</p>\r\n<pre>try {\r\n  const result = await client.query(\r\n    q.Get(q.Ref(q.Collection(\"SpaceUsers\"), \"269412903498547719\"))\r\n  )\r\n} catch (error) {\r\n  console.log(error);\r\n}\r\n\r\n// Result:\r\n\r\n[PermissionDenied: permission denied] {\r\n  name: 'PermissionDenied',\r\n  message: 'permission denied',\r\n  description: 'Insufficient privileges to perform the action.',\r\n ...</pre>\r\n<p>But it will work fine if we try to access Darth's document:</p>\r\n<pre>q.Get(q.Ref(q.Collection(\"SpaceUsers\"), \"269412903498547719\"))\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"SpaceUsers\"), \"269170966886613509\"),\r\n  ts: 1592960287940000,\r\n  data: { email: 'darth@empire.com' }\r\n}</pre>\r\n<h2>Fine-grained memberships</h2>\r\n<p>Just as we can use <strong>Lambda</strong> to define custom behaviors to check if a role can do something, we can also create fine-grained memberships and determine which documents on a collection are members of a role.</p>\r\n<p>Let's create a new user to test this:</p>\r\n<pre>Create(\r\n  Collection(\"SpaceUsers\"),\r\n  {\r\n    data: {\r\n      email: \"han@solo.com\",\r\n      isPilot: true\r\n    },\r\n    credentials: {\r\n      password: \"dontgetcocky\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"SpaceUsers\"), \"269417695003279879\"),\r\n  ts: 1593195586136000,\r\n  data: {\r\n    email: \"han@solo.com\",\r\n    isPilot: true\r\n  }\r\n}</pre>\r\n<p>Now, let's create a new <strong>Pilot</strong> role that will only grant permissions to users with the <strong>isPilot</strong> property. We do that by adding a predicate function to the membership object:</p>\r\n<pre>CreateRole({\r\n  name: \"Pilot\",\r\n  membership: {\r\n    resource: Collection(\"SpaceUsers\"),\r\n    predicate:\r\n      Query(\r\n        Lambda(\r\n          \"ref\",\r\n          Select([\"data\",\"isPilot\"], Get(Var(\"ref\")), false)\r\n        )\r\n      )\r\n  },\r\n  privileges: [\r\n    {resource: Collection(\"Spaceships\"), actions: {create: true}}\r\n  ]\r\n})</pre>\r\n<p>We've added a privilege that simply allows creating documents in the <strong>Spaceships</strong> collection. <br><br>Let’s look at the membership predicate function:</p>\r\n<pre>Query(\r\n  Lambda(\r\n    \"ref\",\r\n    Select(\r\n      [\"data\",\"isPilot\"],\r\n      Get(Var(\"ref\")),\r\n      false\r\n    )\r\n  )\r\n)</pre>\r\n<ul><li><strong>Lambda</strong> will receive a reference to a document and will return whatever <strong>Select</strong> returns.</li><li><strong>Select</strong> will return the value of <strong>isPilot</strong> from the document. If the path <strong>[\"data\",\"isPilot\"]</strong> doesn't exist in the document, it will return <strong>false</strong>.</li></ul>\r\n<p>In plain English: <em>\"if the document in </em><strong><em>SpaceUsers</em></strong><em> contains </em><strong><em>isPilot</em></strong><em> and is set to </em><strong><em>true</em></strong><em>, the logged in user will be able to create documents in the </em><strong><em>SpaceShips </em></strong><em>collection</em><strong><em>“.</em></strong><br><br>As expected, if we try to create a new ship with Darth's token, we will get an error because the <strong>User</strong> role doesn't have that privilege:</p>\r\n<pre>try {\r\n  const result = await client.query(\r\n    q.Create(\r\n      q.Collection(\"Spaceships\"),\r\n      {\r\n        data: {\r\n          name: \"Imperial Destroyer\"\r\n        }\r\n      }\r\n    )\r\n  )\r\n  console.log(result);\r\n} catch (error) {\r\n  console.log(error);\r\n}\r\n\r\n// Result:\r\n\r\n[PermissionDenied: permission denied] {\r\n  name: 'PermissionDenied',\r\n  message: 'permission denied',\r\n  description: 'Insufficient privileges to perform the action.',\r\n  ...</pre>\r\n<p>But if we do it with Han's token instead:</p>\r\n<pre>const result = await client.query(\r\n  q.Create(\r\n    q.Collection(\"Spaceships\"),\r\n    {\r\n      data: {\r\n        name: \"Millennium Falcon\"\r\n      }\r\n    }\r\n  )\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"Spaceships\"), \"269419218694308358\"),\r\n  ts: 1593197039260000,\r\n  data: { name: 'Millennium Falcon' }\r\n}</pre>\r\n<h2>Privileges over UDFs</h2>\r\n<p>We can grant privileges on UDFs just as we can on collections and indexes.</p>\r\n<p>Let's create a simple function that opens the hatch of a spaceship and also writes an entry to the log:</p>\r\n<pre>CreateFunction({\r\n  name: \"OpenHatch\",\r\n  body: Query(\r\n    Lambda(\"shipRef\",\r\n      Do(\r\n        Update(\r\n          Var(\"shipRef\"),\r\n          Let({\r\n            shipDoc: Get(Var(\"shipRef\")),\r\n          }, {\r\n            data:{\r\n              hatchIsOpen: true\r\n            }\r\n          })\r\n        ),\r\n        Create(\r\n          Collection(\"ShipLogs\"),\r\n          {\r\n            data: {\r\n              spaceshipRef: Var(\"shipRef\"),\r\n              status: \"HATCH_OPENED\",\r\n              pilotRef: Identity()\r\n            }\r\n          }\r\n        ),\r\n        \"Hatch open!\"\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<p>This function will:</p>\r\n<ol><li>Receive a reference to a ship</li><li>Modify the ship's document and set <strong>hatchIsOpen</strong> to <strong>true</strong>.</li><li>Create a new document in the <strong>ShipLogs</strong> collection.</li><li>Return <strong>\"Hatch open!\"</strong> at the end.</li></ol>\r\n<p>If this function is unclear, I recommend going back to <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">part 4</a> where we go through functions and transactions.</p>\r\n<p>We'd call this function by simply passing a reference of the spaceship:</p>\r\n<pre>Call(\r\n  Function(\"OpenHatch\"),\r\n  Ref(Collection(\"Spaceships\"), \"266356873589948946\")\r\n)</pre>\r\n<p>Now, let's update the privileges to our <strong>Pilot</strong> role:</p>\r\n<pre>Update(Role(\"Pilot\"), {\r\n  privileges: [\r\n    {\r\n      resource: Collection(\"Spaceships\"),\r\n      actions: {create: true, write: true}\r\n    },\r\n    { resource: Collection(\"ShipLogs\"), actions: {create: true} },\r\n    { resource: Function(\"OpenHatch\"), actions: {call: true} }\r\n  ]\r\n})</pre>\r\n<p>Other than granting our pilots the privilege to call the <strong>OpenHatch</strong> function, we're also granting privileges to the resources that the function needs to execute.</p>\r\n<p>The problem is that by setting <strong>call</strong> to <strong>true,</strong> any pilot would be able to open any hatch of any ship. They could open the hatch of another spaceship by mistake while warping through a wormhole and break the space-time continuum!</p>\r\n<p>That's not good. Let's make sure pilots can only open the hatch of their own ships.</p>\r\n<p>First, let's assign Han to his spaceship:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"Spaceships\"), \"269419218694308358\"),\r\n  {\r\n    data: {\r\n      pilotRef: Ref(Collection(\"SpaceUsers\"), \"269417695003279879\")\r\n    }\r\n  }\r\n)</pre>\r\n<p>Now, let's update our role so that Han can only warp his own ship.</p>\r\n<pre>Update(Role(\"Pilot\"), {\r\n  privileges: [\r\n    {\r\n      resource: Collection(\"Spaceships\"),\r\n      actions: {create: true, write: true}\r\n    },\r\n    { resource: Collection(\"ShipLogs\"), actions: {create: true} },\r\n    {\r\n      resource: Function(\"OpenHatch\"),\r\n      actions: {\r\n        call: Query(\r\n          Lambda(\r\n            \"shipRef\",\r\n            Let(\r\n              {\r\n                shipDoc: Get(Var(\"shipRef\")),\r\n                pilotRef: Select([\"data\",\"pilotRef\"], Var(\"shipDoc\"), null)\r\n              },\r\n              Equals(Identity(), Var(\"pilotRef\"))\r\n            )\r\n          )\r\n        )\r\n      }\r\n    }\r\n  ]\r\n})</pre>\r\n<p>This is our <strong>Lambda</strong>:</p>\r\n<pre>Lambda(\r\n  \"shipRef\",\r\n  Let(\r\n    {\r\n      shipDoc: Get(Var(\"shipRef\")),\r\n      pilotRef: Select([\"data\",\"pilotRef\"], Var(\"shipDoc\"), null)\r\n    },\r\n    Equals(Identity(), Var(\"pilotRef\"))\r\n  )\r\n)</pre>\r\n<p>This <strong>Lambda</strong> is going to receive the same arguments we are using to call the function. So then, we just need to get the spaceship document and check whether the logged in user is the same as the pilot.</p>\r\n<p>If we test this using Han's token on the Falcon:</p>\r\n<pre>const result = await client.query(\r\n  q.Call(\r\n    q.Function(\"OpenHatch\"),\r\n    q.Ref(q.Collection(\"Spaceships\"), \"269419218694308358\")\r\n  )\r\n)\r\nconsole.log(result);\r\n\r\n// Result:\r\n\r\nHatch open!</pre>\r\n<p>As expected, a document was created in the logs with the proper references:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"ShipLogs\"), \"269686129668653575\"),\r\n  \"ts\": 1593451585430000,\r\n  \"data\": {\r\n    \"spaceshipRef\": Ref(Collection(\"Spaceships\"), \"269419218694308358\"),\r\n    \"status\": \"HATCH_OPENED\",\r\n    \"pilotRef\": Ref(Collection(\"SpaceUsers\"), \"269417695003279879\")\r\n  }\r\n}</pre>\r\n<p>If we try to call the same function with a different ship reference, we will get an error though:</p>\r\n<pre>try {\r\n  const result = await client.query(\r\n    q.Call(\r\n      q.Function(\"OpenHatch\"),\r\n      q.Ref(q.Collection(\"Spaceships\"), \"266356873589948946\")\r\n    )\r\n  )\r\n} catch (error) {\r\n  console.log(error);\r\n}\r\n\r\n// Result:\r\n\r\n[PermissionDenied: permission denied] {\r\n  name: 'PermissionDenied',\r\n  message: 'permission denied',\r\n  description: 'Insufficient privileges to perform the action.',\r\n...</pre>\r\n<h2>Final conclusion (to this 5 part series)</h2>\r\n<p>With this article, we've finally reached the end of the series. What an adventure. We've travelled through the galaxy, worked with famous pilots, created spaceships, feeded futuristic holographic UIs with data… and hopefully, also learned some FQL along the way!</p>\r\n<p>We've gone through many common scenarios and problems, but if you ever get stuck you can always get help from <a href=\"https://community.fauna.com/\">Fauna's community</a>.</p>\r\n<p>Don't forget you can also hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>\r\n<p>Farewell, fellow space developer!</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8607"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-07-28T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8534,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "5c661d1b-76c0-4f14-a236-2c80bc768fa8",
        "siteSettingsId": 8534,
        "fieldLayoutId": 4,
        "contentId": 2873,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting started with FQL, FaunaDB’s native query language - part 4",
        "slug": "getting-started-with-fql-faunadbs-native-query-language-part-4",
        "uri": "blog/getting-started-with-fql-faunadbs-native-query-language-part-4",
        "dateCreated": "2020-07-27T08:46:57-07:00",
        "dateUpdated": "2020-09-02T09:09:55-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-fql-faunadbs-native-query-language-part-4",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4",
        "isCommunityPost": true,
        "blogBodyText": "<p>Welcome back, fellow space developer! We will continue our FQL space journey in this five-part series of articles.</p><ul><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">Part 1: a look at FQL and fundamental FaunaDB concepts</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">Part 2: a deep dive into indexes with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">Part 3: a look into the principles of modeling data with FaunaDB</a></li><li>Part 4: a look at how to create custom functions that run straight in FaunaDB</li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5\">Part 5: a look at authentication and authorization in FaunaDB</a></li></ul><p>Today, we're going to take a look at how to create custom functions that run straight in FaunaDB.</p>\r\n<h2>In this article:</h2>\r\n<ul><li>Introduction</li><li>Why use UDFs?</li><li>Your first function</li><li>Creating transactions with Do</li><li>Aborting functions and transactions</li><li>Warping across the galaxy</li><li>Data aggregation</li></ul>\r\n<h2>Introduction</h2>\r\n<p>We've seen in previous articles of this series that FQL is much closer to a functional programming language than other querying languages like SQL or GraphQL.</p>\r\n<p>Much like in any programming language, FQL also allows you to craft complex behaviors by creating functions. We've already seen multiple examples of anonymous functions using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lambda\">Lambda</a> in your FQL queries. It's also possible to encapsulate these custom behaviors into the database by creating user-defined functions (UDFs in short) that can be invoked from your queries or even from other UDFs.</p>\r\n<p>UDFs are somewhat similar to stored procedures found in other databases. Of course, the implementation and capabilities of UDFs in FaunaDB will be very different because of the unique nature of FQL. For example, it's common to use stored procedures to group SQL queries, or to reduce the results sent to the database clients. You wouldn't really need to use an UDF in those situations since these can be accomplished in a regular FQL query executed from the application.</p>\r\n<h2>Why use UDFs?</h2>\r\n<p>I mean, other than because UDFs are super cool, there are a couple of reasons why you'd want to move logic into the database.</p>\r\n<h4><strong>Avoid code duplication</strong></h4>\r\n<p>If you have multiple clients (web, API, mobile, desktop, microservices) written in multiple programming languages, you will probably want to avoid maintaining different versions of the same business logic. By moving some of that logic to the database, you can avoid code duplication, and thus all the effort and confusion that code duplication usually causes.</p>\r\n<h4><strong>Abstraction and decoupling of processes</strong></h4>\r\n<p>As applications grow, you often need to abstract processes and their underlying data. This can be easily accomplished with UDFs. As an added benefit, the process is now decoupled from the rest of your logic. An outdated version of your application (e.g. web or mobile) could keep interacting with FaunaDB without knowing that an UDF has, in fact, been updated multiple times.</p>\r\n<h4><strong>Consistency guarantees</strong></h4>\r\n<p>By having a single version of some business logic running as close to the database as possible, you will ensure your data is consistent. FQL is very expressive which will make this task easier compared to traditional stored procedures written in SQL.</p>\r\n<h2>Your first function</h2>\r\n<p>We'll start with a very simple function just to see the basics.</p>\r\n<p>Here's the latest version of our spaceship document from the previous articles:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n  \"ts\": 1592255653240000,\r\n  \"data\": {\r\n    \"name\": \"Voyager\",\r\n    \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    \"type\": \"Rocket\",\r\n    \"fuelType\": \"Plasma\",\r\n    \"actualFuelTons\": 7,\r\n    \"maxFuelTons\": 10,\r\n    \"maxCargoTons\": 25,\r\n    \"maxPassengers\": 5,\r\n    \"maxRangeLightyears\": 10,\r\n    \"celestialPosition\": {\r\n      \"x\": 2234,\r\n      \"y\": 3453,\r\n      \"z\": 9805\r\n    },\r\n    \"code\": \"VOYAGER\",\r\n    \"colors\": [\r\n      \"RED\",\r\n      \"YELLOW\"\r\n    ]\r\n  }\r\n}</pre>\r\n<p>With this in mind, let's create a function that receives an id and returns an object:</p>\r\n<pre>CreateFunction({\r\n  name: \"GetSpaceship\",\r\n  body: Query(\r\n    Lambda(\"shipId\",\r\n      Let(\r\n        {\r\n          shipDoc: Get(Ref(Collection(\"Spaceships\"),Var(\"shipId\")))\r\n        },\r\n        {\r\n          id: Select([\"ref\",\"id\"], Var(\"shipDoc\")),\r\n          name: Select([\"data\",\"name\"], Var(\"shipDoc\"))\r\n        }\r\n      )\r\n  ))\r\n})</pre>\r\n<p>If you've been following along with this series, there shouldn't be much of a mystery. We've previously covered <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lambda\">Lambda</a>, <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/let\">Let</a>, <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/select\">Select</a>, and <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/var\">Var</a> with detail.</p>\r\n<p>As expected, <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/createfunction\">CreateFunction</a> creates a new function with the specified name and body.</p>\r\n<p>We need to use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/query\">Query</a> function because we want to define a Lambda that will be executed later, not actually execute the Lambda when creating the function.</p>\r\n<p>This how we'd call the function:</p>\r\n<pre>Call(Function(\"GetSpaceship\"), \"266356873589948946\")\r\n\r\n// Result:\r\n\r\n{\r\n  id: \"266356873589948946\",\r\n  name: \"Voyager\"\r\n}</pre>\r\n<p>Of course you could also use this function anywhere in your FQL queries.</p>\r\n<p>Here's an example on how you could use it in combination with a list of results:</p>\r\n<pre>Map(\r\n  Paginate(Documents(Collection(\"Spaceships\"))),\r\n  Lambda(\r\n    \"shipRef\",\r\n    Call(Function(\"GetSpaceship\"), Select([\"id\"], Var(\"shipRef\")))\r\n  )\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  data: [\r\n    {\r\n      id: \"266356873589948946\",\r\n      name: \"Voyager\"\r\n    },\r\n    {\r\n      id: \"266619264914424339\",\r\n      name: \"Explorer IV\"\r\n    },\r\n    {\r\n      id: \"267096263925694994\",\r\n      name: \"Destroyer\"\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p><em><strong>Quick tip:</strong> the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/documents\">Documents</a> function allows you to retrieve a list of references from all the documents in a collection without having to set up an index.</em></p>\r\n<h2>Creating transactions with Do</h2>\r\n<p>Unlike many NoSQL databases, FaunaDB supports ACID transactions. This essentially means that it guarantees the validity of a transaction no matter what: power failure, server crash, gremlins, alien attack... Ok, maybe not in the case of an alien attack, but you get the idea.</p>\r\n<p>Actually, transactions in FaunaDB are ACIDD (not an actual technical term) as they are also Distributed worldwide to all FaunaDB clusters.</p>\r\n<h4><strong>The Do command</strong></h4>\r\n<p>The <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/do\">Do</a> command executes a list of FQL expressions sequentially to form a transaction. Changes committed to the database in each of the expressions are immediately available to the following expressions.</p>\r\n<p>To verify this, let's create a new collection first:</p>\r\n<pre>CreateCollection({name: \"LaserColors\"})</pre>\r\n<p>And then:</p>\r\n<pre>Do(\r\n  // first create a document\r\n  Create(Ref(Collection(\"LaserColors\"), \"123456\"), {\r\n    data: {\r\n      name: \"Pink\"\r\n    }\r\n  }),\r\n  // then update that same document\r\n  Update(Ref(Collection(\"LaserColors\"), \"123456\"), {\r\n    data: {\r\n      hex: \"#ff5c9e\"\r\n    }\r\n  })\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"LaserColors\"), \"123456\"),\r\n  ts: 1592364971590000,\r\n  data: {\r\n    name: \"Pink\",\r\n    hex: \"#ff5c9e\"\r\n  }\r\n}</pre>\r\n<p>As you can see, the document created in the first expression is immediately available.</p>\r\n<p>The <strong>Do</strong> command returns whatever the last command in the sequence returned, so we get the full document with the updated data.</p>\r\n<h2>Aborting functions and transactions</h2>\r\n<p>Obviously, whenever something fails, FaunaDB will let you know about it. You can also define when and how you want a transaction or a function to fail. In FaunaDB this is done using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/abort\">Abort</a> function.</p>\r\n<p>Let's create a simple example:</p>\r\n<pre>Do(\r\n  \"Step 1\",\r\n  \"Step 2\",\r\n  Abort(\"You shall not pass!\"),\r\n  \"Step 3\"\r\n)\r\n\r\n// Result:\r\n\r\nerror: transaction aborted\r\nYou shall not pass!\r\nposition: [\"do\",2]</pre>\r\n<p>Now, if you were executing this (rather useless) query in your application, you'd be getting an exception.</p>\r\n<p>In JavaScript for example:</p>\r\n<pre>try {\r\n  const result = await client.query(\r\n    q.Do(\r\n      \"Step 1\",\r\n      \"Step 2\",\r\n      q.Abort(\"You shall not pass!\"),\r\n      \"Step 3\"\r\n    )\r\n  );\r\n} catch (error) {\r\n  // do something with the error      \r\n}</pre>\r\n<p>As expected, this applies to UDFs too:</p>\r\n<pre>CreateFunction({\r\n  name: \"StopIt\",\r\n  body: Query(\r\n    Lambda(\"bool\",\r\n      If(\r\n        Var(\"bool\"),\r\n        Abort(\"Stopped!\"),\r\n        \"Not stopped!\"\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<p>If we pass <strong>true</strong> to the UDF, the execution of the function will be aborted and an exception will be raised:</p>\r\n<pre>Call(Function(\"StopIt\"), true)\r\n\r\n// Result:\r\n\r\nError: [\r\n  {\r\n    \"position\": [],\r\n    \"code\": \"call error\",\r\n    \"description\": \"Calling the function resulted in an error.\",\r\n    \"cause\": [\r\n      {\r\n        \"position\": [\r\n          \"expr\",\r\n          \"then\"\r\n        ],\r\n        \"code\": \"transaction aborted\",\r\n        \"description\": \"Stopped!\"\r\n      }\r\n    ]\r\n  }\r\n]</pre>\r\n<h2>Warping across the galaxy</h2>\r\n<p>Let's go through a more complex example to give you a better idea on how these concepts work together. We're going to create a <strong>WarpToPlanet</strong> function to propel our ships to infinity and beyond.</p>\r\n<h4><strong>Step 1: Check if we have enough fuel</strong></h4>\r\n<p>I have to admit that my celestial navigation math is a bit rusty, especially if wormholes are involved, so we're just going to assume that a spaceship needs <strong>5</strong> tons of fuel to warp anywhere in the galaxy.</p>\r\n<p>To know how much fuel a ship has left, we can use this property:</p>\r\n<pre>\"actualFuelTons\": 7</pre>\r\n<p>Let's make a function that returns&nbsp;<strong>true</strong> if there is enough fuel to create a wormhole and travel through it:</p>\r\n<pre>CreateFunction({\r\n  name: \"HasEnoughFuelToWarp\",\r\n  body: Query(\r\n    Lambda(\"shipRef\",\r\n      Let(\r\n        {\r\n          shipDoc: Get(Var(\"shipRef\")),\r\n          actualFuelTons: Select([\"data\",\"actualFuelTons\"], Var(\"shipDoc\"))\r\n        },\r\n        GTE(Var(\"actualFuelTons\"), 5)\r\n      )\r\n  ))\r\n})</pre>\r\n<p>This is a very straightforward Lambda:</p>\r\n<ul><li>First, we prepare the Let bindings that we need. In this case, we get the document and extract the <strong>actualFuelTons</strong> property from the document.</li><li>Second, we check that the <strong>actualFuelTons</strong> is greater than or equal to <strong>5</strong></li></ul>\r\n<p>To test it out, we only need to use a reference to our Voyager ship (which we know has <strong>7</strong> tons of fuel available):</p>\r\n<pre>Call(\r\n  Function(\"HasEnoughFuelToWarp\"),\r\n  Ref(Collection(\"Spaceships\"), \"266356873589948946\")\r\n)\r\n\r\n// Result:\r\n\r\ntrue</pre>\r\n<h4><strong>Step 2: Open the wormhole and warp</strong></h4>\r\n<p>Now, let's create a simple function to enable lightspeed on the ship by simply updating a bit of data on its document:</p>\r\n<pre>CreateFunction({\r\n  name: \"OpenWormholeAndWarp\",\r\n  body: Query(\r\n    Lambda(\"shipRef\",\r\n      Update(\r\n        Var(\"shipRef\"),\r\n        Let({\r\n          shipDoc: Get(Var(\"shipRef\")),\r\n          actualFuelTons: Select([\"data\",\"actualFuelTons\"], Var(\"shipDoc\"))\r\n        }, {\r\n          data:{\r\n            actualFuelTons: Subtract(Var(\"actualFuelTons\"), 5)\r\n          }\r\n        })\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<p>Easy, right? We're just subtracting 5 from the <strong>actualFuelTons</strong> using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/subtract\">Subtract</a> function.</p>\r\n<p>Let's test this out on our Destroyer ship which currently has <strong>11</strong> tons of fuel:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"267096263925694994\"),\r\n  \"ts\": 1592513359750000,\r\n  \"data\": {\r\n    \"name\": \"Destroyer\",\r\n    \"actualFuelTons\": 11\r\n    // etc...\r\n  }\r\n}</pre>\r\n<p>To invoke the function, we just need a reference to the document of the ship:</p>\r\n<pre>Call(\r\n  Function(\"OpenWormholeAndWarp\"),\r\n  Ref(Collection(\"Spaceships\"), \"267096263925694994\")\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"Spaceships\"), \"267096263925694994\"),\r\n  ts: 1592513503470000,\r\n  data: {\r\n    name: \"Destroyer\",\r\n    actualFuelTons: 6,\r\n    // etc...\r\n}</pre>\r\n<p>As expected, Destroyer now has <strong>6</strong> tons of fuel left.</p>\r\n<h4><strong>Step 3: Write to the ship's log</strong></h4>\r\n<p>The admiral wouldn't be too happy if we didn't keep a log of what's going on with our ships. We'll create a function that creates a new log entry whenever a ship warps to a new planet.</p>\r\n<p>First, we need a collection to store our logs:</p>\r\n<pre>CreateCollection({name: \"ShipLogs\"})</pre>\r\n<p>And a function to create a new document in that collection:</p>\r\n<pre>CreateFunction({\r\n  name: \"CreateLogEntry\",\r\n  body: Query(\r\n    Lambda([\"shipRef\",\"planetRef\",\"status\"],\r\n      Create(\r\n        Collection(\"ShipLogs\"),\r\n        {\r\n          data: {\r\n            shipRef: Var(\"shipRef\"),\r\n            planetRef: Var(\"planetRef\"),\r\n            status: Var(\"status\")\r\n          }\r\n        }\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<h4><strong>Step 4: All together now</strong></h4>\r\n<p>For our last step, let's see how to combine all these functions to create the super ultimate <strong>WarpToPlanet</strong> function:</p>\r\n<pre>CreateFunction({\r\n  name: \"WarpToPlanet\",\r\n  body: Query(\r\n    Lambda([\"shipRef\",\"planetRef\"],\r\n      If(\r\n        Call(Function(\"HasEnoughFuelToWarp\"), Var(\"shipRef\")),\r\n        Do(\r\n          Call(Function(\"OpenWormholeAndWarp\"), Var(\"shipRef\")),\r\n          Call(\r\n            Function(\"CreateLogEntry\"),\r\n            [Var(\"shipRef\"), Var(\"planetRef\"), \"WARPED_TO_PLANET\"]\r\n          ),\r\n          Let(\r\n            {\r\n              planetDoc: Get(Var(\"planetRef\")),\r\n              planetName: Select([\"data\",\"name\"],Var(\"planetDoc\")),\r\n              shipDoc: Get(Var(\"shipRef\")),\r\n              shipName: Select([\"data\",\"name\"],Var(\"shipDoc\")),\r\n            },\r\n            Concat([\"Welcome \",Var(\"shipName\"),\" to \",Var(\"planetName\")])\r\n          )\r\n        ),\r\n       Abort(\"Not enough fuel!\")\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<p>Let's break this down:</p>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/if\">If</a> will evaluate the result of the <strong>HasEnoughFuelToWarp</strong> function. If it returns true, it will execute the <strong>Do</strong> statement. If it returns false, if it will execute the <strong>Abort</strong> function.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/do\">Do</a> is executing a transaction, like we saw earlier.</li><li>The last expression of the transaction produces a welcome message when a ship arrives on a planet.</li></ul>\r\n<p>Finally, let's test all our hard work!</p>\r\n<p>Let's warp with Voyager to planet Vulkan:</p>\r\n<pre>Call(\r\n  Function(\"WarpToPlanet\"),\r\n  [\r\n    Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    Ref(Collection(\"Planets\"), \"268706982578356743\"),\r\n  ]\r\n)\r\n\r\n// Result:\r\n\r\nWelcome Voyager to Vulkan</pre>\r\n<p>Bravo!</p>\r\n<p>If we check our ship document, we can see the it only has&nbsp;<strong>2</strong> tons of fuel left:<br></p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n  \"ts\": 1592518256580000,\r\n  \"data\": {\r\n    \"name\": \"Voyager\",\r\n    \"actualFuelTons\": 2,\r\n    // etc...\r\n}</pre>\r\n<p>And there's also a new document in the&nbsp;<strong>ShipLogs</strong> collection:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"ShipLogs\"), \"268707463485719047\"),\r\n  \"ts\": 1592518256580000,\r\n  \"data\": {\r\n    \"shipRef\": Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    \"planetRef\": Ref(Collection(\"Planets\"), \"268706982578356743\"),\r\n    \"status\": \"WARPED_TO_PLANET\"\r\n  }\r\n}</pre>\r\n<p>Honestly, there's not much to do in Vulkan and these Vulkans are quite boring.</p>\r\n<p>Let's go back to Earth:</p>\r\n<pre>Call(\r\n  Function(\"WarpToPlanet\"),\r\n  [\r\n    Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n  ]\r\n)\r\n\r\n// Result:\r\n\r\nError: [\r\n  {\r\n    \"position\": [],\r\n    \"code\": \"call error\",\r\n    \"description\": \"Calling the function resulted in an error.\",\r\n    \"cause\": [\r\n      {\r\n        \"position\": [\r\n          \"expr\",\r\n          \"else\"\r\n        ],\r\n        \"code\": \"transaction aborted\",\r\n        \"description\": \"Not enough fuel!\"\r\n      }\r\n    ]\r\n  }\r\n]</pre>\r\n<p>Oh no! We don't have enough fuel to warp to Earth! Well, at least our function works as expected.</p>\r\n<p>Obviously, the logic of this example is extremely simple, but we've covered a number of important points related to UDFs.</p>\r\n<p>First, to operate the <strong>WarpToPlanet</strong> function, our application doesn't need to know anything about the fuel logic, or even about the structure of the related documents. It only needs to pass two references. When (not if) the implementation of the function changes, we won't need to update any code in our application(s).</p>\r\n<p>And second, to call the <strong>WarpToPlanet</strong> function our application needs to know about spaceships and planets, but it doesn't need to know about the <strong>ShipLogs</strong> collection.</p>\r\n<h2>Data aggregation</h2>\r\n<p>Let's see how to use UDFs to aggregate data from multiple documents.</p>\r\n<p>In our first article, the admiral tasked us with feeding his holomap with the position of all spaceships. This worked fine, but now he'd like to be able to go backwards and forwards in time to better understand the movement of the ships.</p>\r\n<p>Obviously, we need to store the position somehow, but the admiral won't tolerate a slow holomap, so it needs to be as fast as possible.</p>\r\n<p>We saw in <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">the previous article</a> that reading a single document will give us the best performance. We also saw this pattern presents some dangers, but since the recorded data will never change, and the number of ships is not very large, this is the perfect scenario for storing all the data in a single document.<br><br>First, let's create a new collection:</p>\r\n<pre>CreateCollection({name: \"ShipPositionsHistory\"})</pre>\r\n<p>And this would be the function:</p>\r\n<pre>CreateFunction({\r\n  name: \"RecordPositions4\",\r\n  body: Query(\r\n    Lambda(\"bool\",\r\n      Do(\r\n        Create(\r\n          Collection(\"ShipPositionsHistory\"),\r\n          Let({\r\n            shipsDocuments: Map(\r\n              Paginate(\r\n                Documents(Collection(\"Spaceships\"))),\r\n                Lambda(\"shipRef\", Get(Var(\"shipRef\"))\r\n              )\r\n            ),\r\n            positions: Map(\r\n              Var(\"shipsDocuments\"),\r\n              Lambda(\"shipDocument\",\r\n                {\r\n                  ref: Select([\"ref\"], Var(\"shipDocument\")),\r\n                  name: Select([\"data\",\"name\"], Var(\"shipDocument\")),\r\n                  position: Select(\r\n                    [\"data\",\"celestialPosition\"],\r\n                    Var(\"shipDocument\")\r\n                  )\r\n                }\r\n              )\r\n            )\r\n          },{\r\n            data: {\r\n              timestamp: Now(),\r\n              positions: Var(\"positions\")\r\n            }\r\n          })\r\n        ),\r\n        \"Positions recorded\"\r\n      )\r\n    )\r\n  )\r\n})</pre>\r\n<p>Again, these are the same FQL commands we've seen multiple times.</p>\r\n<p>This function would first get an array of Spaceships documents (denoted with the variable <strong>shipsDocuments</strong> in the Let). Then, it creates a new document into the <strong>ShipPositionsHistory</strong> collection with an array of ships and their positions.</p>\r\n<p>We are performing this inside a transaction with a simple string on the last step. Otherwise, we'd be returning the complete result of the Create function to our application, which might slow things down a bit.</p>\r\n<p>Now, we'd only need to trigger the function periodically:</p>\r\n<pre>Call(Function(\"RecordPositions\"))\r\n\r\n// Result:\r\n\r\nPositions recorded</pre>\r\n<p>If we check our <strong>ShipPositionsHistory</strong> collection, here is our first document:</p>\r\n<pre>{\r\n  ref: Ref(Collection(\"ShipPositionsHistory\"), \"268613645148094983\"),\r\n  ts: 1592428784478000,\r\n  data: {\r\n    timestamp: Time(\"2020-06-17T21:19:44.239194Z\"),\r\n    positions: {\r\n      data: [\r\n        {\r\n          ref: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n          name: \"Voyager\",\r\n          position: {\r\n            x: 2234,\r\n            y: 3453,\r\n            z: 9805\r\n          }\r\n        },\r\n        {\r\n          ref: Ref(Collection(\"Spaceships\"), \"266619264914424339\"),\r\n          name: \"Explorer IV\",\r\n          position: {\r\n            x: 1134,\r\n            y: 9453,\r\n            z: 3205\r\n          }\r\n        },\r\n        // etc...\r\n      ]\r\n    }\r\n  }\r\n}</pre>\r\n<h2>Conclusion</h2>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In part 5 of this series, we will wrap it all up by going deeper into roles and permissions in FaunaDB.</p>\r\n<p>If you have any questions, don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8543"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "8494",
        "postDate": "2020-07-23T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8489,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "beae97d7-c8a7-4f59-8734-f237081d83bc",
        "siteSettingsId": 8489,
        "fieldLayoutId": 4,
        "contentId": 2835,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Clive - Hannon Hill Engagement Tool Built with FaunaDB",
        "slug": "clive-hannon-hill-engagement-tool-built-with-faunadb",
        "uri": "blog/clive-hannon-hill-engagement-tool-built-with-faunadb",
        "dateCreated": "2020-07-08T14:03:58-07:00",
        "dateUpdated": "2020-07-23T09:29:24-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/clive-hannon-hill-engagement-tool-built-with-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/clive-hannon-hill-engagement-tool-built-with-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>Hannon Hill is known for its Content Management System called <a href=\"https://www.hannonhill.com/products/cascade-cms/\">Cascade CMS</a>, which comes in the form of an instance installed on a machine. This works well because Cascade CMS is a push CMS, meaning only a handful of content managers log in to the system while the duty of serving live data to potentially millions of website visitors is decoupled and delegated to web servers.</p>\r\n<p>However, Hannon Hill's newest product —<a href=\"https://www.hannonhill.com/products/clive/index.html\">Clive</a>, a digital engagement and personalization tool is designed to serve live content directly to the visitors, which means that with a popular enough website, a single machine would not be able to handle all the demand. Because of that, a more scalable solution is required. Instead of taking care of load balancing and manually scaling servers and a database, we have decided to use something different — we decided to go the serverless route, and a stack with AWS Lambda and FaunaDB turned out to be the best solution.</p><figure><img src=\"{asset:8491:url}\" data-image=\"8491\"></figure>\r\n<h2>Stack Overview</h2>\r\n<p>Clive's administrative application accessed by Clive content managers runs on a tried and true, non-serverless solution — Ruby on Rails. Rails pushes data to FaunaDB through the FaunaDB<a href=\"https://github.com/fauna/faunadb-ruby\"> Ruby gem</a>, so that the content is then available for Live API.</p>\r\n<p>On the other hand, the Live API that serves live content to visitors runs on a true serverless stack - AWS Lambda behind an API Gateway with TypeScript and NodeJS FaunaDB library. This allows seamless auto-scaling without any worry — no matter if websites are accessed by several visitors per hour or several millions of visitors per hour, the content will be served without any hiccups.</p>\r\n<p>To be able to easily publish AWS Lambda functions, we use the <a href=\"https://serverless.com/\">Serverless framework</a>, with some additional plugins that allow us to develop locally and deploy quickly. </p><figure><img src=\"{asset:8492:url}\" data-image=\"8492\"></figure>\r\n<h2>Why Serverless?</h2>\r\n<p>Clive gave us the opportunity to modernize our architecture (in parts). Instead of the classic three-tiered architecture, we chose to go serverless with Clive, to avail of these following benefits: :</p>\r\n<ul><li><strong>Auto-scaling</strong>: Both in terms of being able to handle a large number of requests in parallel (throughput) and in terms of max database size.<br><br>We currently handle traffic from as low as 5k requests per hour up to 50k requests per hour depending on the time of the day and we expect the numbers to grow.</li></ul>\r\n<ul><li><strong>Cost savings</strong>: Costs based on usage with zero cost when there is zero or near-zero usage; no need to pay for minimum or pre-defined throughput.<br><br>API gateway costs $3.50 per million requests while Lambda averages out to around $1.00 per million requests. $4.50 per million at an average of 30k requests per hour calculates to $0.135 per hour, which is pricing of somewhere between c5.large and c5.xlarge instances. A legacy non-serverless product we maintain that handles similar load runs currently on 3 c3.xlarge machines that unreserved cost $0.210 per hour. At $0.630 per hour the legacy product costs us almost 5 times as much as Clive does.</li></ul>\r\n<ul><li><strong>Time savings</strong>: No need to maintain the underlying operating system, perform upgrades, etc., the database as a service is made available through an easy-to-use API.<br><br>We average out about 10 hours a month spent on maintaining servers of the legacy non-serverless product while maintenance related tasks for FaunaDB and Lambda were related to client and NodeJS version upgrades less frequently than once a year.</li></ul>\r\n<h2>Using FaunaDB for Clive</h2>\r\n<p>While PostgreSQL serves the administration side and stores administration related data, such as Account, Users, Configurations, etc., Fauna serves the live side and stores live data, such as Forms, Form Submissions, Visitors, Page Views, etc. Thanks to this distinction, if PostgreSQL is down for any reason, live websites are not affected since all the necessary data is available in Fauna. Likewise, if Fauna is down for any reason, users can still use the administrative app to log in and manage content. Content managed by the administrative side but needed by the live side, such as Forms, is synchronized with Fauna in the background.</p>\r\n<p>Thanks to the fact that <strong>Fauna clients are available in many programming languages</strong>, including Ruby, the administrative side is able to fetch the live data and present it to administrators directly, without a need for additional network hops (Ruby-&gt;Fauna as opposed to Ruby-&gt;Lambda-&gt;Fauna). </p>\r\n<p>Some of our client Accounts have already accumulated many millions of Visitors, Visits and Page Views. <strong>Fauna allows administrators to browse that data quickly using cursor-based pagination</strong>, no matter what page the user is currently viewing and no matter how big the dataset is. This is a clear improvement over Spectate’s MySQL where loading large data tables with hundreds of pages was resulting in slugging performance.</p>\r\n<p><strong>Fauna handles parallel transactions very nicely</strong> as well. Thousands or millions of Visitors can be accessing client websites at the same time, yet<strong> no transactional blocking errors occur</strong> (such as \"Lock wait timeout\" errors that we are familiar with from CascadeCMS). Transactional blocking functions properly regardless, <strong>preventing creation of duplicate documents</strong>, double counts etc., without much of an effort. This is a much better developer experience compared to MySQL where we were running into issues with transactional blocking and we had to spend a good amount of time tweaking locking levels to get things to work right - not cause too much contention, yet not create duplicates. A simple example of a seamless transactional locking is the typical \"If(Exists(), Create(), Update())\" query. With default transactional locking levels MySQL is capable of creating duplicate rows when using corresponding \"select\", \"insert\" and \"update\" queries in a transaction if multiple parallel transactions try to insert/update records with the same key, while acquiring a lock on entire table level would quickly result in contention.</p>\r\n<p>At times model/schema changes are necessary. With hundreds of millions of rows, schema updates tend to take a very long time to get applied. To maximize uptime, such updates are typically executed in the background row by row while the app is running. This applies to both MySQL and Fauna. This can be a challenge to implement, since the app needs to be able to handle the data in both schema formats. Thanks to the fact that <strong>Fauna documents do not have to follow a specific schema</strong>, we are able to avoid many such updates altogether, saving us a lot of time, effort and saving our users from running into potential bugs.</p>\r\n<h2>Conclusion</h2>\r\n<p>FaunaDB was able to save us a good amount of DevOps time that we would have otherwise spent on building a scalable solution and maintaining or managing database servers. Because it can handle large volumes of load and it can quickly query from large volume data, our product performs smoothly at all times. The team at Fauna has been quick to respond to our issues, while introducing new functionality at a regular cadence. The documentation has helped us operate on an autopilot for the most part, however, we want to extend a huge shout out to the Fauna community of developers. It is one of the most helpful communities I’ve participated in, and we look forward to more collaboration with them. </p>\r\n<figure><img src=\"{asset:8493:url}\" data-image=\"8493\"></figure>",
        "blogCategory": [
            "3"
        ],
        "mainBlogImage": [
            "8490"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-07-14T05:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8505,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a612fb9c-ab74-404f-9590-929b3f3f4df0",
        "siteSettingsId": 8505,
        "fieldLayoutId": 4,
        "contentId": 2846,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting started with FQL, FaunaDB’s native query language - part 3",
        "slug": "getting-started-with-fql-faunadbs-native-query-language-part-3",
        "uri": "blog/getting-started-with-fql-faunadbs-native-query-language-part-3",
        "dateCreated": "2020-07-13T09:52:11-07:00",
        "dateUpdated": "2020-09-02T09:10:24-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-fql-faunadbs-native-query-language-part-3",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3",
        "isCommunityPost": true,
        "blogBodyText": "<p>Welcome back, fellow space developer! We will continue our FQL space journey in this five-part series of articles.</p>\r\n<ul><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">Part 1: a look at FQL and fundamental FaunaDB concepts</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">Part 2: a deep dive into indexes with FaunaDB</a></li><li>Part 3: a look into the principles of modeling data with FaunaDB</li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">Part 4: a look at how to create custom functions that run straight in FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5\">Part 5: a look at authentication and authorization in FaunaDB</a></li></ul><p>Today we're going to take a look into the principles of modeling data with FaunaDB.<br></p><ul></ul>\r\n<h2>In this article:</h2>\r\n<ul><li>Introduction</li><li>Normalization and denormalization</li><li>Nested data</li><li>References in FaunaDB</li><li>Arrays of references</li><li>Many-to-many relationships</li></ul>\r\n<h2>Introduction</h2>\r\n<p>FaunaDB is a rare breed in the world of databases as it allows you to model and query your data using different paradigms:</p>\r\n<ul><li>Relational</li><li>Documents (schemaless)</li><li>Temporal</li><li>Graph-like</li></ul>\r\n<p>As we'll see in this article, by having the flexibility to switch between different models, you can avoid common pitfalls inherent in each approach.</p>\r\n<p>Today we'll focus on documents and relational modeling techniques.</p>\r\n<h2>Normalization and denormalization</h2>\r\n<p>Document-based databases typically require that you resort to data duplication (denormalization) to be able to produce the answers needed for your application and implement certain access patterns.</p>\r\n<p>Here's a very simplistic example. Say we have millions of stored chat messages with this format:</p>\r\n<pre>{\r\n  author: \"Admiral Ackbar\",\r\n  message: \"It's a trap!\",\r\n  timestamp: 1591475572346\r\n}</pre>\r\n<p>That would make it super fast to retrieve a list of messages with the name of the author in our SpaceChat app.</p>\r\n<p>But what if our users now want to update their name? We'd need to perform millions of write operations. This is slow and impractical, but also very expensive since most document-based databases charge you by the number of document operations.</p>\r\n<p>If you're using a database that does not support ACID transactions, such big updates could even be dangerous as there is no guarantee about the final state of the transaction.</p>\r\n<p><strong>Quick note:</strong> FaunaDB offers ACID transactions, so this wouldn't be a problem here. Besides, since FaunaDB stores each document's complete event history, you could rollback the document to any previous state (as far back as defined by <a href=\"https://docs.fauna.com/fauna/current/api/fql/collections\">history_days</a> on the collection which can be set to forever/indefinite).</p>\r\n<p>In relational databases, this problem is non-existent as data is commonly normalized, or in other words, each bit of data is unique across the whole database. Normalization was born out of the necessity to (1) save money decades ago when storage was extremely expensive, but also (2) help maintain consistent data and avoid the problem of denormalization we just saw.</p>\r\n<p>In a relational database, the name of the user would only exist in a single row of the Users table:</p>\r\n<pre>USERS\r\nId      Name\r\n123     Admiral Ackbar\r\n\r\nMESSAGES\r\nId      Message          authorId\r\n23462   It's a trap!     123</pre>\r\n<p>Cool, so now you'd only need to update the name in a single place.</p>\r\n<p>Ah, but this introduces another problem. To retrieve a message, now the database needs to read data from multiple places on the disk and join that data back together before returning it to the application. The more tables involved when performing a query, the more CPU and disk IO you will consume. Eventually, this will become slow and expensive once you start having users all around the galaxy.</p>\r\n<p>In data modeling, it's all about tradeoffs. Sometimes, you will want to design your model for performance, sometimes for querying flexibility, and sometimes for cost.</p>\r\n<h2>Nested data</h2>\r\n<p>So far, we've been working with single entities stored on single document objects:</p>\r\n<ul><li>A pilot</li><li>A planet</li><li>A spaceship</li></ul>\r\n<p>You could get pretty far with simple documents and indexes, but at some point you will need to model more complex data.</p>\r\n<p>It's possible to use a single document to store multiple data entities together for <em>one-to-one</em> and <em>one-to-few</em> relationships. Let's say we wanted to model the weapons carried by our space pilots:</p>\r\n<pre>{\r\n  name: \"Flash Gordon\",\r\n  weapons: [\r\n    {\r\n      type: \"LASER_GUN\",\r\n      damage: 12\r\n    }\r\n  ]\r\n}</pre>\r\n<p>It seems almost natural in the document-based paradigm to model hierarchical data this way, but beware. There are some important points that need to be taken into consideration and may not be obvious.</p>\r\n<h4><strong>How much data are we nesting?</strong></h4>\r\n<p>We know it's unlikely our pilots will carry more than a dozen weapons. On the other hand, imagine we wanted to model galaxies, stars, planets, etc, for a SpaceMaps app. Since each galaxy can have billions of stars, this might not be a great idea:</p>\r\n<pre>{\r\n  name: \"Solar System\",\r\n  stars: [\r\n    {\r\n      name: \"Sun\",\r\n      brightness: −26.74,\r\n      massKg: 2000000000000000000000000000000\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>Galaxies can have billions of stars and your galaxy document would become huge.</p>\r\n<p>FaunaDB does not impose a size limit on documents, but as they get bigger, performance will start to degrade. The <a href=\"https://docs.fauna.com/fauna/current/api/fql/documents#limits\">documentation</a> warns us that we could start seeing degraded performance with documents larger than 5MB.</p>\r\n<p>Personally, I would strive to keep my documents much smaller than that. Even 1MB can hold a lot of data if you consider The Odyssey (yes, that ancient Greek book) can fit in a <a href=\"https://www.gutenberg.org/ebooks/1727\">700kB text file</a>.</p>\r\n<h4><strong>Will the data grow?</strong></h4>\r\n<p>This nesting pattern is not a great fit for use cases where the data either could grow indefinitely, or is unbound.</p>\r\n<p>For example, in SpaceAdvisor, our app for reviewing space hotels and restaurants, we will definitely not want to store reviews inside the properties documents:</p>\r\n<pre>{\r\n  name: \"The Nebula Gourmet\",\r\n  type: \"RESTAURANT\",\r\n  reviews: [\r\n    {\r\n      title: \"Delicious\",\r\n      stars: 5,\r\n      message: \"Best filet mignon in the whole quadrant!\"\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>Again, one problem here is that documents could potentially become huge and we don't want that. Additionally, accessing heavily nested data often requires more coding, so it should only be done in the context of your access patterns, as we'll discuss in the next few sections.</p>\r\n<h4><strong>How are we going to query the data?</strong></h4>\r\n<p>Another important consideration before nesting entities in a single document is knowing in advance what access patterns we'll need on those entities. This is critical in document-based databases as your design is usually coupled to your access patterns.</p>\r\n<p>For example, you might think that it would be ok to store the planets and their moons together in our SpaceMaps app.</p>\r\n<pre>{\r\n  name: \"Earth\",\r\n  moons: [\r\n    {\r\n      name: \"Luna\",\r\n      massKg: 73420000000000000000000\r\n    }\r\n  ]\r\n}</pre>\r\n<p>A priori, it would seem this pattern is perfect for this use case. After all, it's unlikely a planet will have more than, say, a couple hundred moons in some extreme cases and this number will practically never change.</p>\r\n<p>But what if at some point we wanted to list and sort all the moons in our SpaceMaps app? This nesting pattern wouldn't be appropriate for this use case as we wouldn't be able to index and query the moons properly.</p>\r\n<h4><strong>How often are we going to update the data?</strong></h4>\r\n<p>Finally, even if your use case perfectly fits the nesting pattern, you have to consider how often you are going to update that document. The more frequently you update a document, the less efficient this pattern becomes.</p>\r\n<p>Remember that FaunaDB provides worldwide immediate consistency. Every time you change a document, it needs to be updated to all clusters in the world. There are physical constraints such as the speed of light which make very frequent updates around the world not very practical.</p>\r\n<h4><strong>How fast and how often do we need to retrieve our data?</strong></h4>\r\n<p>Don't get me wrong: if your use case requires the best possible performance, nesting the data into a document might actually be your best option (assuming it's still under 1 MB). In FaunaDB, like in any other document-based database, retrieving a single document is the fastest operation there is. Just take the previous points into consideration if you need to do that.</p>\r\n<h2>References in FaunaDB</h2>\r\n<p>Previously, we saw that each document in FaunaDB has a reference that identifies it with a unique id inside a collection.</p>\r\n<p>Here is a pilot document from a previous article:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n  \"ts\": 1590270525630000,\r\n  \"data\": {\r\n    \"name\": \"Flash Gordon\"\r\n  }\r\n}</pre>\r\n<p>The reference to this document has a unique id of <strong>266350546751848978</strong>, but remember that by itself the id is not very useful. It only makes sense when paired with a collection to create a reference.</p>\r\n<h4><strong>References to other documents</strong></h4>\r\n<p>Obviously, we can also use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/ref\">Ref</a> type to reference other documents. To demonstrate this, let's revisit our SpaceMaps app.</p>\r\n<p><br>We already have a <strong>Planets</strong> collection from a previous article. For reference, here's the document for Earth:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n  \"ts\": 1590977345595000,\r\n  \"data\": {\r\n    \"name\": \"Earth\",\r\n    \"type\": \"TERRESTRIAL\",\r\n    \"color\": \"BLUE\"\r\n  }\r\n}</pre>\r\n<p>Now, let's create a Moons collection:</p>\r\n<pre>CreateCollection({name: \"Moons\"})</pre>\r\n<p>And let's create a moon document with a reference to a planet document:</p>\r\n<pre>Create(\r\n  Collection(\"Moons\"),\r\n  {\r\n    data: {\r\n      name: \"Luna\",\r\n      planetRef: Ref(Collection(\"Planets\"), \"267081091831038483\")\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"Moons\"), \"267691276872188416\"),\r\n  ts: 1591549145540000,\r\n  data: {\r\n    name: \"Luna\",\r\n    planetRef: Ref(Collection(\"Planets\"), \"267081091831038483\")\r\n  }\r\n}</pre>\r\n<p>We just created a <em>one-to-many</em> relationship since it is possible for many moons to share the same planet.</p>\r\n<p>We can now retrieve all moons in our database using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/documents\">Documents</a> function to avoid creating an index like we saw in a previous article:</p>\r\n<pre>Map(\r\n  Paginate(Documents(Collection('Moons'))),\r\n  Lambda(\"moonRef\", Get(Var(\"moonRef\")))\r\n)</pre>\r\n<p>We could also create an index to find all the moons for a given planet:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Moons_by_planet\",\r\n  source: [Collection(\"Moons\")],\r\n  terms: [{ field: [\"data\", \"planetRef\"] }]\r\n})\r\n\r\n// Query the index:\r\n\r\nMap(\r\n  Paginate(\r\n    Match(\r\n      Index(\"all_Moons_by_planet\"),\r\n      Ref(Collection(\"Planets\"), \"267081091831038483\")\r\n    )\r\n  ),\r\n  Lambda(\"moonRef\", Get(Var(\"moonRef\")))\r\n)</pre>\r\n<p><strong>Quick note:</strong> If these FQL commands for indexes are confusing, you might want to revisit the <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">previous article</a> where all these are explained in detail.</p>\r\n<h4><strong>One-to-one constraints</strong></h4>\r\n<p>If you'd like to enforce a <em>one-to-one</em> relationship, you can do that by creating an index with a unique constraint like we saw in the previous article:</p>\r\n<pre>CreateIndex({\r\n  name: \"only_one_Moon_per_planet\",\r\n  source: [\r\n    Collection(\"Moons\")\r\n  ],\r\n  terms: [\r\n    {field: [\"data\", \"planetRef\"]}\r\n  ],\r\n  unique: true\r\n})</pre>\r\n<p>Now, if we try to create another moon on planet Earth, we'll get an error:</p>\r\n<pre>Create(\r\n  Collection(\"Moons\"),\r\n  {\r\n    data: {\r\n      name: \"Luna 2\",\r\n      planetRef: Ref(Collection(\"Planets\"), \"267081091831038483\")\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\nerror: instance not unique\r\ndocument is not unique.\r\nposition: [\"create\"]</pre>\r\n<h4><strong>Foreign keys </strong></h4>\r\n<p>In FaunaDB, like its purely document-based cousins, there is no out-of-the-box concept of foreign keys like in relational databases. That is, without writing a custom function, FaunaDB will not verify if a document exists when creating a reference, nor will it warn you if you are deleting a document that is referenced elsewhere; it will not cascade delete related documents.</p>\r\n<p>This might sound like a deal breaker, but I've found in practice that it's not much of a big deal. Even in the relational world, it's much more common to soft delete rows than to actually delete rows, just in case. Also, many of the big apps and websites you use do not support foreign keys either. They run on purely document-based databases that were the only options available to meet their high global performance needs at the time.</p>\r\n<p>Besides, if you do need this kind of functionality for a particular use case, it can be implemented using a custom FQL function.</p>\r\n<h2>Arrays of references</h2>\r\n<p>You could also model your <em>one-to-few</em> relationships by using arrays of references. This could be more convenient in certain situations where the data is frequently accessed together, but you still want to be able to query the entities independently and more efficiently.<br><br>For example, we could store the moon references in our planet document this way:</p>\r\n<pre>{\r\n  \"name\": \"Earth\",\r\n  \"type\": \"TERRESTRIAL\",\r\n  \"color\": \"BLUE\",\r\n  \"moonRefs\": [\r\n    Ref(Collection(\"Moons\"), \"267691276872188416\")\r\n  ]\r\n}</pre>\r\n<p>Then, to query a planet with all its moon documents you could use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/map\">Map</a>, <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/let\">Let</a>, and <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/select\">Select</a> like we learned previously when querying indexes:</p>\r\n<pre>Let(\r\n  {\r\n    planetDoc: Get(Ref(Collection(\"Planets\"), \"267081091831038483\"))\r\n  },\r\n  {\r\n    planet: Var(\"planetDoc\"),\r\n    moons: Map(\r\n      Select([\"data\", \"moonRefs\"], Var(\"planetDoc\")),\r\n      Lambda(\"moonRef\", Get(Var(\"moonRef\")))\r\n    )\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  planet: {\r\n    ref: Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n    ts: 1591554900130000,\r\n    data: {\r\n      name: \"Earth\",\r\n      type: \"TERRESTRIAL\",\r\n      color: \"BLUE\",\r\n      moonRefs: [Ref(Collection(\"Moons\"), \"267691276872188416\")]\r\n    }\r\n  },\r\n  moons: [\r\n    {\r\n      ref: Ref(Collection(\"Moons\"), \"267691276872188416\"),\r\n      ts: 1591553627340000,\r\n      data: {\r\n        name: \"Luna\",\r\n        planetRef: Ref(Collection(\"Planets\"), \"267081091831038483\")\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<p>Or, if you only wanted to get the names and ids instead of the full documents, you could do this instead:</p>\r\n<pre>Let(\r\n  {\r\n    planetDoc: Get(Ref(Collection(\"Planets\"), \"267081091831038483\"))\r\n  },\r\n  {\r\n    planet: Let({}, {\r\n      id: Select([\"ref\",\"id\"], Var(\"planetDoc\")),\r\n      name: Select([\"data\",\"name\"], Var(\"planetDoc\")),\r\n      moons: Map(\r\n        Select([\"data\", \"moonRefs\"], Var(\"planetDoc\")),\r\n        Lambda(\"moonRef\", Let(\r\n          {\r\n            moonDoc: Get(Var(\"moonRef\"))\r\n          },\r\n          {\r\n            id: Select([\"ref\",\"id\"], Var(\"moonDoc\")),\r\n            name: Select([\"data\",\"name\"], Var(\"moonDoc\")) \r\n          }\r\n        ))\r\n      )\r\n    })\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  planet: {\r\n    id: \"267081091831038483\",\r\n    name: \"Earth\",\r\n    moons: [\r\n      {\r\n        id: \"267691276872188416\",\r\n        name: \"Luna\"\r\n      }\r\n    ]\r\n  }\r\n}</pre>\r\n<p>This query may seem intimidating, but if you inspect it in more detail, you will see that it is only using FQL functions we've already learned in the previous articles. FQL is a lot closer to a functional programming language than a declarative language like SQL, so it might help to think about it that way.</p>\r\n<h2>Many-to-many relationships</h2>\r\n<p>As we've seen in previous examples, many-to-many relationships can be expressed with indexes and/or arrays of references. Let's look at a use case for that.</p>\r\n<p>There is a bit of chaos in the dock and our boss, the admiral, has tasked us with creating a system for managing spaceship repairs. Armed with our new knowledge about references and relationships in FaunaDB, we should be able to solve this in no time!</p>\r\n<p>First, we need to be able to track our personnel:</p>\r\n<pre>CreateCollection({name: \"DockTechnicians\"})</pre>\r\n<p>Let's add all the people working in the dock:</p>\r\n<pre>Create(\r\n  Collection(\"DockTechnicians\"),\r\n  {\r\n    data: {name: \"Johnny Sparkles\"}\r\n  }\r\n)\r\n\r\n// etc...</pre>\r\n<p>We already have a Spaceships collection from our previous articles. For simplicity's sake, we're going to assume all ships are in the dock right now.</p>\r\n<p>Now, to assign a technician to a ship, we could just maintain an array of spaceships references in the technician document:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"DockTechnicians\"), \"267703813461246483\"),\r\n  {\r\n    data: {\r\n      workingOn: [\r\n        Ref(Collection(\"Spaceships\"), \"266356873589948946\")\r\n      ]\r\n    }\r\n  }\r\n)</pre>\r\n<p>Waaait a minute!</p>\r\n<p>The admiral specifically said he not only wanted to know which technicians worked on which spaceships, but also which repairs were in process and how long each repair took.</p>\r\n<h4><strong>Join collections</strong></h4>\r\n<p>In the relational world, it's very common to model many-to-many relationships with an entity using a join table, or a bridging table.</p>\r\n<p>This is done in part because, with a rigid schema, you'd need to add more columns to relate a row to other rows. But there is another important reason which actually solves our problem. When modeling a relationship with an independent entity, you can assign data to that relationship.</p>\r\n<p>What if we actually had a collection to track the repairs that also associated technicians with spaceships?</p>\r\n<pre>CreateCollection({name: \"DockRepairs\"})</pre>\r\n<p>And now, let's create the first repair:</p>\r\n<pre>Create(\r\n  Collection(\"DockRepairs\"),\r\n  {\r\n    data: {\r\n      technicianRefs: [\r\n        Ref(Collection(\"DockTechnicians\"), \"267703813461246483\")\r\n      ],\r\n      shipRef: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n      status: 'PENDING'\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n  ts: 1591562886860000,\r\n  data: {\r\n    technicianRefs: [\r\n      Ref(Collection(\"DockTechnicians\"), \"267703813461246483\")\r\n    ],\r\n    shipRef: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    status: \"PENDING\"\r\n  }\r\n}</pre>\r\n<p>So now we know the status of a ship repair and which technicians are assigned to it. Neat.</p>\r\n<p>Let's start a repair:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n  {\r\n    data: {\r\n      startTimestamp: Time('2355-02-11T05:23:11Z'),\r\n      status: 'IN_PROCESS'\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n  ts: 1591563124590000,\r\n  data: {\r\n    technicianRefs: [\r\n      Ref(Collection(\"DockTechnicians\"), \"267703813461246483\")\r\n    ],\r\n    shipRef: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    status: \"IN_PROCESS\",\r\n    startTimestamp: Time(\"2355-02-11T05:23:11Z\")\r\n  }\r\n}</pre>\r\n<p>And now let's finish the repair:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n  {\r\n    data: {\r\n      endTimestamp: Time('2355-02-11T03:05:35Z'),\r\n      status: 'DONE'\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  ref: Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n  ts: 1591563210950000,\r\n  data: {\r\n    technicianRefs: [\r\n      Ref(Collection(\"DockTechnicians\"), \"267703813461246483\")\r\n    ],\r\n    shipRef: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n    status: \"DONE\",\r\n    startTimestamp: Time(\"2355-02-11T05:23:11Z\"),\r\n    endTimestamp: Time(\"2355-02-11T03:05:35Z\")\r\n  }\r\n}</pre>\r\n<p>Great!</p>\r\n<p>Now with this simple index, we can see all repairs and their info:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_DockRepairs\",\r\n  source: [\r\n    Collection(\"DockRepairs\")\r\n  ]\r\n})</pre>\r\n<p>Not bad, if I say so myself.</p>\r\n<h4><strong>But how long do repairs actually take?</strong></h4>\r\n<p>We could have added a <strong>duration</strong> property to our documents and stored a value when ending a repair, but where is the fun in that? With FQL, there are other ways to accomplish this.</p>\r\n<p>For example, we could just determine the duration when querying our data without having to implement it in our application logic:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_DockRepairs\"))),\r\n  Lambda(\"repairRef\", Let({\r\n    repairDoc: Get(Var(\"repairRef\"))\r\n  },{\r\n    durationMinutes: If(\r\n      Or(\r\n        IsNull(Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\"), null)),\r\n        IsNull(Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\"), null))\r\n      ),\r\n      null,\r\n      TimeDiff(\r\n        Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\")),\r\n        Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\")),\r\n        \"minutes\"\r\n      )\r\n    ),\r\n    repair: Var(\"repairDoc\")\r\n  }))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  data: [\r\n    {\r\n      durationMinutes: 137,\r\n      repair: {\r\n        ref: Ref(Collection(\"DockRepairs\"), \"267705685715714560\"),\r\n        ts: 1591563210950000,\r\n        data: {\r\n          technicianRefs: [\r\n            Ref(Collection(\"DockTechnicians\"), \"267703813461246483\")\r\n          ],\r\n          shipRef: Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n          status: \"DONE\",\r\n          startTimestamp: Time(\"2355-02-11T05:23:11Z\"),\r\n          endTimestamp: Time(\"2355-02-11T03:05:35Z\")\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<p>Woah.</p>\r\n<p>Again, this query may seem complicated, but we already know most of the stuff from previous articles.<br><br>Here's the new part:</p>\r\n<pre>If(\r\n  Or(\r\n    IsNull(Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\"), null)),\r\n    IsNull(Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\"), null))\r\n  ),\r\n  null,\r\n  TimeDiff(\r\n    Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\")),\r\n    Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\")),\r\n    \"minutes\"\r\n  )\r\n)</pre>\r\n<p>What this does is check that <strong>startTimestamp</strong> or <strong>endTimestamp</strong> are not missing from the document. If both exist, then return the time difference in minutes.</p>\r\n<p>Let's go step-by-step.</p>\r\n<ul><li>We already know what <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/select\">Select</a> does from previous articles. In this case, we are giving it a default value of <strong>null</strong> if the path <strong>[\"data\", \"startTimestamp\"]</strong> does not exist in <strong>Var(\"repairDoc\")</strong>.</li></ul>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/isnull\">IsNull</a> will return <strong>true</strong> if a value does not exist and <strong>Select</strong> returns <strong>null</strong>.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/or\">Or</a> will return true if either <strong>startTimestamp</strong> or <strong>endTimestamp</strong> do not exist in the document. If that's the case, then <strong>If</strong> would return <strong>null</strong>.</li><li>If both timestamps do exist in the repair document, we simply calculate the duration using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/timediff\">TimeDiff</a> in minutes.</li></ul>\r\n<h4><strong>Index bindings</strong></h4>\r\n<p>There is another way to solve this. Do you remember index bindings from the<a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\"> previous article</a>? These are computed values calculated beforehand.</p>\r\n<p>The decision to use index bindings comes down to how often you need to know the duration of the repairs. As explained in the previous article, bindings consume more storage but less CPU, so you have to account for that when deciding to use bindings.</p>\r\n<p>That said, here's a possible index to return the duration using a binding and the same conditional logic. We're also returning a custom object instead of the full document, which might make more sense when listing repairs instead of showing a detail view to your users.</p>\r\n<pre>CreateIndex({\r\n  name: \"all_DockRepairs_with_duration\",\r\n  source: {\r\n    collection: Collection(\"DockRepairs\"),\r\n    fields: {\r\n      durationMinutes: Query(\r\n        Lambda(\"repairDoc\",\r\n          If(\r\n            Or(\r\n              IsNull(\r\n                Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\"), null)\r\n              ),\r\n              IsNull(\r\n                Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\"), null)\r\n              )\r\n            ),\r\n            null,\r\n            TimeDiff(\r\n              Select([\"data\", \"endTimestamp\"], Var(\"repairDoc\")),\r\n              Select([\"data\", \"startTimestamp\"], Var(\"repairDoc\")),\r\n              \"minutes\"\r\n            )\r\n          )\r\n        )\r\n      )\r\n    }\r\n  },\r\n  values: [\r\n    { binding: \"durationMinutes\"},\r\n    { field: [\"ref\", \"id\"]},\r\n    { field: [\"data\", \"status\"]}\r\n  ]\r\n})</pre>\r\n<p>Let's query it:&nbsp;</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_DockRepairs_with_duration\"))),\r\n  Lambda(\"result\", Let({},\r\n    {\r\n      id: Select([1], Var(\"result\")),\r\n      status: Select([2], Var(\"result\")),\r\n      durationMinutes: Select([0], Var(\"result\"))\r\n    }\r\n  ))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  data: [\r\n    {\r\n      id: \"267705685715714560\",\r\n      status: \"DONE\",\r\n      durationMinutes: 137\r\n    }\r\n  ]\r\n}</pre>\r\n<h2>Conclusion</h2>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In part 4 of this series, we will continue our space adventure by learning how to create FQL functions to extend the basic functionality of FaunaDB with custom logic.<br><br></p>\r\n<p>If you have any questions don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8506"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-07-10T05:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8399,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "1481adff-f4f2-4a17-bdc1-97f18b5cba04",
        "siteSettingsId": 8399,
        "fieldLayoutId": 4,
        "contentId": 2816,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting started with FQL, FaunaDB’s native query language - part 2",
        "slug": "getting-started-with-fql-faunadbs-native-query-language-part-2",
        "uri": "blog/getting-started-with-fql-faunadbs-native-query-language-part-2",
        "dateCreated": "2020-07-07T10:40:00-07:00",
        "dateUpdated": "2020-09-02T09:10:06-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-fql-faunadbs-native-query-language-part-2",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2",
        "isCommunityPost": true,
        "blogBodyText": "<p>Welcome back, fellow space developer! In <a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\" target=\"_blank\" data-saferedirecturl=\"https://www.google.com/url?q=https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1&source=gmail&ust=1594402251942000&usg=AFQjCNFpE7bOjZCneuUinjlssTW34J8ngw\">part 1</a> of this five-part series we got our first look at FQL and some fundamental FaunaDB concepts.&nbsp;</p>\r\n<ul><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1\">Part 1: a look at FQL and fundamental FaunaDB concepts</a></li><li>Part 2: a deep dive into indexes with FaunaDB</li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">Part 3: a look into the principles of modeling data with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">Part 4: a look at how to create custom functions that run straight in FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5\">Part 5: a look at authentication and authorization in FaunaDB</a></li></ul><ul></ul>\r\n<p>Today we're going to take a more in-depth look into FaunaDB's indexes.</p>\r\n<h2>In this article:</h2>\r\n<ul><li>Recap</li><li>What can indexes do?</li><li>Indexing across multiple collections</li><li>Sorting results</li><li>Filtering results</li><li>Enforcing unique values</li><li>Combining multiple indexes</li><li>Index bindings</li></ul>\r\n<h2>Recap</h2>\r\n<p>We briefly introduced indexes in the previous article, but here's a recap of the FQL commands we learned.</p>\r\n<p>First, we created a simple index to be able to retrieve all our pilots from the Pilots collection:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Pilots\",\r\n  source: Collection(\"Pilots\")\r\n})</pre>\r\n<p>Then, we retrieved a list of references:</p>\r\n<pre>Paginate(\r\n  Match(\r\n    Index(\"all_Pilots\")\r\n  )\r\n)</pre>\r\n<p>Finally, we learned how to use Map, Lambda, and Var to retrieve a list of documents:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Pilots\"))),\r\n  Lambda('pilotRef', Get(Var('pilotRef')))\r\n)</pre>\r\n<h2>What can indexes do?</h2>\r\n<p>So far, we've seen that indexes allow you to retrieve all the documents in a collection, but indexes are much more powerful than that.<br><br>With indexes you can:</p>\r\n<ul><li>Enforce unique constraints</li><li>Sort and filter results</li><li>Create computed values from document data</li></ul>\r\n<h4><strong>Indexes vs SQL views</strong></h4>\r\n<p>If you're coming from the relational world, it can make sense to think about indexes similar to views on a relational database. Views are stored queries that can retrieve data from multiple tables, calculate computed data, join tables to create virtual entities, filter, etc. In a way, FaunaDB's indexes perform similar functions, as we will explore in this article.</p>\r\n<h2>Indexing across multiple collections</h2>\r\n<p>Until now, our indexes have been created on documents from a single collection, but you can configure an index to include documents from multiple collections.</p>\r\n<p>There are many reasons why you might want to do that. Maybe, when designing your database, you'd like to group some collections under a single virtual collection, so to speak. In the relational world, combining database entities under a single entity is known as polymorphism.</p>\r\n<p>To test this, let's create a new collection to store our land vehicles:</p>\r\n<pre>CreateCollection({name: \"Speeders\"})</pre>\r\n<p>Now, with this index, you'd be able to retrieve all the vehicles in the database:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Vehicles\",\r\n  source: [\r\n    Collection(\"Spaceships\"),\r\n    Collection(\"Speeders\")\r\n  ]\r\n})</pre>\r\n<p>When indexing multiple collections, keep in mind that the indexed fields need to be of the same type (string, number, etc) across collections. In the rest of the examples, we'll use indexes with a single collection for simplicity's sake. </p>\r\n<h2>Sorting results</h2>\r\n<p>Indexes also allow us to sort results. Let's create a new index to get all our pilots sorted by their name:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Pilots_sorted_by_name\",\r\n  source: Collection(\"Pilots\"),\r\n  values: [\r\n    { field: [\"data\", \"name\"] },\r\n    { field: [\"ref\"] }\r\n  ]\r\n})</pre>\r\n<p>Here, we're using a <strong>values</strong> object which defines the <strong>output</strong> values for the index.</p>\r\n<p>In this case, we are defining two output values:</p>\r\n<ul><li><strong><tt>[\"data\", \"name\"]</tt></strong> a path referring to the name property of the document.</li><li><strong><tt>[\"ref\"]</tt></strong> another path which will output a reference to the matched document. In a moment, we'll see why we need this.</li></ul>\r\n<p>When using a <strong>values</strong> object, FaunaDB will always sort the results in ascending order by default:</p>\r\n<pre>Paginate(Match(Index(\"all_Pilots_sorted_by_name\")))\r\n\r\n// Results:\r\n\r\n{\r\n  \"data\": [\r\n    [\r\n      \"Buck Rogers\",\r\n      Ref(Collection(\"Pilots\"), \"266359371696439826\")\r\n    ],\r\n    [\r\n      \"Flash Gordon\",\r\n      Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n    ],\r\n    [\r\n      \"Jean-Luc Picard\",\r\n      Ref(Collection(\"Pilots\"), \"266359447111074322\")\r\n    ],\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>As you can see, FaunaDB will output two values per result as defined in the <strong>values</strong> object of the index, and these results are now ordered by those values.</p>\r\n<h4><strong>Reverse order</strong></h4>\r\n<p>If we wanted to get the pilots sorted by their name in descending order, we'd need a new index with the <strong>reverse</strong> setting:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Pilots_sorted_by_name_desc\",\r\n  source: Collection(\"Pilots\"),\r\n  values: [\r\n    { field: [\"data\", \"name\"], reverse: true},\r\n    { field: [\"ref\"] }\r\n  ]\r\n})</pre>\r\n<h4><strong>Getting documents from sorting results</strong></h4>\r\n<p>You can add as many output values as needed without any performance penalty, but we might need to get a document from these types of results:</p>\r\n<pre>[\"Buck Rogers\", Ref(Collection(\"Pilots\"), \"266359371696439826\")]</pre>\r\n<p>So how do we actually get documents?</p>\r\n<p>One option would be using the Select function like we learned in the previous article:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Pilots_sorted_by_name\"))),\r\n  Lambda(\"pilotResult\", Get(Select([1], Var(\"pilotResult\"))))\r\n)</pre>\r\n<p>Since FaunaDB uses zero-based arrays, the trick here is selecting the reference in the second item with <strong>[1]</strong>, then using Get to return a document.</p>\r\n<p>Another option would be to simply configure our Lambda to expect an array with two values:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Pilots_sorted_by_name\"))),\r\n  Lambda([\"name\", \"pilotRef\"], Get(Var(\"pilotRef\")))\r\n)</pre>\r\n<p>In both cases, we'd get the same result:</p>\r\n<pre>{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Pilots\"), \"266359371696439826\"),\r\n      \"ts\": 1590278941740000,\r\n      \"data\": {\r\n        \"name\": \"Buck Rogers\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n      \"ts\": 1590270525630000,\r\n      \"data\": {\r\n        \"name\": \"Flash Gordon\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Pilots\"), \"266359447111074322\"),\r\n      \"ts\": 1590279013675000,\r\n      \"data\": {\r\n        \"name\": \"Jean-Luc Picard\"\r\n      }\r\n    }\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<h2>Filtering results</h2>\r\n<p>Another useful feature of indexes is being able to search and filter results.</p>\r\n<p>To test this, let's create a Planets collection:</p>\r\n<pre>CreateCollection({name: \"Planets\"})</pre>\r\n<p>Then, create some planets with three different types: <strong>TERRESTRIAL</strong>, <strong>GAS</strong>, and <strong>ICE</strong>:</p>\r\n<pre>Create(Collection(\"Planets\"),\r\n  {\r\n    data: {\r\n      name: \"Mercury\",\r\n      type: \"TERRESTRIAL\"\r\n    }\r\n  }\r\n)\r\n\r\nCreate(Collection(\"Planets\"),\r\n  {\r\n    data: {\r\n      name: \"Saturn\",\r\n      type: \"GAS\"\r\n    }\r\n  }\r\n)\r\n\r\n// etc..</pre>\r\n<p>Finally, let's create an index to filter our planets by type:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Planets_by_type\",\r\n  source: Collection(\"Planets\"),\r\n  terms: [\r\n    { field: [\"data\", \"type\"]}\r\n  ]\r\n})</pre>\r\n<p>As we saw earlier, the <strong>terms</strong> object is used as the search <strong>input</strong> for the index, whereas the <strong>values</strong> object defines which data the index will return. With this index, the <strong>values</strong> object is not defined, so the index will return the ref by default.</p>\r\n<p>In this case, we're telling FaunaDB that the search term will use a <strong>field</strong> of the document found at the path <strong>[\"data\", \"type\"]</strong>.</p>\r\n<p>We can now query our index by passing a parameter to Match:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Planets_by_type\"), \"GAS\")),\r\n  Lambda(\"planetRef\", Get(Var(\"planetRef\")))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081152090604051\"),\r\n      \"ts\": 1590967285200000,\r\n      \"data\": {\r\n        \"name\": \"Jupiter\",\r\n        \"type\": \"GAS\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081181884842515\"),\r\n      \"ts\": 1590967313610000,\r\n      \"data\": {\r\n        \"name\": \"Saturn\",\r\n        \"type\": \"GAS\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<h4><strong>Filtering on an array value</strong></h4>\r\n<p>If we wanted to match an item inside an array instead of filtering on a single string, we would only need to pass the term FaunaDB needs to search inside the array.</p>\r\n<p>To test this, let's add some colors to our ships:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n  {\r\n    data: {\r\n      colors: [\"RED\",\"YELLOW\"]\r\n    }\r\n  }\r\n)\r\n \r\n// etc...</pre>\r\n<p>If we now wanted to filter ships based on a single color, we could create this index which uses the <strong>colors</strong> array as a filtering term:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Spaceships_by_color\",\r\n  source: Collection(\"Spaceships\"),\r\n  terms: [\r\n    { field: [\"data\",\"colors\"]}\r\n  ]\r\n})</pre>\r\n<p>And then query it:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Spaceships_by_color\"), \"WHITE\")),\r\n  Lambda(\"shipRef\", Let({\r\n    shipDoc: Get(Var(\"shipRef\"))\r\n  },{\r\n    name: Select([\"data\",\"name\"], Var(\"shipDoc\")),\r\n    colors: Select([\"data\",\"colors\"], Var(\"shipDoc\"))\r\n  }))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  data: [\r\n    {\r\n      name: \"Explorer IV\",\r\n      colors: [\"BLUE\", \"WHITE\", \"RED\"]\r\n    },\r\n    {\r\n      name: \"Navigator\",\r\n      colors: [\"WHITE\", \"GREY\"]\r\n    },\r\n    {\r\n      name: \"Le Super Spaceship\",\r\n      colors: [\"PINK\", \"MAGENTA\", \"WHITE\"]\r\n    }\r\n  ]\r\n}</pre>\r\n<p>FaunaDB is smart enough to understand that if the field used in the <strong>terms</strong> object is an array, then it should search for an item inside that array instead of an exact match on the full array.</p>\r\n<h4><strong>About full text search</strong></h4>\r\n<p>At this time, it's only possible to filter documents using indexes with exact matches. This feature is on Fauna's roadmap, but no official timeframes have been provided yet.</p>\r\n<p>It's certainly possible to solve this in other ways. FQL has a number of functions that will allow you to implement full text search.</p>\r\n<p>Let's create an index first to get all planets:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Planets\",\r\n  source: Collection(\"Planets\")\r\n})</pre>\r\n<p>Now, we could combine <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/filter\">Filter</a>, <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/containsstr\">ContainsStr</a>, and <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lowercase\">LowerCase</a> to make a case insensitive search of the string \"ur\" on the planets' names:</p>\r\n<pre>Map(\r\n  Filter(\r\n    Paginate(Match(Index(\"all_Planets\"))),\r\n    Lambda(\"planetRef\",\r\n      ContainsStr(\r\n        LowerCase(Select([\"data\",\"name\"],Get(Var(\"planetRef\")))),\r\n        \"ur\"\r\n      )\r\n    )\r\n  ),\r\n  Lambda(\"planetRef\", Get(Var(\"planetRef\")))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  data: [\r\n    {\r\n      ref: Ref(Collection(\"Planets\"), \"267081079730471443\"),\r\n      ts: 1590977548370000,\r\n      data: {\r\n        name: \"Mercury\",\r\n        type: \"TERRESTRIAL\",\r\n        color: \"GREY\"\r\n      }\r\n    },\r\n    {\r\n      ref: Ref(Collection(\"Planets\"), \"267081181884842515\"),\r\n      ts: 1590977684790000,\r\n      data: {\r\n        name: \"Saturn\",\r\n        type: \"GAS\",\r\n        color: \"YELLOW\"\r\n      }\r\n    },\r\n    {\r\n      ref: Ref(Collection(\"Planets\"), \"267081222719537683\"),\r\n      ts: 1590977359690000,\r\n      data: {\r\n        name: \"Uranus\",\r\n        type: \"ICE\",\r\n        color: \"BLUE\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<h4><strong>Sorting and filtering at the same time</strong></h4>\r\n<p>You can certainly do both at the same time by combining <strong>terms</strong> and <strong>values</strong> in the same index:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Planets_by_type_sorted_by_name\",\r\n  source: Collection(\"Planets\"),\r\n  terms: [\r\n    { field: [\"data\", \"type\"]}\r\n  ],\r\n  values: [\r\n    { field: [\"data\", \"name\"]},\r\n    { field: [\"ref\"] }\r\n  ]\r\n})</pre>\r\n<p>And then:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Planets_by_type_sorted_by_name\"), \"TERRESTRIAL\")),\r\n  Lambda(\"planetResult\", Get(Select([1], Var(\"planetResult\"))))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n      \"ts\": 1590967227710000,\r\n      \"data\": {\r\n        \"name\": \"Earth\",\r\n        \"type\": \"TERRESTRIAL\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081096484618771\"),\r\n      \"ts\": 1590967232165000,\r\n      \"data\": {\r\n        \"name\": \"Mars\",\r\n        \"type\": \"TERRESTRIAL\"\r\n      }\r\n    },\r\n    // etc ...\r\n  ]\r\n}</pre>\r\n<h2>Enforcing unique values</h2>\r\n<p>Another important function of indexes, besides retrieving documents, is enforcing a unique constraint on the documents that can be created.</p>\r\n<p>For example, to add a unique code to our spaceships:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Spaceships_by_code\",\r\n  source: Collection(\"Spaceships\"),\r\n  terms: [\r\n    {field: [\"data\", \"code\"]}\r\n  ],\r\n  unique: true\r\n})</pre>\r\n<p>This index accomplishes two purposes:</p>\r\n<ul><li>We're configuring it to accept a filtering term with the <strong>terms</strong> object.</li><li>We're ensuring the defined terms are unique across the documents matched by this index by using <strong>unique: true</strong>.</li></ul>\r\n<p>We're using a single term here for simplicity's sake, but you could create a unique constraint over multiple terms much like you'd do in SQL by creating constraints over multiple columns.</p>\r\n<p>Let's test this by creating a new spaceship:</p>\r\n<pre>Create(\r\n  Collection(\"Spaceships\"),\r\n  {\r\n    data: {\r\n      name: \"Rocinante\",\r\n      code: \"ROCINANTE\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"267072793181422099\"),\r\n  \"ts\": 1590959313500000,\r\n  \"data\": {\r\n    \"name\": \"Rocinante\",\r\n    \"code\": \"ROCINANTE\"\r\n  }\r\n}</pre>\r\n<p>So far so good. Let's create another one with the same code:</p>\r\n<pre>Create(\r\n  Collection(\"Spaceships\"),\r\n  {\r\n    data: {\r\n      name: \"Rocinante 2\",\r\n      code: \"ROCINANTE\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\nerror: instance not unique\r\ndocument is not unique.\r\nposition: [\"create\"]</pre>\r\n<p>As expected, FaunaDB throws an error since there is already a ship with the <strong>ROCINANTE</strong> code.</p>\r\n<p><strong>Quick tip:</strong> when using unique constraints, we know in advance that an index can only return a single document. So, instead of using Paginate, we can simply use Get to get a single document:</p>\r\n<pre>Get(Match(Index(\"all_Spaceships_by_code\"), 'ROCINANTE'))\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"267072793181422099\"),\r\n  \"ts\": 1591022503995000,\r\n  \"data\": {\r\n    \"name\": \"Rocinante\",\r\n    \"code\": \"ROCINANTE\"\r\n  }\r\n}</pre>\r\n<h2>Combining multiple indexes</h2>\r\n<p>FQL has a number of functions that allow you to combine results from indexes and other sources in different ways:</p>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/union\">Union</a> will add the results from all indexes.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/intersection\">Intersection</a> will return the results that are similar from each index and discard the rest.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/difference\">Difference</a> will return the results that are unique in the first index and discard the rest.</li></ul>\r\n<figure><img src=\"{asset:8398:url}\" data-image=\"8398\"></figure>\r\n<p><br>To be able to test these, let's add some colors to our planets (please excuse any scientific inaccuracies).</p>\r\n<pre>// Earth\r\n\r\nUpdate(Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n  {data: {color: \"BLUE\"}}\r\n)\r\n\r\n// Etc...</pre>\r\n<p>Let's also create a new index:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Planets_by_color\",\r\n  source: Collection(\"Planets\"),\r\n  terms: [\r\n    { field: [\"data\", \"color\"]}\r\n  ]\r\n})</pre>\r\n<h4><strong>OR filtering with Union</strong></h4>\r\n<p>Union will combine whatever results each index returns. We're just using two indexes here, but you could use any number of indexes.<br><br><em>\"Hey Fauna, get me the planets that are of type </em><strong><em>GAS</em></strong><em> or are </em><strong><em>YELLOW</em></strong><em>\"</em></p>\r\n<pre>Map(\r\n  Paginate(\r\n    Union(\r\n      Match(Index(\"all_Planets_by_type\"), \"GAS\"),\r\n      Match(Index(\"all_Planets_by_color\"), \"YELLOW\")\r\n    )\r\n  ),\r\n  Lambda(\"planetRef\", Get(Var(\"planetRef\")))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081152090604051\"),\r\n      \"ts\": 1590977605890000,\r\n      \"data\": {\r\n        \"name\": \"Jupiter\",\r\n        \"type\": \"GAS\",\r\n        \"color\": \"BROWN\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081181884842515\"),\r\n      \"ts\": 1590977684790000,\r\n      \"data\": {\r\n        \"name\": \"Saturn\",\r\n        \"type\": \"GAS\",\r\n        \"color\": \"YELLOW\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n</pre>\r\n<p>As you can see, Union will skip duplicates since Saturn being a gas giant appears in the results of both indexes.</p>\r\n<h4><strong>AND filtering with Intersection</strong></h4>\r\n<p>Intersection will return only the results that are the same in all indexes. Again, you could use any number of indexes.</p>\r\n<p><em>\"Hey Fauna, get me the planets that are of type</em><strong><em>TERRESTRIAL</em></strong><em> and are </em><strong><em>BLUE</em></strong><em>\"</em></p>\r\n<pre>Map(\r\n  Paginate(\r\n    Intersection(\r\n      Match(Index(\"all_Planets_by_type\"), \"TERRESTRIAL\"),\r\n      Match(Index(\"all_Planets_by_color\"), \"BLUE\")\r\n    )\r\n  ),\r\n  Lambda(\"planetRef\", Get(Var(\"planetRef\")))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081091831038483\"),\r\n      \"ts\": 1590977345595000,\r\n      \"data\": {\r\n        \"name\": \"Earth\",\r\n        \"type\": \"TERRESTRIAL\",\r\n        \"color\": \"BLUE\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n</pre>\r\n<h4><strong>NOT filtering with Difference</strong></h4>\r\n<p>Difference will compare the first index you provide with the rest of the indexes, and return the results that exist only in the first index.<br><br><em>\"Hey Fauna, get me the planets that are </em><strong><em>TERRESTRIAL</em></strong><em> but are not </em><strong><em>BLUE</em></strong><em> nor </em><strong><em>RED</em></strong><em>\"</em></p>\r\n<pre>Map(\r\n  Paginate(\r\n    Difference(\r\n      Match(Index(\"all_Planets_by_type\"), \"TERRESTRIAL\"),\r\n      Match(Index(\"all_Planets_by_color\"), \"BLUE\"),\r\n      Match(Index(\"all_Planets_by_color\"), \"RED\")\r\n    )\r\n  ),\r\n  Lambda(\"planetRef\", Get(Var(\"planetRef\")))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081079730471443\"),\r\n      \"ts\": 1590977548370000,\r\n      \"data\": {\r\n        \"name\": \"Mercury\",\r\n        \"type\": \"TERRESTRIAL\",\r\n        \"color\": \"GREY\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081085891904019\"),\r\n      \"ts\": 1590977561660000,\r\n      \"data\": {\r\n        \"name\": \"Venus\",\r\n        \"type\": \"TERRESTRIAL\",\r\n        \"color\": \"GREY\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<h2>Index bindings</h2>\r\n<p>With index bindings, it's possible to create pre-computed values based on some document data, using pretty much any FQL expression.</p>\r\n<p>These values are calculated beforehand, which makes retrieving these values super efficient as the operation consumes little CPU. The downside is that these computed values consume storage space. Before deciding to use a binding at scale, you should consider if the performance boost is worth the storage cost for your use case.</p>\r\n<p>Let's see a couple examples on how to use index bindings.</p>\r\n<p>Remember our spaceship from the previous article?</p>\r\n<pre>{\r\n  \"name\": \"Voyager\",\r\n  \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n  \"type\": \"Rocket\",\r\n  \"fuelType\": \"Plasma\",\r\n  \"actualFuelTons\": 7,\r\n  \"maxFuelTons\": 10,\r\n  \"maxCargoTons\": 25,\r\n  \"maxPassengers\": 5,\r\n  \"maxRangeLightyears\": 10,\r\n  \"celestialPosition\": {\r\n    \"x\": 2234,\r\n    \"y\": 3453,\r\n    \"z\": 9805\r\n  },\r\n  \"code\": \"VOYAGER\"\r\n}</pre>\r\n<p>So here comes our boss, the fleet admiral. He has 100 ships in the dock that need refueling and wants to know which ships could be filled faster so that he can fill them first and empty the dock as fast as possible.</p>\r\n<p>Easy, right? To do that, we'd only need to sort our ships by <strong>pendingFuelTons</strong>.</p>\r\n<p>But Pier, <strong>pendingFuelTons</strong> is not in the document! We're doomed!</p>\r\n<p>Don't panic my friend, we have the perfect tool to solve this problem.</p>\r\n<p>Index bindings allow you to create computed values dynamically based on the data of the document. In this case, we could just calculate the value <strong>pendingFuelTons</strong> by subtracting <strong>actualFuelTons</strong> from <strong>maxFuelTons</strong>.</p>\r\n<p>So let's create our index:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Spaceships_by_pendingFuelTons\",\r\n  source: {\r\n    collection: Collection(\"Spaceships\"),\r\n    fields: {\r\n      pendingFuelTons: Query(\r\n        Lambda(\"shipDoc\",\r\n          Subtract(\r\n            Select([\"data\",\"maxFuelTons\"], Var(\"shipDoc\")),\r\n            Select([\"data\",\"actualFuelTons\"], Var(\"shipDoc\"))\r\n          )\r\n        )\r\n      )\r\n    }\r\n  },\r\n  values: [\r\n    { binding: \"pendingFuelTons\"},\r\n    { field: [\"data\", \"name\"]}\r\n  ]\r\n})</pre>\r\n<p>The only new FQL function we're using here is Subtract, which simply subtracts the second number from the first.</p>\r\n<p>So let's query our new index:</p>\r\n<pre>Paginate(Match(Index(\"all_Spaceships_by_pendingFuelTons\")))\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    [\r\n      3,\r\n      \"Explorer IV\"\r\n    ],\r\n    [\r\n      3,\r\n      \"Voyager\"\r\n    ],\r\n    [\r\n      10,\r\n      \"Navigator\"\r\n    ],\r\n    [\r\n      18,\r\n      \"Destroyer\"\r\n    ]\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>As you can see, FaunaDB is sorting first by the new computed value <strong>pendingFuelTons</strong> and then by the ship name.</p>\r\n<p>Cool!</p>\r\n<h4><strong>Filtering by the first letter</strong></h4>\r\n<p>Let's create another example. What if we wanted to get all the planets that started with the letter <strong>M</strong>? Our planet documents do not have a <strong>firstLetter</strong> property, but we can solve this with bindings too.</p>\r\n<p>We can create a new index with a binding for the first letter of the name and add a <strong>terms</strong> object to be able to filter the documents by <strong>firstLetter</strong>:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Planets_by_firstLetter\",\r\n  source: {\r\n    collection: Collection(\"Planets\"),\r\n    fields: {\r\n      firstLetter: Query(\r\n        Lambda(\"planetDoc\",\r\n          SubString(Select([\"data\", \"name\"], Var(\"planetDoc\")), 0, 1)\r\n        )\r\n      )\r\n    }\r\n  },\r\n  terms: [\r\n    { binding: \"firstLetter\"}\r\n  ]\r\n})</pre>\r\n<p>As you can see in the <strong>terms</strong> object, we're now telling FaunaDB that the value we want to use for filtering is an index binding instead of a document field.</p>\r\n<p>Great, so let's query the index as usual and pass the letter <strong>M</strong>:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Planets_by_firstLetter\"), \"M\")),\r\n  Lambda(\"planetDoc\", Get(Var(\"planetDoc\")))\r\n)\r\n\r\n// Result:\r\n\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081079730471443\"),\r\n      \"ts\": 1590977548370000,\r\n      \"data\": {\r\n        \"name\": \"Mercury\",\r\n        \"type\": \"TERRESTRIAL\",\r\n        \"color\": \"GREY\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Planets\"), \"267081096484618771\"),\r\n      \"ts\": 1590977464930000,\r\n      \"data\": {\r\n        \"name\": \"Mars\",\r\n        \"type\": \"TERRESTRIAL\",\r\n        \"color\": \"RED\"\r\n      }\r\n    }\r\n  ]\r\n}</pre>\r\n<p>Easy, right?</p>\r\n<p>These bindings are very powerful. We can access all the FQL commands available to produce computed values.</p>\r\n<h4><strong>Filtering by any letter</strong></h4>\r\n<p>As a final example, let's see how we could check if an array produced by a binding includes a search term.</p>\r\n<p><strong>Quick note:</strong> The NGram function is currently undocumented, but will be officially supported in a future release.  You can <a href=\"https://fauna.com/blog/boosting-developer-productivity-with-string-functions\">check more details here</a>.</p>\r\n<pre>CreateIndex({\r\n  name: \"filter_Spaceships_by_letter\",\r\n  source: {\r\n    collection: Collection(\"Spaceships\"),\r\n    fields: {\r\n      nameLetters: Query(\r\n        Lambda(\"shipDoc\",\r\n          NGram(Select([\"data\",\"name\"], Var(\"shipDoc\")),1,1)\r\n        )\r\n      )\r\n    }\r\n  },\r\n  terms: [\r\n    { binding: \"nameLetters\"}\r\n  ]\r\n})</pre>\r\n<p>And query it:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"filter_Spaceships_by_letter\"), \"V\")),\r\n  Lambda(\"shipRef\", Let({\r\n    shipDoc: Get(Var(\"shipRef\"))\r\n  },{\r\n    name: Select([\"data\",\"name\"], Var(\"shipDoc\"))\r\n  }))\r\n)\r\n\r\n// Result:\r\n \r\n{\r\n  data: [\r\n    {\r\n      name: \"Voyager\"\r\n    },\r\n    {\r\n      name: \"Explorer IV\"\r\n    }\r\n  ]\r\n}</pre>\r\n<p>This works because the NGram function produces an array of letters which can be queried by the index.</p>\r\n<pre>NGram(\"FaunaDB\",1,1)\r\n\r\n// Result:\r\n\r\n[\"F\", \"a\", \"u\", \"n\", \"a\", \"D\", \"B\"]</pre>\r\n<p>Or:</p>\r\n<pre>NGram(\"FaunaDB\",2,3)\r\n\r\n// Result:\r\n\r\n[\"Fa\", \"Fau\", \"au\", \"aun\", \"un\", \"una\", \"na\", \"naD\", \"aD\", \"aDB\", \"DB\"]</pre>\r\n<p>You can create all sorts of binding values. For example, you could extract the day of the week from a timestamp using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/dayofweek\">DayOfWeek</a> to get all events that happened on a Friday.</p>\r\n<h4><strong>Binding and unique constraints</strong></h4>\r\n<p>If you're wondering, yes, you can use unique constraints over bindings too.</p>\r\n<p>Imagine we wanted to have keycards with ids for accessing our ships. We know that pilots have a history of forgetting their keycard ids, so these ids should be memorable and obvious. What if we create them based on the ships' names? And, since keycards would only be available for a single ship, these ids should be unique.</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Keycards\",\r\n  source: {\r\n    collection: Collection(\"Spaceships\"),\r\n    fields: {\r\n      keyCardId: Query(\r\n        Lambda(\"shipDoc\",\r\n          UpperCase(\r\n            ReplaceStr(Select([\"data\", \"name\"], Var(\"shipDoc\")), \" \", \"_\")\r\n          )\r\n        )\r\n      )\r\n    }\r\n  },\r\n  values: [\r\n    { binding: \"keyCardId\"}\r\n  ],\r\n  unique: true\r\n})</pre>\r\n<p>If we query this index, you'll see how it all makes sense:</p>\r\n<pre>Paginate(Match(Index(\"all_Keycards\")))\r\n\r\n// Result:\r\n \r\n{\r\n  data: [\r\n    \"DESTROYER\",\r\n    \"EXPLORER_IV\",\r\n    \"LE_SUPER_SPACESHIP\",\r\n    \"NAVIGATOR\",\r\n    \"ROCINANTE\",\r\n    \"VOYAGER\"\r\n  ]\r\n}</pre>\r\n<p>If we now try to create a new ship by using a name we've already used, we get an error. The <strong>all_Keycards</strong> index will prevent two keycards from having the same <strong>keyCardId,</strong> even if we have no unique constraints on the names of the spaceships themselves:</p>\r\n<pre>Create(\r\n  Collection(\"Spaceships\"),\r\n  {\r\n    data: {\r\n      name: \"Le Super Spaceship\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\nerror: instance not unique\r\ndocument is not unique.\r\nposition: [\"create\"]</pre>\r\n<h2>Conclusion</h2>\r\n<p>So that's it for today. Hopefully you learned something valuable!</p>\r\n<p>In part 3 of this series, we will continue our space adventure by learning how to model data in FaunaDB.</p>\r\n<p>If you have any questions don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/PierB\">@pierb</a></p>\r\n<p></p>\r\n<p></p>\r\n<p><strong>Next up:&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">Part 3 - a look into the principles of modeling data with FaunaDB</a></strong></p>\r\n<ul></ul>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "8495"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "8142",
        "postDate": "2020-07-01T05:30:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 8144,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a8547c37-fc9c-4906-9f0c-6b1f58affd8e",
        "siteSettingsId": 8144,
        "fieldLayoutId": 4,
        "contentId": 2742,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The Next Chapter for Fauna: $27M and New Leadership",
        "slug": "funding-to-scale-serverless-data",
        "uri": "blog/funding-to-scale-serverless-data",
        "dateCreated": "2020-06-30T15:43:39-07:00",
        "dateUpdated": "2020-07-01T07:25:56-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/funding-to-scale-serverless-data",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/funding-to-scale-serverless-data",
        "isCommunityPost": false,
        "blogBodyText": "<p>When Matt and I founded Fauna, we envisioned a world in which every development team could ship an application to global scale without operational overhead, without scalability problems, without losing productivity, and without rearchitecting.</p>\r\n<p>Thanks to the hard work of the Fauna team and our partners, that world is now real. 25,000 developers and growing now use Fauna to do exactly that. I feel privileged to be witnessing and contributing to what is clearly a revolution in development methodology and architecture. </p>\r\n<p>We have begun calling this new architecture <a href=\"https://fauna.com/client-serverless\"><em>client-serverless</em></a>. It is a return to a model where smart clients compose APIs individually and concurrently, instead of making coarse-grained requests to co-located application servers and databases. Applications now run in globally distributed web, mobile, and embedded clients, with hundreds of times the processing power that a server in a datacenter used to have. These clients expect their data services to be ubiquitously available, full-featured, and low latency.  </p>\r\n<h2>Application Architecture Is Changing</h2>\r\n<p>Organizations of every size are adopting client-serverless development because it is transformatively more productive, both for new products and for augmentations of existing apps. Jamstack is one of the first application stacks that embodies this new approach. </p>\r\n<p>This world is new, exciting, and messy. Complementary technologies include GraphQL, React.JS, Deno, RedwoodJS, and many others. Our partners like Netlify and Vercel are working hard to deliver the most productive development and deployment experience, and we are working to deliver the mission-critical database tier—not as a managed, provisioned infrastructure service—but as a simple API. </p>\r\n<p>Similar to the way MySQL was foundational to the LAMP stack, FaunaDB is becoming foundational to the new client-serverless stack. We are proud to be the first mover in this space and our strong growth over the last 18 months reflects it. Nevertheless, we have a lot of work ahead of us to continue building FaunaDB and to help lift the ecosystem overall. </p>\r\n<h2>New Money and New People</h2>\r\n<p>That growth has led us to a new, $27M investment in Fauna from Soma Somasegar at Madrona Venture Group and Aaron Schildkrout at Addition Capital, along with our existing investors GV, CRV, and others. We are proud to welcome new angel investors as well, including Tom Preston-Werner, creator of RedwoodJS, and Roger Bamford, distinguished architect at MongoDB and Oracle. </p>\r\n<p>I will admit that raising this round in the current global climate was not easy. We are grateful for the resolve and commitment of our new investors in the midst of the pandemic, and the hard work of the Fauna team, our customers, our community members, and others who assisted with the fundraising effort. </p>\r\n<p>In addition to the fundraise, we have brought on additional executive leadership. Bob Muglia, former CEO of Snowflake, has joined us as executive chair. Along with Bob, Eric Berg, former CPO of Okta, has joined us as our new CEO. Bob brings a wealth of experience from taking both Snowflake and SQL Server to market, and Eric was key to Okta’s rise from 10-person startup to the publicly-traded security infrastructure company it is today. </p>\r\n<p>To my surprise, being CEO of Fauna is the longest job I have ever had. I am an engineer at heart, and will be stepping into the CTO role where I can spend more time hands-on with the product and and with our partners. My co-founder, Matt, is stepping into a Chief Architect role where he can contribute more directly to the deep architectural innovations of Fauna like the transaction pipeline and the multi-tenant process scheduler.</p>\r\n<p>I am excited to welcome Eric and Bob to the team. Fauna’s future is in good hands and our partnership has been very fruitful already. </p>\r\n<h2>Investing in You, the Developer</h2>\r\n<p>We will be focusing on three things as we continue to scale the company: improving the developer experience and making FaunaDB easier to learn, integrating more closely with our partners in the space to make building dynamic applications easier overall, and maturing FaunaDB and our business so that those applications, products, and companies making a bet on us and the client-serverless architecture can themselves mature and scale. </p>\r\n<p>If you believe in a world of limitless global scale where building applications doesn’t require operating servers, overcoming the baggage of legacy technology, or paying monthly fees—a world where the developer is empowered directly to realize their product vision in code—please reach out to us. Because, of course, we are <a href=\"https://fauna.com/careers\">hiring</a>. </p>\r\n<p>Thanks for everything,</p>\r\n<p>Evan and Matt</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [
            "8222"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7655",
        "postDate": "2020-06-23T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7650,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a275c52c-0e0e-4450-a930-33bb28d08410",
        "siteSettingsId": 7650,
        "fieldLayoutId": 4,
        "contentId": 2622,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting started with FQL, FaunaDB’s native query language - part 1",
        "slug": "getting-started-with-fql-faunadbs-native-query-language-part-1",
        "uri": "blog/getting-started-with-fql-faunadbs-native-query-language-part-1",
        "dateCreated": "2020-06-22T10:59:37-07:00",
        "dateUpdated": "2020-09-02T09:10:13-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-fql-faunadbs-native-query-language-part-1",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-1",
        "isCommunityPost": true,
        "blogBodyText": "<p>FaunaDB is a serverless global database designed for low latency and developer productivity.  FQL, its query language, was also designed with these goals in mind. With it, you can create expressive queries that will allow you to harness the full power of FaunaDB.</p>\r\n<p>In this five-part series of articles, we’ll go through the basics of FQL with no need of prior knowledge. If you are skimming and don’t understand something, you probably only need to go back to a previous section.</p>\r\n<ul><li>Part 1: a look at FQL and fundamental FaunaDB concepts</li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">Part 2: a deep dive into indexes with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-3\">Part 3: a look into the principles of modeling data with FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-4\">Part 4: a look at how to create custom functions that run straight in FaunaDB</a></li><li><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-5\">Part 5: a look at authentication and authorization in FaunaDB</a></li></ul><ul></ul>\r\n<p>In this article:</p>\r\n<ul><li>Should you learn FQL if you're already using GraphQL?</li><li>Getting started</li><li>About documents and collections</li><li>Your first collections</li><li>Basic CRUD operations</li><li>Your first index</li><li>Using Lambda() to retrieve a list of documents</li><li>Using Let() and Select() to return custom results</li></ul>\r\n<h2>Should you learn FQL if you're already using GraphQL?</h2>\r\n<p>If you're using FaunaDB's native GraphQL API, you might be wondering if it makes sense to invest time in learning FQL. The answer is yes, absolutely.</p>\r\n<p>As an agnostic querying language, GraphQL is a great option for using FaunaDB straight from your client(s), but FQL will allow you to go beyond data querying and define more sophisticated behaviors right in the database. For example, you can define custom functions in FQL, similar in concept to SQL stored procedures, which can be triggered from GraphQL. See the <a href=\"https://docs.fauna.com/fauna/current/api/graphql/functions\">official docs</a> for more info on this.</p>\r\n<h2>Getting started</h2>\r\n<p>Before embarking on our space adventure, you only need to <a href=\"https://dashboard.fauna.com/accounts/register\">signup for a free FaunaDB account</a>. FaunaDB has a very generous free tier which is more than enough for learning, development, or even light production workloads.</p>\r\n<figure><img src=\"{asset:7651:url}\" data-image=\"7651\"></figure>\r\n<p>Once inside the dashboard, create a new database and you’re good to go.</p>\r\n<figure><img src=\"{asset:7652:url}\" data-image=\"7652\" style=\"max-width: 450px;></figure>\r\n<p>In the dashboard, there is a FaunaDB shell. There you can write queries in FQL and see the results of these queries or any errors that might arise.</p>\r\n<figure><img src=\"></figure>\r\n<p></p>\r\n<p>It’s also possible to install FaunaDB on your development machine using <a href=\"https://docs.fauna.com/fauna/current/start/dev\">an official Docker image</a> if you prefer.</p>\r\n<h2>About documents and collections</h2>\r\n<p>FaunaDB is a NoSQL database. Instead of organizing data in tables and rows, it uses documents and collections.</p>\r\n<p>The smallest units of data in FaunaDB are schemaless <a href=\"https://docs.fauna.com/fauna/current/api/fql/documents\">documents</a> which are basically JSON with some extra <a href=\"https://docs.fauna.com/fauna/current/api/fql/types\">FaunaDB types</a>. These documents are grouped into collections which are simply buckets of documents.</p>\r\n<p>This is what a simple document looks like:</p>\r\n<pre>{\r\n  \"ref\": Ref(Collection(\"Planets\"), \"264471980339626516\"),\r\n  \"ts\": 1588478985090000,\r\n  \"data\": {\r\n    \"name\": \"Vulcan\"\r\n  }\r\n}</pre>\r\n<ul><li><strong><tt>ref</tt></strong> is a reference that uniquely identifies the document inside a <strong><tt>Planets</tt></strong> collection with the id <strong>264471980339626516</strong>. We’ll go over references and the special <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/ref\">Ref type</a> in more detail later.</li><li><strong><tt>ts</tt></strong> is a timestamp of the document's last event (e.g., create, read, update, delete) in microseconds.</li><li><strong><tt>data</tt></strong> is the actual data of the document. You can create any structure you need and use any of the JSON and FaunaDB types. Strings, numbers, references to other documents, nested objects, arrays, etc.</li></ul>\r\n<p>At creation, a document cannot exceed 1MB since that is the limit of a FaunaDB request. You can append more data to a document afterwards.</p>\r\n<h2>Your first collections</h2>\r\n<p>Obviously, before we begin our space adventure, we need a spaceship and a pilot. How else are we going to travel through space?</p>\r\n<p>Let’s create a Spaceships collection using the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/createcollection\">CreateCollection</a> function:</p>\r\n<pre>CreateCollection({name: \"Spaceships\"})\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Collection(\"Spaceships\"),\r\n  \"ts\": 1590269343560000,\r\n  \"history_days\": 30,\r\n  \"name\": \"Spaceships\"\r\n}</pre>\r\n<p>As you can see, the result looks very similar to a document. Pretty much all data in FaunaDB is stored in documents. For now, let’s leave the default values and move on.</p>\r\n<p>Let’s create another a collection for our pilots:</p>\r\n<pre>CreateCollection({name: \"Pilots\"})\r\n</pre>\r\n<p>We're ready now to start creating our first documents.</p>\r\n<h2>Basic CRUD operations</h2>\r\n<h3><strong>Create</strong></h3>\r\n<p>Let’s create our first document with the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/create\">Create</a> function:</p>\r\n<pre>Create(\r\n  Collection(\"Pilots\"),\r\n  {\r\n    data: {\r\n      name: \"Flash Gordon\"\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n  \"ts\": 1590270525630000,\r\n  \"data\": {\r\n    \"name\": \"Flash Gordon\"\r\n  }\r\n}</pre>\r\n<p>Let's break this down:</p>\r\n<ul><li>Create is used to create new documents in FaunaDB.</li><li><strong><tt>Collection(\"Pilots\")</tt></strong> is a reference to the <strong><tt>Pilots</tt></strong> collection.</li><li><strong><tt>{data: {name: \"Flash Gordon\"}}</tt></strong> is the actual data of the document.</li></ul>\r\n<p>So now that we’ve created a pilot, we can create a new spaceship:</p>\r\n<pre>Create(\r\n  Collection(\"Spaceships\"),\r\n  {\r\n    data: {\r\n      name: \"Millennium Hawk\",\r\n      pilot: Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n    }\r\n  }\r\n)</pre>\r\n<p>As you can see, we're now storing a reference to another document in the pilot property. I will cover references and relationships in much more detail in part three of this series.</p>\r\n<p><strong>Quick tip: </strong>SQL users might be tempted to store the actual id in a pilot_id property of the JSON instead of a reference. This would be totally valid but it's recommended to use native FaunaDB references. This will make your FQL queries much simpler as we’ll see later on.</p>\r\n<h3><strong>Read</strong></h3>\r\n<p>To read documents, we use the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/get\">Get function</a> which receives a document reference and returns an actual document:</p>\r\n<pre>Get(\r\n  Ref(Collection(\"Spaceships\"), \"266354515987399186\")\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266354515987399186\"),\r\n  \"ts\": 1590274311000000,\r\n  \"data\": {\r\n    \"name\": \"Millennium Hawk\",\r\n    \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n  }\r\n}</pre>\r\n<h3><strong>Update</strong></h3>\r\n<p>To update a document, we use <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/update\">Update</a>. If we wanted to change the name of our ship, we’d simply run:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"Spaceships\"), \"266354515987399186\"),\r\n  {\r\n    data: {\r\n      name: \"Millennium Falcon\"\r\n    }\r\n  }\r\n)\r\n\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266354515987399186\"),\r\n  \"ts\": 1590274726650000,\r\n  \"data\": {\r\n    \"name\": \"Millennium Falcon\",\r\n    \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n  }\r\n}</pre>\r\n<p>As you can see, only the name has been updated in the document and the pilot remains untouched. It’s also possible to replace an entire document using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/replace\">Replace</a> instead.</p>\r\n<h3><strong>Delete</strong></h3>\r\n<p>On second thought, it’s probably better if we don’t use that copyrighted name for our spaceship. We don’t want to get into trouble with the galactic empire.</p>\r\n<p>As expected, to delete a document we simply use&nbsp;<a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/delete\">Delete</a>:</p>\r\n<pre>Delete (\r\n  Ref(Collection(\"Spaceships\"), \"266354515987399186\")\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266354515987399186\"),\r\n  \"ts\": 1590274726650000,\r\n  \"data\": {\r\n    \"name\": \"Millennium Falcon\",\r\n    \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n  }\r\n}</pre>\r\n<p>Let’s create a new spaceship again to continue with our adventure:</p>\r\n<pre>Create(\r\n  Collection(\"Spaceships\"),\r\n  {\r\n    data: {\r\n      name: \"Voyager\",\r\n      pilot: Ref(Collection(\"Pilots\"), \"266350546751848978\")\r\n    }\r\n  }\r\n)</pre>\r\n<h2>Your first index</h2>\r\n<p>Fetching all documents in a database to check if each document fits a particular criteria would be very slow. In the relational world, this would be comparable in concept to a full table scan.</p>\r\n<p>To solve this problem, Fauna implements <a href=\"https://docs.fauna.com/fauna/current/api/fql/indexes\">indexes</a>. These are database entities that organise your data in such a way that they allow for efficient lookup of multiple documents. Whenever you create new documents, Fauna will know which indexes it needs to update in the background.</p>\r\n<p>As we’ll see in the next article, indexes can span multiple collections and accept parameters for sorting and filtering.</p>\r\n<p>For now, let’s create a simple index to list all the documents in a collection:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Pilots\",\r\n  source: Collection(\"Pilots\")\r\n})\r\n \r\n// Result:\r\n\r\n{\r\n  \"ref\": Index(\"all_Pilots\"),\r\n  \"ts\": 1590278778420000,\r\n  \"active\": true,\r\n  \"serialized\": true,\r\n  \"name\": \"all_Pilots\",\r\n  \"source\": Collection(\"Pilots\"),\r\n  \"partitions\": 8\r\n}</pre>\r\n<p>Again, you can see that an index is just another type of document.</p>\r\n<p>After adding some more pilots to our collection, we can query our new index like this:</p>\r\n<pre>Paginate(\r\n  Match(\r\n    Index(\"all_Pilots\")\r\n  )\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    Ref(Collection(\"Pilots\"), \"266359364060709394\"),\r\n    Ref(Collection(\"Pilots\"), \"266359371696439826\"),\r\n    Ref(Collection(\"Pilots\"), \"266359447111074322\")\r\n  ]\r\n}</pre>\r\n<p>Let’s break this down:</p>\r\n<ul><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/iindex\">Index</a> returns a reference to an index</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/match\">Match</a> accepts that reference and  constructs a set, which is sort of like an abstract representation of the data. At this point, no data has been fetched from FaunaDB yet.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/paginate\">Paginate</a> takes the output from Match, fetches data from FaunaDB, and returns a <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/paginate#page\">Page</a> of results. In this case, this is simply an array of references.</li></ul>\r\n<h3><strong>Using the Documents function to get all the documents of a collection</strong></h3>\r\n<p>The previous index was actually a very simplistic example that served as an introduction to indexes.</p>\r\n<p>Since retrieving all the documents in a collection is a very common need, FaunaDB provides us with the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/documents\">Documents</a> function to avoid the need to create a new index for every collection. It produces exactly the same results as the equivalent index.</p>\r\n<pre>Paginate(Documents(Collection('Pilots')))\r\n\r\n// Result:\r\n \r\n{\r\n  \"data\": [\r\n    Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    Ref(Collection(\"Pilots\"), \"266359364060709394\"),\r\n    Ref(Collection(\"Pilots\"), \"266359371696439826\"),\r\n    Ref(Collection(\"Pilots\"), \"266359447111074322\")\r\n  ]\r\n}</pre>\r\n<h3><strong>Page size</strong></h3>\r\n<p>By default, Paginate returns pages of 64 items. You can define how many items you’d like to receive with the <strong>size</strong> parameter up to 100,000 items:</p>\r\n<pre>Paginate(\r\n  Match(Index(\"all_Pilots\")),\r\n  {size: 2}\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"after\": [\r\n    Ref(Collection(\"Pilots\"), \"266359371696439826\")\r\n  ],\r\n  \"data\": [\r\n    Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    Ref(Collection(\"Pilots\"), \"266359364060709394\")\r\n  ]\r\n}</pre>\r\n<p>Since the number of results, in this case, does not fit in one page, FaunaDB also returns the <strong>after</strong> property to be used as a cursor. You can read more about <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/paginate#cursor\">using cursors in the docs</a>.</p>\r\n<h2>Using Lambda() to retrieve a list of documents</h2>\r\n<p>In some cases, you might want to retrieve a list of references, but generally, you will probably need an actual list of documents.</p>\r\n<p>Initially, you might think the best way to solve this would be by performing multiple queries from your programming language. That would be an anti-pattern which you absolutely want to avoid. You would introduce unnecessary latency and make your application much slower than it needs to be.</p>\r\n<p>For example, in this JavaScript example, you'd be waiting first for the query to get the references and then for the queries to get the documents:</p>\r\n<pre>// Don't do this!\r\nconst result = await client.query(q.Paginate(q.Match(q.Index(\"all_Pilots\")));\r\nconst refs = result.data;\r\nconst promises = result.map(refs.map(ref =&gt; client.query(q.Get(ref))));\r\nconst pilots = await Promise.all(promises);</pre>\r\n<p>Or even worse, by waiting for each and every query that gets a document:</p>\r\n<pre>// Don't do this!\r\nconst result = await client.query(q.Paginate(q.Match(q.Index(\"all_Pilots\")));\r\nconst refs = result.data;\r\nconst pilots = [];\r\nfor (const ref of refs) {\r\n  const pilot = await client.query(q.Get(ref));\r\n  pilots.push(pilot);\r\n}</pre>\r\n<p>The solution is simply to use FQL to solve this neatly in a single query.</p>\r\n<p>Here's the idiomatic solution of getting an actual list of documents from an array of references:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Pilots\"))),\r\n  Lambda('pilotRef', Get(Var('pilotRef')))\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"ref\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n      \"ts\": 1590270525630000,\r\n      \"data\": {\r\n        \"name\": \"Flash Gordon\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": Ref(Collection(\"Pilots\"), \"266359364060709394\"),\r\n      \"ts\": 1590278934520000,\r\n      \"data\": {\r\n        \"name\": \"Luke Skywalker\"\r\n      }\r\n    },\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>We’ve already seen that Paginate returns an array of references, right? The only mystery here is Map and this Lambda thing.</p>\r\n<p>You’ve probably already used a map function in your programming language of choice. It’s a function that accepts an array and returns a new array after performing an action on each item.</p>\r\n<p>Consider this JavaScript example:</p>\r\n<pre>const anotherArray = myArray.map(item =&gt; doSomething(item));\r\n\r\n// which is equivalent to:\r\n\r\nconst anotherArray = myArray.map(function (item) {\r\n  return doSomething(item);\r\n});</pre>\r\n<p>With this in mind, let’s break down this part of our FQL query:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Pilots\"))),\r\n  Lambda(\"pilotRef\", Get(Var(\"pilotRef\")))\r\n)</pre>\r\n<ul><li>Paginate returns an array of references.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/map\">Map</a> accepts an array (from Paginate or other sources), performs an action on each item of this array, and returns a new array with the new items. In this case, the action is performed using <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lambda\">Lambda</a>, which is the Fauna equivalent of what you'd call a simple anonymous function in JavaScript. It's all very similar to the previous JavaScript example.</li><li><strong><tt>Lambda('pilotRef'</tt></strong> defines a parameter called pilotRef for the anonymous function. You can name this parameter anything that makes sense for you. FaunaDB doesn’t care. In this example, the parameter will receive a reference which is why I named it pilotRef.</li><li><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/var\">Var</a> is used to evaluate variables. In this case, it evaluates <strong><tt>\"pilotRef\"</tt></strong> and returns the document reference.</li><li>Finally, Get will receive the reference and return the actual document.</li></ul>\r\n<p>If we were to rewrite the previous FQL query with the JavaScript FaunaDB driver, we could do something like this:</p>\r\n<pre>q.Map(\r\n   q.Paginate(q.Match(q.Index(\"all_Pilots\"))),\r\n  (pilotRef) =&gt; q.Get(pilotRef)\r\n)\r\n\r\n// Or:\r\n\r\nq.Map(\r\n   q.Paginate(q.Match(q.Index(\"all_Pilots\"))),\r\n   q.Lambda(\"pilotRef\", q.Get(q.Var(\"pilotRef\")))\r\n)</pre>\r\n<p><strong>Quick tip:</strong> you can paste JavaScript queries into the FaunaDB shell as well as FQL queries.</p>\r\n<figure><img src=\"{asset:7654:url}\" data-image=\"7654\"></figure>\r\n<h2>Using Let() and Select() to return custom results</h2>\r\n<p>Up until now, our documents have been pretty minimalistic. Let's add some more data to our spaceship:</p>\r\n<pre>Update(\r\n  Ref(Collection(\"Spaceships\"),\"266356873589948946\"),\r\n  {\r\n    data: {\r\n      type: \"Rocket\",\r\n      fuelType: \"Plasma\",\r\n      actualFuelTons: 7,\r\n      maxFuelTons: 10,\r\n      maxCargoTons: 25,\r\n      maxPassengers: 5,\r\n      maxRangeLightyears: 10,\r\n      position: {\r\n        x: 2234,\r\n        y: 3453,\r\n        z: 9805\r\n      }\r\n    }\r\n  }\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"ref\": Ref(Collection(\"Spaceships\"), \"266356873589948946\"),\r\n  \"ts\": 1590524958830000,\r\n  \"data\": {\r\n    \"name\": \"Voyager\",\r\n    \"pilot\": Ref(Collection(\"Pilots\"), \"266350546751848978\"),\r\n    \"type\": \"Rocket\",\r\n    \"fuelType\": \"Plasma\",\r\n    \"actualFuelTons\": 7,\r\n    \"maxFuelTons\": 10,\r\n    \"maxCargoTons\": 25,\r\n    \"maxPassengers\": 5,\r\n    \"maxRangeLightyears\": 10,\r\n    \"position\": {\r\n      \"x\": 2234,\r\n      \"y\": 3453,\r\n      \"z\": 9805\r\n    }\r\n  }\r\n}</pre>\r\n<p>Cool.</p>\r\n<p>So now imagine our application were in fact managing a whole fleet and you needed to show a list of ships to the fleet admiral.</p>\r\n<p>First, we'd need to create an index:</p>\r\n<pre>CreateIndex({\r\n  name: \"all_Spaceships\",\r\n  source: Collection(\"Spaceships\")\r\n})</pre>\r\n<p>Ok, now we just use Paginate, Map, and Lambda like we saw earlier to get all the documents. So we do that but... Oh no!</p>\r\n<p>The fleet admiral is very unhappy about the slow performance of his holomap now.</p>\r\n<p>Sending the complete list with thousands of documents across lightyears of space wasn't a great idea because it's a lot of data. We propose breaking down the results with pages, but the admiral <em>absolutely</em> needs to see all ships at once.</p>\r\n<p><em>\"By the cosmic gods! I don't care how much fuel a ship has!\" </em>shouts the admiral. <em>\"I only want to know its name, id, and position!\".</em></p>\r\n<p>Of course! Let's do that:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"all_Spaceships\"))),\r\n  Lambda(\"shipRef\",\r\n    Let(\r\n      {\r\n        shipDoc: Get(Var(\"shipRef\"))\r\n      },\r\n      {\r\n        id: Select([\"ref\", \"id\"], Var(\"shipDoc\")),\r\n        name: Select([\"data\", \"name\"], Var(\"shipDoc\")),\r\n        position: Select([\"data\", \"position\"], Var(\"shipDoc\"))\r\n      }\r\n    )\r\n  )\r\n)\r\n\r\n// Result:\r\n\r\n{\r\n  \"data\": [\r\n    {\r\n      \"id\": \"266356873589948946\",\r\n      \"name\": \"Voyager\",\r\n      \"position\": {\r\n        \"x\": 2234,\r\n        \"y\": 3453,\r\n        \"z\": 9805\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"266619264914424339\",\r\n      \"name\": \"Explorer IV\",\r\n      \"position\": {\r\n        \"x\": 1134,\r\n        \"y\": 9453,\r\n        \"z\": 3205\r\n      }\r\n    }\r\n    // etc...\r\n  ]\r\n}</pre>\r\n<p>Boom! Now the holomap loads much faster. We can see the satisfaction in the admiral's smile.</p>\r\n<p>Since we already know how Paginate, Map, and Lambda work together, this is the new part:</p>\r\n<pre>Let(\r\n  {\r\n    shipDoc: Get(Var(\"shipRef\"))\r\n  },\r\n  {\r\n    id: Select([\"ref\", \"id\"], Var(\"shipDoc\")),\r\n    name: Select([\"data\", \"name\"], Var(\"shipDoc\")),\r\n    position: Select([\"data\", \"position\"], Var(\"shipDoc\"))\r\n  }\r\n)</pre>\r\n<h3><strong>Let</strong></h3>\r\n<p><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/let\">Let</a> is a function used in FQL to create custom objects. You can even have nested Let functions to format the data with total freedom.</p>\r\n<p>The first part of Let is used to define variables that will be used later on. The docs call these variables \"bindings\". These bindings will be available to any nested Let objects you create.</p>\r\n<p>Here we define a <strong>shipDoc</strong> variable which will store the document returned from Get, which in turn will use the reference from the Lambda parameter:</p>\r\n<pre>{\r\n  shipDoc: Get(Var(\"shipRef\"))\r\n}</pre>\r\n<p>The second part is the actual object that will be returned by Let:</p>\r\n<pre>{\r\n  id: Select([\"ref\", \"id\"], Var(\"shipDoc\")),\r\n  name: Select([\"data\", \"name\"], Var(\"shipDoc\")),\r\n  position: Select([\"data\", \"position\"], Var(\"shipDoc\"))\r\n}</pre>\r\n<h3><strong>Select</strong></h3>\r\n<p><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/select\">Select</a> is used to select data from objects or arrays.</p>\r\n<pre>Select([\"data\", \"name\"], Var(\"shipDoc\"))</pre>\r\n<p>Here, we're telling FaunaDB to select the <strong><tt>name</tt></strong> property from the <strong><tt>data</tt></strong> property of the document stored in the <strong><tt>shipDoc</tt></strong> binding.</p>\r\n<p>This array-like notation <strong><tt>[\"data\", \"name\"]</tt></strong> is called a path in FaunaDB lingo. We're using it here to get to the <strong>name</strong> property, but it can be used with integers to access array items too.</p>\r\n<h2>Conclusion</h2>\r\n<p>So that's it for today. Hopefully, you learned something valuable!</p>\r\n<p>In part 2 of the series, we will continue our space adventure by going deeper into indexes.</p>\r\n<p>If you have any questions, don't hesitate to hit me up on Twitter: <a href=\"https://twitter.com/pierb\">@pierb</a></p>\r\n<p></p>\r\n<p></p>\r\n<p><strong>Next up:&nbsp;&nbsp;</strong><a href=\"https://fauna.com/blog/getting-started-with-fql-faunadbs-native-query-language-part-2\">Part 2 - a deep dive into indexes with FaunaDB</a></p>\r\n<ul></ul>",
        "blogCategory": [
            "10",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "7658"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7580",
        "postDate": "2020-06-15T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7562,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "71602add-a675-43a7-b77f-680075c20d69",
        "siteSettingsId": 7562,
        "fieldLayoutId": 4,
        "contentId": 2574,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Control theory for fun and profit",
        "slug": "control-theory-for-fun-and-profit",
        "uri": "blog/control-theory-for-fun-and-profit",
        "dateCreated": "2020-06-10T10:32:28-07:00",
        "dateUpdated": "2020-06-19T08:19:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/control-theory-for-fun-and-profit",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/control-theory-for-fun-and-profit",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is a distributed system. Like all distributed systems, we have the somewhat vexing problem of an unreliable network and faulty nodes (not byzantine faulty, just the regular slow or dead kind). The chance that a faulty node received a share of work in a request is greatly amplified when the result of one request requires a combination of data from several nodes. In such a scenario that is common for a distributed system—a faulty node could impact many requests. Methods to increase the reliability of your system in the presence of faulty nodes are therefore indispensable. </p>\r\n<p>Whenever you need to request data over a potentially unreliable link and multiple nodes have the data there is an easy method to turn a set of unreliable links into a single virtual link that is both more reliable and faster in the tail in aggregate than any one of the individual links. Just issue requests redundantly. The fastest path of the available paths will return first. Of course, issuing redundant requests has its own problems: increased server load for machines servicing the requests and increased network traffic for both parties.</p>\r\n<figure><img src=\"{asset:7563:url}\" data-image=\"7563\"><figcaption>Issuing redundant requests naively might increase network traffic and overload servers that were previously fine.</figcaption></figure>\r\n<p>Instead of flooding the cluster with redundant messages in every scenario, we could try to determine when we have to send redundant messages. The most straightforward way to do this is to simply wait and see whether the result comes in fast enough. Instead of sending redundant requests right away, the node would wait for a specific delay and send a backup message when it did not receive an answer within the given time-frame.</p>\r\n<p>In fact, almost all the gains of redundant requests can be had if we delay issuing the second request until we have waited out some high percentile of the response distribution. <a href=\"https://ai.google/research/pubs/pub40801\">The Tail at Scale</a> paper recounts how waiting out the ~ 98% percentile response time (they measured a fixed delay of 10ms) before issuing a redundant request reduced the 99.9th percentile from 1800ms to 74ms while only costing them a modest 2% extra utilization.</p>\r\n<h2>The Next Problem</h2>\r\n<p>This is all well and good in theory. There is, of course, an obvious pressing and practical concern: how does one figure out how long one should wait before sending the hedged request? Ideally, it’s small enough since each request that requires backup messages would take at least the time of the delay. But it can’t be too small either since the cluster would be flooded with backup messages. A simplistic and obvious answer is to measure it. We could determine the percentage of messages that we desire backup messages for, measure it directly, and adapt where needed. </p>\r\n<p>Let’s say we aim to wait for the 98% percentile like in the paper. By gathering latency measurements for all our requests, we can determine the exact ‘backup request delay’ to make sure that only 2% of the requests trigger a redundant request. </p>\r\n<figure><img src=\"{asset:7564:url}\" data-image=\"7564\"></figure>\r\n<p>In reality, it’s much more complex, by the time we have set our delay based on our measurements, the latencies might have changed due to external factors. Additionally, by setting this delay, the measurements will change, influencing the measurements which we rely on to take action can have bad side-effects.</p>\r\n<p>In a naive approach where we set the delay each time exactly according to our measurements, we might accidentally make things worse. If due to an anomaly such as a netsplit, our latencies are severely skewed for only a split second we might set an extremely high delay. Setting such a high delay might affect our measurements which then results in an even higher delay, which again has an effect on the percentile, which in turn results in a higher delay, which again results in … . . .</p>\r\n<p>I believe you get the point, a system that acts in a loop where it takes action on measurements that are influenced by its actions could quickly go rogue instead of stabilizing on an optimal value. There are many bad scenarios to avoid: </p>\r\n<ul><li><strong>Slow convergence:</strong> the system takes too long to respond.</li><li><strong>Constantly in flux:</strong> the system continuously jumps up and down instead of stabilizing, typically caused by responding too quickly.</li><li><strong>Snowball effect:</strong> an anomaly results in a poorly chosen value which enforces the anomaly.</li></ul>\r\n<p>In essence, dealing with such a moving target raises even more questions: over what time slice should we measure? How should we evolve the time to wait if we measure the latencies continuously? Weighted average? Exponential? What weightings? </p>\r\n<p>The ideal solution should react rapidly to changes in network conditions and converge to the right value without overshooting it. Now, in our case, getting this a little bit wrong either side isn’t the end of the world but it does have consequences: slow convergence or overshoot depending on the direction from which it arrives either means overly slugging request servicing or overconsumption of computing resources.</p>\r\n<h2>Control Theory To The Rescue</h2>\r\n<p>Fortunately, this kind of problem falls into a well-known area of study which is very common in electronics yet less often encountered in the wild in computer algorithms. In fact, the thermostat in your home has to solve similar issues. It needs to achieve a target by reacting to measurements that are influenced by its own actions (start heating) and external stimuli (an open door lets in the cold) in a constant loop. </p>\r\n<figure><img src=\"{asset:7565:url}\" data-image=\"7565\"></figure>\r\n<p>Check out this visual analogy where a motor is controlling the position of a ball in response to external stimuli. This system has to decide on a course of action based on measurements of a constantly moving target that is influenced both by external stimuli (the hand that moves the ball out of center) as well as its own actions (tilting the plate).</p>\r\n<figure><img src=\"{asset:7570:url}\" data-image=\"7570\"></figure>\r\n<p>(source <a href=\"https://giphy.com/gifs/t2zwKlu9jtw9G\">Giphy</a>)<br></p>\r\n<p>This plate controller is doing exactly what we’d like our software to do. It has a setpoint (keep the ball in the center of the plate), it has an error (the distance of the ball to the setpoint) and it has a mechanism for reducing the error (tilting the plate). Our setpoint is the ratio of requests that trigger a backup read to those that don’t, our error is just the gap between the observed ratio and the target ratio and our mechanic to influence this ratio is the delay before issuing the backup request. </p>\r\n<figure><img src=\"{asset:7567:url}\" data-image=\"7567\"></figure>\r\n<p>How is the plate being controlled? With a PID controller.</p>\r\n<h2>PID Controllers</h2>\r\n<p>PID controllers are classic closed-loop control systems that every tick gather information and take action to bring the system to some desired state. PID is a mathematical answer to this type of problem that was <a href=\"https://en.wikipedia.org/wiki/PID_controller\">invented in the 17th century</a> to keep windmills running at a fixed speed. </p>\r\n<figure><img src=\"{asset:7568:url}\" data-image=\"7568\"></figure>\r\n<p>PID is an initialism standing for Proportional, Integral, Derivative which are the components used to tame the system and a PID controller is the embodiment of this function which has taken many forms over the years; mechanical (a pneumatic device), electrical (chips) and finally the programmatic implementation that can be used to optimize distributed systems.  </p>\r\n<p>The function combines the error <strong>(P)</strong>, the accumulated error over time <strong>(I)</strong> and the predicted error <strong>(D)</strong> each multiplied by a tunable constant respectively. Why does this make sense?</p>\r\n<p>By specifying a weight for these three factors (using the constants Kp, Ki, and Kd), we can finetune how our system should behave to prevent overshooting, accumulating errors or unstable situations.  </p>\r\n<ul><li><strong>Proportional:</strong> we adapt the setpoint relatively to the size of the error. This is meant to trigger a fast reaction.</li><li><strong>Integral:</strong> take into account how the error has behaved over time which helps to remove a systematic error. The integral part receives more influence the longer the error accumulates over time. Which means that systematic errors will eventually be mitigated.</li><li><strong>Derivative:</strong> could be seen as the variable that predicts the future and helps to prevent overshooting the target. The derivative would slow down the process if we are moving too fast towards the target.</li></ul>\r\n<p>See this <a href=\"https://www.youtube.com/watch?v=wkfEZmsQqiA&list=PLn8PRpmsu08pQBgjxYFXSsODEF3Jqmm-y\">video series</a> if you want to really dive in. This is exactly what we want: when our ratio is out of wack we want to safely yet rapidly change the delay to converge back to our desired state keeping the system responsive while minimizing resource usage.  </p>\r\n<h2>Conclusion</h2>\r\n<p>The introduction of backup requests and a delay before we actually issue these requests was only the first step of the approach. By introducing the PID controller as a third element we can more readily adapt to delay when the circumstances change. With these three insights combined into one algorithm, we expect to retain all of the benefits of issuing backup requests while making fewer requests.</p>",
        "blogCategory": [
            "1462"
        ],
        "mainBlogImage": [
            "7579"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2020-04-16T04:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7069,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d0a539e9-db7a-4246-a91f-81674a4ec57f",
        "siteSettingsId": 7069,
        "fieldLayoutId": 4,
        "contentId": 2479,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Discover your most engaged customers using Pivot Tables",
        "slug": "discover-your-most-engaged-customers-using-pivot-tables",
        "uri": "blog/discover-your-most-engaged-customers-using-pivot-tables",
        "dateCreated": "2020-04-14T11:33:52-07:00",
        "dateUpdated": "2020-04-16T07:43:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/discover-your-most-engaged-customers-using-pivot-tables",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/discover-your-most-engaged-customers-using-pivot-tables",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://fauna.com/blog/how-to-spot-tech-trends-early\">In my blog last post</a> in the “Data Discovery” series, I discussed how high-level trends can be captured from public data. In this post, I’ll discuss how to use your own company data to take your product to the next level.</p>\r\n<p>Knowing which users are most engaged with your web product is essential. This knowledge helps you understand which users to reach out to for product feedback, and ultimately how to direct the product roadmap. Distinct from the amount they pay or how vocal they are on your community Slack, usage points to true engagement. Furthermore, examining usage can bring unexpected insights into the planning process for your application as you consider growth and breadth of features for the beginning of this new decade.</p>\r\n<p>Pivot tables are quite possibly the easiest and most effective way to analyze complex data sets. Essentially, they take a table of time series data and “tabularize” the data so that it can be represented in a chart more easily. If this is a bit confusing, bear with me, and keep reading. &#x1f600;</p>\r\n<p>We’ll be using this <a href=\"https://docs.google.com/spreadsheets/d/1AUAAqDHlZC2VOKR5CEavNNsSZ_AALwfN_J36YNeOAF8/edit#gid=0\">dataset in Google Sheets for the tutorial</a>. Please make a copy of it to follow along.</p>\r\n<h2>Preparing Your Data</h2>\r\n<p>Note that this tutorial assumes that basic usage data is being tracked on your app in some flavor of relational datastore. If you don’t have that yet, that’s no problem--you can pull an example dataset from Google Sheets <a href=\"https://docs.google.com/spreadsheets/d/1AUAAqDHlZC2VOKR5CEavNNsSZ_AALwfN_J36YNeOAF8/edit#gid=0\">here</a>. </p>\r\n<p>As an aside, at Fauna, we pull data from <a href=\"https://fauna.com/features\">FaunaDB </a>into <a href=\"https://aws.amazon.com/athena/\">Amazon Athena</a> via <a href=\"https://spark.apache.org/\">Spark</a> for analytics purposes.</p>\r\n<h3>SQL</h3>\r\n<p>Note: Don’t know or need <a href=\"https://www.w3schools.com/sql/sql_intro.asp\">SQL</a>? No problem, just skip to the “Spreadsheet cleanup” section.</p>\r\n<p>Although some analytics platforms automatically export data into a spreadsheet for you, you might need to start with the output from a SQL statement that queries your app usage data. </p>\r\n<p>Here’s an example query that would return the total number of operations and revenue for each user by date:</p>\r\n<pre>select \r\n    u.customer_name customer_name,\r\n    p.date date, \r\n    sum(p.ops) ops, \r\n    sum(p.payment) revenue\r\nfrom \r\n    users u\r\njoin\r\n    payments p\r\non\r\n    u.id = p.user_id\r\ngroup by \r\n    1, 2\r\n</pre>\r\n<p>This query pulls the customer name from a “users” table, and the date and number of operations (e.g. click events), and revenue information from the “payments” table. These may all be joined together using user_id, which is common to both tables (and simply called “id” in the \"users\" table).</p>\r\n<p>Some other helpful hints about this query:&nbsp;</p>\r\n<ul><li>“u” and “p” are used as aliases to make referencing the table names easier in the query,</li><li>group by 1, 2 tells the query to group by the first two items in the select statement, namely customer_name and date ,</li><li><tt>u.customer_name customer_name</tt> is shorthand for renaming the column from \"<tt>u.customer_name</tt>\" to \"<tt>customer_name</tt>\"</li></ul>\r\n<h3>Spreadsheet cleanup&nbsp;</h3>\r\n<p>The resulting dataset looks like this (note that this is made up sample data):\r\n</p>\r\n<figure><img src=\"{asset:7057:url}\" data-image=\"7057\"></figure>\r\n<p>They say that 90% of data science is cleaning data. Well, here’s that part &#x1f605;&nbsp;</p>\r\n<p>\r\nAs you can see, the output is a bit messy. The ops and revenue numbers aren’t rounded. Also, the column titles look like a programmer made them. Shame on us!&nbsp;</p>\r\n<p>\r\nThis cleanup is possible to do in SQL, but faster to do in Google Sheets. The following best practices will make your data much easier to read and work with:&nbsp;</p>\r\n<ul><li>Capitalize the column headers and replace underscores with spaces,&nbsp;</li><li>Bold the first row, make the background black, and make the text white.&nbsp;</li><li>Round the Operations numbers to the nearest integer:&nbsp;<ul><li>Highlight the Operations numbers (click on cell C2 and click command+shift+down arrow on a Mac), click More formats (the “123” button in the editor ribbon), click Number, click the “.0” button twice.</li></ul></li><li>Round the Revenue numbers to the nearest cent:&nbsp;<ul><li>Highlight the Revenue numbers (click on cell D2 and click command+shift+down arrow on a Mac), click “$” button in the editor ribbon.&nbsp;</li></ul></li><li>Select the entire table with command+A and create a Filter to make the data sortable :<ul><li>In the menu bar, select Data -&gt; “Create a filter”.&nbsp;</li></ul></li><li>Freeze the first row In the menu bar, select View -&gt; Freeze -&gt; 1 row.&nbsp;</li></ul>\r\n<p>Formatting options menu:</p>\r\n<figure><img src=\"{asset:7058:url}\" data-image=\"7058\"></figure>\r\n<p>The resulting dataset will be easier to read:\r\n</p>\r\n<figure><img src=\"{asset:7059:url}\" data-image=\"7059\"></figure>\r\n<p>We’re also going to add one “helper column” to the dataset so that we’ll be able to view trends by month more easily. To do this, insert a column to the right of the customer name, call it Month, and paste the formula =month(C2) in cell B2. You can double click the bottom right corner of the cell, then drag down to apply the formula to the rest of the column. Your data should&nbsp;look like this now:\r\n</p>\r\n<figure><img src=\"{asset:7060:url}\" data-image=\"7060\"></figure>\r\n<h2>Creating your pivot table&nbsp;</h2>\r\n<p>You might be saying to yourself: “Finally! The part about the pivot table! Thank goodness.”  At least, that’s what I said while writing this since I blathered on for 3 pages about data prep &#x1f61c;</p>\r\n<p>\r\n\r\nNow that the data is clean, select all of it with control+A, and in the menu bar select Data -&gt; “Pivot table”. Click the Create button when the prompt pops up. You are going to want to use the default setting to open the pivot table (PT) in a new sheet so that your raw data does not get muddied.&nbsp;</p>\r\n<p>\r\nOnce you click Create, you’ll have a sparkly new pivot table tab:\r\n</p>\r\n<figure><img src=\"{asset:7061:url}\" data-image=\"7061\"></figure>\r\n<p>Hmm, perhaps a bit too sparkly. This bad boy needs some data. Let’s start by looking at total customer revenue. To do this, add Customer Name as a row and Revenue as a value. Here’s the resulting pivot table:</p>\r\n<figure><img src=\"{asset:7062:url}\" data-image=\"7062\"></figure>\r\n<p>Nice! So glad Goat Bleats Inc. is getting so much value out of our product:\r\n</p>\r\n<figure><img src=\"{asset:7063:url}\" data-image=\"7063\"><figcaption>A happy customer</figcaption></figure>\r\n<p>That said, we are really interested in seeing how revenue and operations trend over time. To get this data, add Customer Name as a column instead of a row, and add Date as a column. Select all of this data and in the menu click Insert -&gt; Chart. The resulting chart:\r\n</p>\r\n<figure><img src=\"{asset:7064:url}\" data-image=\"7064\"></figure>\r\n<p>Wow, it’s clear that Goat Bleats is slaying the competition and that their revenue is increasing over time. That said, this is a noisy as heck chart. Wouldn’t it be great if we could simply see this data rolled up by month?&nbsp;</p>\r\n<p>\r\nThankfully, there’s a helper column for that!&nbsp;</p>\r\n<p>\r\nSwitch out Date for Month in the Rows section. Then, copy cells A2:E5 (the summarized data without Grand Totals) into a new tab. We do that so that we can have a static version of the dataset for charting.&nbsp;</p>\r\n<h2>\r\nCharting the magic&nbsp;</h2>\r\n<p>Charting is a critical part of the data analysis process. This transforms data that we nerds use and makes it look pretty for the executives that need to use this data to make business decisions.\r\n\r\nClean up this pasted data using the cleanup techniques described above, select the data, and in the menu click Insert -&gt; Chart. In the Chart editor that pops up, change the Chart type to a Stacked area chart, and name the chart “Revenue per customer by month”:\r\n</p>\r\n<figure><img src=\"{asset:7065:url}\" data-image=\"7065\"></figure>\r\n<p>And voila! We have a beautiful chart showing our growth over time:</p>\r\n<figure><img src=\"{asset:7066:url}\" data-image=\"7066\"></figure>\r\n<p>Using the exact same process for Operations, we get the following chart (using a Column chart type):</p>\r\n<figure><img src=\"{asset:7067:url}\" data-image=\"7067\"></figure>\r\n<h2>Conclusion</h2>\r\n<p>Pivot tables are my favorite way to learn about how users actually use a web product. They are also powerful and can help you look like a wizard at work.</p>\r\n<figure><img src=\"{asset:7070:url}\" data-image=\"7070\" alt=\"Hagrid\"><figcaption>Your boss after seeing the pivot table</figcaption></figure>\r\n<p>What’s your favorite way to examine user data, and what metrics do you like most? Please let me know on <a href=\"https://twitter.com/lew\">Twitter</a> or in our <a href=\"https://community.fauna.com/\">Community Slack</a>.</p>",
        "blogCategory": [
            "3",
            "10",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "7071"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7038",
        "postDate": "2020-04-10T06:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7039,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b9c320d3-5ab2-44dd-9ee6-9bd466455d23",
        "siteSettingsId": 7039,
        "fieldLayoutId": 4,
        "contentId": 2449,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The FaunaDB Data Manager",
        "slug": "announcing-the-faunadb-data-manager",
        "uri": "blog/announcing-the-faunadb-data-manager",
        "dateCreated": "2020-04-08T10:21:10-07:00",
        "dateUpdated": "2020-09-21T08:52:15-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-faunadb-data-manager",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-faunadb-data-manager",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce the highly anticipated release of the FaunaDB Data Manager (FDM). The FDM can assist with a variety of import and export tasks, including:</p>\r\n<ol><li>Copying documents, collections, indexes, functions, and roles from one FaunaDB database, at any particular point in time, to another FaunaDB database</li><li>Importing and updating data from:<ol><li>A local directory using JSON or CSV files</li><li>An existing FaunaDB database, including at a specific point in time for auditing, version control, or data recovery purposes</li><li>Importing A SQL database, such as MySQL or Postgres, that is accessible over a JDBC connection</li><li>An AWS S3 bucket using JSON or CSV files</li></ol></li><li>Exporting and backing up data to:&nbsp;<ol><li>A local directory (as JSON files)</li></ol></li><li>Simple ETL (i.e., data formatting):&nbsp;<ol><li>Changing a field or column's name and/or data type</li><li>Setting a primary field (i.e., Ref column)</li><li>Setting the import time of a document</li><li>Ignoring fields</li></ol></li><li>Creating a new database pre-filled with demo data, indexes, and roles for testing purposes</li></ol>\r\n<h2>Limitations</h2>\r\n<ul><li>This tool is currently in Preview mode. We do not advise you write production code depending on it as potentially breaking changes might still be introduced.</li><li>Child databases are not processed. To process a child database, run the FaunaDB Data Manager with an admin key for that child database.</li><li>Keys and tokens are not copied. Since the secret for a key or token is only provided on initial creation, it is not possible to recreate existing keys and tokens. You would need to create new keys and tokens in the target database.</li><li>GraphQL schema metadata is not fully processed. This means that if you import an exported database, or copy one {server} database to another, you need to import an appropriate GraphQL schema into the target database in order to run GraphQL queries.</li><li>Schema documents have an upper limit of 10,000 entries per type. If a source database contains more than 10,000 collections, indexes, functions, or roles, only the first 10,000 of each type are processed and the remainder are ignored.</li><li>When exporting a FaunaDB database to the local filesystem, only collections and their associated documents are exported. A copy of the schema documents describing collections, indexes, functions, and roles is copied to the file fauna_schema. Currently, that schema file cannot be used during import.</li><li>FaunaDB imposes collection-naming rules, specifically that collections cannot be named any of the following: <tt>events, set, self, documents,</tt> or _. Unfortunately, the FaunaDB Data Manager does not have the ability to rename collections during processing. If your CSV, JSON, or JDBC sources have filenames/tables that use these reserved names, processing terminates with an error.</li><li>While the FaunaDB Data Manager works on Windows, only limited testing has been done on that platform. You may experience unexpected platform-specific issues. We certainly plan to expand our Windows testing for the FaunaDB Data Manager for future releases</li></ul>\r\n<h2>Prerequisites</h2>\r\n<p>The FaunaDB Data Manager requires Java 11, or higher.&nbsp;To find the version of java currently being used, execute:</p>\r\n<pre>java -version</pre>\r\n<p>A recent version of Java can be <a href=\"https://www.oracle.com/java/technologies/javase-jdk13-downloads.html\">downloaded here</a><br></p>\r\n<h2>Installing the FDM</h2>\r\n<p><strong>Start by downloading the FDM zip file<a href=\"https://fauna-repo.s3.amazonaws.com/fdm/fdm.zip\"> here</a>.</strong></p>\r\n<p>Extract the zip file, then open your terminal and navigate to the unzipped FDM directory:</p>\r\n<pre>cd fdm-1.14</pre>\r\n<p>If you are running on Windows, ensure that the JAVA_HOME environment variable points to the folder where JDK has been installed, and add Java to your&nbsp;path:</p>\r\n<pre>set JAVA_HOME=C:\\Program Files\\Java\\jdk-13.0.2\r\nset PATH=%PATH%;%JAVA_HOME%\\bin</pre>\r\n<p>And type the following command to output the help menu and make sure it is installed properly:</p>\r\n<pre>./fdm --help</pre>\r\n<p>As you should now see from the example commands printed in your terminal, all import/export methods follow the same basic format:</p>\r\n<pre>./fdm -source &lt;arg&gt; -dest &lt;arg&gt;</pre>\r\n<p>In the above command, -source defines the data source and -dest defines the destination. The subsequent arguments differ based on the source and destination types.</p>\r\n<p>When reading data from a file, directory, or AWS bucket, the FDM examines each file’s contents and auto-detects if the file is supported. File extensions such as .json or .csv are ignored, and only the file contents are used to determine the type. If the type of file is not supported, the file is ignored. </p>\r\n<p>Now, let's look at some specific examples!</p>\r\n<h2>Creating your demo databases and keys</h2>\r\n<p>For this tutorial, we will create two demo databases that we can copy and transform data between.</p>\r\n<p>First, you need to create a new API key by following these steps:&nbsp;<br></p>\r\n<ol><li>Log into&nbsp;<a href=\"https://dashboard.fauna.com/\">https://dashboard.fauna.com/</a></li><li>Click on an existing database, or create a new database</li><li>Click the “Security” button in the left navigation bar</li><li>Click the “New Key” button</li><li>Ensure that the “Role” field is set to “Admin”</li><li>Enter a name for the key, e.g. “FDM”</li><li>Click the “Save” button</li><li>Save the displayed secret: it is only displayed once, and you may need to use it multiple times. If you lose it, you would have to create a new key.</li></ol>\r\n<p>Next, to populate a demo database, paste the displayed key secret into the following command, and run it in your terminal from within the FDM directory:</p>\r\n<pre>./fdm -demo REPLACE_WITH_SECRET</pre>\r\n<p>The output provides you with a key secret that can be used to access&nbsp;the newly created \"fdm-demo\" database:</p>\r\n<pre>Database &lt;fdm-demo&gt; created with secret &lt;fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000&gt;\r\n</pre>\r\n<p><em>Note: the key secrets used in this tutorial have been masked.</em><br></p>\r\n<p>Next, let's create our second demo database and generate a secret for it. Visit <a href=\"https://dashboard.fauna.com/\">https://dashboard.fauna.com/</a> and click \"New database\". Name your new database \"fdm-demo-2\" and click the checkbox to \"Pre-populate with demo data\". Click \"Save\". You should now be in your second demo database \"fdm-demo-2\".</p>\r\n<p>Now, click \"Security\" in the left sidebar. Then, click \"New key\". Keep the Role option as \"Admin\", optionally give the key a name such as “key-for-fdm-demo”, and click \"Save\". Copy the secret displayed on the next screen and save it for other examples in this tutorial.</p>\r\n<p><em>Note: You can use either an admin or server key; however, the roles from a FaunaDB database accessed via server key will not be imported for security reasons.</em></p>\r\n<h2>Copying data between FaunaDB databases</h2>\r\n<p>Now, we are ready to copy our data from one demo database to another. </p>\r\n<p>To do so, use the following command, replacing the source key with the \"fdm-demo\" database secret from the terminal output in the previous section, and the destination key with the \"fdm-demo-2\" secret that we just generated:</p>\r\n<pre>./fdm -source key=fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000 -dest key=fnADnJW5CcACE6qfPsMOf111111R-l8ez111111</pre>\r\n<p>Now, visit <a href=\"https://dashboard.fauna.com/collections/@db/fdm-demo-2\">https://dashboard.fauna.com/collections/@db/fdm-demo-2</a> to see that fdm-demo's collections have been copied into the fdm-demo-2 database. </p>\r\n<p>If we had been copying from JSON or CSV files rather than a FaunaDB database, the FDM would have used the file's base name as the destination collection name.</p>\r\n<h2>Appending or Updating Documents</h2>\r\n<p>Not only can the FDM add new documents to a collection, it can also add to an existing document's event history, providing update functionality. This is controlled by two characteristics that every document possesses: the reference, and a timestamp denoting creation time. The reference and timestamp can be provided to the FDM by the input data and/or the command line argument `-format`. If no timestamp is provided, then the current date and time is used. If no reference is provided, the FDM generates a unique reference for the document before it inserts the document to the destination collection. </p>\r\n<p>If a reference is provided, then three different outcomes are possible:</p>\r\n<ol><li>If a document with the reference does not exist in the collection, then the FDM inserts the document into the destination collection.</li><li>If a document with the provided reference does exist in the collection, but no timestamp was provided to the FDM, the default timestamp of “now” is used and a new version of the document is inserted.</li><li>If a document with both the reference and timestamp already exists in the collection, the document’s history is modified at the point in time specified by the timestamp.</li></ol>\r\n<h3>Example of updating a document</h3>\r\n<p>Let's return to the two databases from the example above, and change some of the data in the source database. To modify the data, visit a document in the fdm-demo database, for example: https://dashboard.fauna.com/collections/documents-edit/address/100/@db/fdm-demo. Next, modify the data, e.g., by changing the State from WA to NY. </p>\r\n<p>Now run the same command from the last section to update fdm-demo-2 with the most recent data from fdm-demo:</p>\r\n<pre>./fdm -source key=fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000 -dest key=fnADnJW5CcACE6qfPsMOf111111R-l8ez111111</pre>\r\n<p>Now visit <a href=\"https://dashboard.fauna.com/collections/address/@db/fdm-demo-2\">https://dashboard.fauna.com/collections/address/@db/fdm-demo-2</a> to see that the data has been updated! Additionally, the source data's document references have been retained.</p>\r\n<h3>Example of adding documents to a collection</h3>\r\n<p>In this example, we will add new documents to the collection \"storehouses\". To do this quickly, let's return to fdm-demo and rename the \"address\" collection to \"storehouses\" by visiting <a href=\"https://dashboard.fauna.com/collections-edit/address/@db/fdm-demo\">https://dashboard.fauna.com/collections-edit/address/@db/fdm-demo</a>.</p>\r\n<p>Now, run the last command yet again:</p>\r\n<pre>./fdm -source key=fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000 -dest key=fnADnJW5CcACE6qfPsMOf111111R-l8ez111111</pre>\r\n<p>Finally, visit <a href=\"https://dashboard.fauna.com/collections/storehouses/@db/fdm-demo-2\">https://dashboard.fauna.com/collections/storehouses/@db/fdm-demo-2</a> to see that documents with new references have been inserted into the storehouses collection.</p>\r\n<h2>Importing data from non-FaunaDB sources</h2>\r\n<p>For all of the import examples in this section, we will set the destination as \"dryrun\". This is useful for checking the data format before actually copying anything over, especially if you're doing any data transformations and need to confirm that you've defined all of your fields properly. Once you feel confident in the import, substitute “dryrun” for your destination database secret.</p>\r\n<h3>Importing from a local file or directory</h3>\r\n<p>To import a file or directory, simply specify the path argument. For example:</p>\r\n<pre>./fdm -source path=/work/fauna/fdm/load/ -dest dryrun</pre>\r\n<pre>./fdm -source path=/work/fauna/fdm/load/data1000_basic.json -dest dryrun</pre>\r\n<h3>Importing over a JDBC connection</h3>\r\n<p>In this case, you first need to download the JDBC driver. Here are handy links for the <a href=\"https://dev.mysql.com/downloads/connector/j/\">MySQL</a> and <a href=\"https://jdbc.postgresql.org/download.html\">Postgres</a> drivers. </p>\r\n<p>Then, in the example below, replace the value of the \"<tt>jdbc=</tt>\" argument with the path to your driver, which will always be a jar file. Replace the value of the \"<tt>driver=</tt>\" argument with the driver name as chosen by the vendor. Replace the value of the \"<tt>url=</tt>\" argument with the connection string. Next, add your username and password. Finally, you can optionally choose a source database and/or table. If any of your JDBC arguments contain an embedded equals (i.e =), then use the fdm.props configuration file instead of the command line.</p>\r\n<pre>&lt;tt&gt;./fdm -source jdbc=/workfauna/JDBC/mysql-connector-java-8.0.18.jar  driver=com.mysql.cj.jdbc.Driver url=jdbc:mysql://localhost:3306/demo user=root password=MY_PASSWORD database=demo table=tab2 -dest dryrun&lt;/tt&gt;</pre>\r\n<h3>Importing from an AWS S3 bucket</h3>\r\n<p>In this case, you just need the path to your AWS bucket and your AWS environment variables, which will likely be already set in your environment. Here is an example of what that looks like. The only argument you should need to replace below is the bucket path.</p>\r\n<pre>&lt;tt&gt;./fdm -source aws=${AWS_SECRET_ACCESS_KEY} id=${AWS_ACCESS_KEY_ID} region=${AWS_DEFAULT_REGION} bucket=//faunadb-work/my_database -dest dryrun&lt;/tt&gt;</pre>\r\n<h2>Transforming data and ETL</h2>\r\n<p>In all of the import, export, and copy options described above, we can perform the following transformations:</p>\r\n<ol><li>Change a field or column's name</li><li>Change a field or column's data type</li><li>Set a primary field (i.e., reference ID)</li><li>Set the creation time of a document</li><li>Ignore fields</li></ol>\r\n<p>All transformations follow the same format:</p>\r\n<pre>&lt;field-name&gt;[-&gt;new-name]:&lt;field-type&gt;[(date_format)],...</pre>\r\n<p>Please see the official <a href=\"https://deploy-preview-470--fauna-docs.netlify.com/fauna/current/fdm/format\">FDM documentation</a> for all of the&nbsp;supported field types.</p>\r\n<p>Let's return to our demo databases again, this time transforming two of the field names (lname and mgr), and changing the manager field to a string:</p>\r\n<pre>&lt;tt&gt;./fdm -source key=fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000 -dest key=fnADnJW5CcACE6qfPsMOf111111R-l8ez111111 -format \"lname-&gt;last_name,mgr-&gt;manager:string\"&lt;/tt&gt;</pre>\r\n<p>Now, visit <a href=\"https://dashboard.fauna.com/collections/employee/@db/fdm-demo-2\">https://dashboard.fauna.com/collections/employee/@db/fdm-demo-2</a> to see that the two field names have changed, and the manager number has been wrapped in quotation marks to denote that it is now a string.</p>\r\n<h2>Copying a FaunaDB from a specific point in time</h2>\r\n<p>You can also optionally define a point in time with the `pit` argument, and export that database snapshot to a local folder, or another database. For example, try editing the timestamp in the command below to reflect the local date and time at which you initially created the \"fdm-demo\" database for this tutorial:</p>\r\n<pre>./fdm -source key=fnADnJHGHwACEw1QnhsY-C000000lSyB4Z000000 pit=\"2020-03-17T09:00:00\" -dest key=fnADnJW5CcACE6qfPsMOf111111R-l8ez111111</pre>\r\n<p>Now visit <a href=\"https://dashboard.fauna.com/collections/employee/@db/fdm-demo-2\">https://dashboard.fauna.com/collections/employee/@db/fdm-demo-2</a> to see that all of our changes have been reverted.</p>\r\n<p>The FDM automatically reads the following time/date formats in the `pit` argument:</p>\r\n<pre>\"dd-MM-yyyy\", \"yyyy-MM-dd\", \"MM/dd/yyyy\", \"yyyy/MM/dd\", \"yyyyMMddTHHmm\", \"dd-MM-yyyyTHH:mm\", \"yyyy-MM-ddTHH:mm\", \"MM/dd/yyyyTHH:mm\", \"yyyy/MM/ddTHH:mm\", \"yyyyMMddTHHmmss\", \"dd-MM-yyyyTHH:mm:ss\", \"yyyy-MM-ddTHH:mm:ss\", \"MM/dd/yyyyTHH:mm:ss\", \"yyyy/MM/ddTHH:mm:ss\"</pre>\r\n<p>This is useful for purposes like auditing, version control, and data recovery. To find out more information about this capability, read more about <a href=\"https://docs.fauna.com/fauna/current/tutorials/temporality.html\">FaunaDB’s temporal features</a>.</p>\r\n<h2>Backup your database yesterday at midnight</h2>\r\n<p>Regularly backing up a database to local storage is a common request. The command below provides the data in JSON format to local storage at a consistent point in time (i.e., midnight yesterday). The command takes three arguments: the key to the database being backed up, yesterday's date, and a path to a directory which should hold the data:</p>\r\n<p><strong>Mac</strong></p>\r\n<pre>./fdm -source key={database_key} pit=`date -v-1d +%F` -dest path=/work/backup</pre>\r\n<p><strong>Linux</strong></p>\r\n<pre>./fdm -source key={database_key} pit=`date -d 'yesterday' '+%F'` -dest path=/work/backup</pre>\r\n<h2>Recovering data from a directory</h2>\r\n<p>If your documents have been backed up as JSON in a directory, the FDM can easily restore them. First, create a new empty database and a key to access it. Then, execute the command below:</p>\r\n<pre>./fdm -source path=/work/backup -dest key={database_key}</pre>\r\n<p>If you must restore to an existing database, then you might have to adjust the schema and data policies. Currently, the FDM only recovers the collections (with default attributes) and the data documents, but it does have a file called “fauna_schema” which contains the definitions&nbsp;of all collections, indexes, functions, and roles.</p>\r\n<h2>Conclusion</h2>\r\n<p>We’d love to hear your thoughts on how this feature works for your use case in the #fdm channel in our <a href=\"https://community.fauna.com/\">Community Slack</a>. We will continue to ideate on how to bring more functionality to imports, including importing data from a JDBC database like MySQL or Postgres.</p>\r\n<p>With the FDM, users are able to easily import and export both small and large amounts of data into and out of a FaunaDB database. In addition, we hope that the getting started experience of bringing in your own data to FaunaDB will be greatly simplified.</p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "7040"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "601",
        "postDate": "2020-04-07T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7034,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "0672c9c6-b1d3-43be-a670-8af2fa21de56",
        "siteSettingsId": 7034,
        "fieldLayoutId": 4,
        "contentId": 2444,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Write With Fauna - Calling all Fauna Enthusiasts!",
        "slug": "write-with-fauna",
        "uri": "blog/write-with-fauna",
        "dateCreated": "2020-04-06T09:27:23-07:00",
        "dateUpdated": "2020-07-23T15:42:49-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/write-with-fauna",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/write-with-fauna",
        "isCommunityPost": false,
        "blogBodyText": "<p>The <a href=\"https://fauna.com/client-serverless\">client-serverless</a> architecture is&nbsp;fast emerging as the modern approach to building dynamic full stack applications. At Fauna, we've had a front row seat to this revolution, and&nbsp;watched the community flourish, building the coolest new SaaS platforms and mobile&nbsp;apps furiously, with the Jamstack developers leading the charge.&nbsp;<br></p>\r\n<p>The&nbsp;“<a href=\"https://www2.fauna.com/write-with-fauna\">Write with Fauna</a>” program aims to&nbsp;develop educational content for the client-serverless&nbsp;stack&nbsp;with a focus on driving&nbsp;material that dives deeper into this methodology, its ecosystem, the best practices, and tutorials, especially as they pertain to using FaunaDB.&nbsp;</p>\r\n<p>If you are building modern applications with rich clients and serverless backends using&nbsp;JavaScript frameworks,&nbsp;serverless functions, 3rd party&nbsp;APIs, GraphQL etc., alongside FaunaDB,&nbsp;the “Write with Fauna” program is a way for you to contribute to the community by writing technical content, and getting paid for doing so.</p>\r\n<p>We expect to pay between $200-$500 in reward (as well as some cool Fauna swag) for&nbsp; approved content. Exceptions may apply based on content quality. </p>\r\n<p></p>\r\n<p><a href=\"https://www2.fauna.com/write-with-fauna\"></a></p>\r\n<figure style=\"width:166px; margin:0 auto\"><a href=\"https://www2.fauna.com/write-with-fauna\"><img src=\"{asset:7107:url}\" data-image=\"7107\"></a></figure>\r\n<p></p>\r\n<h2>Program details</h2>\r\n<p>This program solicits content focused on technical education around serverless development and FaunaDB, with working sample code, wherever applicable. Examples of content include:</p>\r\n<ul><li>Front-end&nbsp;development&nbsp;with popular Javascript frameworks&nbsp;(React, Angular, Vue, RedwoodJS, Svelte, Next.js, Gatsby, etc.), serverless functions and FaunaDB</li><li>Architectural discussions or best practices&nbsp;on building full stack serverless apps<br></li><li>Data modeling strategies for GraphQL, and&nbsp;real-world business applications</li><li>FaunaDB step-by-step tutorials, tips and tricks based on your own experiences</li> <li>Stack compare and contrasts for various frameworks, pros and cons of use</li></ul>\r\n<p>To participate in the program, please follow these instructions:</p>\r\n<ol><li>Submit your proposal with a topic and an abstract using <a href=\"https://www2.fauna.com/write-with-fauna\">this form</a>.</li><li>Fauna will review your submission promptly. If selected, we will contact you to discuss the outline, schedule,&nbsp;placement, and&nbsp;reward.</li><li>The author will build the content and periodically review with our editors until completion.</li><li>Throughout the process, Fauna resources will help you with your questions, content, and edits. </li><li>Upon completion, the author will be compensated with the agreed-upon payment, determined by the length and complexity of the content.</li></ol>\r\n<p>We look forward to your participation! Please <a href=\"https://www2.fauna.com/write-with-fauna\">get started here</a>.</p>",
        "blogCategory": [
            "1461",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "7035"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "7019",
        "postDate": "2020-03-25T02:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 7018,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d9037dc1-05cb-4e8f-b69b-f66317fff533",
        "siteSettingsId": 7018,
        "fieldLayoutId": 4,
        "contentId": 2439,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Week 2 Working From Home Completed – Whew",
        "slug": "week-2-working-from-home-completed",
        "uri": "blog/week-2-working-from-home-completed",
        "dateCreated": "2020-03-25T08:51:20-07:00",
        "dateUpdated": "2020-03-25T09:34:36-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/week-2-working-from-home-completed",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/week-2-working-from-home-completed",
        "isCommunityPost": false,
        "blogBodyText": "<p>If you’ve seen my posts on LinkedIn (<a href=\"https://www.linkedin.com/in/danmeyers99/\">Dan Meyers</a>) over the past week or so, you know that not only is Fauna a 100% distributed environment, but that as an HR Leader, I have worked virtually 8 of the last 10 years leading organizations of 50-22,000 employees. In other words, I understand this working from home (WFH) thing!</p>\n<p>In the spirit of my previous posts, I wanted to share a few more helpful hints to help make this transition a success for you.&nbsp;</p>\n<h2>A new reality</h2>\n<p>For many, you are ending week 2 of WFH and reality is starting to settle in – the good and the not so great.&nbsp;Hopefully you and your co-workers/manager have settled into a groove – you’re having daily/bi-daily or weekly check-in’s via video as well as video team calls to stay in sync work wise, but also to support each other emotionally if needed. Now more than ever, you are most likely burning up the keyboard with IM’s via Slack/Skype to stay connected.</p>\n<p>But let’s talk about the reality of WFH though; it is incredible in so many ways yet challenging in others…</p>\n<p><strong>The good:</strong> no commute and the stresses/expense that comes with it, wear-n-tear on your vehicle, no public transportation, better for the environment, no waiting for coffee from Starbucks (wait, this might be a “not so good”) etc. and the time saved commuting (over 1 hour a day for most) is devoted to other more important priorities such as family or even getting ahead on work items. </p>\n<p><strong>The not so good:</strong> loss of routine, no direct human connection (isolation), distractions at home or even the “Vegas Syndrome”. </p>\n<h2>So what do we do?</h2>\n<p>Much like the ability to succeed in a traditional office setting, preservation in WFH comes from best practices, mindfulness and discipline. In a previous post, I covered off on how to avoid the “isolation” factor, so here, we’ll focus on distractions and “Vegas Syndrome”.</p>\n<h3>Distractions and Vegas Syndrome</h3>\n<figure><img src=\"{asset:7016:url}\" data-image=\"7016\"></figure>\n<p>Distractions! Oh, there are many and given the current situation (Covid-19) those distractions can be compounded by having your children/spouse/partner at home as well. What to do? I wish there was one simple answer, but there isn’t as everyone’s situation may be different. What I do suggest though is to find “your workspace” in the home – think of it as your dedicated cube/office. At the beginning of the day, talk to your spouse/partner/children and review your/their schedule (yes, your children need a schedule too – routine is key for everyone). Make sure to carve out 10-15 minute blocks throughout the day for you/them to connect or for you to do things like taking the dog out, flip the laundry over, grab a snack etc. – this is important for obvious reasons but also to avoid the “Vegas Syndrome”.</p>\n<figure><img src=\"{asset:7017:url}\" data-image=\"7017\"></figure>\n<p>“OK Dan, you’ve said it twice now. What is this “Vegas Syndrome'' you keep referring to?” </p>\n<p>Well, ever been to Las Vegas? The casinos are designed in a way that you lose all sense of time. Day? Night? No clue. Just keep gambling! Same thing can happen when you WFH if you don’t have a routine – next thing you know, 3 days have zipped by, you haven’t showered, are eating horribly and your spouse/kids/dog do not recognize you. A little over the top, but for illustrative purposes, you get the point.</p>\n<h2>How to combat the distractions</h2>\n<p><strong>Keep a similar schedule to when you went into the office.</strong> Really? Yes really, but maybe sleep in an extra 15 minutes &#x1f634;. Still get up at 6am, walk the dog/exercise, take a shower, get dressed (skip the fancy work clothes, but no sweatpants and for your sake, I hope your company still doesn’t require fancy work clothes – so 1990’s!), have breakfast with spouse/kids/dog then head to your dedicated space. Once working, have a virtual coffee/lunch with a co-worker, take those 10-15 minute blocks to get up from the desk/table and end your day at your normal time. Yes, it is fine to check email/respond to items later at night like you may normally do but limit it to 20 minutes max!</p>\n<p>As you have read here and probably on other commentaries posted on-line, there are many benefits and challenges to WFH. Ultimately though, you control your success/frustrations in this new way of working and hopefully these tips will help in your journey. This really can be a great working experience – just remember to pay attention to yourself!</p>\n<h2>#Remote-Work</h2>\n<p>Finally, since Fauna has been a 100% remote org from the start we have a lot of folks who would love to share their experiences and offer advice. Please feel free to reach out to any of us via our <a href=\"https://community.fauna.com/\">Slack community</a>. We also have a specific #remote-work channel where we can all share our experiences - positive and/or negative!</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [
            "7015"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "603",
        "postDate": "2020-03-17T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6956,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "194ed9a8-f2e2-4437-a22c-6eb7dfc1f6c0",
        "siteSettingsId": 6956,
        "fieldLayoutId": 4,
        "contentId": 2408,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Build Fearlessly Podcast - Episode 1: Dissecting Static.fun",
        "slug": "build-fearlessly-podcast-static-fun",
        "uri": "blog/build-fearlessly-podcast-static-fun",
        "dateCreated": "2020-02-27T10:48:09-08:00",
        "dateUpdated": "2020-04-21T10:26:38-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/build-fearlessly-podcast-static-fun",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/build-fearlessly-podcast-static-fun",
        "isCommunityPost": false,
        "blogBodyText": "<p><em>Since publication of this article, ZEIT has been renamed to <a href=\"https://vercel.com/blog/zeit-is-now-vercel\">Vercel</a>.</em></p>\r\n<p>---<br><br></p><p>Allen Hai (<a href=\"https://twitter.com/coetry\">@coetry</a>), software engineer at <a href=\"https://zeit.co/\">ZEIT</a>, discusses how he became interested in software engineering and the journey from his high school computer science class to his dream job at ZEIT. Allen also dives the why and how behind creating the app, <a href=\"https://static.fun/\">static.fun</a>, an open source project that demonstrates ZEIT's support of wildcard domains.</p>\r\n<figure><iframe src=\"https://player.blubrry.com/id/56663534/\" scrolling=\"no\" width=\"100%\" height=\"138px\" frameborder=\"0\"></iframe></figure>\r\n<h2>Show notes</h2>\r\n<p><strong>Static.fun Stack:</strong></p>\r\n<ul><li><a href=\"https://zeit.co/\">ZEIT Now</a></li><li><a href=\"https://fauna.com/\">FaunaDB</a></li><li><a href=\"https://nextjs.org/\">Next.js</a></li><li><a href=\"https://sendgrid.com/\">Twilio Sendgrid</a></li><li><a href=\"https://sentry.io/welcome/\">Sentry</a></li><li><a href=\"https://pusher.com/channels\">Pusher</a></li></ul>\r\n<p><strong>Mentioned resources:</strong></p>\r\n<ul><li><a href=\"https://static.fun/\">Static.fun</a></li><li><a href=\"https://zeit.co/\">ZEIT</a></li><li>Follow Allen on Twitter <a href=\"https://twitter.com/coetry\">@coetry</a></li><li><a href=\"https://developer.mozilla.org/en-US/\">Mozilla developer network</a></li><li><a href=\"https://news.ycombinator.com/\">Y combinator / Hacker News</a></li><li><a href=\"https://lambdaschool.com/\">Lambda School</a></li><li><a href=\"https://www.youtube.com/watch?v=evaMpdSiZKk\">[Video] Guillermo Rauch on Next.js:</a> Universal React Made Easy and Simple - React Conf 2017&nbsp;</li><li><a href=\"https://docs.fauna.com/fauna/current/start/graphql\">Fauna’s native GraphQL API</a></li><li><a href=\"https://svelte.dev/\">Svelte</a></li></ul>\r\n<h6>-------------------</h6>\r\n<p>To learn more about this podcast and Fauna, check out <a href=\"https://podcasts.apple.com/us/podcast/episode-0-jamstack-faunadb-and-faunas-ecosystem/id1502140620?i=1000467897112\">Episode 0: JAMstack, FaunaDB and Fauna’s ecosystem</a> on iTunes.</p>\r\n<p><em><strong>Follow the Build Fearlessly Podcast on <a href=\"https://podcasts.apple.com/us/podcast/build-fearlessly/id1502140620\">iTunes</a> to hear more stories as they develop from the developer community.</strong></em></p>",
        "blogCategory": [
            "6957"
        ],
        "mainBlogImage": [
            "6958"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2020-02-26T08:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6950,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "de7d66bd-1771-4cb6-8d81-2fd42195874d",
        "siteSettingsId": 6950,
        "fieldLayoutId": 4,
        "contentId": 2402,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The FaunaDB Extension for Visual Studio Code",
        "slug": "announcing-the-faunadb-extension-for-visual-studio-code",
        "uri": "blog/announcing-the-faunadb-extension-for-visual-studio-code",
        "dateCreated": "2020-02-24T13:35:11-08:00",
        "dateUpdated": "2020-08-26T11:31:12-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-faunadb-extension-for-visual-studio-code",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-faunadb-extension-for-visual-studio-code",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://marketplace.visualstudio.com/items?itemName=fauna.faunadb\">FaunaDB extension for VS Code</a>&nbsp;allows users to browse their FaunaDB databases, indexes, collections, documents, and functions from right inside of the VS Code sidebar. Users are also able to edit their FaunaDB documents from within VS Code and run Fauna Query Language queries within VS Code against their FaunaDB database.</p>\r\n<h2>What is VS Code?</h2>\r\n<p>VS (Visual Studio) Code is a code editor redefined and optimized for building and debugging modern web and cloud applications. <a href=\"https://code.visualstudio.com/\">VS Code</a> is free and developed by Microsoft for Windows, Linux, and macOS. It includes support for debugging, built-in commands for Git, syntax highlighting, intelligent code completion, snippets, code refactoring, and customization via extensions.</p>\r\n<h2>Getting Started</h2>\r\n<p>Be sure to install <a href=\"https://code.visualstudio.com/\">VS Code</a>, install the FaunaDB extension for VS Code, and create a <a href=\"https://dashboard.fauna.com/accounts/register\">FaunaDB account</a>.</p>\r\n<p>To get started using the FaunaDB VS Code extension, you'll need to set your secret key in order to access database information. Keys can be created in the <a href=\"https://dashboard.fauna.com/\">FaunaDB Dashboard</a> or via the <a href=\"https://github.com/fauna/fauna-shell\">Shell CLI</a>. </p>\r\n<p><em>Important: Generate an \"admin\" key, not a \"server\" key.</em></p>\r\n<p>Once you have the secret key, configure the FaunaDB extension for VS code by going to:&nbsp;<tt>Code</tt> &gt; <tt>Preferences</tt> &gt; <tt>Settings</tt> &gt; <tt>Extensions</tt> &gt; <tt>FaunaDB</tt>.</p>\r\n<p>Then paste the secret key into the Secret Key field, and reload VS Code.</p>\r\n<figure><img src=\"{asset:6946:url}\" data-image=\"6946\"></figure>\r\n<p><tt>faunadb.secretKey:</tt> Your database secret.</p>\r\n<p><em>Warning: Be careful! To avoid exposing this secret, do not commit it to your local </em><em>.vscode</em><em> configuration.</em></p>\r\n<h2>Features</h2>\r\n<ul><li>FaunaDB: Create Query</li><li>FaunaDB: Run Query</li></ul>\r\n<figure><img src=\"{asset:6947:url}\" data-image=\"6947\"></figure>\r\n<h3>Browse database</h3>\r\n<p>With this extension, you can browse the databases, indexes, collections, documents, and functions associated with your FaunaDB database right inside of the VS Code sidebar.</p>\r\n<figure><img src=\"{asset:6948:url}\" data-image=\"6948\"></figure>\r\n<h3>Run queries</h3>\r\n<p>In addition to browsing your data, this extension also allows you to run <a href=\"https://docs.fauna.com/fauna/current/api/fql/\">FQL queries</a> against your FaunaDB database.</p>\r\n<figure><img src=\"{asset:6949:url}\" data-image=\"6949\"></figure>\r\n<h2>Benefits for users of VS Code and FaunaDB</h2>\r\n<p>Using the FaunaDB extension for VS Code, users can access FaunaDB directly from the editor, a faster alternative to visiting the FaunaDB Dashboard in a web browser. </p>\r\n<p>VS Code users have a new NoSQL database option with all of the unique benefits that FaunaDB offers, including 100% ACID transactions. They can use FaunaDB as a stateful component of their apps with ease and avail of its generous free tier. Databases are available for use instantly: no additional provisioning is necessary within FaunaDB, making for seamless serverless development experience. Globally distributed data ensures that data is close to where your users are, thus enabling snappy user experience for your apps<br></p>\r\n<p>Finally, documents created via the extension can be managed via FaunaDB Dashboard as well as FaunaDB Shell and VS Code extension for hassle-free use.</p>\r\n<h2>Conclusion</h2>\r\n<p>With this extension, users are able to browse their FaunaDB databases, indexes, collections, documents, and functions from right inside of the VS Code sidebar.</p>\r\n<p>Please visit <a href=\"https://docs.fauna.com/\">the FaunaDB documentation</a> to learn more. And please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate your feedback into a future release.</p>\r\n<p>What other integrations would you like to see implemented in FaunaDB? Please reach out to me on <a href=\"http://twitter.com/lew\">Twitter</a> or our <a href=\"https://community-invite.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB and Netlify an obvious choice for your next project.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "6952"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1740",
        "postDate": "2020-02-12T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6872,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "308bd498-3b7f-4282-946c-89a111187e12",
        "siteSettingsId": 6872,
        "fieldLayoutId": 4,
        "contentId": 2369,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing Built-in Collection Indexes",
        "slug": "announcing-built-in-collection-indexes",
        "uri": "blog/announcing-built-in-collection-indexes",
        "dateCreated": "2020-02-11T08:27:12-08:00",
        "dateUpdated": "2020-02-12T12:01:16-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-built-in-collection-indexes",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-built-in-collection-indexes",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce a new FQL capability that empowers users to write more concise and powerful FQL statements: built-in collection indexes, along with the Documents function!</p>\n<h2>Built-in collection indexes (Documents function)&nbsp;<sup><em>Preview</em></sup></h2>\n<p>With built-in collection indexes, you no longer need to create your own “collection index” just to track all of the documents that exist within a collection.</p>\n<p>For example, if you have a collection of documents called “widgets”, you would normally have created a collection index:</p>\n<pre>CreateIndex({\n  name: \"all_widgets\",\n  Source: Collection(\"widgets\")\n})</pre>\n<p>So that you could then retrieve all of the “widgets” documents with:</p>\n<pre>Paginate(Match(Index(\"all_widgets\")))</pre>\n<p>Now, that “all_widgets” index is unnecessary. You can simply use this query:</p>\n<pre>Paginate(Documents(Collection(\"widgets\")))</pre>\n<p>The advantages are:</p>\n<ul><li>You no longer need to create one index per collection, just to track a collection’s documents.</li><li>You can remove any existing collection indexes (provided that you update queries that involve them) to save on storage.</li></ul>\n<p>Note: As a <a href=\"https://fauna.com/blog/software-naming-releases\">Preview function</a>, Documents is currently only supported in the JavaScript, Java, and Scala drivers for FaunaDB. Support for Documents in the C#, Go, and Python drivers will be available in a future release.</p>\n<h2>Conclusion</h2>\n<p>With our latest release, users now have access to built-in collection indexes, the new Documents function, and some internal bug fixes and improvements that increase stability and performance.</p>\n<p>As the Documents function is in Preview, please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate feedback into the formal release. Visit our <a href=\"https://docs.fauna.com/\" target=\"_blank\">documentation</a> to learn more.</p>\n<p>What other functions would you like to see implemented in FaunaDB? Please reach out to me on our <a href=\"https://community-invite.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>\n<p></p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "6899"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6380",
        "postDate": "2020-02-06T02:30:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6814,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "c2508758-a963-4964-b546-6b5f99ab29ca",
        "siteSettingsId": 6814,
        "fieldLayoutId": 4,
        "contentId": 2351,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "A Comparison of Serverless Function (FaaS) Providers",
        "slug": "comparison-faas-providers",
        "uri": "blog/comparison-faas-providers",
        "dateCreated": "2020-02-04T04:34:20-08:00",
        "dateUpdated": "2020-06-30T04:53:55-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/comparison-faas-providers",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/comparison-faas-providers",
        "isCommunityPost": false,
        "blogBodyText": "<p>New Function-as-a-service (FaaS) providers are rising and gaining adoption. How do these new providers differentiate\r\n  themselves from the existing big players? We will try to answer these questions by exploring the capabilities of both\r\n  the more popular FaaS offerings and the new players. We will look at where their focus lies, the limitations, what\r\n  programming languages they support, and their pricing models. The goal is to build a reference for those looking to\r\n  choose between providers. Since this should be a collaborative effort, feel free to contact us if you would like to\r\n  see something added.</p>\r\n<p>Before we dive in, let’s briefly discuss the advantages of FaaS over other solutions. Why would you choose to go\r\n  serverless? </p>\r\n<h2>How does FaaS compare to PaaS and CaaS</h2>\r\n<h3>Functions as a Service (FaaS) vs. Platform as a service (PaaS)</h3>\r\n<p>At the highest level, the choice between PaaS and FaaS is a choice of control versus ease of use, and a choice\r\n  between architectures (monolith versus microservices). A platform such as Heroku is the first choice for many proof of\r\n  concepts because getting started is easy and it has powerful tooling to make a developer’s life easier. Typically,\r\n  such applications are monoliths that scale by adding nodes with another instance of the whole application (e.g., in\r\n  Heroku terms: add more dynos). An application can go a long way on such a platform without running into performance\r\n  problems. Still, if one part in your code becomes a bottleneck, this monolith approach makes it harder to scale your\r\n  application. Once the load on your application increases, PaaS providers typically become expensive (<a href=\"https://blog.boltops.com/2018/04/22/heroku-vs-ecs-fargate-vs-ec2-on-demand-vs-ec2-spot-pricing-comparison\">1</a>).\r\n</p>\r\n<p>FaaS, on the other hand, requires you to break up applications into functions, resulting in a microservices\r\n  architecture. In a FaaS-based application, scaling happens at the function level, and new functions are spawned as\r\n  they are needed. This lower granularity avoids bottlenecks, but also has a downside since many small components\r\n  (functions) require orchestration to make them cooperate. Orchestration is the extra configuration work that is\r\n  necessary to define how functions discover each other or talk to each other.</p>\r\n<table>\r\n  <tbody>\r\n    <tr>\r\n      <th></th>\r\n      <th><p><strong>Advantages</strong></p></th>\r\n      <th><p><strong>Disadvantages</strong></p></th>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Platforms (PaaS)</strong></p></td>\r\n      <td><ul>\r\n          <li>very easy to get started</li>\r\n        </ul></td>\r\n      <td><ul>\r\n          <li>less control, especially over scaling</li>\r\n          <li>in general, more expensive</li>\r\n          <li>vendor lock-in</li>\r\n          <li>your process can go down</li>\r\n        </ul></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Functions (FaaS)</strong></p></td>\r\n      <td><ul>\r\n          <li>more control, less flexibility</li>\r\n          <li>a function that crashes won’t bring down your app</li>\r\n        </ul></td>\r\n      <td><ul>\r\n          <li>the orchestration of many functions results in more configuration</li>\r\n        </ul></td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<h3>Containers (CaaS) <strong>vs</strong> Functions (FaaS)</h3>\r\n<p>Although containers and functions solve a very different problem, they are often compared since the type of\r\n  applications that are built with them can be very similar. Both provide ways to easily deploy a piece of code. In the\r\n  container case, one typically still has to decide how many instances need to run while functions are 100%\r\n  auto-scaling. There will always be one container running for a specific service, waiting until it receives a task,\r\n  while a function only runs on request. Containers start up fairly quickly, but are not fast enough to catch up with\r\n  quick bursts in traffic. In contrast, functions just scale along with the burst. </p>\r\n<p>In general, containers require much more configuration, especially because scaling rules must be tweaked in order not\r\n  to over- or underprovision. In FaaS, over- or underprovisioning becomes the concern of the provider while clients are\r\n  provided with a pay-as-you-go model. The pay-as-you-go model is quickly becoming the definition of \"serverless\" since\r\n  it abstracts away the last indication of servers from the developer. Of course, the servers that execute your\r\n  functions are still there and the provider needs to make sure that there are enough of them when a client launches new\r\n  functions. In order to make this manageable, there are typically memory, payload, and execution time limits in place.\r\n  These limits make it easier for FaaS providers to estimate the load at a given time and are the prices we pay to be\r\n  able to run code without caring about servers. </p>\r\n<table>\r\n  <tbody>\r\n    <tr>\r\n      <th></th>\r\n      <th><p><strong>Advantages</strong></p></th>\r\n      <th><p><strong>Disadvantages</strong></p></th>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>Containers (CaaS)</strong></td>\r\n      <td><ul>\r\n          <li>portability (can run everywhere)</li>\r\n          <li>flexibility in environment and language</li>\r\n          <li>no vendor lock-in</li>\r\n          <li>no cold starts, if you keep the containers running</li>\r\n          <li>does not suffer from the typical FaaS&nbsp;memory/payload/execution limits</li>\r\n        </ul></td>\r\n      <td><ul>\r\n          <li>pay for what you don’t use.</li>\r\n          <li>scaling is slower</li>\r\n          <li>scaling configuration required</li>\r\n          <li>need to install your environment</li>\r\n          <li>your process can still go down since containers are meant to keep running</li>\r\n        </ul></td>\r\n    </tr>\r\n    <tr>\r\n      <td><strong>Functions (FaaS)</strong></td>\r\n      <td><ul>\r\n          <li>granular scaling</li>\r\n          <li>no need to install software</li>\r\n          <li>functions don't go down, a new function will run anyway when one crashes</li>\r\n          <li>no scaling configuration</li>\r\n          <li>faster scaling</li>\r\n          <li>pay-as-you-go</li>\r\n        </ul></td>\r\n      <td><ul>\r\n          <li>cold starts</li>\r\n          <li>high granularity, the orchestration of many functions can result in more configuration then needed\r\n          </li>\r\n          <li>memory, payload, and execution limits</li>\r\n        </ul></td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<h2>FaaS providers compared</h2>\r\n<p>Focus: <strong>ease-of-use</strong> vs <strong>configurability </strong>vs <strong>edge computing. </strong>Of\r\n  course, people choose a cloud ecosystem for different reasons (e.g., free Azure credits, deployment via\r\n  CloudFormation, or a specific feature like Google Dataflow) and the logical choice is usually the provider you already\r\n  use. If the ecosystem is not the deciding factor, the focus of the FaaS provider is often the reason why a company\r\n  chooses a particular provider. </p>\r\n<h3>Generality and configurability: </h3>\r\n<p>The biggest providers, such as <a href=\"https://azure.microsoft.com/\">Azure</a>, <a href=\"https://cloud.google.com/\">Google</a>, and <a href=\"https://aws.amazon.com/\">AWS</a>, focus on\r\n  configurability. Their function offerings are often a vital part of communication between other services of their\r\n  ecosystem. The trigger mechanism is therefore separated from the function allowing the function to be activated by\r\n  many things such as database triggers, queue triggers, custom events based on logs, scheduled events, or load\r\n  balancers. If all you need is a REST API based on functions, then this might seem cumbersome. For example, when you\r\n  write an API based on AWS Lambda functions, you need to write the function, deploy it, set up security rules to be\r\n  able to execute them, and configure how they are triggered (e.g., via a load balancer). The number of configuration\r\n  possibilities can make the documentation seem daunting.</p>\r\n<h3>Easy-of-use and developer experience: </h3>\r\n<p>Other providers such as <a href=\"http://netlify.com/\">Netlify</a> and Vercel (formerly&nbsp;<a href=\"https://zeit.co/\">ZEIT</a>) respond to\r\n  that by focusing on a more narrow use case and ease-of-use. In this case, one command is often enough to transform the\r\n  function into a REST or GraphQL API that is ready to be consumed. The starting barrier and learning curve is greatly\r\n  reduced because the functions serve a more specific purpose (APIs). Besides that, they provide an impressive toolchain\r\n  to easily debug, deploy, and version your functions. For example, both support file system routing which allows you to\r\n  simply drop a function in a folder so that it becomes accessible as an API with the same path, which also works\r\n  locally. Since the functions live on the same origin as your frontend code, you do not have to deal with CORs issues.\r\n</p>\r\n<p>Their function offering also integrates perfectly with the rest of their offerings, which are aimed at facilitating\r\n  JAMstack applications and typical Single Page Applications (SPAs). JAMstack sites are relatively static sites that\r\n  make heavy use of serverless offerings to populate the smaller dynamic parts. </p>\r\n<ul>\r\n  <li>Vercel Functions: Deploy with the <a href=\"https://github.com/zeit/now\">Now</a> ecosystem and you have a scalable\r\n    API under the /api endpoint that can also be simulated locally. When using Functions inside a <a href=\"https://nextjs.org/\">Next.js</a> app, you receive the benefits of Webpack and Babel.<br><br></li>\r\n  <li>Netlify Functions: Deploy with the Netlify CI by just pushing your code to Github. By integrating with their\r\n    Identify product (similar to Auth0), Functions automatically receive user information. Netlify also provides\r\n    one-click add-ons such as databases from which the environment variables will be automatically injected.<br><br>\r\n  </li>\r\n</ul>\r\n<p>You can say that Netlify and Vercel compare to AWS/Azure Functions/Google Functions as a PaaS platform like Heroku\r\n  compares to AWS: they abstract away the complexities of functions to provide a very smooth developer experience with\r\n  less setup and overhead. Under the hood, they both rely on AWS to run their functions. Their free tier does not only\r\n  include function invocations; it also comes with everything you need to deploy a website such as hosting, CI builds,\r\n  and CDN bandwidth.</p>\r\n<h3>Edge computing</h3>\r\n<p>If low latency is necessary and data always needs to be real-time, then <a href=\"https://www.cloudflare.com/products/cloudflare-workers/\">Cloudflare Workers</a>, <a href=\"https://www.stackpath.com/products/edge-computing/serverless-scripting/\">EdgeEngine</a>, <a href=\"https://aws.amazon.com/lambda/edge/\">Lambda@Edge</a>, or <a href=\"https://fly.io/\">Fly.io</a> are probably the\r\n  best choices. Their focus is essentially edge computing, or in other words, bringing the functions as close as\r\n  possible to the end-users to reduce latency to an absolute minimum. At the time of writing (November, 2019),\r\n  Cloudflare Workers have 194 points of presence, EdgeEngine sports 45+ locations, and Fly.io is at 16 locations. </p>\r\n<p>Most of them run functions straight on the V8 engine instead of NodeJS, which allows for lower latency than their\r\n  competitors. Due to that, they initially only supported JavaScript but all have recently added support for WebAssembly which should (theoretically) allow most languages. Fly.io recently dropped the V8 runtime to allow for more&nbsp;intensive workloads and worked&nbsp;around cold-starts by keeping processes running.&nbsp;EdgeEngine and Cloudflare\r\n  Workers are probably not meant for very CPU intensive tasks since the runtime limitations are expressed in CPU time. A\r\n  function can exist for a long time, as long as it does not actively use the CPU longer than the runtime limitation.\r\n  For example, an idle function that is waiting for the result of a network call is not spending any CPU time. This makes these providers a great choice to build a 'middleware' backend that spends most of its time forwarding and waiting on&nbsp;requests.&nbsp;&nbsp;</p>\r\n<p>Fly.io differentiates by focusing on a&nbsp;powerful API and&nbsp;an open-source runtime which allows you to build and test your apps in a local environment. In a recent update, they moved away from the V8 engine in favour of docker which removed typical serverless&nbsp;limits and explains&nbsp;why their pricing is now longer&nbsp;based on the duration of a call.&nbsp;This turns Fly.io&nbsp;in a service to run docker images on servers in different cities. This service comes with a&nbsp;global router to connect users to the nearest available instance and the automatic addition of VMs on popular locations as your application's traffic increases. This makes Fly.io a great service to run heavier tasks close to the edge such as a full-fledged rails application.</p>\r\n<p>It is noteworthy that Vercel provides a unique <a href=\"https://zeit.co/docs/v2/serverless-functions/edge-caching\">Edge\r\n    Caching </a>system which approximates the serverless edge experience. This system caches data from your serverless\r\n  functions at configurable intervals, which gives users fast data access, although the data is not real-time. </p>\r\n<h3>Do it yourself</h3>\r\n<p>Finally, there is a whole new set of offerings that are starting to move away from the serverless aspect of FaaS.\r\n  Many have bumped into the limits (payload, memory) of functions and then tried to get around those limits. The typical\r\n  solution was to start running Docker containers. Frameworks like OpenFaaS, the FN Project, Fission, OpenWhisk, and\r\n  Kubeless are aiming to provide a framework that allows you to deploy your own FaaS solution. They aim to deliver a\r\n  similar experience to a true FaaS provider, but it’s important to note that this FaaS is all but serverless since you\r\n  are again responsible for managing clusters and scaling. One can argue that deploying Kubeless on a managed Kubernetes\r\n  service comes very close and it would not be surprising if these open source frameworks motivate other companies to\r\n  start their own FaaS offerings. In fact, some of these already serve as the basis for a commercial service provider.\r\n  OpenWhisk powers IBM's FaaS offering while a fork of the Fn project is the technology behind Oracle Functions. Fly.io\r\n  also deserves a space in this category since they rolled out an open source runtime. The future will definitely be\r\n  interesting; we expect to see FaaS offerings for each niche. </p>\r\n<table>\r\n  <tbody>\r\n    <tr>\r\n      <th colspan=\"6\"><p><strong>Comparing the focus of FaaS providers</strong></p></th>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n      <td class=\"subheader\" colspan=\"6\"><p><strong>Do it yourself</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p>Open-</p>\r\n<p>FaaS</p></td>\r\n      <td><p>Fn project</p></td>\r\n      <td><p>Fission</p></td>\r\n      <td><p>Open-</p>\r\n<p>Whisk</p></td>\r\n      <td><p>Kubeless</p></td>\r\n      <td><p>Fly.io*</p></td>\r\n    </tr>\r\n\r\n    <tr class=\"subheader-row\">\r\n      <td class=\"subheader\" colspan=\"6\"><p><strong>Configurability</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p>AWS </p>\r\n<p>Lambda</p></td>\r\n      <td><p>Google </p>\r\n<p>Functions</p></td>\r\n      <td><p>Azure</p>\r\n<p>Functions</p></td>\r\n      <td><p>IBM</p>\r\n<p>Cloud Functions</p></td>\r\n      <td><p>Alibaba</p>\r\n<p>Functions</p></td>\r\n      <td><p>Oracle </p>\r\n<p>Functions</p></td>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n      <td class=\"subheader\" colspan=\"6\"><p><strong>Ease-of-use</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td colspan=\"2\"><p>Vercel Functions</p></td>\r\n      <td colspan=\"2\"><p>Netlify Functions</p></td>\r\n      <td colspan=\"2\"><p>Fly.io*</p></td>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n      <td class=\"subheader\" colspan=\"6\"><p><strong>Edge computing</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p>Cloudflare Workers</p></td>\r\n      <td><p>Edge</p>\r\n<p>Engine</p></td>\r\n      <td><p>Fly.io</p></td>\r\n      <td><p>Lambda</p>\r\n<p>@Edge</p></td>\r\n      <td colspan=\"2\"><p>Vercel Functions + Edge</p>\r\n<p>Cache*</p></td>\r\n    </tr>\r\n  </tbody>\r\n\r\n</table>\r\n<div class=\"caption\">\r\n  <p>A table categorizing all FaaS providers according to their main focus</p>\r\n  <p>Providers starred with * have noteworthy features in that domain although it’s not their main focus</p>\r\n</div>\r\n<h3>Limits</h3>\r\n<p>One of the main pitfalls of using Functions as a Service is the limits. Some developers are not aware of those when\r\n  they get started, or they underestimate how easily an application can bump into these limits over time. The number of\r\n  workarounds that can be found online where results are stored temporarily on another location such as AWS S3 shows how\r\n  many developers bumped into these. Do-it-yourself solutions are left out of the comparison since those typically allow\r\n  extensive configuration of the limitations. These limitations are there for a reason; they make sure that the load\r\n  remains predictable for the provider, which allows them to provide lower latencies and better scaling. Azure is the\r\n  only provider that has no payload limits, which might explain why they are a bit behind performance-wise. Programmers\r\n  have struggled with these limitations and have found several <a href=\"https://www.stackery.io/blog/RequestEntityTooLargeException-aws-lambda-message-invocation-limits/\">creative\r\n    solutions </a>around them. Some providers came up with their own solutions to improve the developer experience.&nbsp;</p>\r\n<table class=\"comparison-smaller\">\r\n  <tbody>\r\n    <tr>\r\n      <td class=\"invisible-cell\" rowspan=\"2\"></td>\r\n      <th colspan=\"2\"><p><strong>Memory (MB)</strong></p></th>\r\n      <th colspan=\"2\"><p><strong>Execution time</strong></p></th>\r\n      <th colspan=\"2\"><p><strong>Payloads (MB)</strong></p></th>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n      <td><p><strong>Default</strong></p></td>\r\n      <td><p><strong>Max</strong></p></td>\r\n      <td><p><strong>Default</strong></p></td>\r\n      <td><p><strong>Max</strong></p></td>\r\n      <td><p><strong>Request</strong></p></td>\r\n      <td><p><strong>Response</strong></p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n\r\n      <td colspan=\"7\"><p><strong>Non-Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Vercel Functions</strong></p></td>\r\n      <td><p>1024</p></td>\r\n      <td><p>3008</p></td>\r\n      <td><p>10s-900s*2</p></td>\r\n      <td><p>10s-900s</p></td>\r\n      <td><p>5</p></td>\r\n      <td><p>5</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Netlify Functions</strong></p></td>\r\n      <td><p>1024</p></td>\r\n      <td><p>1024*3</p></td>\r\n      <td><p>10s*3</p>\r\n        <br></td>\r\n      <td><p>10s*3</p></td>\r\n      <td><p>6</p></td>\r\n      <td><p>6</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>IBM Cloud Functions</strong></p></td>\r\n      <td><p>256</p></td>\r\n      <td><p>2048</p></td>\r\n      <td><p>1m</p></td>\r\n      <td><p>10m</p></td>\r\n      <td><p>5</p></td>\r\n      <td><p>5</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Google cloud Functions</strong></p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>2048</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>540s</p></td>\r\n      <td><p>10</p></td>\r\n      <td><p>10</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Oracle Functions</strong> *4</p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>1024</p></td>\r\n      <td><p>30s</p></td>\r\n      <td><p>120s</p></td>\r\n      <td><p>6</p></td>\r\n      <td><p>6 </p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Azure Functions</strong> *5</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>1536</p></td>\r\n      <td><p>5m</p></td>\r\n      <td><p>10m</p></td>\r\n      <td><p>No limit</p></td>\r\n      <td><p>No limit</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>AWS Lambda</strong></p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>3008</p></td>\r\n      <td><p>3s</p></td>\r\n      <td><p>15m</p></td>\r\n      <td><p>6*1</p>\r\n        <br></td>\r\n      <td><p>6*1</p>\r\n        <br></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Alibaba Functions</strong></p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n      <td><p>600s</p></td>\r\n      <td><p>6*1</p></td>\r\n      <td><p>6*1</p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n\r\n      <td colspan=\"7\"><p><strong>Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Cloudflare Workers</strong></p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>No limit\r\n          <br>10ms CPU*6</p></td>\r\n      <td><p>No limit\r\n          <br>10ms CPU </p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>EdgeEngine</strong></p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>15s\r\n          <br>5ms CPU*6</p></td>\r\n      <td><p>15s\r\n          <br>5ms CPU</p></td>\r\n      <td><p class=\"na\">?</p></td>\r\n      <td><p class=\"na\">?</p>\r\n        <br></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>AWS Lambda@Edge</strong></p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>128</p></td>\r\n      <td><p>30s</p></td>\r\n      <td><p>5s</p></td>\r\n      <td><p>50Mb</p></td>\r\n      <td><p>40 KB</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Fly.io</strong> *7</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>No limit</p></td>\r\n      <td><p>No limit</p></td>\r\n      <td><p>No limit</p></td>\r\n      <td><p>No limit</p></td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<div class=\"caption-left-aligned\">\r\n\r\n  <p> *1: These are the specifics for synchronous requests. Asynchronous requests only provide 128Kb.</p>\r\n  <p> *2: Vercel’s execution time limits change depending on the plan and range between 10 seconds and 15 minutes.</p>\r\n  <p> *3: Netlify allows you to tweak your function limits (probably within AWS limits) for custom plans. </p>\r\n  <p> *4: Oracle Functions also limit the amount of applications (10) and functions (20) per tenancy.</p>\r\n  <p> *5: Azure Functions have no specified limits, but the host machine’s limit is 1.5Gb.</p>\r\n  <p> *6: Cloudflare Workers and EdgeEngine are measured in CPU time. A function can use up to x ms of CPU time but can\r\n    run for a long time as long as it’s waiting and not using the CPU. Cloudflare has a limit of 10ms or 50 ms CPU\r\n    depending on the plan and no function duration limit while EdgeEngine has 5ms and a function limit of 15s and offers\r\n    custom plans via sales. </p>\r\n  <p> *7: Fly.io moved&nbsp;towards a global container runtime offering. The memory limits depend on the machine you select to run your container on, Besides of that there are no limits.<br></p>\r\n</div>\r\n<h2>Pricing<br></h2>\r\n<p>The main four FaaS providers (Azure, AWS, Google, IBM) are very <a href=\"https://www.altexsoft.com/blog/cloud/comparing-serverless-architecture-providers-aws-azure-google-ibm-and-other-faas-vendors/\">comparable</a>.\r\n  Only Google comes out more expensive as can be verified using this online <a href=\"http://serverlesscalc.com/\">calculator</a>. The new kids on the block are slightly pricier since they do\r\n  things quite differently. They abstract away a lot of work for you by automatically provisioning load balancers, or\r\n  making the deployment process much easier with local development tools, debugging tools, versioning, etc. These\r\n  providers aim to eliminate devops completely, so comparing their prices with bare-metal FaaS providers is like\r\n  comparing Heroku pricing with AWS. Pricing for custom setups is left out since they will basically depend on your own\r\n  infrastructure. </p>\r\n<table class=\"comparison-smaller\">\r\n  <tbody>\r\n    <tr>\r\n      <td rowspan=\"2\"></td>\r\n      <th colspan=\"3\"><p><strong>Free tier</strong></p></th>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n\r\n      <td><p><strong>Requests</strong></p></td>\r\n      <td><p><strong>GB seconds</strong></p></td>\r\n      <td><p><strong>Hours / Month</strong></p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n      <td></td>\r\n      <td colspan=\"3\"><p><strong>Non-Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Netlify</strong></p></td>\r\n      <td><p>125K *</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>100 *</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Vercel</strong></p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>20 *</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>IBM Cloud Functions</strong></p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p>400K</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Oracle Functions</strong></p></td>\r\n      <td><p>2M</p></td>\r\n      <td><p>400K</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Alibaba Functions</strong></p></td>\r\n      <td><p>1M</p></td>\r\n      <td><p>400K</p></td>\r\n      <td></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Google</strong>\r\n          <br><strong>Functions</strong></p></td>\r\n      <td><p>2M</p></td>\r\n      <td><p>400K</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>AWS Lambda</strong></p></td>\r\n      <td><p>1M</p></td>\r\n      <td><p>400K</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Azure Functions</strong></p></td>\r\n      <td><p>1M</p></td>\r\n      <td><p>400K</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n      <td></td>\r\n      <td colspan=\"3\"><p><strong>Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Cloudflare Workers</strong></p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>EdgeEngine</strong></p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Fly.io</strong></p></td>\r\n      <td colspan=\"3\"><p>$10/mo of service credit</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Lambda@Edge</strong></p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n      <td><p class=\"na\">NA</p></td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<div class=\"caption-left-aligned\">\r\n  <p>*: Both Netlify as Vercel have a pricing model which starts with different plans, starting with a free plan. Other\r\n    plans start with a basic fee (which includes other services besides functions). Depending on the plan you choose,\r\n    these prices might differ, and there are also custom plans available.</p>\r\n  <p></p>\r\n</div>\r\n<table class=\"comparison-smaller\">\r\n  <tbody>\r\n    <tr>\r\n      <td rowspan=\"2\"></td>\r\n      <th colspan=\"3\"><p><strong>Production pricing</strong></p></th>\r\n    </tr>\r\n    <tr class=\"subheader-row\">\r\n      <td><p><strong>Requests</strong></p></td>\r\n      <td><p><strong>GB seconds</strong></p></td>\r\n      <td><p><strong>Hours / Month</strong></p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n      <td></td>\r\n      <td colspan=\"3\"><p><strong>Non-Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Netlify</strong></p></td>\r\n      <td><p>$0.000038</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p>1h per 1000 requests</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Vercel</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p>$0.2 / h</p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>IBM Cloud Functions</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p>$0.000017</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Oracle Functions</strong></p></td>\r\n      <td><p>$0.0000002 </p></td>\r\n      <td><p>$0.00001417</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Alibaba Functions</strong></p></td>\r\n      <td><p>$0.0000002</p></td>\r\n      <td><p>$0.00001668</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Google</strong>\r\n          <br><strong>Functions</strong></p></td>\r\n      <td><p>$0.0000004</p></td>\r\n      <td><p>$0.0000025</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>AWS Lambda</strong></p></td>\r\n      <td><p>$0.0000002</p></td>\r\n      <td><p>$0.00001667</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Azure Functions</strong></p></td>\r\n      <td><p>$0.0000002</p></td>\r\n      <td><p>$0.000016</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr class=\"separator-row\">\r\n      <td></td>\r\n      <td colspan=\"3\"><p><strong>Edge offerings</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Cloudflare Workers</strong></p></td>\r\n      <td><p>$0.0000005*2</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>EdgeEngine</strong></p></td>\r\n      <td><p>$0.0000006*1</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Fly.io</strong></p></td>\r\n      <td><p>$0.000001015 / sec *3<br></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n    <tr>\r\n      <td><p><strong>Lambda@Edge</strong></p></td>\r\n      <td><p>0,0000006</p></td>\r\n      <td><p>$0.00005001</p></td>\r\n      <td><p><strong>NA</strong></p></td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<div class=\"caption-left-aligned\">\r\n  <p>*1: EdgeEngine starts with a $10/month subscription which includes 15M requests. </p>\r\n  <p>*2: Cloudflare Workers starts with a $5/month subscription which includes 10M requests. Cloudflare Workers include\r\n    storage which is billed separately.</p><p>*3: Fly.io pricing depends on the selected instance since their recent update:&nbsp;<a href=\"https://fly.io/docs/pricing/\">https://fly.io/docs/pricing/</a></p>\r\n</div>\r\n<h3>Languages</h3>\r\n<p>Most engineers prefer to program in a specific language. When moving to FaaS, this is not always possible since not\r\n  every language is supported by the providers. Additionally, some languages have significantly higher cold starts than\r\n  others. In theory, when deploying your own custom FaaS, any language is possible since they are built on top of Docker\r\n  and meant to be extensible. Of course, that might require a lot of work when providers such as IBM Cloud&nbsp;Functions and\r\n  Oracle Functions, who build upon OpenWhisk and Fn Project respectively, do not already have native support for all\r\n  languages. This table provides an overview of supported languages per service. Only the darker green ones can be\r\n  considered fully supported. </p>\r\n<div class=\"ritz grid-container\" dir=\"ltr\"> <table class=\"waffle\" cellspacing=\"0\" cellpadding=\"0\"> <thead> <tr> <th class=\"row-header freezebar-origin-ltr\"></th> <th id=\"0C0\" style=\"width:100px\" class=\"column-headers-background\">A</th> <th id=\"0C1\" style=\"width:100px\" class=\"column-headers-background\">B</th> <th id=\"0C2\" style=\"width:100px\" class=\"column-headers-background\">C</th> <th id=\"0C3\" style=\"width:100px\" class=\"column-headers-background\">D</th> <th id=\"0C4\" style=\"width:100px\" class=\"column-headers-background\">E</th> <th id=\"0C5\" style=\"width:100px\" class=\"column-headers-background\">F</th> <th id=\"0C6\" style=\"width:100px\" class=\"column-headers-background\">G</th> <th id=\"0C7\" style=\"width:100px\" class=\"column-headers-background\">H</th> <th id=\"0C8\" style=\"width:100px\" class=\"column-headers-background\">I</th> <th id=\"0C9\" style=\"width:100px\" class=\"column-headers-background\">J</th> <th id=\"0C10\" style=\"width:100px\" class=\"column-headers-background\">K</th> <th id=\"0C11\" style=\"width:100px\" class=\"column-headers-background\">L</th> <th id=\"0C12\" style=\"width:100px\" class=\"column-headers-background\">M</th> <th id=\"0C13\" style=\"width:100px\" class=\"column-headers-background\">N</th> <th id=\"0C14\" style=\"width:100px\" class=\"column-headers-background\">O</th> <th id=\"0C15\" style=\"width:100px\" class=\"column-headers-background\">P</th> <th id=\"0C16\" style=\"width:100px\" class=\"column-headers-background\">Q</th> <th id=\"0C17\" style=\"width:100px\" class=\"column-headers-background\">R</th> </tr> </thead> <tbody> <tr style=\"height:20px;\"> <th id=\"0R0\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">1</div></th> <td class=\"s0\"></td> <td class=\"s1\">OpenFaaS</td> <td class=\"s1\">Fn project</td> <td class=\"s1\">Fission</td> <td class=\"s1\">OpenWhisk</td> <td class=\"s1\">Kubel <br>-ess</td> <td class=\"s1\">AWS <br>Lambda</td> <td class=\"s1\">Google <br>Functions</td> <td class=\"s1\">Azure <br>Functions</td> <td class=\"s1\">IBM <br>Cloud Functions</td> <td class=\"s1\">Alibaba <br>Functions</td> <td class=\"s1\">Oracle <br>Functions</td> <td class=\"s1\">Vercel&nbsp;<br>Functions</td> <td class=\"s1\">Netlify <br>Functions</td> <td class=\"s1\">Cloudflare <br>Workers</td> <td class=\"s1\">EdgeEngine</td> <td class=\"s2\">Fly.io</td> <td class=\"s1\">Lambda@Edge</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R1\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">2</div></th> <td class=\"s3\">JS</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R2\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">◉</div></th> <td class=\"s3\">Go</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R3\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">4</div></th> <td class=\"s3\">Python</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">▲</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R4\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">5</div></th> <td class=\"s3\">Ruby</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R5\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">6</div></th> <td class=\"s3\">Java</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◉</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">▲</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R6\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">7</div></th> <td class=\"s3\">C#</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R7\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">8</div></th> <td class=\"s3\">C++</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R8\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">9</div></th> <td class=\"s3\">Elixir</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R9\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">10</div></th> <td class=\"s3\">Haskell</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R10\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">11</div></th> <td class=\"s3\">Erlang</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R11\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">12</div></th> <td class=\"s3\">Cobol</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R12\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">13</div></th> <td class=\"s3\">Rust</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R13\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">14</div></th> <td class=\"s3\">PHP</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">▲</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R14\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">15</div></th> <td class=\"s3\">Swift</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s4\">✔︎</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">▲</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R15\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">16</div></th> <td class=\"s3\">Bash</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s4\">✔︎</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◼︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">▲</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> <tr style=\"height:20px;\"> <th id=\"0R16\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">17</div></th> <td class=\"s3\">Powersh</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s6\">◉</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> <td class=\"s6\">▲</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s6\">◉</td> <td class=\"s5\">✕</td> <td class=\"s5\">✕</td> <td class=\"s6\">◆</td> <td class=\"s6\">◆</td> <td class=\"s4\">✔︎</td> <td class=\"s5\">✕</td> </tr> </tbody> </table></div>\r\n<div class=\"caption-left-aligned\">\r\n<p>◼︎ AWS Lambda runtimes in theory allow you to run any language but require extra work. At the moment, custom runtimes\r\n  were announced, they open sourced C++ and Rust runtimes, while other partners developed runtimes for Elixir, Elrang,\r\n  PHP, and Cobol. </p>\r\n<p>▲ Azure has experimental support for these languages, but it is not advised to use them in production.</p>\r\n<p>◉ Projects like OpenFaaS, Fn project, and OpenWhisk can in theory run any Docker container as a function. Therefore,\r\n  any language is supported, but it might require extra work. For example, OpenFaaS requires you to create OpenFaaS\r\n  templates, and OpenWhisk requires you to write something called Docker actions. </p>\r\n<p>◆ via Web assembly so we assume at the moment that if it compiles to WebAssembly, it can be run.</p>\r\n</div>\r\n<h3>Performance</h3>\r\n<p>Performance is typically the difference between an engaged customer and a bored client. FaaS performance is measured\r\n  in function call latency. This is very hard to compare across providers for two reasons: </p>\r\n<ul>\r\n  <li><strong>Cold starts: </strong>When a function starts for the first time, it will respond slower than usual.</li>\r\n  <li><strong>Different idle instance lifetimes:</strong> Each provider keeps its functions alive for a different amount\r\n    of time to mitigate cold starts.</li>\r\n  <li><strong>Language and technology-specific: </strong>Cold starts and execution time <a href=\"https://medium.com/the-theam-journey/benchmarking-aws-lambda-runtimes-in-2019-part-i-b1ee459a293d\">are very\r\n      language-specific</a>. The way the language is executed (compiled, interpreted, runtime, etc.) can have a\r\n    significant impact on the performance. For example, both EdgeEngine and Cloudflare Workers run JavaScript straight\r\n    on the V8 engine instead of on Node, which apparently decreases cold start latency significantly. </li>\r\n</ul>\r\n<p></p>\r\n<p>Between the three major providers, AWS is definitely still in the lead (<a href=\"https://mikhail.io/serverless/coldstarts/big3/\">1</a>,<a href=\"http://pages.cs.wisc.edu/~swift/papers/atc18-serverless.pdf\">2</a>) in keeping cold starts low across all\r\n  languages. They also exhibit the most consistent performance. Azure is last with cold starts and overall performance\r\n  that is significantly worse than both Google and AWS. Nuweba has created a <a href=\"https://www.faastest.com/\">benchmarking website</a> to compare these providers periodically.<br> </p>\r\n<figure><img src=\"{asset:6961:url}\" data-image=\"6961\"></figure>\r\n<p>(source: <a href=\"https://mikhail.io/serverless/coldstarts/big3/\">https://mikhail.io/serverless/coldstarts/big3/</a>)\r\n</p>\r\n<p><a href=\"https://www.cloudflare.com/learning/serverless/serverless-performance/\">Cloudflare Workers</a> and <a href=\"https://www.stackpath.com/products/edge-computing/serverless-scripting/\">EdgeEngine</a> both provide a lower\r\n  cold start latency than regular FaaS providers thanks to the way they execute their functions. Both of these together\r\n  with <a href=\"https://aws.amazon.com/lambda/edge/\">Lambda@Edge</a> also aim to reduce the latency that is perceived by\r\n  the caller by deploying functions in multiple locations and executing the functions as close to the caller as\r\n  possible. Since Cloudflare Workers currently have most locations, they can probably provide the lowest average latency\r\n  of the three edge providers. Cloudflare did a <a href=\"https://www.cloudflare.com/learning/serverless/serverless-performance/\">comparison</a> between their own\r\n  workers, Lambda, and Lambda@Edge from which the results indicate that Cloudflare is several times faster. </p>\r\n<figure><img src=\"{asset:6962:url}\" data-image=\"6962\"></figure>\r\n<p>(Source: <a href=\"https://www.cloudflare.com/learning/serverless/serverless-performance/\">https://www.cloudflare.com/learning/serverless/serverless-performance/</a>)\r\n</p>\r\n<p>There is also an ongoing independent <a href=\"https://serverless-benchmark.com/\">serverless benchmark</a> project\r\n  that is benchmarking both cold starts and execution time. It aims to extend the benchmarks to different providers than\r\n  the main three and already has benchmarks for AWS Lambda, Azure Functions, Google Cloud Functions, IBM Cloud&nbsp; &nbsp;Functions, and Cloudflare Workers. The results are in line with what we expect: AWS Lambda is consistently the fastest\r\n  for pure processing jobs yet is beaten in the domain of cold starts by Cloudflare. IBM Cloud Functions do very\r\n  well, but also have very high worst-case latencies. Note that the results are continuously updated and the images\r\n  provided here are snapshots. The results vary strongly from day-to-day. </p>\r\n<p>The work of independent benchmarks is a great help to determine which functions are best suited for your problem.\r\n  Providers like Vercel and Netlify that rely on another provider will probably exhibit the same performance as the\r\n  underlying technology, which in both of these cases is AWS Lambda. At the time of writing, little work has been done\r\n  to benchmark the performance of other frameworks.<br> </p>\r\n<figure><img src=\"{asset:6963:url}\" data-image=\"6963\"></figure>\r\n<p>(Source: <a href=\"https://serverless-benchmark.com/\">https://serverless-benchmark.com</a><a href=\"https://www.cloudflare.com/learning/serverless/serverless-performance/\">/</a> as of 26 November 2019)</p>\r\n<figure><img src=\"{asset:6964:url}\" data-image=\"6964\"></figure>\r\n<p></p>\r\n<p>(Source: <a href=\"https://serverless-benchmark.com/\">https://serverless-benchmark.com</a><a href=\"https://www.cloudflare.com/learning/serverless/serverless-performance/\">/</a> as of 26 November 2019)<br><br>For do-it-yourself FaaS frameworks, keeping cold start latencies low, scaling, and distribution are your\r\n  responsibility. Different frameworks provide different configuration options for you to work with, but it will\r\n  probably be extremely difficult to compete with true serverless providers.</p>\r\n<h3>Overview</h3>\r\n<p></p>\r\n<p>In a global overview, this is how they all compare.</p>\r\n<div class=\"ritz grid-container\">\r\n  <table class=\"waffle\" cellspacing=\"0\" cellpadding=\"0\">\r\n    <thead>\r\n      <tr>\r\n        <th class=\"row-header freezebar-origin-ltr\"></th>\r\n        <th id=\"0C0\" style=\"width:100px\" class=\"column-headers-background\">A</th>\r\n        <th id=\"0C1\" style=\"width:100px\" class=\"column-headers-background\">B</th>\r\n        <th id=\"0C2\" style=\"width:100px\" class=\"column-headers-background\">C</th>\r\n        <th id=\"0C3\" style=\"width:100px\" class=\"column-headers-background\">D</th>\r\n        <th id=\"0C4\" style=\"width:100px\" class=\"column-headers-background\">E</th>\r\n        <th id=\"0C5\" style=\"width:100px\" class=\"column-headers-background\">F</th>\r\n        <th id=\"0C6\" style=\"width:100px\" class=\"column-headers-background\">G</th>\r\n        <th id=\"0C7\" style=\"width:100px\" class=\"column-headers-background\">H</th>\r\n        <th id=\"0C8\" style=\"width:100px\" class=\"column-headers-background\">I</th>\r\n        <th id=\"0C9\" style=\"width:100px\" class=\"column-headers-background\">J</th>\r\n        <th id=\"0C10\" style=\"width:100px\" class=\"column-headers-background\">K</th>\r\n        <th id=\"0C11\" style=\"width:100px\" class=\"column-headers-background\">L</th>\r\n        <th id=\"0C12\" style=\"width:100px\" class=\"column-headers-background\">M</th>\r\n      </tr>\r\n    </thead>\r\n    <tbody>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R1\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">2</div></th>\r\n        <td class=\"fauna-blue\"></td>\r\n        <td class=\"fauna-blue\">AWS <br>Lambda</td>\r\n        <td class=\"fauna-blue\">Google <br>Function</td>\r\n        <td class=\"fauna-blue\">Azure<br>Function</td>\r\n        <td class=\"fauna-blue\">Alibaba Function</td>\r\n        <td class=\"fauna-blue\">IBM<br>Cloud Functions</td>\r\n        <td class=\"fauna-blue\">Oracle <br>Function</td>\r\n        <td class=\"fauna-blue\">Vercel&nbsp;<br>Function</td>\r\n        <td class=\"fauna-blue\">Netlify<br>Function</td>\r\n        <td class=\"fauna-blue\">Cloudflare<br>Workers</td>\r\n        <td class=\"fauna-blue\">EdgeEngine</td>\r\n        <td class=\"fauna-blue\">Fly.io</td>\r\n        <td class=\"fauna-blue\">Lambda@Edge</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R2\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">3</div></th>\r\n        <td>Focus</td>\r\n        <td>general</td>\r\n        <td>general</td>\r\n        <td>general</td>\r\n        <td>general</td>\r\n        <td>general</td>\r\n        <td>general</td>\r\n        <td>Easy JAM,<br>Edge</td>\r\n        <td>Easy<br>JAM</td>\r\n        <td>Edge</td>\r\n        <td>Edge</td>\r\n        <td>Easy Edge</td>\r\n        <td>Edge</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R3\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">4</div></th>\r\n        <td>Speed</td>\r\n        <td>+++</td>\r\n        <td>++</td>\r\n        <td>+</td>\r\n        <td>?</td>\r\n        <td>++</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>+++</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R4\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">5</div></th>\r\n        <td>Free tier</td>\r\n        <td>1M reqs,<br>400K Gb/s</td>\r\n        <td>2M reqs,<br>400K Gb/s</td>\r\n        <td>1M reqs,<br>400K Gb/s</td>\r\n        <td>1M reqs,<br>400K Gb/s</td>\r\n        <td>Unlimited reqs<br>400K Gb/s</td>\r\n        <td>2M reqs,<br>400K Gb/s</td>\r\n        <td>20 hours /month</td>\r\n        <td>100 hours /month</td>\r\n        <td>None</td>\r\n        <td>None</td>\r\n        <td>$25</td>\r\n        <td>None</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R5\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">6</div></th>\r\n        <td>Pricing</td>\r\n        <td>$0.0000002 / request<br><br><br>$0.0000025<br> Gb/s</td>\r\n        <td>$0.0000004 / request<br><br><br>$0.00001667 Gb/s</td>\r\n        <td>$0.0000002 / request<br><br><br>$0.000016<br>Gb/s</td>\r\n        <td>$0.0000004 / request<br><br><br>$0.00001668 Gb/s</td>\r\n        <td>$0.000017<br>Gb/s</td>\r\n        <td>$0.0000002 / request<br><br><br>$0.00001417<br>Gb/s</td>\r\n        <td>$0.2 / h</td>\r\n        <td>$0.000038 /request</td>\r\n        <td>$0.0000005<br>/request</td>\r\n        <td>$0.0000006<br>/request</td>\r\n        <td>$0.000001015 / sec</td>\r\n        <td>$0.0000500<br>1/request</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R6\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">7</div></th>\r\n        <td>Languages</td>\r\n        <td class=\"s9\">JS<br>Go,<br>Python<br>Ruby<br>Java<br>C#<br>PowerSh</td>\r\n        <td class=\"s9\">JS,<br>Go,<br>Python</td>\r\n        <td class=\"s9\">JS,<br>C#</td>\r\n        <td class=\"s9\">JS<br>Python<br>Java<br>PHP</td>\r\n        <td class=\"s9\">JS,<br>Go,<br>Python,<br>Ruby,<br>Java,<br>C#</td>\r\n        <td class=\"s9\">JS,<br>Go,<br>Python,<br>Ruby</td>\r\n        <td class=\"s9\">JS,<br>Go,<br>Python</td>\r\n        <td class=\"s9\">JS,<br>go</td>\r\n        <td class=\"s9\">JS<br>Webassembly</td>\r\n        <td class=\"s9\">JS<br>Webassembly</td>\r\n        <td class=\"s9\">JS</td>\r\n        <td class=\"s9\">JS<br>Python</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R7\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">8</div></th>\r\n        <td>Memory Limits</td>\r\n        <td>128-3008</td>\r\n        <td>128-2048</td>\r\n        <td>?-1536</td>\r\n        <td></td>\r\n        <td>128-2048</td>\r\n        <td>128-1024</td>\r\n        <td>1024-3008</td>\r\n        <td>1024-1024</td>\r\n        <td>128-128</td>\r\n        <td>128-128</td>\r\n        <td>128-128</td>\r\n        <td>128-128</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R8\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">9</div></th>\r\n        <td>Execution time Limits</td>\r\n        <td>15m</td>\r\n        <td>9m</td>\r\n        <td>10m</td>\r\n        <td>10m</td>\r\n        <td>10m</td>\r\n        <td>2m</td>\r\n        <td>15m</td>\r\n        <td>10s</td>\r\n        <td>No limit 10ms CPU</td>\r\n        <td>15s</td>\r\n        <td>?</td>\r\n        <td>5s</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R9\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">10</div></th>\r\n        <td>Payload request</td>\r\n        <td>6Mb</td>\r\n        <td>10Mb</td>\r\n        <td>No limit?</td>\r\n        <td>6Mb</td>\r\n        <td>5Mb</td>\r\n        <td>6Mb</td>\r\n        <td>5Mb</td>\r\n        <td>6Mb</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>50Mb</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R10\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">11</div></th>\r\n        <td>Payload<br>response</td>\r\n        <td>6Mb</td>\r\n        <td>10Mb</td>\r\n        <td>No limit?</td>\r\n        <td>6Mb</td>\r\n        <td>5Mb</td>\r\n        <td>6Mb</td>\r\n        <td>5Mb</td>\r\n        <td>6Mb</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>?</td>\r\n        <td>40Kb</td>\r\n      </tr>\r\n      <tr style=\"height:20px;\">\r\n        <th id=\"0R11\" style=\"height: 20px;\" class=\"row-headers-background\"><div class=\"row-header-wrapper\" style=\"line-height: 20px;\">12</div></th>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n        <td></td>\r\n      </tr>\r\n    </tbody>\r\n  </table>\r\n</div>\r\n<h2>Conclusion</h2>\r\n<p>A while ago, choosing a FaaS provider was a relatively easy task. Today, we are spoiled with diversity since FaaS\r\n  providers are springing up like mushrooms. With such a wide range of providers, it becomes harder and harder to follow\r\n  up on what exists and how they differ. Realizing that some new offerings have an entirely different focus\r\n  (easy-of-use, edge, do-it-yourself) already brings you one step closer to choosing the right provider. In an attempt\r\n  to make your choice more comfortable, we have researched and compared various providers on topics such as focus,\r\n  limitations, pricing, languages, and performance. We hope that this becomes a basis for further discussion as such a\r\n  comparison is ideally a collaborative effort. </p>\r\n<p></p>\r\n<style type=\"text/css\">\r\n  .invisible-table,\r\n  .invisible-table tr {\r\n    background-color: rgba(0, 0, 0, 0)\r\n  }\r\n.s4, .s5, .s6, .s7, .s8 {\r\n  font-size: 10px;\r\n  color: rgba(0,0,0,0.3);\r\n}\r\n\r\n.fauna-blue {\r\n    background-color: rgba(50, 64, 203, 0.8);\r\ncolor: white;\r\n}\r\n\r\n.s5 {\r\nbackground-color: #f4cccc;\r\n}\r\n\r\n\r\n.s7 {\r\nbackground-color: #d9ead3;\r\n}\r\n\r\n\r\n.s4 {\r\nbackground-color: #b6d7a8;\r\n}\r\n\r\n.s8 {\r\nbackground-color: #efefef;\r\n}\r\n\r\n  .invisible-table table tr {\r\n    background: rgba(255, 255, 255, 0.6);\r\n  }\r\n\r\n  .comparison-smaller p {\r\n    font-size: 14px !important;\r\n  }\r\n\r\n  table {\r\n    display: flex;\r\n    justify-content: center;\r\n    border-collapse: collapse;\r\n    text-align: left;\r\n    width: 101%;\r\n  }\r\n\r\n\r\n  table .na {\r\n    color: rgba(255, 0, 0, 0.4);\r\n    font-size: 10px !important;\r\n  }\r\n\r\n  table tr {\r\n    background: rgba(255, 255, 255, 0.6);\r\n    border-bottom: 1px solid\r\n  }\r\n\r\n  table th {\r\n    background-color: rgba(50, 64, 203, 0.8);\r\n    color: white;\r\n  }\r\n\r\n  table th,\r\n  table td {\r\n    padding: 10px 20px;\r\n  }\r\n\r\n  table td span {\r\n    background: #eee;\r\n    color: dimgrey;\r\n    display: none;\r\n    font-size: 10px;\r\n    font-weight: bold;\r\n    padding: 5px;\r\n    position: absolute;\r\n    text-transform: uppercase;\r\n    top: 0;\r\n    left: 0;\r\n  }\r\n\r\n  table td ul {\r\n    padding-inline-start: 10px;\r\n  }\r\n\r\n  table td ul li {\r\n    position: relative;\r\n    font-size: 15px;\r\n    list-style: none\r\n  }\r\n\r\n  table td ul li:before {\r\n    content: \"\";\r\n    background-color: black;\r\n    border: 1px white solid !important;\r\n    border-radius: 5px;\r\n    width: 5px;\r\n    height: 5px;\r\n    position: absolute;\r\n    left: -15px;\r\n    top: 12px;\r\n  }\r\n\r\n  table .subheader-row {\r\n    border: 0px solid rgba(0, 0, 0, 0);\r\n    background-color: rgba(0, 0, 0, 0.04)\r\n  }\r\n\r\n  table .subheader-row td {\r\n    border: 0px solid rgba(0, 0, 0, 0);\r\n  }\r\n\r\n  table .separator-row {\r\n    border: 1px solid rgba(0, 0, 0, 0);\r\n    background-color: rgba(50, 64, 203, 0.2);\r\n  }\r\n\r\n  table .separator-row p {\r\n    margin-bottom: 0px;\r\n\r\n  }\r\n\r\n  table .subheader p {\r\n    margin-bottom: 0px;\r\n  }\r\n\r\n  .caption {\r\n    display: flex;\r\n    flex-direction: column;\r\n\r\n    align-items: center;\r\n    justify-content: center;\r\n  }\r\n\r\n  .caption-left-aligned {\r\n    display: flex;\r\n    flex-direction: column;\r\n\r\n    align-items: left;\r\n    justify-content: center;\r\n    padding-left: 30px;\r\n    padding-right: 30px;\r\n  }\r\n\r\n  .caption p,\r\n  .caption-left-aligned p {\r\n    color: rgba(0, 0, 0, 0.4);\r\n    font-size: 12px !important;\r\n    margin: 0;\r\n  }\r\n\r\n\r\n\r\n  /* Simple CSS for flexbox table on mobile */\r\n  @media(max-width: 800px) {\r\n    table thead {\r\n      left: -9999px;\r\n      position: absolute;\r\n      visibility: hidden;\r\n    }\r\n\r\n    table tr {\r\n      border-bottom: 0;\r\n      display: flex;\r\n      flex-direction: row;\r\n      flex-wrap: wrap;\r\n      margin-bottom: 40px;\r\n    }\r\n\r\n    table td {\r\n      border: 1px solid;\r\n      margin: 0 -1px -1px 0;\r\n      padding-top: 35px;\r\n      position: relative;\r\n      width: 50%;\r\n    }\r\n\r\n    table td span {\r\n      display: block;\r\n    }\r\n  }\r\n</style>",
        "blogCategory": [
            "1530"
        ],
        "mainBlogImage": [
            "6826"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6421",
        "postDate": "2020-01-24T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6787,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d5ab4163-d731-4b66-a29a-4ace430bf055",
        "siteSettingsId": 6787,
        "fieldLayoutId": 4,
        "contentId": 2344,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Lessons Learned Livin' La Vida JAMstack",
        "slug": "lessons-learned-livin-la-vida-jamstack",
        "uri": "blog/lessons-learned-livin-la-vida-jamstack",
        "dateCreated": "2020-01-24T08:37:09-08:00",
        "dateUpdated": "2020-01-24T09:33:01-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/lessons-learned-livin-la-vida-jamstack",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/lessons-learned-livin-la-vida-jamstack",
        "isCommunityPost": false,
        "blogBodyText": "<p>Being a serverless distributed database, the <a href=\"http://jamstack.org/\">JAMstack world</a> is a natural fit for our system, but long before we were chasing JAMstack developers, we were using the stack ourselves. We've been building and operating a couple of JAMstack powered tools for about a year and I thought we could share some of our experiences.</p>\n<figure><a href=\"https://www.youtube.com/watch?v=qSV1ms6GjfU&feature=youtu.be\" target=\"_blank\"><img src=\"{asset:6783:url}\" data-image=\"6783\" style=\"max-width: 150px;\" alt=\"Learn more\"></a></figure>\n<p>Here at Fauna, we have two JAMstack apps: the first is our FaunaDB Console that FaunaDB users use to create and manage their databases and the second is our documentation site. Given that we weren't really targeting JAMstack developers a year ago, we didn't choose to build JAMstack sites to seem hip and trendy but because the approach solved meaningful problems for us.  </p>\n<p>If you’re not familiar with the term JAMstack, the JAM stands for Javascript, APIs, and Markdown. The idea is that modern websites and apps can use client-side Javascript calling out to APIs to fetch and push data, served via static HTML pages that are potentially rendered from a more user-friendly Markdown-type language. Being rendered client-side and served as static files, JAMstack sites can scale aggressively using CDNs for distribution and operations are dramatically simplified by removing the need for web servers or server-side rendering.</p>\n<h2>FaunaDB Console</h2>\n<figure><img src=\"{asset:6785:url}\" data-image=\"6785\" style=\"max-width: 750px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p><br>With the dashboard app, we had an interesting requirement that the application should be runnable in the cloud for our hosted service, but also should be available on a developer's machine for ease of development and runnable behind the firewall of the enterprise customers we were supporting. While it's not hard to run a webserver, building a JAMstack application made it easy to deliver a bundle of files that users could deploy in any environment.</p>\n<p>We had a tight deadline to deliver the new dashboard application, so going with the JAMstack also freed us from a number of operational concerns that sped up delivery. Early on we identified Netlify as a great tool to further simplify our operational life. Netlify's ability to provide previews of features being worked on in task branches greatly accelerated our review process with the UX and product groups.</p>\n<p>Our dashboard app largely only talks to FaunaDB itself; FaunaDB uses https for communication which makes it a piece of cake for web applications to talk to without needing middleware to talk to the database. This further cemented the JAMstack as the proper solution for us as all of the APIs the dashboard apps needs are available in all of the environments we want to run in.</p>\n<h2>FaunaDB Docs</h2>\n<figure><img src=\"{asset:6786:url}\" data-image=\"6786\" style=\"max-width: 750px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p><br>A couple of months after kicking off the dashboard development work we took on revamping our documentation site. Historically the doc site was built using a homegrown doc toolkit. The homegrown system had some advanced functionality for generating examples, but it had not been well maintained and the new builds for even simple changes could take hours.</p>\n<p>For the new doc site, we chose to go with the popular open-source static site generator <a href=\"https://antora.org/\">Antora</a>. Antora is built specifically for generating technical documentation using the powerful <a href=\"https://asciidoctor.org/\">Asciidoctor</a> markup language. Asciidoctor has a broader feature set than Markdown specifically targeting technical writers with syntax for admonitions and cross-references.</p>\n<p>Just like source code, our documentation is stored in git and compiled from Asciidoctor into static HTML pages. Just like our dashboard, our doc site is a JAMstack app served via CDN to make it scalable and rendered client-side, cutting down on the number of web servers we need to manage. Javascript is used locally on the site to provide powerful tooltips and search capabilities. It’s a model JAMstack site.</p>\n<p>Being a JAMstack site means we can take full advantage of Netlify for managing the doc site as well. Automatic per branch deploys make it trivial for reviewers to verify new doc changes, including advanced client-side functionality like search and rich tooltips. Moving to this new toolset has allowed us to move from multiple hours for a build to a few minutes; we can go from a bug report to a fix on the site in 10 minutes.</p>\n<p>Overall our experience building and running JAMstack apps has been fantastic and I can say without reservation that we would do so again if given the opportunity. The simplicity of operations can’t be beaten and having an app that can move between environments gracefully is a major win for development and testing. The biggest win however is being able to easily have per branch reviews to improve the team’s ability to test and review changes. Building apps using the JAMstack has made it easier for us to work together as a team and to focus on building new functionality for our users.</p>\n<p>If you’d like to learn more about the advantages of JAMstack apps, please check out some great talks our engineering team has given <a href=\"https://youtu.be/qSV1ms6GjfU\">here</a> and <a href=\"https://www.youtube.com/watch?v=k0GgiGowU6Q\">here</a>.</p>",
        "blogCategory": [
            "8",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "6782"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6421",
        "postDate": "2020-01-13T08:15:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6768,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9723325e-1473-46ea-8622-8501b84ac495",
        "siteSettingsId": 6768,
        "fieldLayoutId": 4,
        "contentId": 2330,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Fauna Engineering: Looking Back at 2019",
        "slug": "fauna-engineering-looking-back-at-2019",
        "uri": "blog/fauna-engineering-looking-back-at-2019",
        "dateCreated": "2020-01-13T08:15:00-08:00",
        "dateUpdated": "2020-01-13T08:24:14-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/fauna-engineering-looking-back-at-2019",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/fauna-engineering-looking-back-at-2019",
        "isCommunityPost": false,
        "blogBodyText": "<p>There’s nothing like the end of the year to take a moment to reflect on what has passed in the previous months and what excitement lies ahead in the coming ones. FaunaDB and the team that builds it has changed a lot over the past year and we’d like to take a moment to reflect back on it with you.</p>\n<h2>Jepsen</h2>\n<p>2019 started off with a bang for Fauna with the release of FaunaDB 2.6 and its accompanying Jepsen report proving the correctness of the system. Getting FaunaDB to that point was no small feat, but the response from the software development community made it all worthwhile. As the year went on we incorporated Jepsen into our build and release process so we could ensure that kept that hard-earned correctness as new features and performance improvements were added.</p>\n<p>A lot of developers in the JAMstack community are unfamiliar with Jepsen and the level of rigor that it proves, but I think all developers can agree that it’s nice to not worry about the correctness of your datastore! </p>\n<p><we can=\"\" call=\"\" out=\"\" here=\"\" that=\"\" we=\"\" plan=\"\" to=\"\" do=\"\" another=\"\" jepsen=\"\" report,=\"\" probably=\"\" in=\"\" the=\"\" next=\"\" year.=\"\" evan=\"\" and=\"\" matt=\"\" weigh=\"\" on=\"\" whether=\"\" this=\"\" is=\"\" something=\"\" still=\"\" want=\"\"></we></p>\n<h2>New and Improved Dashboard</h2>\n<p>Quickly on the heels of the 2.6 FaunaDB release came the launch of our much improved FaunaDB dashboard. While our former dashboard was entirely serviceable, the new dashboard brought a whole new level of ease of use to managing FaunaDB accounts and databases.</p>\n<p>There’s a common meme that developers don’t like UIs and prefer command-line interfaces wherever possible. However, over the past few months we’ve received a huge number of compliments on the dashboard and how it makes learning and managing FaunaDB a breeze. The design and development team have worked hard to address feedback over the past year to make it even better, adding a FaunaDB shell in the browser and a great new system for managing user-defined functions!</p>\n<p>We have a great roadmap ahead for 2020 with continued usability enhancements and support for teams within Fauna Cloud. If you ever have thoughts about how the dashboard could be better, please <a href=\"https://community.fauna.com/\">join our community</a> and let us know!</p>\n<h2>GraphQL</h2>\n<p>Late spring brought the launch of built-in support for GraphQL to FaunaDB, and with it, a new and easier way to store data in the cloud. By just uploading a schema the system itself is able to handle the heavy lifting of setting up tables and indexes, making it quick and easy to get started with using FaunaDB. </p>\n<p>July brought support for GraphQL to the console making it one-stop shopping for setting up and exploring FaunaDB using GraphQL. If you aren’t familiar with using GraphQL with FaunaDB you should check out this excellent series of blog posts from one of the lead developers of the GraphQL functionality.</p>\n<p>Looking into the new year we have a robust roadmap planned for the GraphQL functionality including support for Apollo Federation, expanded GraphQL specification support, and some new custom resolvers to make it even easier to work with FaunaDB.</p>\n<h2>Delightful Documentation</h2>\n<p>One of our biggest improvements over the past year has been on the documentation side of the house. FaunaDB is a rich system with a lot of functionality, but if you can’t figure out how to make it work (or if it exists!) then it doesn’t matter. A big push for us at Fauna was to revamp the documentation to make it easier to find content and more comprehensive.</p>\n<p>Thanks to the hard work of our writers and great research from our UX team we were able to make massive improvements to this system. This year we introduced fast local search to make it easier to find the content you are looking for and new rich tooltips to make it easier to understand how larger queries are composed. New tutorials were added and existing ones were rewritten and cleaned up. Behind the scenes, we built a new doc pipeline that makes it possible for us to go from bug report to live fix in minutes.</p>\n<p>2020 will bring expanded tutorials, richer examples throughout the doc set, and more thorough coverage of both FQL and GraphQL. If you ever have suggestions for how the documentation could be improved please let us know at <a href=\"mailto:docs@fauna.com\">docs@fauna.com</a>.</p>\n<h2>Functions Galore</h2>\n<p>The last half of 2019 brought a barrage of FaunaDB releases with an equally large set of new functions. This was not an accident; we focused this year on streamlining our release and testing processes so that we could move to a near-monthly release cadence. This has made it possible for us to react more quickly to your feedback and in response, we delivered a number of new FQL features to make FaunaDB easier to use and more powerful.</p>\n<p>As part of this process, we also introduced our preview release process. New functions in FQL are released with the Javascript and JVM drivers so that we can get feedback from the community before we lock down the behavior. We are committed to minimizing churn in the API so that developers do not have to change their apps regularly to keep up-to-date with FaunaDB, and this preview function has been very helpful for us in ensuring that we deliver the functionality our users need before we commit to a certain behavior.</p>\n<p>We have a deep roadmap of new features queued up for 2020; some changes will make it easier to get started with FQL and others will deepen what FQL can do.</p>\n<p>All told we’re extremely proud of the progress we made this year at Fauna and are looking forward to seeing how much further we can push FaunaDB in 2020. Our goal is to help you folks build fearlessly; to make data storage simple with no operations and no configuration.</p>",
        "blogCategory": [
            "1461",
            "1462",
            "1530"
        ],
        "mainBlogImage": [
            "6769"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-12-19T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6685,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "4a69b321-d485-45b6-8031-d265c344cdae",
        "siteSettingsId": 6685,
        "fieldLayoutId": 4,
        "contentId": 2307,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing UDF in Console",
        "slug": "announcing-udf-in-console",
        "uri": "blog/announcing-udf-in-console",
        "dateCreated": "2019-12-17T14:29:21-08:00",
        "dateUpdated": "2019-12-19T11:03:29-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-udf-in-console",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-udf-in-console",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce the release of user-defined function (UDF) support in the FaunaDB Console! While UDFs have been available in FQL for some time, they can now be easily managed using the Console GUI. </p>\n<p>Generally speaking, a UDF is comparable to a stored procedure or server-side function that has its parameters set by a user and often is seen as a “programming shortcut”. In FaunaDB, UDFs are FQL <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/lambda\">Lambda</a> functions (which can be anonymous), and they can be exposed through the GraphQL API, by using the <a href=\"https://docs.fauna.com/fauna/current/api/graphql/directives/d_resolver\">@resolver</a> directive on fields in the Query and Mutation types. You can learn more about FaunaDB GraphQL UDFs in our <a href=\"https://docs.fauna.com/fauna/current/api/graphql/functions\">documentation</a>.</p>\n<h1>UDFs in action</h1>\n<p>Creating UDFs in the Console is quite simple. From the homepage, click on one of your Databases. In the left-hand navigation bar, click on the “Functions” button and then click the green “New Function button”:</p>\n\n<figure><img src=\"{asset:6712:url}\" data-image=\"6712\"></figure><p><br></p><p>You’ll then see the New Function screen. For this example, let’s create a function that simply doubles the input for a query (more advanced examples are given <a href=\"https://docs.fauna.com/fauna/current/api/graphql/functions\">here</a>). This query adds the variable “x” to itself, thereby doubling it.</p>\n<p>Let’s call the function “double”, and specify the function body as:</p>\n<pre>Query(\nLambda('x', Add(Var('x'), Var('x')))\n)</pre>\n\n<figure><img src=\"{asset:6713:url}\" data-image=\"6713\"></figure><p><br></p><p>Once saved, you should see the function listed in the Functions list:</p><figure><img src=\"{asset:6714:url}\" data-image=\"6714\"></figure>\n\n<p>Note: you can use the Role field to apply a role’s privileges to a UDF, which is how you specify permissions in FaunaDB.</p>\n<p>You can create more functions at your whim, and delete functions when needed:</p>\n\n<figure><img src=\"{asset:6715:url}\" data-image=\"6715\"></figure><p><br></p><p>Using your new function is straightforward as well. Simply wrap the function name in quotes in a <tt>Call()command</tt>, i.e. <tt>Call(\"double\", 2)</tt>:</p><figure><img src=\"{asset:6716:url}\" data-image=\"6716\"></figure>\n\n<p>And that’s about it! When it comes to UDFs, the only limit is your creativity &#x1f609;</p>\n<h1>Conclusion</h1>\n<p>With our latest cloud release, users now have the ability to create and manage UDFs for a database directly in the FaunaDB Console.</p>\n<p>Visit our <a href=\"https://docs.fauna.com/fauna/current/api/graphql/functions\">documentation</a> to learn more. Please reach out to me on <a href=\"http://twitter.com/lew\">Twitter</a> and our <a href=\"https://community-invite.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "6686"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-12-17T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6667,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "421cdaad-d9ec-4d90-bc62-aa19f14e1a24",
        "siteSettingsId": 6667,
        "fieldLayoutId": 4,
        "contentId": 2289,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing the FaunaDB Integration for ZEIT",
        "slug": "announcing-the-faunadb-integration-for-zeit",
        "uri": "blog/announcing-the-faunadb-integration-for-zeit",
        "dateCreated": "2019-12-11T12:08:59-08:00",
        "dateUpdated": "2020-04-21T10:26:26-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-faunadb-integration-for-zeit",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-faunadb-integration-for-zeit",
        "isCommunityPost": false,
        "blogBodyText": "<p><em>Since publication of this article, ZEIT has been renamed to <a href=\"https://vercel.com/blog/zeit-is-now-vercel\">Vercel</a>.</em></p><p>---</p><p>We’re excited to announce the FaunaDB integration for ZEIT.</p>\r\n<figure><a href=\"https://zeit.co/integrations/faunadb\"><img src=\"{asset:6660:url}\" data-image=\"6660\" style=\"max-width: 400px;\"></a></figure>\r\n<h2>Overview of ZEIT integration with FaunaDB</h2>\r\n<p>This integration allows users to get started with a sample app using FaunaDB and ZEIT Now in 5 minutes or less. Users can connect to their FaunaDB instance and display their collections for a given database. Future functionality is planned to include performing CRUD operations directly from ZEIT.</p>\r\n<h2>What is ZEIT?</h2>\r\n<p>For the performance-obsessed, <a href=\"https://zeit.co/\">ZEIT</a> is the easiest way to deploy modern websites and apps. No more time lost on painstakingly configuring DNS, SSL, or CDN — ZEIT does all of that automatically, for free.</p>\r\n<h2>Integration Capabilities</h2>\r\n<h3>What it does</h3>\r\n<p>We’ve created a simple ZEIT Integration which asks for a FaunaDB API key and displays the user’s collections from a database.</p>\r\n<h3>Prerequisites</h3>\r\n<ul><li><a href=\"https://dashboard.fauna.com\">Sign in to your FaunaDB account</a></li><li>Have at least one database with collections<ul><li>If you are new to FaunaDB, <a href=\"https://dashboard.fauna.com/db-new/\">create a database here</a> & check the “Pre-populate with demo data” button<br><br><img src=\"{asset:6661:url}\" data-image=\"6661\" style=\"max-width: 400px;></li></ul></li><li>Sign in to your <a href=\"><br></li><li>Sign in to your <a href=\"https://zeit.co\">ZEIT account</a><br></li><li>Ensure that you have a working version of <a href=\"https://www.npmjs.com/\">npm</a> on your machine<br></li><li>Ensure that ZEIT Now CLI is installed and logged in to your ZEIT account (<a href=\"https://zeit.co/docs#install-now-cli\">instructions here</a>)</li></ul>\r\n<h3>Steps for integrating</h3>\r\n<ol><li>Go to the <a href=\"https://zeit.co/integrations/faunadb\">FaunaDB integration page</a> | Add<br></li><li>Select user account | Add (again)<br></li><li>Create root admin key&nbsp;<a href=\"https://dashboard.fauna.com/keys\">here</a><br><br><img src=\"{asset:6668:url}\" data-image=\"6668\"><br></li><li>Paste root admin secret into the integration | Save Secret<br></li><li>Select a project | Create new project | enter faunadb-zeit-sample-app | CREATE<br></li><li>“Link To Project” the database you want to connect. To select child databases, click on the database name hyperlink<img src=\"{asset:6671:url}\" data-image=\"6671\"><br><br></li><li>Click the “Open in CodeSandbox” link, and continue to follow these steps in the <a href=\"https://codesandbox.io/s/github/fauna/faunadb-zeit-sample-app\">CodeSandbox app</a>:<ol><li>Click on the Deployment menu (the rocket icon in the sidebar)<br></li><li>Click the \"Fork Sandbox\" button<br></li><li>Return to the Deployment menu<br></li><li>Click \"Now\" (You'll need to sign in to Now when you're deploying for the first time)<br><img src=\"{asset:6664:url}\" data-image=\"6664\" style=\"max-width: 550px;><br><br></li><li>Click the \"><br></li><li>Click the deployment link and wait for building to complete to verify that the application lists the collections in your database<br><img src=\"{asset:6665:url}\" data-image=\"6665\"></li></ol></li></ol><p>Alternatively, to step 7, you can Clone & deploy the sample app yourself:</p>\r\n<ol><li>Clone & deploy sample app:&nbsp;<ol><li>git clone&nbsp;<a href=\"https://github.com/fauna/faunadb-zeit-sample-app\" target=\"_blank\">https://github.com/fauna/faunadb-zeit-sample-app</a></li><li>cd<a href=\"https://github.com/fauna/faunadb-zeit-sample-app\" target=\"_blank\"> faunadb-zeit-sample-app</a></li><li>now --prod</li></ol></li><li>Navigate to the project production URL (already in the clipboard on macOS) & confirm DB collections are listed.<br></li></ol>\r\n<img src=\"{asset:6666:url}\" data-image=\"6666\" style=\"max-width: 300px;\">\r\n<h2>Conclusion</h2>\r\n<p>With this simple integration, users can get started with a sample app using FaunaDB and ZEIT Now in 5 minutes or less.</p>\r\n<p>Please visit <a href=\"https://docs.fauna.com/fauna/current/start/zeit.html\">the FaunaDB documentation</a> to learn more. And please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate your feedback into a future release.</p>\r\n<p>What other integrations would you like to see implemented in FaunaDB? Please reach out to me on <a href=\"http://twitter.com/lew\">Twitter</a> or our <a href=\"https://community.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB and ZEIT an obvious choice for your next project.</p></li></ul>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [
            "6669"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-12-11T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6655,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "c90a7cfd-e6bc-4008-bab0-b0df6b6ef921",
        "siteSettingsId": 6655,
        "fieldLayoutId": 4,
        "contentId": 2277,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing Cloud Onboarding Guide Rails",
        "slug": "announcing-cloud-onboarding-guide-rails",
        "uri": "blog/announcing-cloud-onboarding-guide-rails",
        "dateCreated": "2019-12-11T08:54:41-08:00",
        "dateUpdated": "2019-12-11T11:30:44-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-cloud-onboarding-guide-rails",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-cloud-onboarding-guide-rails",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce two new tutorials in the FaunaDB Console: <strong>Basic CRUD</strong> and <strong>GraphQL</strong>.</p>\n<p>As new users continue to join the FaunaDB community at a growing rate, we know that the use cases for the database continue to diversify. With that growth comes new questions for how to get acclimated to FaunaDB’s unique and powerful query language. Also, some users want to dive straight into trying out GraphQL queries.</p>\n<p>Our goal is to ensure that any user can create their first database and then add, update, delete, and modify new data within that database during their first 5 minutes as a FaunaDB user.</p>\n<p>Over time, we plan to add more advanced use cases to our tutorial suite so that users who have been with FaunaDB since the beginning can continue to level up their skill set.</p>\n<h1>Getting started with tutorials</h1>\n<p>Console tutorials can be found by clicking the (?) button in the top right of the <a href=\"http://dashboard.fauna.com/\">Console</a>:</p>\n<figure><img src=\"{asset:6651:url}\" data-image=\"6651\" style=\"max-width: 350px;\"></figure>\n<p><br>New users are automatically prompted to try out one of the two available tutorials:</p>\n<figure><img src=\"{asset:6652:url}\" data-image=\"6652\" style=\"max-width: 550px;\"></figure>\n<p></p>\n<p>These two tutorials are designed to cover the basic functionality within FaunaDB. The Basic CRUD tutorial also provides a primer for FQL syntax. For example, a query that users will learn and use:</p>\n<figure><img src=\"{asset:6653:url}\" data-image=\"6653\" style=\"max-width: 350px;\"></figure>\n<p>Running this query in the Console provides the expected result:</p>\n<figure><img src=\"{asset:6654:url}\" data-image=\"6654\" style=\"max-width: 550px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p>By design, these tutorial steps are relatively self-explanatory :)</p>\n<h1>Conclusion</h1>\n<p>With our latest Console release, users now have access to getting-started tutorials. Visit our <a href=\"https://docs.fauna.com/\">documentation</a> to learn more.</p>\n<p>What other tutorials would you like to see implemented in the FaunaDB Console? Please reach out to me on <a href=\"https://twitter.com/lew\">Twitter</a> and on our <a href=\"https://community.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>\n",
        "blogCategory": [
            "8",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "6657"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "3110",
        "postDate": "2019-12-10T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6645,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "5c8ba136-b4cc-4a30-9480-14f2bb613031",
        "siteSettingsId": 6645,
        "fieldLayoutId": 4,
        "contentId": 2267,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "ABAC + GraphQL",
        "slug": "abac-graphql",
        "uri": "blog/abac-graphql",
        "dateCreated": "2019-12-06T06:08:55-08:00",
        "dateUpdated": "2020-02-12T16:54:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/abac-graphql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/abac-graphql",
        "isCommunityPost": false,
        "blogBodyText": "<p>In this article, we will learn about FaunaDB's ABAC capabilities, and how they can be used with FaunaDB's native GraphQL API. To do so, we will go through an example use case showing how to build a comprehensive authorization implementation with FaunaDB's GraphQL API.</p>\n<p>First, let’s start by revisiting the main aspects of the ABAC model.</p>\n<h2>What is ABAC?</h2>\n<p>ABAC stands for Attribute-Based Access Control. As its name indicates, it’s an authorization model that establishes access policies based on attributes. Attributes are characteristics that describe any of the different elements in the system. These can be one of the following types: </p>\n<ul><li>User attributes: describe the user attempting the access (e.g., role, department, clearance level, etc).</li><li>Action attributes: describe the action being attempted (e.g., read, write, delete, etc).</li><li>Resource attributes: describe the resource being accessed (e.g., owner, sensitivity, creation date, etc).</li><li>Environmental attributes: describe the environmental conditions on which the access attempt is being performed (e.g., location, time, day of the week, etc).</li></ul>\n<p>Through the combination of these attributes, ABAC allows the definition of fine-grained access control policies on a new level. This ability to express truly complex access rules is the key aspect of the ABAC model, and what makes it stand out from its predecessors.</p>\n<p>FaunaDB implements ABAC as part of its integral <a href=\"https://docs.fauna.com/fauna/current/security/\">security model</a>, which allows the definition of attribute-based policies through its FQL API.</p>\n<h2>An example use case</h2>\n<p>Now that we've gone through the principles of the ABAC model, let's introduce an example use case that demonstrates FaunaDB's ABAC and GraphQL API features working together.</p>\n<p>Since we will be building a GraphQL service, we are going to use the GraphQL Schema Definition Language (SDL) to model our domain. SDL is the most simple and intuitive, yet powerful and expressive, tool to describe a GraphQL schema. Its syntax is well-defined and is part of the official <a href=\"https://graphql.org/learn/schema\">GraphQL specification</a>.</p>\n<p>So, for the domain model defined in the following GraphQL schema:</p>\n<pre>type User {\n  username: String! @unique\n  role: UserRole!\n}\n\nenum UserRole {\n  MANAGER\n  EMPLOYEE\n}\n\ntype File {\n  content: String!\n  confidential: Boolean!\n}\n\ninput CreateUserInput {\n  username: String!\n  password: String!\n  role: UserRole!\n}\n\ninput LoginUserInput {\n  username: String!\n  password: String!\n}\n\ntype Query {\n  allFiles: [File!]!\n}\n\ntype Mutation {\n  createUser(input: CreateUserInput): User! @resolver(name: \"create_user\")\n  loginUser(input: LoginUserInput): String! @resolver(name: \"login_user\")\n}</pre>\n<p>We are going to implement the following access rules: </p>\n<ol><li>Allow employee users to read public files only.</li><li>Allow manager users to read both public files and, only during weekdays, confidential files.</li></ol>\n<p>As you might have already noticed, these access rules include all of the different ABAC attribute types described earlier.</p>\n<p>We will define the access rules through the FQL API, and then verify that they are working as expected by executing some queries from the GraphQL API. </p>\n<p>With our goals already set, let’s put our hands to work!</p>\n<h1>Importing the schema</h1>\n<p>First, let’s import the example schema into a new database. Log into the FaunaDB <a href=\"https://dashboard.fauna.com/\">Cloud Console</a> with your credentials. If you don’t have an account yet, you can sign up for free in a few seconds.</p>\n<p>Once logged in, click the <tt>NEW DATABASE</tt> button from the home page:</p>\n<figure><img src=\"{asset:6629:url}\" data-image=\"6629\"></figure>\n<p>Choose a name for the new database, and click the <tt>SAVE</tt> button: </p>\n<figure><img src=\"{asset:6630:url}\" data-image=\"6630\"></figure>\n<p>Next, we will import the GraphQL schema listed above into the database we just created. To do so, create a file named&nbsp;<tt>schema.gql</tt> containing the schema definition.&nbsp;Then, select the <tt>GRAPHQL</tt> tab from the left sidebar, click the <tt>IMPORT SCHEMA</tt> button, and select the newly-created file: </p>\n<figure><img src=\"{asset:6631:url}\" data-image=\"6631\"></figure>\n<p>The import process creates all of the necessary database elements, including collections and indexes, for backing up all of the types defined in the schema. In the next step, we will implement the custom resolver functions for the <tt>createUser</tt> and <tt>loginUser</tt> Mutation fields.</p>\n<h1>Implementing custom resolvers</h1>\n<p>When looking at the schema, you might notice that the <tt>createUser</tt> and <tt>loginUser</tt> Mutation fields have been annotated with a special directive named <tt><a href=\"https://docs.fauna.com/fauna/current/api/graphql/directives/d_resolver\">@resolver</a></tt>. This is a directive provided by the FaunaDB GraphQL API, which allows us to define a custom behavior for a given Query or Mutation field. </p>\n<p>On the database end, a template <a href=\"https://docs.fauna.com/fauna/current/api/graphql/functions\">User-defined function</a> (UDF) is created during the import process for each of the fields annotated with the <tt>@resolver</tt> directive. A UDF is a custom FaunaDB function, similar to a <em>stored procedure</em>, that enables users to define a tailor-made operation in <a href=\"https://docs.fauna.com/fauna/current/api/fql/\">FQL</a>. This function is then used as the resolver of the annotated field.</p>\n<p>Now, let’s continue to implement the UDF for the <tt>createUser</tt> field resolver. First, select the <tt>SHELL</tt> tab from the left sidebar:</p>\n<figure><img src=\"{asset:6632:url}\" data-image=\"6632\"></figure>\n<p>As explained before, a template UDF has already been created during the import process. When called, this template UDF prints an error message stating that it needs to be updated with a proper implementation. In order to update it with the intended behavior, we are going to use FQL's <tt><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/update\">Update</a></tt> function.</p>\n<p>So, let’s copy the following FQL query into the command panel, and click the <tt>RUN QUERY</tt> button:</p>\n<pre>Update(Function(\"create_user\"), {\n  \"body\": Query(\n    Lambda([\"input\"],\n      Create(Collection(\"User\"), {\n        data: {\n          username: Select(\"username\", Var(\"input\")),\n          role: Select(\"role\", Var(\"input\")),\n        },\n        credentials: {\n          password: Select(\"password\", Var(\"input\"))\n        }\n      })  \n    )\n  )\n});</pre>\n<p>Your screen should look similar to:</p>\n<figure><img src=\"{asset:6633:url}\" data-image=\"6633\"></figure>\n<p>The <tt>create_user</tt> UDF will be in charge of properly creating a <tt>User</tt> document along with a password value. The password is stored in the document within a special object named <em>credentials</em> that cannot be retrieved back by any FQL function. As a result, the password is securely saved in the database making it impossible to read from either the FQL or the GraphQL APIs. The password will be used later for authenticating a <tt>User</tt> through a dedicated FQL function named <tt><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/login\">Login</a></tt>, as explained next.</p>\n<p>Now, let’s add the proper implementation for the UDF backing up the <tt>loginUser</tt> field resolver through the following FQL query:</p>\n<pre>Update(Function(\"login_user\"), {\n  \"body\": Query(\n    Lambda([\"input\"],\n      Select(\n        \"secret\",\n        Login(\n          Match(Index(\"unique_User_username\"), Select(\"username\", Var(\"input\"))), \n          { password: Select(\"password\", Var(\"input\")) }\n        )\n      )\n    )\n  )\n});</pre>\n<p>Copy the query listed above and paste it into the Shell’s command panel, and click the <tt>RUN QUERY</tt> button:</p>\n<figure><img src=\"{asset:6634:url}\" data-image=\"6634\"></figure>\n<p>The <tt>login_user</tt> UDF will attempt to authenticate a User with the given username and password credentials. As mentioned before, it does so via the <tt><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/login\">Login</a></tt> function. The <tt>Login</tt> function verifies that the given password matches the one stored along with the <tt>User</tt> document being authenticated. Note that the password stored in the database is not output at any point during the login process. Finally, in case the credentials are valid, the <tt>login_user</tt> UDF returns an authorization token called a <em>secret</em> which can be used in subsequent requests for validating the <tt>User</tt>’s identity.</p>\n<p>With the resolvers in place, we will continue with creating some sample data. This will let us try out our use case and help us better understand how the access rules are defined later on.</p>\n<h1>Creating sample data</h1>\n<p>First, we are going to create a <em>manager</em> user. Select the <tt>GRAPHQL</tt> tab from the left sidebar, copy the following mutation into the GraphQL Playground, and click the <em>Play</em> button:</p>\n<pre>mutation CreateManagerUser {\n  createUser(input: {\n    username: \"bill.lumbergh\"\n    password: \"123456\"\n    role: MANAGER\n  }) {\n    username\n    role\n  }\n}\n</pre>\n<p>Your screen should look like this:</p>\n<figure><img src=\"{asset:6635:url}\" data-image=\"6635\"></figure>\n<p>Next, let’s create an <em>employee</em> user by running the following mutation through the GraphQL Playground editor:</p>\n<pre>mutation CreateEmployeeUser {\n  createUser(input: {\n    username: \"peter.gibbons\"\n    password: \"abcdef\"\n    role: EMPLOYEE\n  }) {\n    username\n    role\n  }\n}\n</pre>\n<p>You should see the following response:</p>\n<figure><img src=\"{asset:6636:url}\" data-image=\"6636\"></figure>\n<p></p>\n<p>Now, let’s create a <em>confidential</em> file by running the following mutation:</p>\n<pre>mutation CreateConfidentialFile {\n  createFile(data: {\n    content: \"This is a confidential file!\"\n    confidential: true\n  }) {\n    content\n    confidential\n  }\n}</pre>\n<p>As a response, you should get the following:</p>\n<figure><img src=\"{asset:6637:url}\" data-image=\"6637\"></figure>\n<p>And lastly, create a <em>public</em> file with the following mutation:</p>\n<pre>mutation CreatePublicFile {\n  createFile(data: {\n    content: \"This is a public file!\"\n    confidential: false\n  }) {\n    content\n    confidential\n  }\n}</pre>\n<p>If successful, it should prompt the following response:<br></p>\n<figure><img src=\"{asset:6638:url}\" data-image=\"6638\"></figure>\n<p>Now that all the sample data is in place, let’s continue with defining the access rules that we described earlier. The access rules determine how the sample data we just created should be accessed.</p>\n<h1>Defining the access rules</h1>\n<p>In FaunaDB, access rules are defined in the form of roles. A role consists of the following data:</p>\n<ul><li>name —  the name that identifies the role</li><li><a href=\"https://docs.fauna.com/fauna/current/security/abac#privileges\">privileges</a> — specific actions that can be executed on specific resources </li><li><a href=\"https://docs.fauna.com/fauna/current/security/abac#membership\">membership</a> — specific identities that should have the specified privileges</li></ul>\n<p>Roles are created through the <tt><a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/createrole\">CreateRole</a></tt> FQL function, as shown in the following example snippet:<br></p>\n<pre>CreateRole({\n  name: \"role_name\",\n  membership: [ \n    // ... \n  ],\n  privileges: [ \n    // ... \n  ]\n})</pre>\n<p>A privilege specifies how a resource in FaunaDB (e.g., a database, collection, document, index, etc.) can be accessed through a set of predefined actions (e.g. <tt>create</tt>, <tt>read</tt>, <tt>delete</tt>, <tt>history_read</tt>, etc.) and a <a href=\"https://docs.fauna.com/fauna/current/security/abac#predicates\">predicate function</a>. A predicate function is an FQL read-only function that returns <tt>true</tt> or <tt>false</tt> to indicate whether the access is permitted or not. The main purpose of a predicate function defined in the context of a role  privilege is to read the attributes of the resource being accessed, and establish access policies based on them.&nbsp;</p>\n<p>Here’s an example snippet that shows how to define a privilege that establishes <tt>read</tt> access on <em>public</em> files for the <tt>File</tt> collection:</p>\n<pre>privileges: [\n  {\n    resource: Collection(\"File\"),\n    actions: {\n      // Read and establish rule based on action attribute\n      read: Query(\n        // Read and establish rule based on resource attribute\n        Lambda(\"fileRef\",\n          Not(Select([\"data\", \"confidential\"], Get(Var(\"fileRef\"))))\n        )\n      )\n    }\n  }\n]\n</pre>\n<p>In addition, attributes of the subject attempting the access, as well as environmental attributes, are available within the privileges’ predicate functions too. This enables establishing further policies based on them, if required.&nbsp;</p>\n<p>\nMembership defines a set of collections whose documents should have the role’s privileges. As in the case of privileges, a predicate function can be provided when defining the membership as well. The main purpose of a predicate function defined in the context of a role membership is to read the attributes of the user attempting access, and establish access policies based on them.&nbsp;</p>\n<p>\nHere’s an example snippet that shows how to define the membership for documents in the User collection that have a <tt>MANAGER</tt> role:\n</p>\n<pre>membership: {\n  resource: Collection(\"User\"),\n  predicate: Query(\n    // Read and establish rule based on user attribute\n    Lambda(\"userRef\", \n      Equals(Select([\"data\", \"role\"], Get(Var(\"userRef\"))), \"MANAGER\")\n    )\n  )\n}\n</pre>\n<p>\nIn this case, the attributes of the resource being accessed are not available in the membership predicate function. Environmental attributes can be retrieved here though, in case they are required for defining further access rules based on them.&nbsp;</p>\n<p>\nIn sum, FaunaDB roles are a very flexible mechanism that allows defining access rules based on all of the system elements attributes, with different levels of granularity. The place where the rules are defined — privileges or membership — determines their granularity and the attributes that are available, and will differ with each particular use case.&nbsp;</p>\n<p>\nNow that we have covered the basics of how roles work, let’s continue by creating the access rules for our example use case!\n\nIn order to keep things neat and tidy, we’re going to create two roles, one for each of the access rules. This will allow us to extend the roles with further rules in an organized way if required later. Nonetheless, be aware that all of the rules could also have been defined together within just one role if needed.&nbsp;</p>\n<p>\nLet’s implement the first rule:&nbsp;</p>\n<p><em>\n\n“Allow employee users to read public files only.”&nbsp;</em></p>\n<p>\nIn order to create a role meeting these conditions, we are going to use the following query:\n</p>\n<pre>CreateRole({\n  name: \"employee_role\",\n  membership: {\n    resource: Collection(\"User\"),\n    predicate: Query( \n      Lambda(\"userRef\",\n        // User attribute based rule:\n        // It grants access only if the User has EMPLOYEE role.\n        // If so, further rules specified in the privileges\n        // section are applied next.        \n        Equals(Select([\"data\", \"role\"], Get(Var(\"userRef\"))), \"EMPLOYEE\")\n      )\n    )\n  },\n  privileges: [\n    {\n      // Note: 'allFiles' Index is used to retrieve the \n      // documents from the File collection. Therefore, \n      // read access to the Index is required here as well.\n      resource: Index(\"allFiles\"),\n      actions: { read: true } \n    },\n    {\n      resource: Collection(\"File\"),\n      actions: {\n        // Action attribute based rule:\n        // It grants read access to the File collection.\n        read: Query(\n          Lambda(\"fileRef\",\n            Let(\n              {\n                file: Get(Var(\"fileRef\")),\n              },\n              // Resource attribute based rule:\n              // It grants access to public files only.\n              Not(Select([\"data\", \"confidential\"], Var(\"file\")))\n            )\n          )\n        )\n      }\n    }\n  ]\n})\n</pre>\n<p>Select the <tt>SHELL</tt> tab from the left sidebar, copy the above query into the command panel, and click the <tt>RUN QUERY</tt> button:</p>\n<figure><img src=\"{asset:6647:url}\" data-image=\"6647\"></figure>\n<p><br>Next, let’s implement the second access rule:</p>\n<p><em>“Allow manager users to read both public files and, only during weekdays, confidential files.”</em></p>\n<p>In this case, we are going to use the following query:</p>\n<pre>CreateRole({\n  name: \"manager_role\",\n  membership: {\n    resource: Collection(\"User\"),\n    predicate: Query(\n      Lambda(\"userRef\", \n        // User attribute based rule:\n        // It grants access only if the User has MANAGER role.\n        // If so, further rules specified in the privileges\n        // section are applied next.\n        Equals(Select([\"data\", \"role\"], Get(Var(\"userRef\"))), \"MANAGER\")\n      )\n    )\n  },\n  privileges: [\n    {\n      // Note: 'allFiles' Index is used to retrieve the \n      // documents from the File collection. Therefore, \n      // read access to the Index is required here as well.\n      resource: Index(\"allFiles\"),\n      actions: { read: true } \n    },\n    {\n      resource: Collection(\"File\"),\n      actions: {\n        // Action attribute based rule:\n        // It grants read access to the File collection.\n        read: Query(\n          Lambda(\"fileRef\",\n            Let(\n              {\n                file: Get(Var(\"fileRef\")),\n                dayOfWeek: DayOfWeek(Now())\n              },\n              Or(\n                // Resource attribute based rule:\n                // It grants access to public files.\n                Not(Select([\"data\", \"confidential\"], Var(\"file\"))),\n                // Resource and environmental attribute based rule:\n                // It grants access to confidential files only on weekdays.\n                And(\n                  Select([\"data\", \"confidential\"], Var(\"file\")),\n                  And(GTE(Var(\"dayOfWeek\"), 1), LTE(Var(\"dayOfWeek\"), 5))  \n                )\n              )\n            )\n          )\n        )\n      }\n    }\n  ]\n})\n</pre>\n<p>Copy the query into the command panel, and click the <tt>RUN QUERY</tt> button:</p>\n<figure><img src=\"{asset:6648:url}\" data-image=\"6648\"></figure>\n<p><br>At this point, we have created all of the necessary elements for implementing and trying out our example use case! Let’s continue with verifying that the access rules we just created are working as expected...</p>\n<h1>Putting everything in action</h1>\n<p>Let’s start by checking the first rule: </p>\n<p><em>“Allow employee users to read public files only.”</em></p>\n<p>The first thing we need to do is log in as an employee user so that we can verify which files can be read on its behalf. In order to do so, execute the following mutation from the GraphQL Playground console:</p>\n<pre>mutation LoginEmployeeUser {\n  loginUser(input: {\n    username: \"peter.gibbons\"\n    password: \"abcdef\"\n  })\n}</pre>\n<p>As a response, you should get a <em>secret</em> access token. The secret represents that the user has been authenticated successfully:</p>\n<figure><img src=\"{asset:6641:url}\" data-image=\"6641\"></figure>\n<p>At this point, it’s important to remember that the access rules we defined earlier are not directly associated with the secret that is generated as a result of the login process. Unlike other authorization models, the secret token itself does not contain any <em>authorization</em> information on its own, but it’s just an <em>authentication</em> representation of a given document.</p>\n<p>As explained before, access rules are stored in roles, and roles are associated with documents through their membership configuration. After authentication, the secret token can be used in subsequent requests to prove the caller’s identity and determine which roles are associated with it. This means that access rules are effectively verified in every subsequent request and not only during authentication. This model enables us to modify access rules dynamically without requiring users to authenticate again.</p>\n<p>Now, we will use the secret issued in the previous step to validate the identity of the caller in our next query. In order to do so, we need to include the secret as a <a href=\"https://docs.fauna.com/fauna/current/api/graphql/endpoints#bearer-token\"><em>Bearer Token</em></a> as part of the request. To achieve this, we have to modify the <tt>Authorization</tt> header value set by the GraphQL Playground. Since we don’t want to miss the admin secret that is being used as default, we’re going to do this in a new tab.</p>\n<p>Click the plus (+) button to create a new tab, and select the <tt>HTTP HEADERS</tt> panel on the bottom left corner of the GraphQL Playground editor. Then, modify the value of the <tt>Authorization</tt> header to include the secret obtained earlier, as shown in the following example. Make sure to change the <em>scheme</em> value from <tt>Basic</tt> to <tt>Bearer</tt> as well:</p>\n<pre>{\n  \"authorization\": \"Bearer fnEDdByZ5JACFANyg5uLcAISAtUY6TKlIIb2JnZhkjU-SWEaino\"\n}\n</pre>\n<p>With the secret properly set in the request, let’s try to read all of the files on behalf of the employee user. Run the following query from the GraphQL Playground: </p>\n<pre>query ReadFiles {\n  allFiles {\n    data {\n      content\n      confidential\n    }\n  }\n}\n</pre>\n<p>In the response, you should see the public file only:</p>\n<figure><img src=\"{asset:6642:url}\" data-image=\"6642\"></figure>\n<p>Since the role we defined for employee users does not allow them to read confidential files, they have been correctly filtered out from the response!</p>\n<p>Let’s move on now to verifying our second rule:</p>\n<p><em>“Allow manager users to read both public files and, only during weekdays, confidential files.”</em></p>\n<p>This time, we are going to log in as the employee user. Since the login mutation requires an <em>admin</em> secret token, we have to go back first to the original tab containing the default authorization configuration. Once there, run the following query:</p>\n<pre>mutation LoginManagerUser {\n  loginUser(input: {\n    username: \"bill.lumbergh\"\n    password: \"123456\"\n  })\n}</pre>\n<p>You should get a new secret as a response:</p>\n<figure><img src=\"{asset:6643:url}\" data-image=\"6643\"></figure>\n<p>Copy the secret, create a new tab, and modify the Authorization header to include the secret as a Bearer Token as we did before. Then, run the following query in order to read all of the files on behalf of the manager user:</p>\n<pre>query ReadFiles {\n  allFiles {\n    data {\n      content\n      confidential\n    }\n  }\n}</pre>\n<p>As long as you’re running this query on a weekday (if not, feel free to update this rule to include weekends), you should be getting both the public and the confidential file in the response:</p>\n<figure><img src=\"{asset:6646:url}\" data-image=\"6646\"></figure>\n<p></p>\n<p>And, finally, we have verified that all of the access rules are working successfully from the GraphQL API!</p>\n<h1>Conclusion</h1>\n<p>In this post, we have learned how a comprehensive authorization model can be implemented on top of the FaunaDB GraphQL API using FaunaDB's built-in ABAC features. We have also reviewed ABAC's distinctive capabilities, which allow defining complex access rules based on the attributes of each system component.</p>\n<p>While access rules can only be defined through the FQL API at the moment, they are effectively verified for every request executed against the FaunaDB GraphQL API. Providing support for specifying access rules as part of the GraphQL schema definition is already planned for the future.</p>\n<p>In short, FaunaDB provides a powerful mechanism for defining complex access rules on top of the GraphQL API covering most common use cases without the need of third-party services.</p>",
        "blogCategory": [
            "8",
            "10",
            "5603"
        ],
        "mainBlogImage": [
            "6649"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-12-05T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6627,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d20cc35f-2752-4c79-be5a-c2c2385e7c9d",
        "siteSettingsId": 6627,
        "fieldLayoutId": 4,
        "contentId": 2249,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing New FQL Features",
        "slug": "announcing-new-fql-features",
        "uri": "blog/announcing-new-fql-features",
        "dateCreated": "2019-12-04T13:56:59-08:00",
        "dateUpdated": "2020-02-12T16:47:37-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-new-fql-features",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-new-fql-features",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce new FQL capabilities that empower users to write more concise and powerful FQL statements with:</p>\n<ul><li>Date/Time arithmetic functions</li><li>Any/All functions</li><li>Type cast functions</li><li>Type check functions</li></ul>\n<h2>New preview features</h2>\n<p>Please note that these new functions are currently in <a href=\"https://fauna.com/blog/software-naming-releases\">Preview mode</a>, which means that they will be fully supported in the JVM and JS drivers only and will continue to improve over the next 2 months. We invite you to send us feedback about their use. To learn more, please check out our <a href=\"https://docs.fauna.com/\">documentation</a>.</p>\n<h3>Date/Time arithmetic functions</h3>\n<p>The following Date/Time arithmetic functions have been implemented:</p>\n<p><strong>TimeAdd(), TimeSubtract(), and TimeDiff()</strong></p>\n<pre>TimeAdd(Date(\"1970-01-01\"), 1, \"day\")\nTimeSubtract(Time(\"1970-01-01T00:00:00Z\"), 10, \"minutes\")\nTimeDiff(Date(\"1970-01-01\"), Date(\"1970-01-02\"), \"days\")</pre>\n<h3>Any() and All() functions</h3>\n<p>Any() and All() reduce their parameter to a boolean: are any items true, or are all items true, respectively. They work like Or() and And() but for sets, arrays, and pages. </p>\n<p>Unlike the existing boolean functions, Any() and All() have default values for empty inputs. <tt>(Any([])</tt> returns <tt>false</tt>, and <tt>All([])</tt> returns <tt>true</tt>).</p>\n<pre>Any(Map(Paginate(Match(\"user_ages_by_name\", \"Alice\")), Lambda(\"age\", GTE(Var(\"age\"), 18)))</pre>\n<h2>Type cast functions&nbsp;</h2>\n<p>The following type casting functions have been implemented:&nbsp;</p>\n<p><strong>ToDouble, ToArray, ToObject, and ToInteger</strong></p>\n<pre>ToDouble(1) =&gt; 1.0\nToInteger(3.14) =&gt; 3\nToArray({x: 10, y: 20}) =&gt; [[\"x\", 10], [\"y\", 20]]\nToObject([[\"x\", 10], [\"y\", 20]]) =&gt; {x: 10, y: 20}</pre>\n<h2>Type check functions&nbsp;</h2>\n<p>The following type check functions have been implemented:&nbsp;</p>\n<p><strong>IsNumber, IsDouble, IsInteger, IsBoolean, IsNull, IsBytes, IsTimestamp, IsDate, IsString, IsArray, IsObject, IsRef, IsSet, IsDoc, IsLambda, IsCollection, IsDatabase, IsIndex, IsFunction, IsKey, IsToken, IsCredentials, </strong>and<strong> IsRole&nbsp;</strong></p>\n<p>\nAn example of one of these in use:\n</p>\n<pre>IsDouble(3.14) =&gt; true</pre>\n<h3>New GA Features&nbsp;</h3>\n<p>We’re excited to announce that the new FQL functions that we released in Preview mode <a href=\"https://fauna.com/blog/announcing-new-functions-in-fql\">back in September</a> are now generally available. This means that the functions are now also available in our Go, C#, and Python drivers. It also means that these functions are part of the FQL core product and you can expect them to be fully reliable.&nbsp;</p>\n<p>\nNew GA functions include <strong>Range, Reduce, Format, </strong>and<strong> Merge.&nbsp;</strong></p>\n<h2>\nConclusion&nbsp;</h2>\n<p>With our latest FaunaDB update, users now have access to new FQL functionality including date arithmetic and type cast and check functions.</p>\n<p>\n\nThese methods are in Preview! So please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate feedback into the formal release, and visit our <a href=\"https://docs.fauna.com/fauna/2.7.0/guides/rbac\">documentation</a> to learn more.&nbsp;</p>\n<p>\nWhat other functions would you like to see implemented in FaunaDB? Please reach out to me on our <a href=\"https://community-invite.fauna.com/\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "8",
            "1462",
            "1530"
        ],
        "mainBlogImage": [
            "6626"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6421",
        "postDate": "2019-12-03T09:49:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6518,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "32c9e527-bc26-4b41-9bba-7007354b5fa7",
        "siteSettingsId": 6518,
        "fieldLayoutId": 4,
        "contentId": 2233,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "What We Value in Fauna Engineering",
        "slug": "what-we-value-in-fauna-engineering",
        "uri": "blog/what-we-value-in-fauna-engineering",
        "dateCreated": "2019-12-03T09:49:23-08:00",
        "dateUpdated": "2019-12-03T09:55:10-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/what-we-value-in-fauna-engineering",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/what-we-value-in-fauna-engineering",
        "isCommunityPost": false,
        "blogBodyText": "<p>It's not uncommon to see people roll their eyes when managers and companies start talking about values; values are one of those things that are very easy to talk about and orders of magnitude harder to live up to. At a team gathering in Boston this year, we sat down as a full engineering team to write down the values we believe we should work by. It's not uncommon for management teams to show up with a list of commandments brought from on high telling a team how they should act, but to truly own the culture, it's critical that they are driven by the team itself.</p>\n<p>The process we used was relatively simple; folks were randomly broken up into five groups and asked to talk in their group about what they value about the way we work at Fauna and what they aspired for us to rise to. The groups were given 30 minutes or so to talk about what was important to them while I made an emergency run to Starbucks for coffee and breakfast sandwiches.</p>\n<p>Once the conversation portion was done, the teams were asked to sum up what they value in a handful of sentences and to post them on notepads in the front of the room. We then reviewed all of the ideas as a group, asking for clarification on some of them, and diving deeper on others about what made people cherish those values.</p>\n<p>Not shockingly, many of the values from the various groups showed up multiple times and with varying phrasings. To keep our list of values short and to the point, I took the list and outside of the event, worked to condense them into a handful of phrases. The challenge was reducing them to something pithy enough to easily state, without reducing them to something useless or trite.</p>\n<p>Projects like this often take a backburner to hot day to day work, so all too many days later, I brought the values to the management team to review and edit, and then to the larger engineering group.</p>\n<p>In the end, we walked away with these:</p>\n<p><strong>We communicate openly<br>We support others generously<br>We build as a team; we are not our code<br>We build the product we want to use and operate<br>We solve meaningful problems with knowledge and persistence<br>We build with an eye toward the future while grounded in the present<br>We are stronger for our diverse backgrounds, educations, and experiences<br>Research, experimentation, and metrics are the foundations of everything we build</strong></p>\n<p>I think that all things considered they are a good reflection of who we are and how we act. Do we live up to them every day? No. But even within a week of sharing them, I’ve found them useful for clarifying my thinking and decision making, and for having conversations about how we can get closer to these ideals.</p>\n<p>The next step is to make sure we’re living these values. Our first step will be taking each value and week by week reviewing a different one in our one on ones to see how folks throughout the team see us living up to them, and where we need to improve. Just stating our ideals isn’t enough; we need to be reviewing our actions to make sure they aren’t just nice words.</p>\n<p>This was my first time driving a process like this with a team and while I don’t know if it was the best process, I’m happy with the results. I would love to talk with other developers and leaders about how you’ve defined your values and steps you’ve taken to embed them in your culture; you can find me on Twitter as <a href=\"https://twitter.com/mataway\">@mataway</a>.</p>\n<p>If these sound like values that you’d like to work with we’re hiring! Please take a look at our <a href=\"https://fauna.com/careers\">jobs page</a> and see if there is something that would excite you.</p>",
        "blogCategory": [
            "1461",
            "1462"
        ],
        "mainBlogImage": [
            "6517"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6436",
        "postDate": "2019-11-20T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6435,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "22d79a2e-6319-4480-a426-0979687661b0",
        "siteSettingsId": 6435,
        "fieldLayoutId": 4,
        "contentId": 2214,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Building an Authentication SaaS with FaunaDB",
        "slug": "building-an-authentication-saas-with-faunadb",
        "uri": "blog/building-an-authentication-saas-with-faunadb",
        "dateCreated": "2019-11-18T14:26:37-08:00",
        "dateUpdated": "2019-11-20T09:28:13-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/building-an-authentication-saas-with-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/building-an-authentication-saas-with-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>I created <a href=\"https://www.authdog.com/\">Authdog</a> to make an authentication layer easy to integrate in any piece of code (mobile, web app, desktop, service to service, script etc.) while keeping the service at the lowest price possible for the end user. Authdog provides a fully-fledged dashboard in which users can understand authentication workflow out-of-the-box without having to integrate any other tracking service to get log data. It offers the same quality of service to businesses of any size, from start-ups to big corporations, without them having to worry about scalability of authentication.<br></p>\n<h2>Authdog Stack</h2>\n<p>Authdog uses Lambda function as a backend to permit ideal scalability and pricing while growing up. Serverless was chosen due to its simplicity to manage multiple staging/credentials and its native features integrated (KMS encryption, Webpack bundling system, CloudFormation template creation etc.). As Authdog aims to provide a security layer to various companies of any scale, opting for a backend completely serverless (including the database) will have a very positive impact in the company's growth, and will help to reduce maintenance and ops costs as well. <a href=\"https://fauna.com/\">FaunaDB’s</a> low latency characteristics offer the ability for Authdog’s consumer applications to get access to their applications faster.</p>\n<h3>Tools by category</h3>\n<p>Client Side (Netlify): </p>\n<ul><li>Next.js</li><li>React (UI components)</li><li>Redux (State Management)</li></ul>\n<p>Backend (AWS / API Gateway/SQS :</p>\n<ul><li>Apollo (serverless) </li><li><a href=\"https://serverless.com/\">Serverless framework</a><ul><li>Accounts<ul><li>OAuth2.0 workflow</li><li>Authorization / ABAC</li></ul></li><li>Tenants / Applications<ul><li>Authorization / ABAC</li></ul></li></ul></li><li>SQS / Pusher (Notification system)</li></ul>\n<p>As for Data Storage, we use FaunaDB to store and manage all Authdog account data (user profile, tenants, security preferences etc.) and external app data (external apps metadata, groups, rules, permissions, users of Authdog-registered apps, etc.).</p>\n<h2>Why we chose FaunaDB</h2>\n<p>When designing the architecture of Authdog, I faced the question of scale. I want to build Authdog into a global authentication service that caters to the needs of businesses both large and small. I opted for a serverless stack in part to address this need. Using a serverless architecture enables Authdog to achieve desired scale. However, the data layer posed challenges. Databases have always been a bottleneck to highly scalable applications. </p>\n<p>To address this issue, I first looked at a distributed/replicated pgSQL approach, but quickly realized that making a pgSQL infrastructure has a higher cost to permit high availability, especially for thousands of concurrent reads. You need to manage and adapt your infrastructure with physical nodes (Master/Slave replication), per staging environment. In contrast, FaunaDB permits you to have multiple staging environments which won't require such DevOps (no concurrency issue at scale), and the ability to set up Dev/Uat staging (similar performance as production) available 24/7 for testing/development. All of this comes at a very low cost because you're paying only when you're using the database, whereas with a pgSQL infrastructure, you're paying all the time, or have to enable the node when needed, which requires extra DevOps.</p>\n<p>So I evaluated various NoSQL databases (MongoDB, DynamoDB, Voldemort, Riak) that offered the hope to scale horizontally. However, I still felt limited by the pricing of these solutions, at scale, and none of them had all of the features I needed. They all seemed to scale at the expense of some other capability essential to my application. For example, they might scale, but <a href=\"https://docs.fauna.com/fauna/current/comparisons/compare-faunadb-vs-mongodb#consistency-models\">without consistency</a>, or with excessive DevOps. </p>\n<p>Then, along came FaunaDB offering a very appealing, scale-able pricing model with all of the features I needed:</p>\n<h3>Serverless, yet ACID </h3>\n<p>This was the main appeal of FaunaDB. Of course, Dynamo is serverless as well, but it isn't ACID-compliant. From my understanding, FaunaDB is the only NoSQL solution that's 100% ACID-compliant, without any caveats with respect to the number of documents, keys, or data sharding. We can trust that our data will be correct, no matter how we organize it, or what types of queries we write against it  [<a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">1</a>,<a href=\"https://fauna.com/blog/faunadbs-official-jepsen-results\">2</a>,<a href=\"https://jepsen.io/analyses/faunadb-2.5.4\">3</a>].</p>\n<h3>Relational NoSQL data model</h3>\n<p>We knew that we couldn't solve every problem with NoSQL. We would need, at some point, to have structured data and to make relationships between entities. And I think this is what new databases like FaunaDB are trying to achieve [<a href=\"https://medium.com/fauna/relational-nosql-yes-that-is-an-option-5dddef1b57cc\">1</a>,<a href=\"https://docs.fauna.com/fauna/current/whitepapers/relational.html\">2</a>].</p>\n<h3>Easy scalability</h3>\n<p>When you are growing your business, you don't want to spend all your time doing DevOps. You don't want to spend all your time designing an architecture that can support more users. Given that my database would be storing all of my account and external app data, scalability was crucial. With FaunaDB, I knew that I would be able to support millions of users the same way I was using it in development with only one or two users [<a href=\"https://fauna.com/blog/a-comparison-of-scalable-database-isolation-levels\">1</a>]. </p>\n<h3>Handsfree geo-distribution</h3>\n<p>For an authentication app, having high availability is crucial. If you don't have authentication available, you can't use any feature of your application. So it's important to be globally distributed just in case one datacenter is not available. You want multiple replicas all over the world using AWS, Azure, Google Cloud, IBM Cloud. And the ability to synchronously load balance between those regions [<a href=\"https://www.datanami.com/2018/12/12/has-faunadb-cracked-the-code-for-global-transactionality/\">1</a>].</p>\n<h3>Native ABAC/security system</h3>\n<p>I need this feature for managing my FaunaDB database without having to share my admin credentials with all the people I will be working with. So, I can create a key for low-level administration, and another key for an end user who might only need to consume my fauna database as read-only. It's very customizable. This built-in functionality is very useful for what I'm trying to achieve [<a href=\"https://docs.fauna.com/fauna/current/security/abac.html\">1</a>].</p>\n<h3>Document/Object store with powerful indexing</h3>\n<p>With FaunaDB, you've got this abstraction of objects. You can store json objects straight in your database and retrieve them with <a href=\"https://docs.fauna.com/fauna/current/tutorials/indexes/\">indexes</a>. It's a bit more complex than MongoDB, because you have to define your index manually, but in the end you get better performance because you are defining exactly what you need to index. In contrast, with MongoDB, the indexes are created quite naively; you're not indexing a specific column, so you don't have the best indexes for your queries.</p>\n<h3>Native GraphQL interface (OpenCRUD)</h3>\n<p>When I started development on AuthDog, FaunaDB had not yet <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">released the native GraphQL API</a>, so I did not use it for this particular project. Historically, I've developed on top of Apollo Serverless, but I would like to use FaunaDB's OpenCRUD API in the future. Even for this project, Authdog will be divided into smaller microservices, so there will be microservices using this native GraphQL interface, at least for centralizing existing other microservices interfaces.</p>\n<h3>Pricing</h3>\n<p>FaunaDB offers a usage-based <a href=\"https://fauna.com/pricing\">pricing</a> model, including a free tier that is unthrottled. With FaunaDB, I pay for only the resources consumed by my queries, not idle time. Should my usage cross the free tier, I am billed for the excess. As a service operator, I find comfort in the fact that my application will never be blocked should I see spikes in load (which I do hope to see assuming Authdog is successful!).</p>\n<h2>Our Wishlist</h2>\n<p>Although I've been very happy with FaunaDB, there are a few more features from their roadmap that I'm excited to try out in the future:</p>\n<h3>Native cascading features (ability to delete records in FaunaDB that’re linked across collections)</h3>\n<p>This is something I really like in ORMs. You can define all of the relationships between models, and then when you delete a model which is linked to another model, it will delete all of the linked models without you needing to write any custom code. So it will prevent bugs caused by connected collections not being deleted and causing random data to be sitting somewhere without you knowing where it fits in and what it's linked to. In general, I'd like to see a few more relational features in FaunaDB. </p>\n<h3>Real-time subscriptions</h3>\n<p>In all of my applications, I would like to get real-time insights when my data changes, and then notify my users, so you don't have to refresh the page to see if the data has changed. So for now, my plan is to use a simple queue service on AWS, but having something provided directly by the database would be a huge plus.</p>\n<h3>Easy backup/Migration</h3>\n<p>I've heard that Fauna's temporality is a sort of built-in auditing/version control that allows you to access a snapshot of your database at any point in time. However, I wish there were an easier way from within the dashboard to replicate a database's previous or current state. From my understanding, I would need to write some custom code to do this.</p>\n<h2>Conclusion</h2>\n<p>FaunaDB offers features that were critical for the development of Authdog, including native ABAC, scalability, and 100% ACID transactions. It’s the only serverless databases that checked all of the boxes for me. Furthermore, I’m excited to see even more features like streaming and migration coming in the next few months. </p>\n<p>Shipping a product as fast as possible is critical as a startup founder, and FaunaDB has helped me in my mission.</p>\n<h2>About the Author</h2>\n<p>David Barrat, founder of Authdog, is a Data Engineer within the Data Engineering & Dynamic Regulatory Reporting group at Novartis Pharmaceuticals in Basel, Switzerland, with a focus on Architecture, Data Integrity, and Security to make Clinical applications safer and more robust for large audiences.</p>\n<p>Note: Authdog is still under development & testing. First public release is expected early 2020 (late Q1/early Q2).</p>",
        "blogCategory": [
            "3",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "6434"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6421",
        "postDate": "2019-11-12T08:57:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6420,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "4e3f4a8a-3743-42cb-bc89-c08991d76397",
        "siteSettingsId": 6420,
        "fieldLayoutId": 4,
        "contentId": 2199,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB Outage Review",
        "slug": "faunadb-outage-review",
        "uri": "blog/faunadb-outage-review",
        "dateCreated": "2019-11-12T08:57:16-08:00",
        "dateUpdated": "2019-11-12T09:22:25-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-outage-review",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-outage-review",
        "isCommunityPost": false,
        "blogBodyText": "<p>As software engineers, we all know too well that the question is not if you’ll hit a bug or have an outage, but when. However, as a database system provider, our responsibilities in this area are higher than most as our downtime becomes your downtime.</p>\n<p>From 9/23 to 10/17, FaunaDB experienced a series of issues that impacted our users and I want to take some time to walk through them, how we dealt with them, and our steps to prevent them from reoccurring. I also want to touch briefly on our testing process so that you can get a sense of how serious we take our responsibility to you.</p>\n<h2>Election Storms</h2>\n<p>One of the first issues that arose in this period of instability was what we now refer to as \"election storms\". To understand this bug it is important to first understand a bit about FaunaDB. FaunaDB requires there to be a globally shared log to track transactions. We segment this log by the number of nodes in each replica to share the load. Each segment uses RAFT to manage consensus about the log segment across replicas.</p>\n<p>When all is well in the system, a leader is elected in each RAFT ring and transactions flow into their various log segments. However, if an election is called because, for example, the node with the leader goes down, there is a subsecond pause in the flow of transactions while a new leader is elected. In a properly functioning system, this election is unnoticeable. </p>\n<p>In the case of this incident, there was one replica that had two replicas connected to it on connections with very similar latency. Due to the network topology of the system, the one replica was in the middle of the other two. Occasionally a new leader election would be called, and the outer nodes would end up repeatedly calling elections due to a very narrow window where the nodes on the edge would think they were elected, and then get a request for an election from the far node. This pattern could loop effectively indefinitely. Eventually, natural fluctuations in latency would cause an election to stick and a leader was chosen, but during that \"storm\", no transactions could be processed. </p>\n<p>One of the stats we track in the system is the number of elections being called as it is an indicator of overall system health. We were able to see that elections were spiking and identified that restarting one of the involved nodes rectified the issue, but it took significant debugging and research of the related logs to figure out the cause of the issue.</p>\n<p>As is frequently the case with these sorts of distributed consensus algorithms, the solution was to add some variance to the call for an election time such that any cycle of elections would be more quickly broken. Being this was a fundamental change to the system, the engineering team moved cautiously, restarting nodes involved in the election storms as necessary keep the system operating. This fix was rolled out to production in version 2.9.1 and no further election storm events have been seen.</p>\n<h2>Poison Pill</h2>\n<p>In the midst of triaging these election storms, a separate issue occurred which caused two additional outages on 10/9 and 10/10. With the Calvin algorithm, there is a transaction log that all replicas write to. Transactions are written to the log in a globally ordered fashion; as soon as a quorum of nodes acknowledge that a transaction has been written to their logs and at least one node writes the transaction to storage, a response can be given back to the requestor. Given that all of the replicas have the same data, as long as they apply the transactions in the same order we can guarantee that the transaction will be replayed the same way throughout the system.</p>\n<p>It is important to note that with FaunaDB the entry is written to the log and then separately executed and stored. Being a deterministic transactional system, transactions must be applied in order. </p>\n<p>With the first incident, a transaction made it into the log that could be written to the log, but then could not be applied to storage. We have guards in place to reject transactions that cannot be applied to storage, but in this case, the data was in the system from a time before we had these guards and was picked up by a new index building task. Unfortunately, even with the large amounts of introspection we have in the system, it was not clear at first blush what was occurring and took a while to triage and develop a remediation.</p>\n<p>With the second incident, a malformed transaction was introduced to the write log by an internal administrative tool. The method that caused the write to fail was different from the one from the first incident, and it required using the remeditation technique two times to get the system to return to normal.</p>\n<p>Since then we have hardened the system in a number of ways to prevent this category of problems. In 2.9.1 (currently live in production) we put guards in place to defend against existing oversized data from blocking the transaction pipeline. We've also added admin commands to make it easy for the on-call engineer to remove a blocking transaction. In our upcoming release, we will add functionality to automatically quarantine a write that fails to apply so that the transaction log will continue to flow. The failure to apply in all three cases was a side effect of constraints in our storage layer. In addition to the measures above, we are also researching new storage engines that will be more flexible than our current one.</p>\n<h2>Latency Spikes in SA and Singapore</h2>\n<p>The final incident in the series of issues was a large spike in latency for users in South America and Asia on 10/18. The on-call team began looking into the regions involved and saw that the connection latency between our new compute-only nodes in South America and Asia and the rest of the cluster had sky-rocketed. </p>\n<p>Compute-only nodes are nodes that are configured to process user requests, but do not have their own copy of the data, so they must request data to calculate a transaction from the rest of the cluster. The nodes were set up this way to prevent user data from being stored outside of areas protected by GDPR and US Privacy Shield. </p>\n<p>The team ran test queries from the impacted regions and saw that queries routed over the public internet to the data replicas in North America were faster than queries routed through our compute-only replicas. With no local data, these compute-only nodes had to make multiple internal read requests over high latency connections to data nodes in North America. In comparison, with queries routed directly to the North America replicas, the initial connection had high latency, but the internal read requests were over extremely low latency connections. Unfortunately this undermined the expected benefit of local compute nodes for the new regions. </p>\n<p>We have decided to temporarily decommission the compute-only nodes until we can bring them back as full data replicas.</p>\n<h2>How We Test</h2>\n<p>It would be well worth a full article on our testing process, but I think it is important to at least briefly talk about it to give some insight into the process.</p>\n<p>We currently have four test suites that we run:</p>\n<ul><li><strong>Single machine test suite</strong> - this is a suite of tests that can be run on a developer's machine. Some folks might call these smoke tests as we use them to validate pull requests, but they take quite a bit longer to run than a standard unit or smoke test suite. These tests stand up a FaunaDB cluster and run a variety of tests. There are also thousands of unit tests of various subsystems as a part of this process.</li><li><strong>Regression suite</strong> - this is a battery of tests that operate against a cluster running on multiple machines. This test suite runs for about eight hours and executes a number of tests with controlled replica and node failures. Due to the long run time, this suite is run nightly. It can be thought of as a preprogrammed \"chaos monkey\", causing node and replica failures in a pattern calculated by the dev team to make sure failure cases that could impact availability or correctness are caught.</li><li><strong>Jepsen</strong> - since the release of our Jepsen report in early 2019, we have run Jepsen on a regular basis to validate that we are maintaining our correctness claims as we make changes to fundamental systems like the RAFT changes mentioned above. Jepsen immensely stresses database systems and has been a good method to validate our system. We also use it as a soak tester in some cases as it can be run continuously.</li><li><strong>YCS</strong><strong>B</strong> - we use YCSB as a primary performance test framework as it is widely understood in the industry. YCSB is not meant for distributed databases, but we have built custom workloads to be able to stress FaunaDB in more interesting ways. YCSB is also run nightly so that we can catch performance regressions more easily.</li></ul>\n<p>We also have a very robust code review process to ensure all FaunaDB changes are thoroughly reviewed and vetted. Two reviewers are required to approve any commit to master.</p>\n<h2>Next Steps</h2>\n<p>We have now been running with version 2.9.1 for a couple of weeks and have had no incidents related to FaunaDB since its release. It was important to us to make sure that we were confident in our fixes before we came back to our community to say the issues were addressed. In the future, we would like to get the cycle time from incident to this sort of writeup to be shorter, but we will tend to err on the side of caution; quick bandage fixes are not the way to maintain and grow a database system like FaunaDB.</p>\n<p>As part of the incidents above we have also refined our incident response process and replaced our former status page with an industry standard which is both easier for us to use and easier for you to consume. Those status updates are also published to our <a href=\"https://twitter.com/faunastatus?s=20\">@faunastatus</a> twitter account and to #cloud-status in our community Slack.</p>\n<p>In addition to the fixes mentioned above, there is a lot of research going on to minimize latency spikes in the system and improve FaunaDB’s performance and operational stability. The fixes mentioned above are just part of the work that we're doing to make sure FaunaDB is the stable, globally available system you need. </p>\n<p>If you have any questions about anything from this post, or about our processes in general, please feel free to reach out to me at <a href=\"mailto:matt.attaway@fauna.com\" target=\"_blank\">matt.attaway@fauna.com</a> or in our <a href=\"https://community.fauna.com/\">community Slack</a> and I will be happy to answer them. From the engineering team, we highly appreciate your support during the incidents and are hard at work to ensure that data storage is the easiest part of building your applications.</p>\n<p>As you can tell from the incidents above, building a global deterministic transactional database brings an endless number of interesting problems to solve. If you think these sound like the sort of challenges you’d like to tackle, <a href=\"https://fauna.com/careers\">we’re hiring</a>!</p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "6419"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "5873",
        "postDate": "2019-10-31T07:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6383,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "679512c8-3ed8-4163-bb64-571b9e49dd3c",
        "siteSettingsId": 6383,
        "fieldLayoutId": 4,
        "contentId": 2177,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Software Naming & Releases",
        "slug": "software-naming-releases",
        "uri": "blog/software-naming-releases",
        "dateCreated": "2019-10-30T13:38:34-07:00",
        "dateUpdated": "2019-10-30T13:40:58-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/software-naming-releases",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/software-naming-releases",
        "isCommunityPost": false,
        "blogBodyText": "<p>If you have monitored our release cadence closely over the last several months, you will have seen that we have substantially increased our velocity. As we shifted our focus with FaunaDB to be a cloud-based offering, it provided freedom in release schedule and we’ve taken advantage of that.</p><p>Of course, there are a series of common ‘descriptors’ of software releases that carry weight across the industry. Unfortunately, each of these terms carries its own baggage based on your prior experience and understanding of software release schedules.</p><p>We’d like to clarify what versioning, and naming, means for our users when they use FaunaDB. Keep in mind that there are multiple ways to use our products (web console, direct endpoint, Javascript driver, Java driver, Scala driver, Golang driver, etc..) As such, much of the complexity that lies within our versioning and release schemes are opaque to our users. But...sharing our perspective and approach is important when ensuring clarity.</p><h2>Release Cadence & Numbering</h2><p>In general, we intend to release monthly. This can be more, or less, frequent as the complexities of managing a globally distributed, consistent, multi-API datastore are several. However, a frequent release schedule ensures we are bringing the features that you, our users, desire (and sometimes demand) to market more frequently while also ensuring stability.</p><p>We have broadly adopted <a href=\"https://semver.org/\">Semantic Versioning</a> (semver) for our release and adhere to the principles that:</p><ul><li>a MAJOR version is where we make incompatible API changes</li><li>a MINOR version is where we add functionality in a backwards compatible manner</li><li>a PATCH version is where we make backwards compatible bug fixes</li></ul><p>Or, perhaps:</p><ul><li>a MAJOR version is where we create awesome features that might require work to update your application if you want to benefit from them. But, no rush, they are there when you are ready.</li><li>a MINOR version is awesome features for free! (all disclaimers apply) little work, new features, much win.</li><li>a PATCH version is where stability improves, you may not ‘see’ a difference but we’ll put all the internals changes in release notes.</li></ul><p>With that said, flexibility is key and a feature may slip into a PATCH if necessary but we commit to maintaining backwards compatibility in the manner identified above. </p><h2>General Availability (GA)</h2><p>We get it, we release often. We also get it that changing your production code can be onerous. But, take heart, we face the same challenges and requirements ourselves daily. When we do push a release to cloud and drivers as GA, our commitment is that it will be stable, simple to consume, and compatible with previous versions.</p><p>A ‘breaking change’ (for Fauna) is one that requires you to do work to adopt the new driver version. If we break existing apps this is the most serious category of bug we will ever face. </p><h3>Preview</h3><p>Similarly, there are API features that we believe are important, that we have designed in partnership with our community which we are excited about. We’d love to have people from various backgrounds review our work in various emotional states and share their feedback in detail. These ‘preview’ features represent our current point-of-view on implementation and are a glimpse into the new use cases the community desires.</p><p>Some preview features require us to introduce breaking changes or fundamentally rework the API. Since, sometimes our initial idea was just not so great and it has to be improved/simplified. Typically these breaking changes are driven by our community's feedback. We aren’t perfect, yet...but we want to better each release.</p><h3>Experimental</h3><p>As mentioned FaunaDB appears to some as an API and some as a web console (and hopefully both). For those who use the console frequently, there are some exciting features that we have been experimenting with for which we would love you, but we need your feedback.</p><p>If you are the sort of human who enjoys seeing what we are dreaming about from a UI/UX perspective, knows that things break and that’s sad but how we learn, and that excites you...or maybe you like to see our dreams translated in a user interface? Well, our dreams (aka experimental features) end up in our web console. If you feel like playing with them and sharing your feedback, that would be immensely valuable to us.</p><h3>End-of-Life (EOL)</h3><p>Speedy releases shouldn’t mean constant, meaningless upgrades to maintain the ability to get support from ourselves or the community. As we approach the next major release (at time of this writing 3.0) we will support each major release of our product for 12 months from the release date, and we actively maintain the last minor release of the most recent major branch. So, for instance, as we working on 3.x.x we will maintain 2.latest.x.</p><p>In addition, we recognise that -- at some point -- it will become necessary to retire features (particularly at the API level) and as that time approaches we will document and share our approach more broadly.</p><h2>Conclusion</h2><p>At some point, in the not too distant future, we will write this up in proper legal terms and put on the website and in support agreements. However, sharing in the language of a human was an important first step.</p><p>Thank you for building your application on FaunaDB and, most importantly, for trusting us with your feedback.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "6382"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-10-28T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6372,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "f24a3715-52fe-4471-83f0-2f459d33718c",
        "siteSettingsId": 6372,
        "fieldLayoutId": 4,
        "contentId": 2166,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing Fauna’s membership in GraphQL Foundation",
        "slug": "announcing-faunas-membership-in-graphql-foundation",
        "uri": "blog/announcing-faunas-membership-in-graphql-foundation",
        "dateCreated": "2019-10-25T13:34:17-07:00",
        "dateUpdated": "2019-10-28T15:47:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-faunas-membership-in-graphql-foundation",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-faunas-membership-in-graphql-foundation",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re thrilled to announce <a href=\"https://www.prnewswire.com/news-releases/graphql-foundation-launches-interactive-landscape-and-welcomes-new-members-from-open-source-summit-europe-300945869.html\">Fauna’s membership in the GraphQL Foundation</a>. With this membership, we join an elite group of innovators in the GraphQL space, including 8base, Elementl, Expedia, Prisma, Shopify, and Twitter.</p><blockquote>\"At Fauna, we have seen first-hand the productivity and architectural benefits GraphQL offers developers. We believe that an open ecosystem and shared specification are a benefit to all. We’re excited to contribute to the GraphQL community and collaborate on its continued innovation.\" - Matt Freels, CTO, Fauna, Inc.</blockquote><h2>FaunaDB + GraphQL</h2><p>GraphQL has been of central and growing importance at Fauna. Earlier this year, in June, we released <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">native GraphQL functionality</a> into FaunaDB. This feature set supported importing GraphQL schemas into FaunaDB via the FaunaDB Console. The update also included an integration with GraphQL Playground, which is one of the most popular ways developers interact with GraphQL.</p><p>It’s important to mention that <strong>FaunaDB brings same guarantees to GraphQL that we do to FQL</strong> (<a href=\"https://docs.fauna.com/fauna/current/api/fql/\">Fauna Query Language</a>). FaunaDB's GraphQL API leverages the Calvin-inspired shared core to offer developers uniform access to transactional consistency, user authorization, data access, quality of service (QoS), and temporal storage.</p><ul><li><strong>Temporality:</strong> FaunaDB is the only database that provides built-in temporality support with no limits on data history. With per-query snapshots, any API in FaunaDB can return data at any given time.</li><li><strong>Consistency:</strong> FaunaDB offers the highest consistency levels for its transactions. These strong consistency guarantees are automatically applied to all APIs.</li><li><strong>Authorization:</strong> Unlike most databases that control access at a table level, FaunaDB provides access control at the document level. This fine-grained access control is applicable to all APIs, be it GraphQL or SQL.</li><li><strong>Shared Data Access:</strong> In keeping with Fauna's dedication to modern polyglot development, data written by one API (e.g., GraphQL) can be read and modified by another API (e.g., FQL). In contrast, most other databases limit APIs to their specific datasets.</li><li><strong>QoS: </strong>FaunaDB’s built-in prioritization policies for concurrent workloads are enforced at the database level or with access keys. All API access automatically adheres to these QoS definitions.</li></ul><p>We also work with popular IDEs and GraphQL providers, including <a href=\"https://fauna.com/blog/building-a-job-posting-platform-with-faunadb-and-apollo\">Apollo</a> and <a href=\"https://blog.hasura.io/joining-postgres-data-with-faunadb-serverless-hasura-remote-joins/\">Hasura</a>. FaunaDB offers a <a href=\"https://fauna.com/pricing\">generous free tier</a> to help you kick the tires on your project. <a href=\"https://dashboard.fauna.com/accounts/login\">Try it out now</a> &#x1f604; </p><h2>Growing importance</h2><p>For FaunaDB, GraphQL has grown from being a nice-to-have feature to a core piece of our product offering and company strategy. Indeed, in the last few months, our Monthly Active user adoption for GraphQL has grown to 40%:</p><figure><img src=\"{asset:6375:url}\" data-image=\"6375\"></figure><p><strong>This means that 40% of users who use FaunaDB this month are running GraphQL queries</strong>. We expect this percentage to grow even more throughout this year as we continue to add rich features to our GraphQL implementation.</p><h2>Continuing to contribute with partial updates</h2><p>Since our summer release of native GraphQL, we’ve continued to release important features for our developer community, including <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-4-updating-your-schema\">partial schema updates</a> and <a href=\"https://fauna.com/blog/no-more-n-1-problems-with-faunadbs-graphql-api\">batch solutions</a>. </p><p>A quick aside to explain partial update:</p><p>In order to tell to the import process to not create all of these elements automatically, we need to use Fauna’s built-in @embedded directive. If annotated with the @embedded directive, no ID field nor CRUD operations are generated for the given object. </p><p>We can create a file with an updated schema using the @embedded directive for defining our new Address object type:</p><pre>type User {<br/>   username: String! @unique<br/>   address: Address<br/>}<br/>type Address @embedded {<br/>   street: String!<br/>   city: String!<br/>   state: String!<br/>   zipCode: String!<br/>}</pre><h2>Conclusion</h2><p>Joining the GraphQL Foundation takes us one step closer to becoming an inextricable piece of this rich landscape of bleeding edge web tools. We are excited to continue to innovate in this space. We know that GraphQL is a core aspect of the <a href=\"https://www.netlify.com/tags/graphql/\">JAMstack ecosystem</a>, and intend to further cement FaunaDB as the default database for modern web projects.&nbsp;</p><p>What other features for GraphQL would you like to see implemented in FaunaDB? Check out how others are using GraphQL in Fauna’s <a href=\"https://community-invite.fauna.com/\">Community Slack</a>, and describe any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "6371"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6158",
        "postDate": "2019-10-18T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6157,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "004554c9-ce94-4ddf-94b9-ab9251361a7a",
        "siteSettingsId": 6157,
        "fieldLayoutId": 4,
        "contentId": 2119,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing Advanced String and Aggregate Functions in FQL",
        "slug": "advanced-string-aggregate-functions-in-fql",
        "uri": "blog/advanced-string-aggregate-functions-in-fql",
        "dateCreated": "2019-10-01T09:35:01-07:00",
        "dateUpdated": "2020-03-27T14:27:06-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/advanced-string-aggregate-functions-in-fql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/advanced-string-aggregate-functions-in-fql",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce new FQL capabilities that empower users to write more concise and powerful FQL statements with aggregate functions for sets (Count, Mean, Sum); string predicate functions (StartsWith, EndsWith, ContainsStr, ContainsStrRegex, RegexEscape); the MoveDatabase function; and the Now function.<br></p>\n\n<h2>Aggregate functions for sets</h2>\n<p>Following up on the recent addition of the Reduce() function, we have added new functions for common aggregations. Count(), Mean(), and Sum() functions all work on pages, arrays, and sets. Min() and Max() have also been enhanced to work on sets (as well as pages and arrays).</p>\n<h3>Count() function</h3>\n<p>Count() can be used to return a count of elements in any set, array, or page. </p>\n<pre>Count(Match(Index(\"all_animals\"))) //=&gt; 30</pre>\n<h3>Mean() and Sum() functions</h3>\n<p>We have added a couple functions for computing the mean and sum of indexed numeric data.</p>\n<pre>Mean(Match(\"user_ages_by_name\", \"Alice\")) //=&gt; 43.75</pre>\n<pre>Sum(Match(\"user_ages_by_name\", \"Bob\")) //=&gt; 1384</pre>\n<h2>String predicate functions</h2>\n<p>We have added some additional functions which make it easier to search for specific strings with Filter() by returning true or false rather than an index or match structure.</p>\n<h3>ContainsStr(), StartsWith(), and EndsWith() functions</h3>\n<p>These functions allow you to test if a search string contains a substring.</p>\n<p>ContainsStr() returns true if the search string appears anywhere in the test string:</p>\n<pre>ContainsStr(\"Alice is 7 years old\", \"7\") //=&gt; true</pre>\n<p>StartsWith() and EndsWith() test if the search string is a prefix or suffix of the test string, respectively:</p>\n<pre>StartsWith(\"Alice is 7 years old\", \"Alice\") //=&gt; true</pre>\n<pre>EndsWith(\"Alice is 7 years old\", \"old\") //=&gt; true</pre>\n<h3>ContainsStrRegex() function</h3>\n<p>This function allows you to check if a string matches a regex, which can be useful for more advanced predicates.</p>\n<pre>ContainsStrRegex(\"Alice is 7 years old\", \"\\d+\") //=&gt; true</pre>\n<h3>RegexEscape() function</h3>\n<p>In order to be able to construct regular expressions from input data, it can be useful to escape the input, transforming it into a regex that matches the actual input value. RegexEscape() lets you do just that:</p>\n<pre>ContainsStrRegex(\"Alice has $10\", RegexEscape(\"$10\")) //=&gt; true</pre>\n<h2>Even more functions!</h2>\n<h3>MoveDatabase() function&nbsp;<sup><em>Preview</em></sup></h3>\n<p>MoveDatabase() provides the ability to move a database to another point in a database hierarchy, similar to the `mv` command for your file system. For example, say you are using databases to represent environments, and you move an app database from one to the other:</p>\n<pre>MoveDatabase(Database(\"appdb\", Database(\"dev\")), Database(\"prod\"))</pre>\n<p>When moving a database, keys associated with the moved database move along with it, so previously distributed access keys continue to work. However, keys associated with the old parent may no longer be able to read or write into the moved database.</p>\n<h3>Now() function</h3>\n<p>Previously, the only way to get the current time within a transaction (technically speaking, the transaction’s snapshot read time) was by passing the special argument \"now\" to Time(). Now we have a dedicated function for it, which we hope is easier to find if you are new to FQL.</p>\n<pre>Now() //=&gt; Time(\"2019-10-18T09:00:00.000Z\")</pre>\n<h2>Translating OpenCRUD predicates</h2>\n<h3>Highlighting our users</h3>\n<p>The request for many of these new functions comes from our friends at <a href=\"https://graphcms.com/\">GraphCMS</a>, who helped identify additional areas for growth in our FQL API. Creating these new functions enables us to translate <a href=\"https://www.opencrud.org/\">OpenCRUD</a> predicates to queries on a single, field-sorted index, which was a request from GraphCMS that we were excited to fulfill.</p>\n<p>Given the index below, which has no terms, and covers the “X” field and the document ref:</p>\n<pre>{   \"name\": \"coll_by_x\",  \"source\": Collection(\"a_collection\"),  \"terms\": [],  \"values\": [{\"field\": [\"data\", \"x\"]}, {\"field\": \"ref\"}]}</pre>\n<p>The following are FQL representations of <a href=\"https://github.com/opencrud/opencrud\">OpenCRUD</a> single-field predicates:</p>\n<h3>Strings</h3>\n<p><strong>field_contains (contains string):</strong></p>\n<pre>Filter(Match(\"coll_by_x\"), (x, r) =&gt; ContainsStr(x, &lt;value&gt;))</pre>\n<p><strong>field_starts_with (starts with string):</strong></p>\n<pre>Filter(Match(\"coll_by_x\"), (x, r) =&gt; StartsWith(x, &lt;value&gt;))</pre>\n<p><strong>field_ends_with (ends with string):</strong></p>\n<pre>Filter(Match(\"coll_by_x\"), (x, r) =&gt; EndsWith(x, &lt;value&gt;))</pre>\n<h3>Comparison</h3>\n<p><strong>field_lte (less than):</strong></p>\n<pre>Range(Match(\"coll_by_x\"), [], &lt;value&gt;)</pre>\n<p><strong>field_gte (greater than):</strong></p>\n<pre>Range(Match(\"coll_by_x\"), &lt;value&gt;, [])</pre>\n<h2>Conclusion</h2>\n<p>With our latest updates, users now have access to additional&nbsp;powerful regex and aggregate functionality. </p>\n<p>These methods are in Preview! So please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate feedback into the formal release, and visit our <a href=\"https://docs.fauna.com/\">documentation</a> to learn more.</p>\n<p>What other functions would you like to see implemented in FaunaDB? Please reach out to me on our <a href=\"https://community-invite.fauna.com/\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "6159"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "475",
        "postDate": "2019-09-30T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6153,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "96c21e06-957b-4ca2-b70f-c766abaaddb5",
        "siteSettingsId": 6153,
        "fieldLayoutId": 4,
        "contentId": 2115,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How FaunaDB Saves JAMstack Developers from Data Loss",
        "slug": "how-faunadb-saves-jamstack-developers-from-data-loss",
        "uri": "blog/how-faunadb-saves-jamstack-developers-from-data-loss",
        "dateCreated": "2019-09-27T09:02:03-07:00",
        "dateUpdated": "2019-10-01T09:43:31-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/how-faunadb-saves-jamstack-developers-from-data-loss",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/how-faunadb-saves-jamstack-developers-from-data-loss",
        "isCommunityPost": false,
        "blogBodyText": "<p>A developer likes solving large and complex issues for their customers by writing flashy programs which interact with the desktop, web, mobile, or other devices. To accomplish this, the developer requires a solid data platform to store and retrieve data. The better aligned the data platform is with the developers’ requirements, the less mundane tasks the developer has to build and the more time they can spend writing exciting code for their customers. </p>\n<p>An example of a data platform requirement can be transactional consistency. Some databases adhere to eventual consistency, while FaunaDB has globally consistent transactions. While both can be used, if the developer has a requirement of globally consistent data and they choose an eventually consistent database as part of their data platform, the developer will be building an excessive amount of logic into the application. If an efficient data platform was chosen, the developer could spend more time on solving the customer’s application needs and a flashier application. </p>\n<p>In this article I will discuss some of the data protection issues related to data loss and how a developer can avoid or recover from them. While transactions play a large part in this I am going to focus on other types of data loss, such as catastrophic or schema loss. </p>\n<h2>Catastrophic Failures </h2>\n<p>One of the first data safety issues to explore is a catastrophic failure of a site, single business provider, geographical region. With today’s availability standards this is very important but often forgotten. FaunaDB keeps multiple live copies of the data on several geographically dispersed replicas on different cloud providers (see map below). Due to the FaunaDB’s ACID transactional consistency model (<a href=\"https://fauna.com/blog/faunadbs-official-jepsen-results\">see Jepsen</a>) the application can utilize any or all sites and achieve the same consistent results. In addition to the multiple copies of live data, FaunaDB has a built-in traditional backup to offsite storage. This allows FaunaDB to store the data and transaction logs in an encrypted offsite location. These files can be used to reconstruct FaunaDB entire cluster to the current point in time in case of a major catastrophic failure.</p>\n<figure><img src=\"{asset:6149:url}\" data-image=\"6149\" style=\"max-width: 600px;\"></figure>\n<h2>Accidentally Deleted Document</h2>\n<p>Another issue which often affects developers is the ability to restore accidentally removed or updated documents. In FaunaDB, this is simple because of FaunaDB’s temporal capabilities. This restoration of data can be accomplished in seconds utilizing only the FaunaDB FQL language and appropriate permissions. Let outline the three simple steps to Un-Delete your document in seconds: </p>\n<ol><li>You need to identify the refs to the document(s) which were affected.</li><li>List the events for each affected document locating the timestamp for the most recent remove event:<br><img src=\"{asset:6150:url}\" data-image=\"6150\" style=\"max-width: 375px;\"></li><li>Remove the delete event for the document by providing the document reference and the timestamp for the delete event:<br><img src=\"{asset:6151:url}\" data-image=\"6151\" style=\"max-width: 375px;\"></li></ol>\n<p>While there are limits on how old the data is that can be undeleted, this is controlled by the developer when creating a collection (i.e. table) and defaults to 30 days. In many other databases the developer often has to contact a DBA to do some type of restore from offline media.</p>\n<h2>Accidentally Deleted Collection or Database </h2>\n<p>The accidental dropping of database or collection is another one of the many actions developers like to safeguard against. While the same methodology of modifying the events of the database or collection could be applied to instantaneously recover a database or collection, the developer currently does not have access to internal schema events. Fauna is currently planning to release a simple way to modify these events to allow the undeleting of databases and collections. In this way a developer will be able to correct the accidental dropping in seconds instead of hours or days. </p>\n<h2>Data Security </h2>\n<p>Data loss can also occur because of theft of data. First and foremost, all data, in monition on a public network or at rest on disk drives, is encrypted. FaunaDB has an elaborate security model intended for the cloud which can be read about in a <a href=\"https://fauna.com/blog/data-security-in-the-age-of-cloud-native-apps\">previous blog post</a>. </p>\n<p>In addition, FaunaDB has a new security feature called Attribute-Based Access Control (ABAC). This is an alternative to an all-or-nothing security model and is commonly used in applications to restrict access to specific data based on the user’s role. ABAC is an extension of role-based access control (RBAC), where users are assigned roles that grant them specific privileges. The benefit of ABAC is that privileges can be dynamically determined based on attributes of the user, the documents to be accessed or modified, or context during a transaction (e.g. time of day). </p>\n<h2>Conclusion&nbsp;</h2>\n<p>While data loss is something that no one ever wants to experience, we all need to prepare for the event and have a plan for the various scenarios. Fauna’s cloud solution builds into the base architecture encryption and the ability to recover from catastrophic system failures. FaunaDB’s temporal nature to provide it users with an impressive ability to instantaneously recover data which might have been accidentally removed. Preparing for a data loss event is like having life insurance:  you never want to use it, but your family is glad that you have it!</p>",
        "blogCategory": [
            "1462"
        ],
        "mainBlogImage": [
            "6152"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "6140",
        "postDate": "2019-09-26T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6139,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9a322d14-73a7-49af-a582-bf0b2b0f7922",
        "siteSettingsId": 6139,
        "fieldLayoutId": 4,
        "contentId": 2101,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Building a Job Posting Platform with FaunaDB and Apollo",
        "slug": "building-a-job-posting-platform-with-faunadb-and-apollo",
        "uri": "blog/building-a-job-posting-platform-with-faunadb-and-apollo",
        "dateCreated": "2019-09-25T13:31:29-07:00",
        "dateUpdated": "2019-09-26T09:51:48-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/building-a-job-posting-platform-with-faunadb-and-apollo",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/building-a-job-posting-platform-with-faunadb-and-apollo",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://app.talenth.co/\">TalentHub</a> started as a small weekend project and evolved into a full-fledged open source job posting platform. The idea came from a simple problem my friends had while they were looking for a job, which was a need for a clean, intuitive job board with location support. In the future, we plan to include applicant tracking functionality as well. </p>\n<p>Ultimately, the mission is to make a single open source platform for employers and employees to help them solve job and talent matching challenges.</p>\n<p>You can view the app in production <a href=\"https://app.talenth.co\">here</a>. TalentHub's code is 100% open-source on Github; you can look through it <a href=\"https://github.com/spacehelmet/talenthub\">here</a>.</p>\n<h2>TalentHub Stack </h2>\n<p>TalentHub uses <a href=\"https://www.apollographql.com/docs/apollo-server/\">Apollo Server</a> as a thin middleware between FaunaDB and Apollo Client. The main reason for having a middleware layer was security and more control over resolvers. It hides auth tokens that would live in client otherwise, and gives more flexibility in resolvers, such as hashing passwords and generating tokens.</p><figure><img src=\"{asset:6146:url}\" data-image=\"6146\"></figure>\n\n<h2>Why TalentHub chose FaunaDB</h2>\n<p>Let's start with the need. Initially, TalentHub had its own back-end server that I wrote in Ruby on Rails. It was originally hosted on Heroku, and I also tried DigitalOcean. However, it became pretty cumbersome to maintain the server side, pricing tiers, and managing the performance. Eventually, it was clear to me that having my own server slowed down the product delivery cycle and prevents shipping new features as fast as I would like. </p>\n<p>So I started to look for database as a service (DBaaS) providers. As a bonus, I wanted it to have a GraphQL API so I could reuse a lot of components in the front-end. </p>\n<p>The services I came across after some research were:</p>\n<ul><li>Firebase</li><li>graph.cool</li><li>FaunaDB</li><li>Scaphold.io</li></ul>\n<p>I decided against Firebase because of the incident with Parse (I mean when it got shut down). I felt there was no guarantee that Firebase would not have the same fate. Similarly, Scaphold.io was acquired by Amazon. Also, I love experimenting with new tech (early adopter), and supporting teams doing something new. </p>\n<p>So, at the end of the day, I was left to choose between graph.cool and FaunaDB. While graph.cool looks cool (as they also claim in their naming) and has a lot of supporters, blog posts, and examples, I preferred FaunaDB's architecture, community, and <a href=\"https://fauna.com/pricing\">pricing strategy</a>. Specifically, the serverless nature of FaunaDB and guaranteed global transactions appeal from an architectural standpoint.</p>\n<p>Another decision-making factor was that FaunaDB has a user friendly <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">GraphQL interface</a> with a visual dashboard for managing keys, databases, and sanity checking everything. It is also worth noting that FaunaDB has a nice and clean CLI. It was also pretty easy to setup and start using the system. </p>\n<p>Finally, I discovered that the community around FaunaDB is awesome. It is still small, but I am sure that you will not get the same level of support and care anywhere else. This was definitely something I considered while choosing the service. I would recommend joining FaunaDB’s <a href=\"https://community-invite.fauna.com/\">Community Slack</a> to stay up to date and ask any questions you have.</p>\n<p><strong>So, I would say a friendly CLI, native GraphQL support, a nice community, and reasonable pricing were the main FaunaDB selling points.</strong></p>\n<h2>What's next for TalentHub and FaunaDB?</h2>\n<p>While I have been very happy with FaunaDB, there are a few features I would like to see in the future. For example, the list includes stuff like:</p>\n<ul><li><strong>Granular permission levels for access keys: </strong><br>At some point I had to share FaunaDB access keys in the front end and had a need to prevent users from taking control over the DB. So one thing I wanted to see was a flexible ABAC system. I've learned that the Fauna team recently added this feature with version 2.7, in response to requests from the community.</li><li><strong>Flexible field filtering:<br></strong>Again, I had a need to perform a fuzzy matching on certain fields from the database. It's not possible to do that now, so I ended up doing filtering on the client side.</li><li><strong>More control over field resolvers in GraphQL interface:<br></strong>Sometimes, one may need to enforce a field value in a mutation or have a conditional query, which is not achievable right now. A possible workaround here could be writing a thin middleware as a&nbsp;<a href=\"https://www.netlify.com/docs/functions/\">Netlify function</a> that will suck raw data from FaunaDB, process it, and spit it out to the client.</li></ul>\n<p>An example of this kind of middleware could look like:import { ApolloServer } from 'apollo-server-lambda'</p>\n<pre>import { GraphQLClient } from 'graphql-request'\nimport { typeDefs } from './utils/schema'\nimport { customQuery } from './utils/queries'\n\nconst client = new GraphQLClient(process.env.FAUNADB_API, {\n  headers: {\n    authorization: `Bearer ${process.env.FAUNADB_KEY}`,\n  }\n})\n\nconst resolvers = {\n  Query: {\n    customQuery: async () =&gt; {\n      const response = await client.request(customQuery)\n      return response.customQuery\n    }\n  }\n}\n\nconst server = new ApolloServer({\n  typeDefs,\n  resolvers\n})\n\nexports.handler = server.createHandler()</pre>\n<p>This code snippet uses ApolloServer to build a GraphQL interface on top of Fauna. An obvious drawback here is that it will slow down the response and add up unnecessary cycles to response time.</p>\n<h2>Conclusion</h2>\n<p>In the early stages of a startup, when you still need to validate your product/market fit hypothesis, you want to ship a product as fast as possible. Ideally, you will want to spend 0 minutes on things that don’t bring direct value to the customer like setting up and maintaining back-end infrastructure.</p>\n<p>Of course, you can scaffold a server on your favorite language and framework, but it can quickly become time-consuming and costly to maintain it. You may argue that over time it will pay out, you’ll have more control over it. Agreed, but that’s not your goal right now. You need to ship a product as fast as possible with minimal effort. You need to be lean. You need to be agile.</p>\n<p>That’s where <a href=\"https://fauna.com/faunadb\">FaunaDB makes perfect sense</a> to me. With this service, I can go fully production-ready with a database-as-a-service in less than an hour, while it would have taken a day or two to containerize and self-host my own data and back-end, plus the headaches of maintaining it. </p>\n<p>FaunaDB allowed me to build my service without worrying about the data infrastructure and all the operational burdens than historically come with it.</p>",
        "blogCategory": [
            "3"
        ],
        "mainBlogImage": [
            "6143"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-09-16T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5917,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e042b362-ce9b-45e4-b452-d629c723b315",
        "siteSettingsId": 5917,
        "fieldLayoutId": 4,
        "contentId": 2051,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing the FaunaDB Add-on for Netlify",
        "slug": "announcing-the-faunadb-add-on-for-netlify",
        "uri": "blog/announcing-the-faunadb-add-on-for-netlify",
        "dateCreated": "2019-09-09T10:42:46-07:00",
        "dateUpdated": "2019-09-16T09:14:24-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-faunadb-add-on-for-netlify",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-faunadb-add-on-for-netlify",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re excited to announce the FaunaDB Add-on for <a href=\"https://netlify.com\">Netlify</a>, which empowers users to create a fully-featured, globally-distributed data back-end for their applications within seconds.</p>\n<figure><a href=\"https://docs.fauna.com/fauna/current/start/netlify\"><img src=\"{asset:5919:url}\" data-image=\"5919\" style=\"max-width: 375px;\"></a></figure>\n<p>This integration allows users to instantly add FaunaDB to any <a href=\"https://fauna.com/blog/jamstack\">JAMstack</a>&nbsp;project managed with Netlify, and to manage the database instance via FaunaDB Console.<br></p>\n<h2>What is Netlify?</h2>\n<p>Netlify is a unified platform that makes it easy to deploy, scale, and maintain JAMstack websites and apps.  Continuous deployment, hosting, serverless functions, forms, and identity, and more are all easy to add to your projects via Netlify’s dashboard and command line tools.  Sites on Netlify are extremely fast and secure—the code you deploy gets prebuilt and globally distributed across multiple content delivery networks on multiple clouds to be as close to end users as possible. Netlify replaces servers & maintenance with an automated, productive workflow built for developers.</p>\n<h2>Add-on Capabilities</h2>\n<p>The FaunaDB Add-on for Netlify enables users to seamlessly plug in a globally distributed datastore into their applications with an instant GraphQL backend for Netlify apps, without any provisioning. They can also associate databases with their FaunaDB account so that they can manage them within <a href=\"https://dashboard.fauna.com\">FaunaDB Console</a>.</p>\n<p>Once users have a <a href=\"https://app.netlify.com/signup\">Netlify account</a>, creating a database within your site can be done in using a few simple commands:</p>\n<pre>npm i netlify-cli -g\n\nmkdir my_project\n\ncd my_project\n\nnetlify init</pre>\n<p>This installs the <a href=\"https://www.netlify.com/docs/cli/\">Netlify CLI tools</a>, and create and initialize a Netlify project.<br></p>\n<pre>netlify addons:create fauna</pre>\n<p>This command creates a FaunaDB instance for your new site.<br></p>\n<pre>netlify addons:auth fauna</pre>\n<p>This prompts you to sign up with FaunaDB, or login if you already have an account. Once logged in, you are prompted to name your database and import it into your account. This allows you to interact with your database directly using the FaunaDB Console.<br></p>\n<p>The complete documentation for this add-on can be found <a href=\"https://docs.fauna.com/fauna/current/start/netlify\">here</a>.</p>\n<h2>A New Login Option</h2>\n<p>Users can now also <a href=\"https://dashboard.fauna.com\">create an account and login to FaunaDB</a> with their Netlify account credentials using OAuth:</p>\n<figure><img src=\"https://lh4.googleusercontent.com/i16rgdxannsljhYOnwiFYE1Rj5KGq8ydbSIGE9MxcBCh3ylH5_Nnp1ZgHyeMSnj_q-B-SADpPkLukBtyVDXospNLjTC1_TRJiCtJKalLqsLQSg0fXm-o-PXsUnBqb21Ys-0WXh8O\" width=\"361\" height=\"463\" data-image=\"j7qoy5n0y2z9\"></figure>\n<h2>Benefits for users of Netlify and FaunaDB</h2>\n<p>The FaunaDB Add-on for Netlify extends the productivity of the serverless experience to application data, something that has been in strong demand within the <a href=\"https://jamstack.org/\">JAMstack community</a>. Serverless data eliminates all database operations, which has until now been a cumbersome, error-prone and manual effort for developers, and one that required significant upfront planning.</p>\n<p>FaunaDB is the only <a href=\"https://www.netlify.com/blog/2018/07/09/building-serverless-crud-apps-with-netlify-functions-faunadb/\">serverless cloud database</a> with a direct Add-on for Netlify, making it the perfect choice for developers who want to build stateful apps on Netlify. Databases created via the Add-on are available for use instantly via FaunaDB’s native <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">GraphQL</a> API.</p>\n<p>Globally distributed data ensures that data is close to where your users are, thus enabling a snappy user experience for apps deployed on Netlify’s global fabric. Database instances created with this Add-on can be managed via FaunaDB Console as well as FaunaDB Shell for hassle-free use.</p>\n<h2>Conclusion</h2>\n<p>With this Add-on, anyone using FaunaDB and Netlify can come up with an idea and launch it immediately, just by focusing on the frontend. Netlify users can use FaunaDB as a stateful component of their apps with ease, and avail themselves of our generous free tier.</p>\n<p>Please visit <a href=\"https://docs.fauna.com/fauna/current/start/netlify\">the FaunaDB documentation</a> to learn more. And please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate your feedback into a future release.</p>\n<p>What other integrations would you like to see implemented in FaunaDB? Please reach out to me on <a href=\"https://linkedin.com/in/lewisking\">LinkedIn</a> and our <a href=\"https://community-invite.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB and Netlify an obvious choice for your next project.</p>",
        "blogCategory": [
            "8",
            "1531"
        ],
        "mainBlogImage": [
            "5918"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "475",
        "postDate": "2019-09-12T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5928,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "3e50d89e-603b-4660-8f91-b51e216d9355",
        "siteSettingsId": 5928,
        "fieldLayoutId": 4,
        "contentId": 2062,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Rename with No Regrets: One Check to Make Before You Upgrade Your FaunaDB Driver",
        "slug": "rename-with-no-regrets-one-check-to-make-before-you-upgrade-your-fauna-driver",
        "uri": "blog/rename-with-no-regrets-one-check-to-make-before-you-upgrade-your-fauna-driver",
        "dateCreated": "2019-09-11T09:03:23-07:00",
        "dateUpdated": "2019-09-12T10:18:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/rename-with-no-regrets-one-check-to-make-before-you-upgrade-your-fauna-driver",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/rename-with-no-regrets-one-check-to-make-before-you-upgrade-your-fauna-driver",
        "isCommunityPost": false,
        "blogBodyText": "<p>Continually evaluating ourselves and our products to produce the best overall experience for our community is at the forefront of our daily work here at Fauna. We built FaunaDB to make it easy for developers to design and build globally distributed applications, and we continually reflect on the feedback our users give us to make that experience even better. This constant self-assessment caused us to re-evaluate the use of two core terms in FaunaDB, “<strong>class</strong>” and “<strong>instance</strong>”.  </p>\n<p>Programmers and DBA’s who are familiar with NoSQL or SQL databases are accustomed to a specific database vernacular.  When getting started with FaunaDB, experienced database programmers and DBAs were confused and hindered by the terms “<strong>class</strong>” and “<strong>instance</strong>” because our usage did not match up with the rest of the industry’s. In order to improve the experience for both new and seasoned database programmers, Fauna decided to change these two terms to align with established NoSQL database terminology.  <strong>Thus, in FaunaDB 2.7 we’ve renamed an “instance” to a “document” and a “class” to  a “collection”.  </strong></p>\n<p>The choice of replacement terms was difficult, as we view FaunaDB as a relational NoSQL database. Do we choose a term more suitable towards the SQL/relational spectrum such as ‘table’ and ‘row’, or more suitable towards NoSQL and use ‘collection’ and ‘document’? Ultimately, the NoSQL terms of ‘collection’ and ‘document’ were chosen because FaunaDB’s ability to work with flexible schema was viewed to be a more precise description of its functionality.  </p>\n<p>In this post, you will see how the rename will affect your everyday use of FaunaDB, as well as how to actually enable the naming changes by upgrading to 2.7.</p>\n<h2>Details of the Rename</h2>\n<p>The effect of the name change is far reaching and includes drivers, documentation, error messages, and API responses. While the change was extensive, we have worked to ensure 100% backward compatibility when an application connects using a version 2.6 or lower driver.</p>\n<p>When using 2.7 drivers, the API functions CreateClass(), Class() and Classes() have been deprecated. While you can still currently use the deprecated functions in the 2.7 driver, they will be removed in a future version of Fauna.  In addition, several of the API function’s response documents have changed. Any place in the returned response, which previously contained any form of the two words class or instance, have been replaced with their corresponding counterpart <em>collection</em> or <em>document</em>.  </p>\n<h2>Upgrading from 2.6 to 2.7 and above</h2>\n<p>Thankfully, upgrading to use this new terminology is straightforward. To enable these naming changes, you only need to upgrade your driver to version 2.7 or greater. </p>\n<p>Before upgrading your driver, check your application’s codebase to see if you parse the response document for the terms ‘class’ or ‘instance’. If you parse the response for these words, you will need to update your code appropriately--probably a simple search/replace will fix it.</p>\n<p>A practical example of the API change is the <a href=\"https://docs.fauna.com/fauna/current/api/fql/write/createclass\">CreateClass()</a> function’s response document shown below.  While the function has been deprecated and replaced with <a href=\"https://docs.fauna.com/fauna/current/api/fql/write/createcollection\">CreateDocument()</a> it can still be used. The response shown below is from a version 2.6 driver, and maintains a 100% backward compatible format even though the application is connected to a 2.7 FaunaDB server.</p>\n<p>&lt;&lt;&lt; CreateClass Response from a 2.6 or earlier driver &gt;&gt;&gt;</p>\n<figure><img src=\"{asset:5926:url}\" data-image=\"5926\" style=\"max-width: 700px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p>In comparison, the response document below is from  the new FaunaDB 2.7 driver working with the new 2.7 FaunaDB server. The response below will be identical for either the CreateClass() command or the CreateCollection() command when working with a 2.7 or newer driver. <em>Notice that the <strong>ref</strong> field in the version 2.6 output above had the term <strong>class</strong> which has now been changed to <strong>collection</strong> below.</em> As previously noted, any form of the word class is transformed. The 2.6 output has the plural form <strong><em>classes </em></strong>which is transformed with the 2.7 driver to the plural form <em><strong>collections</strong></em>.</p>\n<p>&lt;&lt;&lt; CreateDocument or CreateClass Response from the 2.7 driver.  &gt;&gt;&gt;</p>\n<figure><img src=\"{asset:5927:url}\" data-image=\"5927\" style=\"max-width: 700px;></figure>\n<span id=\" selection-marker-start\"=\"\" class=\"redactor-selection-marker\">﻿<span id=\"selection-marker-end\" class=\"redactor-selection-marker\"></span></figure>\n<p>While the change is relatively straightforward, it’s important to take time to do a quick check to make sure you aren’t surprised when you upgrade your driver.</p>\n<h2>Conclusion</h2>\n<p>At Fauna, one of our top priorities is ensuring our user <a href=\" https:=\" \"=\"\" community-invite.fauna.com=\"\">community</a> has a fantastic development experience. In this case, we realized it was important to evolve the product to use the familiar database terminology of ‘collection’ and ‘document’. However, we also knew it was key to minimize the impact to existing customers and applications. Conveniently, FaunaDB intelligently handles different driver versions, and for most users, a quick search and replace should be all you need to update your code.</p>\n<p>While change is never easy, we know FaunaDB must evolve to be the best database server for our community. Do you have any ideas for how FaunaDB can evolve to make your life easier?  Please reach out via email or join our Slack community and let us know!</p>",
        "blogCategory": [],
        "mainBlogImage": [
            "5929"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-09-05T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5876,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "85dacc92-ea19-45f7-b4d8-23e16709b218",
        "siteSettingsId": 5876,
        "fieldLayoutId": 4,
        "contentId": 2041,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Announcing New Global Regions for FaunaDB:  South America and Asia",
        "slug": "announcing-new-global-regions-for-faunadb-south-america-and-asia",
        "uri": "blog/announcing-new-global-regions-for-faunadb-south-america-and-asia",
        "dateCreated": "2019-08-29T12:59:55-07:00",
        "dateUpdated": "2020-10-05T11:07:20-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-new-global-regions-for-faunadb-south-america-and-asia",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-new-global-regions-for-faunadb-south-america-and-asia",
        "isCommunityPost": false,
        "blogBodyText": "<p>Today, we are announcing the addition of two new regions to FaunaDB: the first in Sao Paulo, Brazil, and the second in Singapore. We're excited to expand FaunaDB's globally distributed footprint to these geographies.&nbsp;</p>\n<p>For these two regions, we have chosen to expand our cluster on Google Cloud Platform. Each region contains a full copy of your data. Your FaunaDB requests are routed to the closest region, eliminating the need to hop across the network to access a replica in the U.S. or in Europe. Therefore, we expect users in South America and Asia to see a significant <a href=\"https://app.fauna.com/status\">performance boost</a>. With these additions, FaunaDB now has a physical presence on four continents around the globe.</p>\n<figure><img src=\"{asset:5898:url}\" data-image=\"5898\"></figure>\n<h3>Community usage</h3>\n<p>The idea to expand the reach of our cloud coverage has been driven by our users, who are developing serverless apps all around the world. We continually strive for performance improvements in all aspects of FaunaDB.</p>\n<p>If you’re deploying applications in these regions, we’d be curious to hear about your experiences. Please do reach out and let me know on <a href=\"https://community-invite.fauna.com\">Community Slack</a>!</p>",
        "blogCategory": [
            "8",
            "1530",
            "1531"
        ],
        "mainBlogImage": [
            "5899"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-09-03T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 4979,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d7dcc8e9-0e6b-48a4-a702-6eea88410703",
        "siteSettingsId": 4979,
        "fieldLayoutId": 4,
        "contentId": 1807,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing New Functions in FQL",
        "slug": "announcing-new-functions-in-fql",
        "uri": "blog/announcing-new-functions-in-fql",
        "dateCreated": "2019-08-14T11:09:23-07:00",
        "dateUpdated": "2019-09-05T14:13:52-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-new-functions-in-fql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-new-functions-in-fql",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re pleased to announce new FQL capabilities that provide better support for query patterns common to smaller scale deployments, and favor flexibility and expressivity over raw scalability.  With these updates, users are able to write more concise and powerful FQL statements with Range(), Filter(), Reduce(), and Merge() functions.<br></p>\n<p>Please note that these new functions are in <em>Preview mode</em>, which means that they will be continuing to improve over the next month and we invite you to send us feedback about their use.</p>\n<p>New functionality has been added via a combination of new FQL functions and enhancements to existing ones. To learn more, please check out our <a href=\"https://docs.fauna.com/fauna/2.7.0/guides/rbac\">documentation</a>.<br></p>\n<h2>Range() function&nbsp;<sup><em>Preview</em></sup></h2>\n<h4>Range(set, from, to)</h4>\n<p>\n</p>\n<p>Range() provides the ability to limit a <a href=\"https://docs.fauna.com/fauna/current/api/fql/sets\">set</a> based on lower and upper bounds of its natural order. For example, given the index:</p>\n<pre>{ \n  \"name\": \"users_by_name\",\n  \"source\": Collection(\"users\"),\n  \"terms\": [],\n  \"values\": [{\"field\": [\"data\", \"name\"]}, {\"field\": \"ref\"}]\n}</pre>\n<p>Range() may be used to get a range of users by name:  </p>\n<pre>Range(Match(\"users_by_name\"), \"Brown\", \"Smith\")</pre>\n<p>If a set’s tuples are longer than the bounds passed to Range(), the limit applies based on the prefix the bound covers. For example, if the above index covered discrete fields for last name and first name, an array argument to Range() will be interpreted as a tuple prefix:</p>\n<pre>Range(Match(\"users_by_last_first\"), [\"Brown\", \"A\"], \"Smith\")</pre>\n<p>Range() is inclusive. Other range-like predicates to be added are RangeLT, RangeLTE, RangeGT, and RangeGTE, which let you bound the set only on one side, or which can be combined to specify upper and lower bound exclusivity.<br></p>\n<h2>Filter() function</h2>\n<h4>Filter(set/array/page, predicate)</h4>\n<p>Before this release, Filter() took an array or page and filtered its elements based on a predicate function. It has been enhanced to work on sets, in order to enable more ergonomic pagination and the ability to compose it with other set modifiers. </p>\n<p>For example, let’s use Filter() on our index above to find all names over a certain length:</p>\n<pre>Filter(Match(\"users_by_name\"), (name, r) =&gt; GT(Length(name), 20))</pre>\n<h2>Reduce() function&nbsp;<sup><em>Preview</em></sup></h2>\n<h4>Reduce(set/array/page, init, fn)</h4>\n<p>We’ve added a Reduce() function to FQL. You may be familiar with “fold” or “reduce” from functional languages, which let you <em>reduce</em> a container such as an array, down to a summary value. FQL’s new Reduce() function brings this capability to sets, arrays, and pages in your FaunaDB queries. </p>\n<p>Here’s how to count the number of users with an “all_users” index:</p>\n<pre>Reduce(Match(\"all_users\"), 0, (x, count) =&gt; Add(1, count))</pre>\n<h2>Format() function <sup><em>Preview</em></sup></h2>\n<h4>Format(template, v1, …, vn)</h4>\n<p>The new Format() function takes a string template and one or more values which are inserted into the template based on c-style formatting rules:</p>\n<pre>Format(\"Alex is %d years old.\", 12)</pre>\n<p>See the <a href=\"https://docs.fauna.com/fauna/current/api/fql/functions/format\">documentation on Format()</a> for more details, including template options.</p>\n<h2>Merge() function&nbsp;<sup><em>Preview</em></sup></h2>\n<h4>Merge(obj1, obj2, lambda)</h4>\n<p>Merge() combines two objects, and takes an optional lambda which lets you resolve conflicts between the two objects.</p>\n<p>By default, if there is a conflict, the value from the second object is used:</p>\n<pre>Merge({ \"a\": 1, \"b\": 1 }, { \"b\": 2, \"c\": 2 })\n \n{ \"a\": 1, \"b\": 2, \"c\": 2 }</pre>\n<p>The optional lambda is used to control how conflicts are resolved:</p>\n<pre>Merge({ \"a\": 1, \"b\": 1 }, { \"b\": 2, \"c\": 2 }, (k, l, r) =&gt; Add(l, r))\n \n{ \"a\": 1, \"b\": 3, \"c\": 2 }</pre>\n<h2>Translating OpenCRUD predicates</h2>\n<p><strong>Listening to our users</strong></p>\n<p>The request for many of these new functions came from our friends at <a href=\"https://graphcms.com\">GraphCMS</a>, who helped identify additional areas for growth in our FQL API. Creating these new functions has enabled us translate <a href=\"https://www.opencrud.org/\">OpenCRUD</a> predicates into queries on a single field sorted index, which was another request from GraphCMS that we were excited to fulfill.</p>\n<p>Given the below index, which has no partition terms, and covers a field and the document ref:<br></p>\n<pre>{ \n  \"name\": \"coll_by_x\",\n  \"source\": Collection(\"a_collection\"),\n  \"terms\": [],\n  \"values\": [{\"field\": [\"data\", \"x\"]}, {\"field\": \"ref\"}]\n}</pre>\n<p>Below are FQL representations of <a href=\"https://github.com/opencrud/opencrud\">OpenCRUD</a> single field equality predicates:<br></p>\n<p><strong>field</strong><strong> (equals):</strong></p>\n<pre>Range(Match(\"coll_by_x\"), &lt;value&gt;)</pre>\n<p><strong>field_not</strong><strong> (not equals):</strong></p>\n<pre>Filter(Match(\"coll_by_x\"), (x, r) =&gt; Not(Equals(x, &lt;value&gt;)))</pre>\n<p><strong>field_in</strong><strong> (in list):</strong></p>\n<pre>Union(Map(&lt;values&gt;, n =&gt; Range(Match(\"coll_by_x\"), n))</pre>\n<p><strong>field_not_in</strong><strong> (not in list):</strong></p>\n<pre>Filter(Match(\"coll_by_x\"), (x, r) =&gt; And(Map(&lt;values&gt;, v =&gt; Not(Equals(v, x)))))</pre>\n<h1>Conclusion</h1>\n<p>Range(), Filter(), Reduce(), Format(), and Merge() allow you to write more expressive FQL, and push more computational logic down to the database. Please visit our <a href=\"https://docs.fauna.com/fauna/2.7.0/guides/rbac\">documentation</a> to learn more. And please <a href=\"https://twitter.com/fauna\">let us know</a> what you think so that we can incorporate your feedback into the formal release.</p>\n<p>What other functions would you like to see implemented in FaunaDB? Please reach out to me on our <a href=\"https://community-invite.fauna.com\">Community Slack</a> and describe any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "5868"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "3110",
        "postDate": "2019-08-26T07:30:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5702,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "0f29c8fc-1e32-4f38-a38a-bcb0cf45f8b3",
        "siteSettingsId": 5702,
        "fieldLayoutId": 4,
        "contentId": 2001,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How FaunaDB's GraphQL API Solves the n+1 Problem",
        "slug": "no-more-n-1-problems-with-faunadbs-graphql-api",
        "uri": "blog/no-more-n-1-problems-with-faunadbs-graphql-api",
        "dateCreated": "2019-08-23T12:04:50-07:00",
        "dateUpdated": "2020-08-29T16:36:24-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/no-more-n-1-problems-with-faunadbs-graphql-api",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/no-more-n-1-problems-with-faunadbs-graphql-api",
        "isCommunityPost": false,
        "blogBodyText": "<p>Dealing with n+1 problems is a common issue when building a GraphQL server. In this post, we’ll learn what the n+1 problem is, how it's solved in most GraphQL servers, and why you don’t need to care about it with FaunaDB’s GraphQL API.</p>\r\n<h2>The n+1 problem</h2>\r\n<p>Before diving into the aspects of the n+1 problem, let’s review how GraphQL queries are handled on the server side. When a query hits the server, it is processed by the so-called <em>resolver</em> functions. A resolver is a field-specific function, in charge of fetching the proper data for a given field. This means that for every field present in the query, the server calls its corresponding resolver function in order to fetch the requested data.</p>\r\n<p>In most implementations, the order in which the server calls each resolver function is the same as how the fields appear in the query graph. While resolvers for fields at the same level may be called at the same time, values for child fields are resolved after their parent field. This is known as a <em>breadth-first</em> <em>search</em> in graph theory.</p>\r\n<p>Now, the n+1 problem occurs when <em>processing a query involves performing one initial request to a data source along with n subsequent requests in order to fetch all of the data.</em> In order to understand this more clearly, let’s take a look at an example.</p>\r\n<p>Suppose we have a schema which establishes a relation between a <tt>Book</tt> and an <tt>Author</tt> type:</p>\r\n<pre>type Book {\r\n  title: String!\r\n  author: Author!\r\n}\r\n\r\ntype Author {\r\n  firstName: String!\r\n  lastName: String!\r\n}\r\n\r\ntype Query {\r\n  allBooks: [Book]!\r\n}</pre>\r\n<p>And we call the <tt>allBooks</tt> query to get all of the <tt>Book</tt>s along with their <tt>Author</tt>s:</p>\r\n<pre>query {\r\n  allBooks {     # fetches books (1 request)\r\n    title       \r\n    author {     # fetches authors for each book (n requests for n authors)\r\n      firstName\r\n      lastName\r\n    }\r\n  }\r\n}</pre>\r\n<p>As explained above, the server goes through each field by following the order of the graph, and calls the proper resolver functions in order to process the query. It starts by making one request (1) to fetch the data for the <tt>books</tt> field, and then make multiple subsequent requests (n) for all of the associated <tt>author</tt> fields. Thus, as a result, there will be n+1 requests in order to fetch all of the data.</p>\r\n<p>If our data source was a SQL-like database, this might result in the following queries:</p>\r\n<pre>SELECT * FROM Book\r\nSELECT * FROM Author WHERE authorId = 1\r\nSELECT * FROM Author WHERE authorId = 2\r\nSELECT * FROM Author WHERE authorId = 3\r\n...</pre>\r\n<p>At this point, you may have noticed that this is a very inefficient and unpredictable way of retrieving data. When working with large sets, this can easily lead to notorious performance issues. Moreover, you might even end up requesting the same element more than once!</p>\r\n<p>Most data sources already support fetching a selection of elements in a single operation. So, shouldn’t there be a way to rearrange how a query is processed in order to have one request instead of the n subsequent requests?</p>\r\n<h2>Batching to the rescue</h2>\r\n<p>Fortunately, there’s a solution to this problem through an old well-known technique: batching. Most GraphQL server libraries offer the ability to defer the resolution of a resolver function and batch all of their executions together. In other words, rather than immediately fetching the data for each field one by one, all fields sharing the same batch resolver get their data at once with a single request.</p>\r\n<p>Let’s take a look at the previous query and see how it is processed using batching:<br></p>\r\n<pre>query {\r\n  books {        # fetches books (1 request)\r\n    title       \r\n    author {     # defers the fetching of author (no request)\r\n      firstName\r\n      lastName\r\n    }\r\n  }              # fetches all authors for each book at once (1 request)\r\n}</pre>\r\n<p>This time, the server goes through the query, fetches the data for the <tt>books</tt> field, and defers the resolution of the <tt>author</tt> fields for later. Once all of the author fields have been traversed, a single request is made in order to fetch the data for all of them.</p>\r\n<p>If we were working with a SQL-like database for storing the data, we might get the following queries:</p>\r\n<pre>SELECT * FROM Book\r\nSELECT * FROM Author WHERE authorId IN (1, 2, 3, ...)</pre>\r\n<p>As you may have already noticed, we have to perform only two queries now to get all of the data! Thanks to batching, we have reduced the number of queries to a constant number and thus avoid potential performance issues.</p>\r\n<p>Although this a well-known solution for the n+1 problem, we still need to take care and handle each case individually ourselves. Libraries like <a href=\"https://github.com/graphql/dataloader\">Dataloader</a> or <a href=\"https://www.apollographql.com/docs/apollo-server/features/data-sources.html\">Apollo's data sources</a> do a great job at easing the implementation of deferred resolvers and batching, and they even provide additional features for handling other cases. However, we cannot escape the fact that we still need to put some code together in order to cover every possible n+1 problem in our server. Since this is such a common issue, you might be wondering if there’s another way to solve all of the potential n+1 problems once and for all...</p>\r\n<h2>One query to rule them all</h2>\r\n<p>With FaunaDB’s GraphQL API, we’ve taken things a step further. First, we have leveraged one of Fauna’s distinctive features: the <a href=\"https://docs.fauna.com/fauna/current/api/fql/\">Fauna Query Language</a> or FQL. FQL is a powerful and comprehensive language that allows complex and precise manipulation and retrieval of data stored within FaunaDB. While not a general-purpose programming language, it provides much of the functionality expected from one.</p>\r\n<p>With FQL at our disposal, we have used resolvers slightly differently from how they are used normally. Whenever FaunaDB’s GraphQL API server receives a query, all of its fields are first resolved into an FQL expression. That is, instead of immediately fetching the data field-by-field, a built-in resolver automatically generates the equivalent FQL expression for each field. Once an expression has been assigned to all fields, all expressions are combined into one FQL query. As a result, <strong>the complete GraphQL query is translated into a single FQL query</strong>. Finally, this FQL query is executed and all data for all of the fields is fetched at once.</p>\r\n<p>Let’s take a look at the previous example one last time and see how this works:</p>\r\n<pre>query {\r\n  books {        # generates an FQL expression for fetching books (no request)\r\n    title       \r\n    author {     # generates an FQL expression for fetching the author (no request)\r\n      firstName\r\n      lastName\r\n    }\r\n  }              # composes all expressions into one single FQL query\r\n}                # and fetches all of the data at once (1 request)</pre>\r\n<p>In this case, the server simply assigns an FQL expression for the <tt>books</tt> field, and another FQL expression for the <tt>author</tt> field. Both expressions are then be composed into one query and the data for the <tt>books</tt> and <tt>author</tt> fields is fetched together.</p>\r\n<p>As mentioned, the complete GraphQL query is translated into a single FQL query similar to the one below:</p>\r\n<pre>Map(\r\n  Paginate(Match(Index(\"allBooks\"))),\r\n  Lambda(\"bookRef\",\r\n    Let(\r\n      {\r\n        \"book\": Get(Var(\"bookRef\")),\r\n        \"author\": Get(Select([\"data\", \"author\"], Var(\"book\")))\r\n      },\r\n      {\r\n        \"title\": Select([\"data\", \"title\"], Var(\"book\")),\r\n        \"author\": {\r\n          \"firstName\": Select([\"data\", \"firstName\"], Var(\"author\")),\r\n          \"lastName\": Select([\"data\", \"lastName\"], Var(\"author\"))\r\n        }\r\n      }\r\n    )\r\n  )\r\n);</pre>\r\n<p>This means that <strong>any given GraphQL query always incurs only one single request to the database.</strong> By design, we have resolved the n+1 problem in order to cover any possible use case—regardless of what query has to be processed. And all of this is done without you even having to write a single line of code!</p>\r\n<h2>Conclusion</h2>\r\n<p>In this post, we have discussed the aspects of the n+1 problem, how batching can help you to keep the number of requests in line, and how this recurrent issue has already been solved for any possible use case within FaunaDB’s GraphQL API.</p>\r\n<p>Throughout this post, we have used a rather simple query to easily illustrate what the n+1 problem is and how it can be solved in different ways. In a real world scenario, GraphQL queries can be vastly complex and even using a traditional optimization technique like batching can require several requests to a data source. In contrast, FaunaDB’s GraphQL API guarantees that for any GraphQL query you make—no matter the complexity—there is always one single request.</p>\r\n<p>If you are interested in learning more about FaunaDB’s GraphQL API, you can start with this <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">simple tutorial</a>.</p>",
        "blogCategory": [
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "5758"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2019-08-23T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5333,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "ca18df80-68a8-4744-80ef-c80c825d326e",
        "siteSettingsId": 5333,
        "fieldLayoutId": 4,
        "contentId": 1926,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Demystifying Database Systems, Part 4: Isolation levels vs. Consistency levels",
        "slug": "demystifying-database-systems-part-4-isolation-levels-vs-consistency-levels",
        "uri": "blog/demystifying-database-systems-part-4-isolation-levels-vs-consistency-levels",
        "dateCreated": "2019-08-19T15:11:45-07:00",
        "dateUpdated": "2019-08-26T17:10:06-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/demystifying-database-systems-part-4-isolation-levels-vs-consistency-levels",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/demystifying-database-systems-part-4-isolation-levels-vs-consistency-levels",
        "isCommunityPost": false,
        "blogBodyText": "<p>In several recent posts, we discussed two ways to trade off correctness for performance in database systems. In particular, I wrote two posts (<a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">Part 1</a> & <a href=\"https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">Part 2</a>) on the subject of isolation levels, and one <a href=\"https://fauna.com/blog/demystifying-database-systems-introduction-to-consistency-levels\">post</a> on the subject of consistency levels.</p>\r\n<p>For many years, database users did not have to simultaneously understand the concept of isolation levels and consistency levels. Either a database system provided a correctness/performance tradeoff using isolation levels, or it provided a correctness/performance tradeoff using consistency levels, but never both. This resulted in a blurring of these concepts to the point that many people --- even experts in the field --- confuse isolation levels with consistency levels and vice versa. For example, <a href=\"https://www.infoq.com/presentations/state-serverless-computing/\" target=\"_blank\" data-saferedirecturl=\"https://www.google.com/url?q=https://www.infoq.com/presentations/state-serverless-computing/&source=gmail&ust=1566513542836000&usg=AFQjCNGiKhOiv5I2VBwMPw85Q-VmlDv7Bw\">this talk</a> by a PhD student at Berkeley (incorrectly) refers to causal consistency as an “isolation level”. And <a href=\"http://db.csail.mit.edu/pubs/silo.pdf\" target=\"_blank\" data-saferedirecturl=\"https://www.google.com/url?q=http://db.csail.mit.edu/pubs/silo.pdf&source=gmail&ust=1566513542836000&usg=AFQjCNHAl2VTTbJrHpzZgHVt4eFaPLCx6g\">this paper</a> from well-known researchers at MIT and Harvard --- including a Turing Award laureate --- (incorrectly) call snapshot isolation and serializability “consistency” levels. I am confident that all these well-known researchers know the difference between isolation levels and consistency levels. However, as long as isolation and consistency could not be tuned within the same system, there has been little necessity for precision in the parlance around these terms. </p>\r\n<p>In modern times, systems are being released that provide both a set of isolation levels and a set of consistency levels to choose from. Unfortunately, due to the historical failure in our community to be careful in our parlance, these new systems continue the legacy of blurring the distinction between isolation and consistency, which has resulted in “isolation levels” or “consistency levels” containing a mixture of isolation and consistency guarantees, and wide-spread confusion. Database users need more education on the difference between isolation levels and consistency levels and how they interact with each other in order to make an informed decision on how to trade off correctness for performance. This post is designed to be a resource for these types of decisions.</p>\r\n<h2>Background material</h2>\r\n<p>This post is designed to be a self-contained overview of the difference between isolation levels and consistency levels. You do not need to read my previous posts that I linked to above in order to read this post. Nonetheless, some of the definitions and fundamental concepts from those posts are important background material for understanding this post. To avoid making you read those previous posts, in this section, I will summarize the pertinent background material (but for more detail and elaboration, please see the previous posts).</p>\r\n<p><strong>Database isolation</strong> refers to the ability of a database to allow a transaction to execute as if there are no other concurrently running transactions (even though in reality there can be a large number of concurrently running transactions). The overarching goal is to prevent reads and writes of temporary, incomplete, aborted, or otherwise incorrect data written by concurrent transactions.</p>\r\n<p></p>\r\n<p>If the application developer is able to ensure the correctness of their code when no other concurrent processes are running, a system that guarantees <strong>perfect</strong> isolation will ensure that the code remains correct even when there is other code running concurrently in the system that may read or write the same data. Thus, in order to achieve perfect isolation, the system must ensure that when transactions are running concurrently, the final state is equivalent to a state of the system that would exist if they were run serially. This perfect isolation level is called <strong>serializability</strong>. </p>\r\n<p>One of our previous posts <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">discussed some famous anomalies</a> in application code that can occur at levels of isolation below serializability, and also some <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">widely-used reduced isolation levels</a>,  of which the most notable are snapshot isolation and read committed. A detailed understanding of these anomalies and reduced isolation levels is not critical in order to understand the material we discuss below in this post.</p>\r\n<p><strong>Database consistency </strong>is defined in <a href=\"https://fauna.com/blog/demystifying-database-systems-introduction-to-consistency-levels\">different ways depending on the context</a>, but when a modern system offers multiple <strong>consistency levels</strong>, they define consistency in terms of the client view of the database. If two clients can see different states at the same point in time, we say that their view of the database is <strong>inconsistent </strong>(or, more euphemistically, operating at a “<strong>reduced consistency level</strong>”). Even if they see the same state, but that state does not reflect writes that are known to have committed previously, their view is inconsistent with the known state of the database. </p>\r\n<p>We explained <a href=\"https://fauna.com/blog/demystifying-database-systems-introduction-to-consistency-levels\">in our previous post</a> that the notion of a <strong>consistency level </strong>originated by researchers of shared-memory multiprocessor systems. The goal of the early work was to reason about how and when different threads of execution, which may be running concurrently and accessing overlapping sets of data in shared memory, should see the writes performed by each other. We gave a bunch of examples of consistency levels with examples using schedule figures similar to the one below, in which different threads of execution write and read data as time moves forward from left to right. In the figure below, we have four threads of execution: P1, P2, P3, and P4.  P1 writes the value 5 to x, and thread P2 writes the value 10 to y after P1’s write to x completes. P3 and P4 read the new value (10) for y during overlapping periods of time. P3 and P4 then read x during overlapping periods of time. P3 starts slightly earlier and sees the old value, while P4 starts (and completes) later sees the new value. </p>\r\n<figure><img src=\"{asset:5526:url}\" data-image=\"5526\"></figure>\r\n<p>To some degree, our example schedule is consistent: although P3 and P4 read different values for x, since P4 initiated its read after P3, they can believe that the official value of x in the system did not change until a point in time in between these two read requests. P3 saw the new value of y (10) before reading the old value of x (0). Therefore, it believes that the write to y happened before the write to x. No thread observes a write ordering different from the write to y occuring before the write to x (P1 and P2 do not perform any reads, so they do not “see” anything, and P4 sees the new values of both x and y, and therefore did not see anything to contradict P3’s observation that the write to y happened before the write to x). Therefore, all threads can agree on a consistent, global sequential ordering of writes. This level of consistency is called <strong>sequential consistency</strong></p>\r\n<p><strong>S</strong><strong>equential consistency</strong> is a very high level of consistency, but it is not perfect. The reason why it is not perfect is that the globally observed write order contradicts reality. In reality, the write to x happens before the write to y. Furthermore, P3 sees a stale value for x: it reads the old value of x (0) long after the write of 5 to x has completed. This return of the non-current version of X is another example of contradicting reality. Perfect consistency (in this post, we are going to call <strong>linearizability </strong>“perfect” even though <a href=\"https://fauna.com/blog/demystifying-database-systems-introduction-to-consistency-levels\"><strong>strict consistency</strong> has slightly stronger guarantees</a>) ensures both: that every thread of execution observes the same write ordering AND that write ordering matches reality (if write A completes before write B starts, no thread will see the write to B happening before the write to A). This guarantee is also true for reads: if a write to A completes before a read of A begins, then the write of A will be visible to the subsequent read of A (assuming A was not overwritten by a different write request in the interim). </p>\r\n<p>When expanding the traditional consistency diagrams such as the one from our example to a transactional model, we annotate each read and write request with the transaction identifier of the transaction that initiated each request. If each thread of execution can only process one transaction at a time, and transactions can not be processed by more than one thread of execution, then the traditional timeline consistency diagrams need only be supplemented with rectangular boundaries indicating the start and end point of each transaction within a thread of execution, as shown in the figure below.</p>\r\n<figure><img src=\"{asset:5527:url}\" data-image=\"5527\"></figure>\r\n<p>Once you include transactions in these consistency diagrams, the concept of database isolation must be considered (isolation is the I of ACID within the ACID set of guarantees for transactions). Depending on the isolation level, a write within a transaction may not become visible until the entire transaction is complete. Isolation guarantees thus place limitations on how and when writes become visible to threads of execution that read database state. Yet, consistency guarantees also specify how and when writes become visible! This conceptual overlap of isolation and consistency guarantees is the source of much confusion.  </p>\r\n<blockquote>Ignorance is dangerous! You need to understand the difference between isolation guarantees and consistency guarantees. And you need to know what you are getting for both types of guarantees.</blockquote>\r\n<p>Let’s look at an example. The diagram below shows a system running two transactions in different threads of execution.  Each transaction reads the current value of X, adds one to it, and writes it back out. In any serializable execution, the final value of X after running both transactions should be two higher than the initial value. But in this example, we have a system running at perfect, linearizable consistency, but the final value of X is only one higher than the initial value. Clearly, this is a correctness bug.</p>\r\n<figure><img src=\"{asset:5593:url}\" data-image=\"5593\"></figure>\r\n<p>The basic problem is that historically, as we described above, consistency levels are only designed for single-operation actions. They generally do not model the possibility of a group of actions that commit or abort atomically. Therefore, they do not protect against the case where writes are performed only temporarily, but ultimately get rolled back if a transaction aborts. Furthermore, without explicit synchronization constructs, consistency levels do not protect against reads (on which subsequent writes in that same transaction depend) becoming immediately stale as a result of a concurrent write by a different process. (This latter problem is the problem that was shown in Figure 3).</p>\r\n<p>The bottom line is that as soon as you have the concept of a transaction --- a group of read and write operations --- you need to have rules for what happens during the timeline between the first of the operations of the group and the last of the operations of the group. What operations by other threads of execution are allowed to occur during this time period between the first and last operation and which operations are not allowed? These set of rules are defined by isolation levels that <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">I discussed previously</a>. For example, serializable isolation states that the only concurrent operations that are allowed to occur are those which will not cause a visible change in database state relative to what the state would have been if there were no concurrent operations.</p>\r\n<p>The bottom line: as soon as you allow transactions, you need isolation guarantees. </p>\r\n<p>What about vice versa? Once you have isolation guarantees, do you need consistency guarantees?</p>\r\n<p>Let’s look at another example. The diagram below shows a system running three transactions. In one thread of execution, a transaction runs (call it T1) that adds one to X (it was originally 4 and now it is 5). In the other thread of execution, a transaction runs (call it T2) that writes the current value of X to Y (5). After the transaction returns, a new transaction runs (call it T3) that reads the value of X and sees the old value (4). The result is entirely serializable (the equivalent serial order is T3 then T1 then T2). But it violates strict serializability and linearizability because it reorders T3 before T2 even though T3 started in real time after T2 completed. Furthermore, it violates sequential consistency because T2 and T3 are run in the same thread of execution, and under sequential consistency, the later transaction, T3, is not allowed to see an earlier value of X than the earlier transaction, T2.  This could result in a bug in the application: many applications are unable to handle the phenomenon of database state going backwards in time across transactions --- and especially not within the same session.  </p>\r\n<figure><img src=\"{asset:5760:url}\" data-image=\"5760\"></figure>\r\n<p>Guarantees of isolation without any guarantees of consistency are not particularly helpful. As an extreme example: assume that a database started empty and then grows over time during the lifetime of an application. A system that guarantees perfect (serializable) isolation without any consistency guarantees could theoretically return the empty set (no results) for every single read-only transaction without violating its serializability guarantee. The serial order that the system is equivalent to is simply a serial order where the read-only transactions happen before the first write transaction. In essence, the serializability guarantee allows transactions to “go back in time” --- as long as the final result is equivalent to some serial order, the guarantee is upheld. Without some kind of consistency guarantee, there are no restrictions on which serial order it needs to be equivalent to. </p>\r\n<p>In general, all of the time travel bugs that I discussed in <a href=\"https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">a previous post</a> in the context of serializability are possible at any isolation level. <strong>Isolation levels only specify what happens for concurrent transactions</strong>. If two transactions are not running at the same time, no isolation guarantee in the world will specify how they should be processed. They are already perfectly isolated from each other by virtue of not running concurrently with each other! This is what makes time travel correctness bugs possible at even the highest levels of isolation. If you want guarantees for nonconcurrent transactions: for example, you want to be sure that a later transaction reads the writes of an earlier transaction, <strong>you need consistency guarantees. </strong></p>\r\n<p>Many, but not all, consistency levels make real-time guarantees for nonconcurrent operations. For example, strict consistency and linearizable/atomic consistency both ensure that all nonconcurrent operations are ordered by real-time. Even lower consistency levels, such as sequential consistency and causal consistency make real time guarantees of operations within the same thread of execution. </p>\r\n<p>So the bottom line is the following: once you have transactions with more than one operation, you need isolation guarantees. And if you need any kind of guarantee for non-concurrent transactions, you need consistency guarantees. Most of the time, if you are using a database system, you need both. </p>\r\n<p>Therefore, as a database user, you need to figure out what you need in terms of isolation guarantees, and you also need to figure out what you need in terms of consistency guarantees. And if that wasn’t hard enough, you then need to figure out how to map what you think you need to the various options that your system gives you. This is where it gets really tricky, because many systems don’t view isolation and consistency as separate guarantees that can be intermixed arbitrarily. Rather, they only allow certain combinations of isolation guarantees and consistency guarantees, and give each combination a name. To make matters worse, the common industry practice is to call each name for a potential combination an “isolation level”, even though it is really a combination of an isolation level and a consistency level. We will give some examples in the next section.</p>\r\n<blockquote>“Isolation levels” are really a mix of isolation and consistency guarantees.</blockquote>\r\n<p>For my loyal readers that read everything I write, you might have been confused by my two posts on isolation guarantees when read together as a unit. In the <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">earlier of these two posts</a>, I claimed that serializability is the highest possible isolation level. Then, in the <a href=\"https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">next post</a>, I described a whole bunch of isolation levels there were seemingly “higher” or “more correct” --- “one-copy serializability”, “strong session serializability”, “strong write serializability”, “strong partition serializability” and “strict serializability”. </p>\r\n<p>But now that we have gotten to this point in this post --- if you have understood what I’ve written so far, hopefully now the answer to why this is not a contradiction is obvious. Serializability is indeed the highest possible isolation level. All variations of serializability that we discussed --- one-copy serializability, strong session serializability, strong write serializability, strong partition serializability, and strict serializability have identical isolation guarantees: they all guarantee <strong>serializability</strong>.  </p>\r\n<p>The only difference between these so-called isolation levels is their consistency guarantee. One-copy serializability guarantees sequential consistency where no thread of execution can process more than one transaction.  Strong session serializability guarantees sequential consistency where each session corresponds to a separate thread of execution. Strong write serializability corresponds to a linearizability guarantee for write transactions, but only sequential consistency for read-only transactions. Strong partition serializability guarantees linearizable consistency within a partition, but only sequential consistency across partitions. And strict serializability guarantees linearizable consistency at all times for all reads and writes (across non-concurrent transactions).</p>\r\n<p>The same method can be used to understand other so-called “isolation levels”. For example, the isolation levels of WEAK SNAPSHOT ISOLATION, STRONG SNAPSHOT ISOLATION, and STRONG SESSION SNAPSHOT ISOLATION are overviewed <a href=\"http://www.vldb.org/conf/2006/p715-daudjee.pdf\">in this paper</a>. All of these “isolation levels” make equivalent isolation guarantees: all guarantee snapshot isolation and are thus susceptible to write skew anomalies. The only difference is that STRONG SNAPSHOT ISOLATION guarantees linearizable consistency alongside SNAPSHOT ISOLATION’s imperfect isolation guarantee, while the other levels pair lower levels of consistency with snapshot isolation.</p>\r\n<h2>Conclusion</h2>\r\n<p>Database system vendors will continue to use single terms to describe a particular mixture of isolation levels and consistency levels. My recommendation is to avoid getting confused by these loaded definitions and break them apart before starting to reason about them. If you see an “isolation level” with three or more words, chances are, it is really an isolation level combined with a consistency level. Break apart the term into the component isolation and consistency guarantees, and then carefully consider each guarantee separately. What does your application require as far as isolation of concurrently running transactions? Does the isolation guarantee that the isolation level is giving you match your requirements? Furthermore, what does your application require as far as the ordering of non-concurrent transactions? Does the consistency guarantee that you extracted from the complex term match your requirements?</p>\r\n<p>As a separate point, it is worth measuring the difference in performance you get between high isolation and consistency levels and lower ones. The quality of the architecture of the system can have a significant effect on the amount of performance drop for high isolation and consistency levels. Poorly designed systems will push you into choosing lower isolation and consistency levels by virtue of a dramatic performance drop if you choose higher levels. All systems will have some performance drop, but well-designed systems will observe a much less dramatic drop.  </p>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [
            "5532"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "5049",
        "postDate": "2019-08-16T09:21:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 5057,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "1df7d715-3f36-4456-8b74-52a0cf4565d8",
        "siteSettingsId": 5057,
        "fieldLayoutId": 4,
        "contentId": 1850,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Code Splitting in React with Lazy Components",
        "slug": "code-splitting-in-react-with-lazy-components",
        "uri": "blog/code-splitting-in-react-with-lazy-components",
        "dateCreated": "2019-08-16T09:21:16-07:00",
        "dateUpdated": "2019-08-16T11:31:18-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/code-splitting-in-react-with-lazy-components",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/code-splitting-in-react-with-lazy-components",
        "isCommunityPost": false,
        "blogBodyText": "<p>Often during application development, we do not care much about the size of our bundle. However, as the code base grows, we need to think about it not only for speed and performance, but also to take responsibility for users’ data consumption—especially on mobile devices. So how do we load our assets as quickly as possible, and only when the user needs them, by taking advantage of concurrent browser requests?</p>\r\n<figure><img src=\"{asset:5051:url}\" data-image=\"5051\" style=\"max-width: 300px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\r\n<p>Font: <a href=\"https://docs.pushtechnology.com/cloud/latest/manual/html/designguide/solution/support/connection_limitations.html\">https://docs.pushtechnology.com/cloud/latest/manual/html/designguide/solution/support/connection_limitations.html</a></p>\r\n<p>A good option for this is to use the Code Split technique. This technique consists of splitting the code into smaller chunks and loading these chunks only when they are required. Let’s see a use case.</p>\r\n<p>In <a href=\"https://dashboard.fauna.com/\">Fauna Dashboard</a>, we have pages where we are using code editors with heavy dependencies even after gzip. We do not want to put this in the main chunk, because this will make the first rendering longer and more costly than necessary on pages where the code editor is not needed. For this case, splitting code is a good option and I’ll show you how to do it without any headaches. To get started, let’s code something and run some analyses.</p>\r\n<p>Let’s use <a href=\"https://github.com/facebook/create-react-app\">create-react-app</a> to start a new React project and create the components:</p>\r\n<pre>create-react-app lazy-demo\r\ncd lazy-demo\r\nyarn add @reach/router react-ace\r\n</pre>\r\n<p><strong>App.jsx</strong></p>\r\n<pre>import React from \"react\";\r\nimport { Router, Link } from \"@reach/router\";\r\nimport Home from \"./Home\";\r\nimport Code from \"./Code\";\r\n\r\nconst App = () =&gt; {\r\n  return (\r\n    &lt;Router&gt;\r\n      &lt;Home path=\"/\"&gt; /&gt;\r\n      &lt;Code path=\"code\" /&gt;\r\n    &lt;/Router&gt;\r\n  );\r\n};\r\n\r\nexport default App;</pre>\r\n<p><strong>Home.jsx</strong></p>\r\n<pre>import React from \"react\";\r\nimport { Link } from \"@reach/router\";\r\n\r\nconst Home = () =&gt; (\r\n  &lt;&gt;\r\n    &lt;h1&gt;Home&lt;/h1&gt;\r\n    &lt;Link to=\"/code\"&gt;Go to code editor&lt;/Link&gt;\r\n  &lt;/&gt; \r\n);\r\n\r\nexport default Home;</pre>\r\n<p><strong>Code.jsx</strong></p>\r\n<pre>import React from \"react\";\r\nimport CodeEditor from \"./CodeEditor\";\r\n\r\nconst Code = () =&gt; (\r\n  &lt;&gt;\r\n    &lt;h1&gt;Code&lt;/h1&gt;\r\n    &lt;CodeEditor /&gt;\r\n  &lt;/&gt;\r\n);\r\n\r\nexport default Code;</pre>\r\n<p><strong>CodeEditor.jsx</strong></p>\r\n<pre>import React, { Lazy } from \"react\";\r\nimport AceEditor from \"react-ace\";\r\n\r\nimport \"brace/mode/java\";\r\nimport \"brace/theme/github\";\r\n\r\nconst CodeEditor = () =&gt; (\r\n  &lt;AceEditor mode=\"java\" theme=\"github\" onChange={() =&gt; {}} name=\"code\" /&gt;\r\n);\r\n\r\nexport default CodeEditor;</pre>\r\n<p>Now let’s take a look at how our bundle is generated and how to optimize it using <a href=\"https://github.com/danvk/source-map-explorer#readme\">source-map-explorer</a>.</p>\r\n<pre>yarn add source-map-explorer</pre>\r\n<p><strong>package.json</strong></p>\r\n<pre>\"scripts\": {\r\n    \"analyze\": \"source-map-explorer 'build/static/js/*.js'\",\r\n</pre>\r\n<pre>yarn build\r\nyarn analyze</pre>\r\n<p>After running the analyses, we can see a simple chart in the browser:</p>\r\n<figure><img src=\"{asset:5052:url}\" data-image=\"5052\"></figure>\r\n<p></p>\r\n<p>We can also look at the time it takes to load the page. For a better comparison, I’m loading this page with cache disabled and on a slow 3G connection.</p>\r\n<figure><img src=\"{asset:5053:url}\" data-image=\"5053\"></figure>\r\n<p>As we can see in the image, all our code is in only one piece. It does not use the power of parallel requests and loads unnecessary code the first time, which causes some delay in the first rendering. To do this, we can turn our CodeEditor component into a slow component.</p>\r\n<p><strong>LazyCodeEditor</strong></p>\r\n<pre>import React, { Suspense } from 'react'\r\nconst CodeEditor = React.lazy(() =&gt; import('./CodeEditor'))\r\n\r\nexport default function LazyCodeEditor(props) {\r\n  return (\r\n    /* Here you can pass a better fallback if you want to. */\r\n    &lt;Suspense fallback=\"{&lt;&gt;&lt;/&gt;}&gt;\r\n      &lt;CodeEditor {...props} /&gt;\r\n    &lt;/Suspense&gt;\r\n  )\r\n}\r\n</pre>\r\n<ul><li><strong>React.lazy</strong> allow us to do dynamic imports.</li><li><strong>Suspense</strong> will render the component only when it is resolved. You can use the <strong>fallback </strong>props to pass a loader component.</li></ul>\r\n<p>So we’re loading the <strong>CodeEditor</strong> only when this code is called and not at build time. The <strong>Suspense</strong> component will wait for the <strong>CodeEditor</strong> to be resolved and display it, meanwhile displaying the fallback. Now, to see the difference, we’ll import the <strong>LazyCodeEditor</strong> instead of the <strong>CodeEditor</strong> and run some analyses again.</p>\r\n<p><strong>Code.jsx</strong></p>\r\n<pre>import React from \"react\";\r\nimport LazyCodeEditor from \"./LazyCodeEditor\";\r\n\r\nconst Code = () =&gt; (\r\n  &lt;&gt;\r\n    &lt;h1&gt;Code&lt;/h1&gt;\r\n    &lt;LazyCodeEditor /&gt;\r\n  &lt;/&gt;\r\n);\r\n\r\nexport default Code;</pre>\r\n<pre>yarn build\r\nyarn analyze</pre>\r\n<p>We can see the <tt>brace</tt> and <tt>react-ace</tt> split into a different chunk:</p>\r\n<figure><img src=\"{asset:5054:url}\" data-image=\"5054\"></figure>\r\n<p>And we can see that initial load time has decreased.</p>\r\n<figure><img src=\"{asset:5055:url}\" data-image=\"5055\"></figure>\r\n<p><br>Our app only loads the chunks related to the code-editor when the user needs them:</p>\r\n<figure><img src=\"{asset:5056:url}\" data-image=\"5056\"></figure>\r\n<p></p>\r\n<p>Now, we have an app which uses <strong>Code Splitting</strong> to have better performance and user experience as well. I hope you have enjoyed this article and it has been useful to you. Thanks for reading!</p>\r\n<p><strong>REFERENCES</strong></p>\r\n<ul><li><a href=\"https://reactjs.org/docs/code-splitting.html\">https://reactjs.org/docs/code-splitting.html</a></li></ul>\r\n<p></p>\r\n<p></p>\r\n<p><em>Originally published on&nbsp;<a href=\"https://www.brunoquaresma.dev/code-split-with-lazy-components/\" target=\"_blank\">Bruno Quaresma Blog</a>.</em></p>\r\n<p>---------</p>\r\n<p></p>\r\n<p></p>",
        "blogCategory": [
            "3",
            "10",
            "1462"
        ],
        "mainBlogImage": [
            "5268"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-08-08T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 3118,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "8fa45dce-f6df-4fbb-8d94-82743e057bb1",
        "siteSettingsId": 3118,
        "fieldLayoutId": 4,
        "contentId": 1670,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How to Spot Tech Trends Early Using Stack Overflow and GitHub",
        "slug": "how-to-spot-tech-trends-early",
        "uri": "blog/how-to-spot-tech-trends-early",
        "dateCreated": "2019-08-06T15:34:53-07:00",
        "dateUpdated": "2019-09-27T10:16:55-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/how-to-spot-tech-trends-early",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/how-to-spot-tech-trends-early",
        "isCommunityPost": false,
        "blogBodyText": "<p>As a product manager at Fauna, it’s been important to understand what’s emerging in the wider technology ecosystem. Fauna is a database company that is shaking up the industry by focusing on developers first. With our recently released <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">GraphQL API</a>, we continue to innovate in ways that matter to our tech-savvy community.</p>\n<p>I was recently asked by one of our executives, “how can we validate our assumption that <a href=\"https://medium.com/@MarutiTech/what-is-serverless-architecture-what-are-its-criticisms-and-drawbacks-928659f9899a\">serverless architecture</a> is a growing trend?” This is an open-ended question with no single answer. After cracking a smile and dusting off my analyst hat, I decided I’d do some digging. And I’m excited to teach you how to do this very digging yourself with the following tutorial!</p>\n<h3>What’s a proxy for actual tech activity, not just noise?</h3>\n<p>My first thought was to check Twitter. Unfortunately, Twitter now provides access to its “Firehose” of tweets only through a <a href=\"https://developer.twitter.com/en/docs/tweets/batch-historical/overview\">paid option</a>. Third-party research firms also offer hashtag analysis at a hefty premium.</p>\n<p>As a sort-of-developer and a long time data analyst, I knew that Stack Overflow (SO) questions and GitHub repos point to the truth over buzzwords. With these two sources, I hit the jackpot. I decided to look for the number of SO questions and GitHub repos over time to gauge the popularity of the serverless architecture.</p>\n<h3>But “how do I query these sources?” you ask</h3>\n<p>Much to my delight, both Stack Overflow and GitHub provide access to their data for free and public use. Thanks, Stack Overflow and GitHub!</p>\n<h2>Stack Overflow</h2>\n<p><a href=\"https://data.stackexchange.com/stackoverflow/query/1064625\">Stack Overflow</a> offers their own Data Explorer. It’s bare bones, and works perfectly for simple queries. I recommend spending some time taking a look at their popular queries--some have over 100,000 views!</p>\n<p>The query I wrote to analyze the number of SO questions over time:</p>\n<pre>select \n  concat(datepart(year, CreationDate),'-', datepart(month, CreationDate)) as dt,\n  count(1) num_serverless_mentions\nfrom \n  Posts \nwhere \n  Title LIKE '%serverless%'\ngroup by \n  concat(datepart(year, CreationDate),'-', datepart(month, CreationDate))\n</pre>\n<p>Let’s dissect what this query is doing:</p>\n<ul><li>The concat() function turns <tt>“CreationDate”</tt> into a YYYY-MM date format that Google Sheets can easily turn into a time series graph</li><li><tt>count(1)</tt> counts each record in the Posts table that matches our conditions</li><li><tt>Title LIKE '%serverless%' </tt> locates all of the different posts with serverless anywhere in the post title<ul><li><em>Replace serverless with any interesting tech trend or language to perform your own analysis </em>&#x1f638;</li></ul></li><li>The group by clause ensures that all posts are counted, in monthly buckets</li>\n</ul>\n<p>The resulting data is visualized here:</p>\n<figure><img src=\"{asset:3121:url}\" data-image=\"3121\" alt=\"Stack Overflow posts mentioning serverless\" style=\"max-width: 550px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p>Wow! Just look at that growth over time. Up and to the right we go &#x1f680;</p>\n<h2>Github</h2>\n<p><a href=\"https://bigquery.cloud.google.com/table/bigquery-public-data:github_repos.sample_contents?pli=1\">GitHub</a> offers a subset of their data as a free and public dataset on <a href=\"https://bigquery.cloud.google.com/table/bigquery-public-data:github_repos.sample_contents?pli=1\">Google BigQuery</a> (note: requires Google login). Tables offered include data about repos, languages, commits, etc.</p>\n<p>The query I wrote to analyze the number of GitHub repos over time:</p>\n<pre>select\n    concat(cast(year(created_at) as string),'-',cast(month(created_at) as string)) dt,\n    count(1) num_serverless_repos\nFROM (TABLE_DATE_RANGE([githubarchive:day.], \n    TIMESTAMP('2017-01-01'), \n    TIMESTAMP('2019-06-01')\n  ))\n  where repo.name like '%serverless%'\ngroup by 1\norder by 1</pre>\n<p>A tad more complex! Let’s dissect what this query is doing:</p>\n<ul><li>As in the SO query, the <tt>concat()</tt> function turns “created_at” into a YYYY-MM date format that Google Sheets can easily turn into a time series graph.</li><li><tt>FROM (TABLE_DATE_RANGE([githubarchive:day.]</tt>... requires some explanation. The way this data is stored in BigQuery is in daily tables. To perform a time series query across days, we need to select a range of tables by date. The <a href=\"https://cloud.google.com/bigquery/docs/reference/legacy-sql#table-date-range\">TABLE_DATE_RANGE function</a> takes three inputs: the table prefix (i.e. githubarchive:day.), and the start and end dates.<ul><li>Note that BigQuery limits the date range to 1000 days/tables. So performing this analysis across many years requires a UNION between result tables.</li></ul></li><li><tt>repo.name like '%serverless%'</tt> picks out all of the different posts with \"serverless\" anywhere in the repo name</li><li><tt>group by 1</tt> and <tt>order by 1</tt> tell the query to organize the query by the date, i.e. the first line in the select statement.</li></ul>\n<p>The resulting data is visualized here:</p>\n<figure><img src=\"{asset:3120:url}\" data-image=\"3120\" alt=\"GitHub repos with serverless titles\" style=\"max-width: 550px;\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\n<p>Even more up and to the right! Literal &#x1f4c8;</p>\n<h2>Conclusion</h2>\n<p>As shown in both charts, serverless is a tech trend that extends beyond being a buzzword. Tens of thousands of repos have been created in the last 3 years with serverless in the title, and the number of Stack Overflow posts continues to grow over time.&nbsp;</p>\n<p>\nThis research reaffirms Fauna’s serverless-first approach. Our customers have stated the value clearly: without needing to worry about provisioning and ops, they are able to focus on what they enjoy doing: building meaningful products.&nbsp;</p>\n<p>\nIt’s clear that by the start of 2017, serverless architecture was hitting its stride. What tech trends are you interested in investigating, and getting ahead of? I invite you to <a href=\"https://github.com/lewisisgood/tech-trends\" target=\"_blank\">run my queries using your own keywords</a>, and share the results <a href=\"https://linkedin.com/in/lewisking\">with me</a> or on our <a href=\"https://community-invite.fauna.com/\">Community Slack channel</a>.</p>",
        "blogCategory": [
            "1530",
            "5603"
        ],
        "mainBlogImage": [
            "3377"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "3110",
        "postDate": "2019-08-01T06:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 3109,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "67064a38-3306-4407-8eb4-141928001cfa",
        "siteSettingsId": 3109,
        "fieldLayoutId": 4,
        "contentId": 1661,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with GraphQL, Part 4: Updating your Schema",
        "slug": "getting-started-with-graphql-part-4-updating-your-schema",
        "uri": "blog/getting-started-with-graphql-part-4-updating-your-schema",
        "dateCreated": "2019-07-31T14:39:59-07:00",
        "dateUpdated": "2019-08-21T14:43:21-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-graphql-part-4-updating-your-schema",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-graphql-part-4-updating-your-schema",
        "isCommunityPost": false,
        "blogBodyText": "<p>In previous articles, we explored how to <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">set up your development environment and query your GraphQL schema</a>, how to <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-2-relations\">add relationships to your GraphQL queries</a>, and lastly, how to <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-3-the-unique-directive\">add unique constraints to your GraphQL schema</a>. In this article, we’ll look at different ways you can update your current schema, and explore what you can do with Fauna’s built-in <tt>@embedded</tt> directive when it comes to schema design.</p>\r\n<p>The schema is a key piece of any GraphQL API and its correct definition determines a successful interaction between clients and servers. Unlike other APIs, GraphQL uses a strong type system to define the data and the operations that can be performed. Once established, this schema is what enables, among other things, the extensive querying flexibility distinctive of a GraphQL API.</p>\r\n<p>As most pieces of software, the schema needs to evolve over time, and, sooner or later, we will face a scenario in which it needs to be updated. As in other areas, GraphQL offers great flexibility when it comes to schema updates.</p>\r\n<p>Let’s continue our exploration with how we can extend an existing schema through Fauna’s GraphQL API!</p>\r\n<h2>Updating your schema</h2>\r\n<p>In the last article, we defined a simple <tt>User</tt> object type with a unique <tt>username</tt> field:</p>\r\n<pre>type User {\r\n  username: String! @unique\r\n}</pre>\r\n<p>Now, we’re going to extend the <tt>User</tt> object by adding a new <tt>Address</tt> object type field to it. If you are familiar with Domain-Driven Design, we want to model the new <tt>Address</tt> object type as a <em>value object</em>. That is, we don’t care about an <tt>Address</tt>’s particular identity; we just want it to be part of the <tt>User</tt> object. From the API perspective, this means that we don’t need to query an <tt>Address</tt> on its own, but we will always do it through the <tt>User</tt> object.&nbsp;</p>\r\n<p>\r\nDuring the import process, unless defined otherwise, all object types are considered <em>entities</em> by default. This means that an <tt>ID</tt> field, as well as top-level CRUD operations, is created automatically for the given object. At the database layer, a Collection, as well as an Index, is associated for the imported object.&nbsp;</p>\r\n<p>\r\nIn order to tell to the import process to not create all of these elements automatically, we need to use Fauna’s built-in <tt>@embedded</tt> directive. If annotated with the <tt>@embedded</tt> directive, no <tt>ID</tt> field nor CRUD operations are generated for the given object. At the database layer, the annotated type is an embedded object within the document associated with the parent object type. All of the necessary metadata for the embedded object is stored in this document as well.&nbsp;</p>\r\n<p>\r\nLet’s create a file with the updated schema using the <tt>@embedded</tt> directive for defining our new <tt>Address</tt> object type:</p>\r\n<pre>type User {\r\n  username: String! @unique\r\n  address: Address\r\n}\r\n\r\ntype Address @embedded {\r\n  street: String!\r\n  city: String!\r\n  state: String!\r\n  zipCode: String!\r\n}</pre>\r\n<p>Import this schema into FaunaDB by logging into the Cloud Console. Select the database you created in previous articles and go to the <tt>GRAPHQL</tt> section from the left menu bar. Click on the <tt>UPDATE SCHEMA</tt> button and select the file containing the updated schema.</p>\r\n<figure><img src=\"{asset:3103:url}\" data-image=\"3103\"></figure>\r\n<p>At the database layer, the update process creates any missing collections, indexes, and functions. If any of those elements already exist, the API overrides its metadata with the new GraphQL schema information. If no new schema information is provided for elements already containing GraphQL metadata, they remain the same.</p>\r\n<p>At this point, it is important to remember that the GraphQL schema information is stored in the form of <em>metadata</em> among the different database elements. During the update process, only this metadata might be overridden with the new given schema definition while the actual data always remains unmodified. This means that the GraphQL schema can be updated safely without worrying about data loss.</p>\r\n<p>Now, let’s create a new <tt>User</tt>, along with its <tt>Address</tt> information, by running the following query from Console’s GraphQL Playground:</p>\r\n<pre>mutation CreateUser {\r\n  createUser(data: {\r\n    username: \"jerry\"\r\n    address: {\r\n      street: \"129 West 81st Street Apt 5A\",\r\n      city: \"New York\"\r\n      state: \"NY\"\r\n      zipCode: \"10024\"\r\n    }\r\n  }) {\r\n    username\r\n    address {\r\n      street\r\n      city\r\n      state\r\n      zipCode\r\n    }\r\n  }\r\n}</pre>\r\n<p>As a result, you should see the following response:</p>\r\n<pre>{\r\n  \"data\": {\r\n    \"createUser\": {\r\n      \"username\": \"jerry\",\r\n      \"address\": {\r\n        \"street\": \"129 West 81st Street Apt 5A\",\r\n        \"city\": \"New York\",\r\n        \"state\": \"NY\",\r\n        \"zipCode\": \"10024\"\r\n      }\r\n    }\r\n  }\r\n}</pre>\r\n<p>If you query information on any pre-existing user, you should notice that everything works as expected, and that previous data has remained unchanged. We have successfully updated the GraphQL schema definition without affecting any pre-existing data!&nbsp;</p>\r\n<h2>Overriding your schema&nbsp;</h2>\r\n<p>In this section, we look at how you can override an existing schema. Overriding a schema works very differently from the update process we have described above, and it’s very important to learn the differences between these two types of operations.</p>\r\n<p>When overriding a schema, all previous GraphQL metadata is removed. All database elements, such as collections, indexes and functions, which were previously annotated with GraphQL metadata, are also removed. Unlike the update operation, when overriding a schema, actual data might be erased from the database. Any other elements that do not have GraphQL metadata won’t be affected. Since this is an operation that might cause data loss, you must be careful and understand its effects.</p>\r\n<p>Now, let’s create a new database to explore how the override schema process works. From the Cloud Console home page, click on the <tt>NEW DATABASE</tt> button:</p>\r\n<figure><img src=\"{asset:3104:url}\" data-image=\"3104\"></figure>\r\n<p>Give a name for the new database and save it:</p>\r\n<figure><img src=\"{asset:3105:url}\" data-image=\"3105\"></figure>\r\n<p>Once the new database is ready, let’s reuse the previous file in order to import the GraphQL schema we have used before:</p>\r\n<pre>type User {\r\n  username: String! @unique\r\n  address: Address\r\n}\r\n\r\ntype Address @embedded {\r\n  street: String!\r\n  city: String!\r\n  state: String!\r\n  zipCode: String!\r\n}</pre>\r\n<p>From the Cloud Console, select <tt>GRAPHQL</tt> from the left menu bar, click the <tt>IMPORT SCHEMA</tt> button, and choose the GraphQL schema file listed above:</p>\r\n<figure><img src=\"{asset:3106:url}\" data-image=\"3106\"></figure>\r\n<p>Once the schema is imported, let’s create a new <tt>User</tt> as follows:</p>\r\n<pre>mutation CreateUser {\r\n  createUser(data: {\r\n    username: \"george\"\r\n    address: {\r\n      street: \"1344 Queens Blvd\",\r\n      city: \"Long Island City\"\r\n      state: \"NY\"\r\n      zipCode: \"11101\"\r\n    }\r\n  }) {\r\n    username\r\n    address {\r\n      street\r\n      city\r\n      state\r\n      zipCode\r\n    }\r\n  }\r\n}</pre>\r\n<p>You should see the following response:</p>\r\n<pre>{\r\n  \"data\": {\r\n    \"createUser\": {\r\n      \"username\": \"george\",\r\n      \"address\": {\r\n        \"street\": \"1344 Queens Blvd\",\r\n        \"city\": \"Long Island City\",\r\n        \"state\": \"NY\",\r\n        \"zipCode\": \"11101\"\r\n      }\r\n    }\r\n  }\r\n}</pre>\r\n<p>Now that we have an initial schema and some data already loaded into the database, let’s see what happens when we override the current GraphQL schema.</p>\r\n<p>Create a new file containing the following schema, which does not include the <tt>User</tt> nor the <tt>Address</tt> object type:</p>\r\n<pre>type Todo {\r\n  title: String!\r\n  completed: Boolean\r\n}\r\n\r\ntype Query {\r\n  allTodos: [Todo!]\r\n  todosByCompletedFlag(completed: Boolean!): [Todo!]\r\n}</pre>\r\n<p>From the <tt>GRAPHQL</tt> view in the Cloud Console, select the <tt>OVERRIDE SCHEMA</tt> button. You should see a warning message explaining the effects of the override operation:\r\n</p>\r\n<figure><img src=\"{asset:3107:url}\" data-image=\"3107\"></figure>\r\n<p></p>\r\n<p>Select <tt>OVERRIDE</tt> and choose the schema file mentioned above. The operation takes around one minute to complete; this extra time is necessary to allow all cluster nodes to process the deletions.</p>\r\n<p>Once the override is finished, let’s try to create a new <tt>User</tt> again:</p>\r\n<pre>mutation CreateUser {\r\n  createUser(data: {\r\n    username: \"elaine\"\r\n    address: {\r\n      street: \"16 West 75th Street Apt 2G\",\r\n      city: \"New York\"\r\n      state: \"NY\"\r\n      zipCode: \"10023\"\r\n    }\r\n  }) {\r\n    username\r\n    address {\r\n      street\r\n      city\r\n      state\r\n      zipCode\r\n    }\r\n  }\r\n}</pre>\r\n<p>Since the <tt>User</tt> and <tt>Address</tt> objects are no longer present in the overridden schema, you should get an error response:</p>\r\n<pre>{\r\n  \"data\": null,\r\n  \"errors\": [\r\n    {\r\n      \"message\": \"Cannot query field 'createUser' on type 'Mutation'. Did you mean 'createTodo'? (line 2, column 3):\\n  createUser(data: {username: \\\"george\\\", address: {street: \\\"1344 Queens Blvd\\\", city: \\\"Long Island City\\\", state: \\\"NY\\\", zipCode: \\\"11101\\\"}}) {\\n  ^\",\r\n      \"locations\": [\r\n        {\r\n          \"line\": 2,\r\n          \"column\": 3\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}</pre>\r\n<p>If you try to find the <tt>User</tt> collection, you will notice that it has been removed along with all of its documents:</p>\r\n<figure><img src=\"{asset:3108:url}\" data-image=\"3108\"></figure>\r\n<p>We have now learned how the data is affected when overriding a GraphQL schema!</p>\r\n<h2>Conclusion</h2>\r\n<p>In this post, we have explored the two different ways that a GraphQL schema can be updated. We have learned the important differences between these two methods, as well as how Fauna’s built-in <tt>@embedded</tt> directive can help you to model the schema in more advanced ways.</p>\r\n<p>Stay tuned for more about GraphQL and FaunaDB, as we explore how to import a schema for an existing database, how to write custom mutations, how to paginate over a list of objects, and much more!</p>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "3111"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2019-07-25T09:40:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1859,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "720c5a28-9db5-4204-b8b8-e4f781e5fb46",
        "siteSettingsId": 1859,
        "fieldLayoutId": 4,
        "contentId": 1420,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Demystifying Database Systems, Part 3: Introduction to Consistency Levels",
        "slug": "demystifying-database-systems-introduction-to-consistency-levels",
        "uri": "blog/demystifying-database-systems-introduction-to-consistency-levels",
        "dateCreated": "2019-07-16T09:59:02-07:00",
        "dateUpdated": "2019-08-26T17:04:20-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/demystifying-database-systems-introduction-to-consistency-levels",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/demystifying-database-systems-introduction-to-consistency-levels",
        "isCommunityPost": false,
        "blogBodyText": "<p>Database systems typically give users the ability to trade off correctness for performance. We have spent the previous <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">two</a> <a href=\"https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">posts</a> in this series discussing one particular way to trade off correctness for performance: database isolation levels. In distributed systems, there is a whole other category for trading off correctness for performance: consistency levels. There are an increasing number of distributed database systems that are giving their users multiple different consistency levels to choose from, thereby allowing the user to specify which consistency guarantees are needed from the system for a particular application. Similar to isolation levels --- weaker consistency levels typically come with better performance, and thus come with the same types of temptations as reduced isolation levels.</p>\r\n<p>In this post, we give a short tutorial on consistency levels --- explaining what they do, and how they work. Much of the existing literature and online discussion of consistency levels are done in the context of multi-processor or distributed systems that operate on a single data item at a time, without the concept of a “transaction”. Instead, we give some direction for how to think about consistency levels in the context of ACID-compliant database systems.</p>\r\n<h2>What is a consistency level?</h2>\r\n<p>The definition of <strong>consistency </strong>is fundamentally dependent on context. In general, consistency refers to the ability of a system to ensure that it complies (without fail) to a predefined set of rules. However, this set of rules changes based on context. For example, the C of ACID and the C of CAP both refer to consistency. However, the set of rules implied by these two contexts are totally different. In ACID, the rules refer to application-defined semantics. A system that guarantees the C of ACID ensures that processing a transaction does not violate referential integrity constraints, foreign-key constraints, and any other application-specific constraints (such as: “every user must have a name”). In contrast, the consistency C of CAP refers to the rules related to making a concurrent, distributed system appear like a single-threaded, centralized system. Reads at a particular point in time have only one possible result: they must reflect the most recent completed write (in real-time) of that data item, no matter which server processed that write. </p>\r\n<p>One point of confusion that we can eliminate at this point is that the phrase “consistency level” is not typically used in the context of ACID consistency. This is because the C of ACID is almost entirely the responsibility of the application developer --- only the developer can ensure that the code they place inside a transaction does not violate application semantics when it is run in isolation.&nbsp;</p>\r\n<blockquote>ACID is really a misnomer --- really it should be AID, since only those three (atomicity, isolation, and durability) are in the realm of system guarantees. [Joe Hellerstein claims that he was taught that the C in ACID was only included to make an LSD pun, but acknowledges that this may be apocryphal.]</blockquote>\r\n<p>When we talk about <strong>consistency levels</strong>, we’re really referring the C of CAP. In this context, perfect consistency --- usually referred to as “strict consistency” --- would imply that the system ensures that all reads reflect all previous writes --- no matter where those writes were performed. Any consistency level below “perfect” consistency enables situations to occur where a read does not return the most recent write of a data item.  [Side note: the C of CAP in the <a href=\"https://users.ece.cmu.edu/~adrian/731-sp04/readings/GL-cap.pdf\">original CAP paper</a> refers to something called “atomic consistency” which is slightly weaker than strict consistency but still considered “perfect” for practical purposes. We’ll discuss the difference later in this post.]</p>\r\n<p>Depending on how a particular system is architected, perfect consistency becomes easier or harder to achieve. In poorly designed systems, achieving perfection comes with a prohibitive performance and availability cost, and users of such systems are pushed to accept guarantees significantly short of perfection. However, even in well-designed systems, there is often a non-trivial performance benefit achieved by accepting guarantees short of perfection. </p>\r\n<h2>An overview of well-known consistency levels</h2>\r\n<p>The notion of a <em>consistency level</em> originated by researchers of shared-memory multi-processor systems. The goal of the early work was to reason about how and when different threads of execution, which may be running concurrently and accessing overlapping sets of data in shared memory, should see the writes performed by each other. As such, most of the initial work focused on reads and writes of individual data items, rather than at the level of groups of reads and writes within a transaction. We will begin our discussion using the same terminology as the original research and models, and then discuss how to apply these ideas to transactions afterward.</p>\r\n<figure><img src=\"{asset:5759:url}\" data-image=\"5759\"></figure>\r\n<p>In <strong>sequential consistency</strong>, all writes --- no matter which thread made the write, and no matter which data item was written to --- are globally ordered. Every single thread of execution must see the writes occurring in this order. For example, if one thread saw data X being updated to 5, and then later saw Y being updated to 10, every single thread must see the update of X happening before the update of Y. If any thread sees the new value of Y but the old value of X, sequential consistency would be violated. This example is shown in Figure 1. In this figure, time gets later as you move to the right in the figure, and there are 4 threads of execution: P1, P2, P3, and P4. Every thread (that reads X and Y) sees the update of X from 0 to 5 happening before the update of Y from 0 to 10. Threads: P1 and P2 write X and Y respectively, but do not read either one. Thread P3 sees the new value of X and subsequently sees the old value of Y. This is only possible if the update to X happened before the update to Y. Thread P4 only sees the new values of X and Y, so it does not see which one happened first. Thus, all threads agree that it is possible that the update of X happened before the update to Y. Contrast this to Figure 2, below, in which P3 and P4 see clearly different orders of the updates to X and Y --- P3 sees the new value of X (5) and subsequently the old value of Y (0), while P4 sees the new value of Y (10) and subsequently the old value of X (0). Figure 2 is thus not a sequentially consistent schedule.</p>\r\n<p></p>\r\n<figure><img src=\"{asset:3082:url}\" data-image=\"3082\"></figure>\r\n<p>In general, sequential consistency does not place any requirements on how to order the writes.&nbsp;In our example the write to X happened in real-time before the write to Y. Nonetheless, as long as every thread agrees to see the write to Y as happening before the write to X, sequential consistency allows the official history to be different than what occurred according to real-time (the only restriction is that writes and reads originating from the same thread of execution cannot be reordered). See Figure 3 for an example of this. </p>\r\n<figure><img src=\"{asset:3079:url}\" data-image=\"3079\"></figure>\r\n<p></p>\r\n<p>In contrast to sequential consistency, <strong>strict consistency</strong> <strong>does </strong>place real-time requirements on how to order the writes.&nbsp;</p>\r\n<p>It assumes that it is always possible to know what time it currently is with zero error --- i.e. that every thread of execution agrees on the precise current time. The order&nbsp;of the writes in the sequential order must be equal to the real-time that these writes were issued. Furthermore, every read operation must read the value of the most recent write in real-time --- no matter which thread of execution initiated that write. In a distributed system (and even multi-processor single-server systems), it is impossible in practice to have a global agreement on the precise current time, which renders strict consistency to be mostly of theoretical interest. </p>\r\n<p>None of Figures 1, 2, or 3 above satisfy strict consistency because they all contain either a read of x=0 or a read of y=0 after the value of x or y has been written to a new value. However, Figure 4 below satisfies strict consistency since all reads reflect the most recent write in real-time:</p>\r\n<figure><img src=\"{asset:3084:url}\" data-image=\"3084\"></figure>\r\n<p></p>\r\n<p>In a distributed/replicated system, where writes and reads can originate anywhere, the highest level of consistency obtained in practice is <strong>linearizability</strong> (also known as “atomic consistency” which is what it is called in the CAP theorem). Linearizability is very similar to strict consistency: both are extensions of sequential consistency that impose real time constraints on writes. The difference is that the linearizability model acknowledges that there is a period of time that occurs between when an operation is submitted to the system, and when the system responds with an acknowledgement that it was completed. In a distributed system, the sending of the write request to the correct location(s) --- which may include replication --- can occur during this time period. A linearizability guarantee does not place any ordering constraints on operations that occur with overlapping start and end times. The only ordering constraint is for operations that do not overlap in time --- only in those cases does the earlier write have to be seen before the later write.</p>\r\n<figure><img src=\"{asset:3083:url}\" data-image=\"3083\"></figure>\r\n<p></p>\r\n<p>Figure 5 above shows an example of a schedule that is linearizable, but not strictly consistent. It is not strictly consistency since the read of X by P3 is initiated (and returns) slightly after the write of X by P1, but still sees the old value. Nonetheless, it is linearizable because this read of X by P3 and write of X by P1 overlap in time, and therefore linearizability does not require the read of X by P3 to see the result of the write of X by P1.</p>\r\n<blockquote>While linearizability and strict consistency are stronger than sequential consistency, sequential consistency is by itself a very high level of consistency, and there exist many consistency levels below it.</blockquote>\r\n<p><strong><br>Causal consistency</strong> is a popular and useful consistency level that is slightly below sequential consistency. In sequential consistency, all writes must be globally ordered --- even if they are totally unrelated to each other. Causal consistency does not enforce orderings of unrelated writes. However, if a thread of execution performs a read of some data item (call it X) and then writes that data item or a different one (call it Y), it assumes that the subsequent write may have been <em>caused</em> by the read. Therefore, it enforces the order of X and Y --- specifically all threads of execution must observe the write of Y after the write of X.</p>\r\n<figure><img src=\"{asset:3080:url}\" data-image=\"3080\"></figure>\r\n<p></p>\r\n<p>As an example, compare Figure 6 (above) with Figure 2. In Figure 2, P3 saw the write to X happening before the write to Y, but P4 saw the write to Y happening before the write to X. This violates sequential consistency, but not causal consistency. However, in Figure 6, P2 read the write to X before performing the write to Y. That places a causal constraint between the write to X and Y --- Y must happen after X. Therefore, when P4 sees the write to Y without the write to X, causal consistency is violated.</p>\r\n<p><strong>Eventual consistency</strong> is even weaker --- even causally dependent writes may become visible out of order. For example, despite violating every other consistency guarantee that we have discussed so far, Figure 6 does not necessarily violate eventual consistency. The only guarantee in eventual consistency is that if there are no writes for a “long” period of time (where the definition of “long” is system dependent), every thread of execution will agree on the value of the last write. So as long as P4 eventually sees the new value of X (5) at some later point in time (not shown in Figure 6), then eventual consistency is maintained.</p>\r\n<h2>Strong consistency vs. Weak consistency</h2>\r\n<p>Strict consistency and linearizability/atomic consistency are typically thought of as “strong” consistency levels. In many cases, sequential consistency is also referred to as a strong consistency level. The key feature that each of these consistency levels share is that the state of the database goes through a universally agreed-upon sequence of state changes. This allows the realities of replication to be hidden to the end-user.&nbsp;</p>\r\n<p>In essence, the view of the database to the user is that there is only one copy of the database that continuously makes state transitions in a forward direction. In contrast, weaker consistency levels (such as causal consistency and eventual consistency) allow different views of the database state to see a different progression of steps in database state ---- a clear impossibility unless there is more than one copy of the database.&nbsp;</p>\r\n<blockquote>Thus, for weaker levels of consistency, the application developer must be explicitly aware of the replicated nature of the data in the database, thereby increasing the complexity of development relative to strong consistency. </blockquote>\r\n<h2>Transactions and consistency levels</h2>\r\n<p>As we discussed above, consistency levels have historically been defined in terms of individual reads or writes of a data item. This makes the discussion of consistency levels hard to apply to the context of database systems in which groups of read and write operations occur together in atomic transactions. Indeed, both the research literature and vendor documentation (for those vendors that offer multiple consistency levels) are extremely confusing and do not take a uniform approach when it comes to applying consistency levels in the context of database systems.</p>\r\n<p>The simplest way to reason about consistency levels in the presence of database transactions is to make only minor adjustments to the consistency models we discussed earlier. We still view consistency as threads of execution sending read and write requests to a shared data store. The only difference is that we annotate each read and write request with the transaction identifier of the transaction that initiated each request. If each thread of execution can only process one transaction at a time, and transactions can not be processed by more than one thread of execution, then the traditional timeline consistency diagrams need only be supplemented with rectangular boundaries indicating the start and endpoint of each transaction within a thread of execution, as shown in the figure below.</p>\r\n<figure><img src=\"{asset:3078:url}\" data-image=\"3078\"></figure>\r\n<p>The presence of a transaction in a consistency diagram adds additional constraints corresponding to AID of ACID: all of the reads and writes within the transaction succeed or fail together (atomicity), they are isolated from other concurrently running transactions (the degree of isolation will depend on the isolation level), and writes of committed transactions will outlive all kinds of system failure (durability).</p>\r\n<p>The atomicity and durability guarantees of transactions are pretty easy to cognitively separate from consistency guarantees because they deal with fundamentally different concepts. The problem is isolation. Consistency guarantees specify how and when writes are visible to threads of execution that read database state. Isolation guarantees also place limitations on when writes become visible. This conceptual overlap of isolation and consistency guarantees is the source of much confusion.  In the next post of this series, I plan to give a tutorial for understanding the difference between isolation levels and consistency levels. </p>\r\n<p></p>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [
            "3064"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1820",
        "postDate": "2019-07-17T09:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1844,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "77ca0256-e5f8-42a5-b792-aa224771d805",
        "siteSettingsId": 1844,
        "fieldLayoutId": 4,
        "contentId": 1405,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing FaunaDB 2.7",
        "slug": "announcing-faunadb-2-7",
        "uri": "blog/announcing-faunadb-2-7",
        "dateCreated": "2019-07-12T14:48:10-07:00",
        "dateUpdated": "2019-08-29T15:51:54-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-faunadb-2-7",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-faunadb-2-7",
        "isCommunityPost": false,
        "blogBodyText": "<p>The Fauna team is very pleased to announce the availability of release 2.7 of FaunaDB. This release brings numerous enhancements in security and usability. The sections below summarize the highlights of this release.<br></p>\n<h2>Attribute-based access control</h2>\n<p>Attribute-based access control (ABAC) is designed as a replacement for FaunaDB’s current resource-based security model. It&nbsp;is an alternative to an all-or-nothing security model and is commonly used in applications to restrict access to specific data based on the user's role. Until 2.7, access within FaunaDB was restricted to 4 pre-defined roles. Users would authenticate using keys or tokens and could have any one of the following roles - admin, server, server- read-only, or client. These roles have different levels of access and a developer would pick one that is closest to end users access profile.&nbsp;<a href=\"https://docs.fauna.com/fauna/2.7.0/guides/rbac\">With ABAC, now developers can define custom roles that match the applications’ requirements</a> and only allow access to the data that the user needs to see.</p>\n<p>Just like in the <a href=\"https://fauna.com/blog/data-security-in-the-age-of-cloud-native-apps\">previous releases</a>, end users use keys to authenticate to the system and keys are assigned roles. At any given time a key can have only one role. These user-defined roles are composed of a set of privileges. Privileges are predefined actions like create, delete, read, write etc. to specific resources. In FaunaDB a resource can a database, a collection, an index or even a role or a document. This model provides a lot of flexibility on how access can be controlled within an application.</p>\n<figure><img src=\"{asset:1847:url}\" data-image=\"1847\"></figure>\n<h4>Validating actions with predicate functions</h4>\n<p>One of the unique features of ABAC in FaunaDB is the ability to associate a predicate lambda, which is a read-only function that returns a boolean for an action. The function determines whether a user can access a specific resource based on a defined condition. In the following example,&nbsp;executed in Fauna Shell, the role only provides access to users who are defined as “vip”s:</p>\n<pre>todo-app&gt; CreateRole({\n    name: \"can_manage_todos\",\n    membership: [\n        // ...\n    ],\n    privileges: [\n        {\n            resource: Collection(\"todos\"),\n            actions: {\n                create: Query(Lambda(newData =&gt;\n                    Select([\"data\", \"vip\"], Get(Identity()))\n                )),\n                // ...\n            }\n        }\n    ]\n})</pre>\n<h4>Membership</h4>\n<p>FaunaDB also allows users to define membership rules. Membership rules contain a resource collection, e.g. a collection with a list of users. A predicate function can be used to determine membership dynamically, e.g. only a “vip” can update a certain record:<br></p>\n<pre>todo-app&gt; CreateRole({\n    name: \"can_manage_todos\",\n    membership: [\n        {\n            resource: Collection(\"users\"),\n            predicate: Query(Lambda(ref =&gt;\n                Select([\"data\", \"vip\"], Get(ref))\n            ))\n        }\n    ],\n    privileges: [\n        // ...\n    ]\n})</pre>\n<h4>Cloud Console support</h4>\n<p>In addition to support in Fauna Shell, we have also added ABAC support to the Cloud Console. You can access this feature by navigating to a database, clicking \"Security\" in the left sidebar, and then clicking \"Manage Roles\". Our built-in roles are not editable, so you need to click \"New Role\" to get started.</p>\n<figure><img src=\"{asset:1861:url}\" data-image=\"1861\"></figure>\n<h4>Putting it together</h4>\n<p>ABAC is ubiquitous in databases, but FaunaDB’s implementation is peerless. The ability to associate a lambda to the membership or privilege definitions notably improves on the standard implementation of roles. </p>\n<figure><img src=\"{asset:1865:url}\" data-image=\"1865\"></figure>\n<h2>FaunaDB Shell, on the web</h2>\n<p>We're thrilled to announce another commonly requested feature from users:&nbsp; a web-based version of the Fauna Shell.&nbsp;</p>\n<p>Users can now access our Shell command-line from any browser, anywhere with an internet connection.&nbsp;FQL commands and functions can now be run directly in the browser.&nbsp;This is especially helpful for new users hoping to get started quickly with FaunaDB, without needing to install CLI tools on their local computer.</p>\n<p>You can access this by drilling into a database and then clicking \"Shell\" in the sidebar. This is a great way to practice queries on-the-fly, especially for users new to FQL. You can issue multiple queries in one shot with the \"Open file\" button, and download all of your results by clicking \"Download\":</p>\n<figure><img src=\"{asset:1863:url}\" data-image=\"1863\"></figure>\n<h2>Renaming classes and instances</h2>\n<table><tbody><tr><td style=\"width:450px;\">Old Name</td><td style=\"width:450px;\">New Name</td></tr><tr><td>Class</td><td>Collection</td></tr><tr><td>Instance</td><td>Document</td></tr></tbody></table>\n<p></p>\n<p>One of the biggest user-facing changes in this release is the <strong>renaming of \"classes\" to \"collections\"</strong> and of <strong>\"instances\" to \"documents\"</strong>. Usability testing has repeatedly surfaced these terms as a source of confusion for new users. As such, we have renamed these terms.</p>\n<p>We decided to go with \"collection\" because it’s more inclusive of flexible data models and resonates well with developers from a NoSQL background. Because databases that use the term \"collections\" typically also use the word \"documents\" to denote the objects in those collections, \"instances\" have been renamed to \"documents\". We have also found that most new users start with document-based data models to improve productivity during the early prototyping stage of application development.</p>\n<p>This rename appears throughout FaunaDB and the documentation, especially in the <a href=\"https://deploy-preview-164--fauna-docs.netlify.com/fauna/2.7.0/api/fql/\">FQL reference</a>.</p>\n<p> </p>\n<pre>&gt; CreateCollection({ name: \"boons\" })\n{ ref: Ref(id=boons, collection=Ref(id=collections)),\n  ts: 1527274777496292,\n  history_days: 30,\n  name: 'boons' }</pre>\n<p>Starting with 2.7.0, the use of “Classes” and “Instances” are deprecated and they will eventually be removed in a future release. If you are using classes in your code, please be advised to change them to collections within the next two minor releases of FaunaDB. </p>\n<h2>Recursive user-defined functions</h2>\n<p>It's finally here! Recursion has been one of the most commonly requested features from our community, so we have added the ability to make recursive calls to user-defined functions. Since recursive calls can be very resource-intensive, the number of calls is restricted to a hard limit of 200. In the example below, we see how one can implement a custom <em>foldLeft</em> function with recursive calls. </p>\n<pre>&gt; CreateFunction({\n  name: \"foldLeft\",\n  \"body\": Query(\n    Lambda(\n      [\"lastResult\", \"array\"],\n      If(IsEmpty(Var(\"array\")),\n        Var(\"lastResult\"),\n        Let(\n          {\n            elem: Select(0, Take(1, Var(\"array\"))),\n            tail: Drop(1, Var(\"array\"))\n          },\n          Call(\"foldLeft\",\n            Add(Var(\"lastResult\"), Var(\"elem\")), Var(\"tail\")\n          )\n        )\n      )\n    )\n  )\n})</pre>\n<p>We are very excited to see how you like this change, and how you are using it in your applications, so please reach out to share your thoughts. You can either <a href=\"mailto:product@fauna.com\" target=\"_blank\">email us</a> or ping us in the <a href=\"https://community-invite.fauna.com/\" target=\"_blank\">Community Slack</a>.</p>\n<h2>Time to Number functions</h2>\n<p>FaunaDB 2.7.0 introduces new capabilities for time to number conversions. Fauna has two special data types dealing with dates and times. The first data type is date, which stores a calendar date. The second is a timestamp (ts), which stores an instant in time expressed as a calendar date and time of day. These new functions allow developers to extract various valuable information from a timestamp field. Here is a summary of these new functions:<br><br></p>\n<table><tbody><tr><td>Function</td><td>Description</td><td>Example</td></tr><tr><td>DayofMonth</td><td>Returns the day of the month from a timestamp</td><td>&gt; DayOfMonth(Time(\"2019-04-29T12:51:17Z\")) <br>29</td></tr><tr><td>DayofWeek</td><td>Returns the day of the week from a timestamp</td><td>&gt; DayOfYear(Time(\"2019-04-29T12:51:17Z\")) <br>119</td></tr><tr><td>DayofYear</td><td>Returns the day of the year from a timestamp</td><td>&gt; DayOfWeek(Time(\"2019-04-29T12:51:17Z\")) <br>1</td></tr><tr><td>Hour</td><td>Returns the hour from a timestamp</td><td>&gt; Hour(Time(\"2019-04-29T12:51:17Z\")) <br>12</td></tr><tr><td>Minute</td><td>Returns the minute from a timestamp</td><td>&gt; Minute(Time(\"2019-04-29T12:51:17Z\")) <br>51</td></tr><tr><td>Second</td><td>Returns the second from a timestamp</td><td>&gt; Second(Time(\"2019-04-29T12:51:17Z\")) <br>17</td></tr><tr><td>Month</td><td>Returns the month from a timestamp</td><td>&gt; Month(Time(\"2019-04-29T12:51:17Z\")) <br>4</td></tr><tr><td>Year</td><td>Returns the year from a timestamp</td><td>&gt; Year(Time(\"2019-04-29T12:51:17Z\")) <br>2019</td></tr></tbody></table>\n<p></p>\n<h2>Conclusion</h2>\n<p>With 2.7, we give users far more control in how user groups are able to work with and access data in FaunaDB. We’ve also made these improvements easily accessible in <a href=\"https://dashboard.fauna.com\">our Cloud Console</a>. Furthermore, users can now use their favorite FQL commands and functions directly in the browser with FaunaDB Web Shell.</p>\n<p>Renaming classes to collections and instances to documents adds clarity for users both new and existing. With recursive UDFs and new time to number functions, we’ve improved usability and implemented a few fan favorite requests.</p>\n<p>To learn more please check out our <a href=\"https://docs.fauna.com/fauna/2.7.0/guides/rbac\">documentation</a>, <a href=\"https://docs.fauna.com/fauna/2.7.0/tutorials/rbac\">ABAC tutorial</a>, and <a href=\"https://deploy-preview-160--fauna-docs.netlify.com/fauna/2.7.0/security/#user-defined-roles\">security examples</a>. We also recommend checking out more <a href=\"https://digitalguardian.com/blog/what-role-based-access-control-rbac-examples-benefits-and-more\">examples on the web</a>.<br></p>\n<p>  </p>\n<p>We also welcome your feedback! Please email me at <a href=\"mailto:product@fauna.com\">product@fauna.com</a> and let us know of any other features that would make FaunaDB an obvious choice for your next project.<br></p>\n<style>\n\ntable { background: white; }\ntable p { margin-bottom: 0; }\ntable tbody tr { border-bottom: 1px solid #dee2e6; }\ntable tbody tr:last-child { border-bottom: 0; }\ntable tbody td { border: 0; vertical-align: top; padding: 20px;}\ntable tbody tr:first-child td { vertical-align: bottom; font-weight:bold; }\ntable tbody td p { line-height: 1.3; }\n\n@media screen and (max-width: 768px) {\n.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }\ntable { width:auto;}\ntable tbody td { font-size: 13px; }\n}\n@media screen and (min-width: 992px) and (max-width: 1400px) {\n.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }\ntable { width:auto;}\ntable tbody td { font-size: 13px; }\n}\n</style>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "1850"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2019-06-28T08:06:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1812,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "20e3cbe5-b966-4089-a449-538aa7d7fad4",
        "siteSettingsId": 1812,
        "fieldLayoutId": 4,
        "contentId": 1376,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Demystifying Database Systems, Part 2:  Correctness Anomalies Under Serializable Isolation",
        "slug": "demystifying-database-systems-correctness-anomalies-under-serializable-isolation",
        "uri": "blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation",
        "dateCreated": "2019-06-12T16:28:28-07:00",
        "dateUpdated": "2019-08-22T13:04:49-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation",
        "isCommunityPost": false,
        "blogBodyText": "<p>Most database systems support multiple <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">isolation levels</a> that enable their users to trade off exposure to various types of <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">application anomalies and bugs</a> for (<a href=\"http://www.vldb.org/pvldb/vol10/p613-faleiro.pdf\">potentially small</a>) increases in potential transaction concurrency. For decades, the highest level of “bug-free correctness” offered by commercial database systems was “SERIALIZABLE” isolation in which the database system runs transactions in parallel, but in a way that is equivalent to as if they were running one after the other. This isolation level was considered <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">“perfect”</a> because it enabled users that write code on top of a database system to avoid having to reason about bugs that could arise due to concurrency. As long as particular transaction code is correct in the sense that if nothing else is running at the same time, the transaction will take the current database state from one correct state to another correct state (where “correct” is defined as not violating any semantics of an application), then serializable isolation will guarantee that the presence of concurrently running transactions will not cause any kind of race conditions that could allow the database to get to an incorrect state.&nbsp;</p>\r\n<blockquote>In other words, serializable isolation generally allows an application developer to avoid having to reason about concurrency, and only focus on making single-threaded code correct.</blockquote>\r\n<p>In the good old days of having a “database server” which is running on a single physical machine, serializable isolation was indeed sufficient, and database vendors never attempted to sell database software with stronger correctness guarantees than SERIALIZABLE. However, as distributed and replicated database systems have started to proliferate in the last few decades, anomalies and bugs have started to appear in applications even when running over a database system that guarantees serializable isolation. As a consequence, database system vendors started to release systems with stronger correctness guarantees than serializable isolation, which promise a lack of vulnerability to these newer anomalies. In this post, we will discuss several well known bugs and anomalies in serializable distributed database systems, and modern correctness guarantees that ensure avoidance of these anomalies. </p>\r\n<h2>What Does “Serializable” Mean in a Distributed/Replicated System?</h2>\r\n<p>We defined “serializable isolation” above as a guarantee that even though a database system is allowed to run transactions in parallel, the final result is equivalent to as if they were running one after the other. In a replicated system, this guarantee must be strengthened in order to avoid the anomalies that would only occur at lower levels of isolation in non-replicated systems. For example, let’s say that the balance of Alice’s checking account ($50) is replicated so that the same value is stored in data centers in Europe and the United States. Many systems do not replicate data synchronously over such large distances. Rather, a transaction will complete at one region first, and its update to the database system may be replicated afterwards.  If a withdrawal of $20 is made concurrently in the United States and Europe, the old balance is read ($50) in both places, $20 is removed from it, and the new balance ($30) is written back in both places and replicated to the other data center. This final balance is clearly incorrect ---- it should be $10 --- and was caused by concurrently executing transactions. But the truth is, the same outcome could happen if the transactions were serial (one after the other) as long as the replication is not included as part of the transaction (but rather happens afterwards).&nbsp;</p>\r\n<blockquote>Therefore, a concurrency bug results despite equivalence to a serial order.</blockquote>\r\n<p>Rony Attar, Phil Bernstein, and Nathan Goodman <a href=\"https://ieeexplore.ieee.org/document/5010293\">expanded the concept of serializability</a> in 1984 to define correctness in the context of replicated systems. The basic idea is that all the replicas of a data item behave like a single logical data item. When we say that a concurrent execution of transactions is “equivalent to processing them in a particular serial order”, this implies that whenever a data item is read, the value returned will be the most recent write to that data item by a previous transaction in the (equivalent) serial order --- no matter which copy was written by that write. In this context “most recent write” means the write by the closest (previous) transaction in that serial order. In our example above, either the withdrawal in Europe or the withdrawal in the US will be ordered first in the equivalent serial order. Whichever transaction is second --- when it reads the balance --- it must read the value written by the first transaction. Attar et. al. named this guarantee “<strong>one copy serializability</strong>” or “1SR”, because the isolation guarantee is equivalent to serializability in an unreplicated system with “one copy” of every data item.</p>\r\n<h2>Anomalies are Possible Under Serializability; Anomalies are Possible Under One-copy Serializability</h2>\r\n<p>We just stated that one-copy serializability in replicated systems is the identical isolation guarantee as serializability in unreplicated systems. There are many, many database systems that offer an isolation level called “serializability”, but <a href=\"https://fauna.com/blog/a-comparison-of-scalable-database-isolation-levels\">very few (if any) replicated database systems that offer an isolation level called “one-copy serializability</a>”. To understand why this is the case, we need to explain some challenges in writing bug-free programs on top of systems that “only” guarantee serializability. </p>\r\n<p>A serializable system only guarantees that transactions will be processed in an equivalent way to <strong>some </strong>serial order. The serializability guarantee by itself doesn’t place any constraints on what this serial order is. In theory, one transaction can run and commit. Another transaction can come along --- a long period of time after the first one commits --- and be processed in such a way that the resulting equivalent serial order places the later transaction before the earlier one. In a sense, the later transaction “time travels” back in time, and is processed such that the final state of the database is equivalent to that transaction running prior to transactions that completed prior to when it began. A serializable system doesn’t prevent this. Nor does a one-copy serializable system. Nonetheless, in single-server systems, it is easy and convenient to prevent time-travel. Therefore, the vast majority of single-server systems that guarantee “serializability” also prevent time-travel. Indeed, it was so trivial to prevent time travel that most commercial serializable systems did not consider it notable enough to document their prevention of this behavior. </p>\r\n<blockquote>In contrast, in distributed and replicated systems, it is far less trivial to guarantee a lack of time travel, and many systems allow some forms of time travel in their transaction processing behavior. </blockquote>\r\n<p>The next few sections describe some forms of time-travel anomalies that occur in distributed and/or replicated systems, and the types of application bugs that they may cause. All of these anomalies are possible under a system that only guarantees one-copy serializability. Therefore, vendors typically document which of these anomalies they do and do not allow, thereby potentially guaranteeing a level of correctness higher than one-copy serializability. At the end of this post, we will classify different correctness guarantees and which time-travel anomalies they allow.</p>\r\n<h2>The Immortal Write</h2>\r\n<p>Let’s say the user of an application currently has a display name of “Daniel”, and decides to change it to “Danny”. He goes to the application interface, and changes his display name accordingly. He then reads his profile to ensure the change took effect, and confirms that it has. Two weeks later, he changes his mind again, and decides he wants to change his display name to “Danger”. He goes to the interface and changes his display name accordingly and was told that that change was successful. But when he performs a read on his profile, it still lists his name as “Danny”. He can go back and change his name a million times. Every time, he is told the change was successful, but the value of his display name in the system remains “Danny”. </p>\r\n<figure><img src=\"{asset:1841:url}\" data-image=\"1841\"></figure>\r\n<p>What happened? All of the future writes of his name travelled back in time to a point in the serial order directly before the transaction that changed his name to “Danny”. The “Danny” transaction therefore overwrote the value written by all of these other transactions, even though it happened much earlier than these other transactions in real time. The system decided that the serial order that it was guaranteeing equivalence to has the “Danny” transaction after all of the other name-change transactions --- it has full power to decide this without violating its serializability guarantee. [Side note: when the “Danny” transaction and/or the other name-change transactions also perform a read to the database as part of the same transaction as the write to the name, the ability to time-travel without violating serializability becomes much more difficult. But for “blind write” transactions such as these examples, time-travel is easy to accomplish.]</p>\r\n<p>In multi-master asynchronously replicated database systems, where writes are allowed to occur at either replica, it is possible for conflicting writes to occur across the replicas. In such a scenario, it is tempting to leverage time travel to create an immortal blind write, which enables straightforward conflict resolution without violating the serializability guarantee. </p>\r\n<h2>The Stale Read</h2>\r\n<p>The most common type of anomaly that appears in replicated systems but not in serializable single-server systems is the “stale read anomaly”. For example, Charlie has a bank account with $50 left in the account. He goes to an ATM and withdraws $50. Afterwards, he asks for a receipt with his current bank balance. The receipt (incorrectly) states that he has $50 left in his account (when, in actuality, he now has no money left). As a result, Charlie is left with an incorrect impression of how much money he has, and may make real life behavioral mistakes (for example, splurging on a new gadget) that he wouldn’t have done if he had the correct impression of the balance of his account. This anomaly happened as a result of a stale read: his account certainly used to have $50 in it. But when the ATM did a read request on the bank database to get his current balance, this read request did not reflect the write to his balance that happened a few seconds earlier when he withdrew money from his account. </p>\r\n<figure><img src=\"{asset:1843:url}\" data-image=\"1843\"></figure>\r\n<p>The stale read anomaly is extremely common in asynchronously replicated database systems (such as read replicas in MySQL or Amazon Aurora). The write (the update to Charlie’s balance) gets directed to one copy, which is not immediately replicated to the other copy. If the read gets directed to the other copy before the new write has been replicated to it, it will see a stale value.</p>\r\n<p>Stale reads do not violate serializability. The system is simply time travelling the read transaction to a point in time in the equivalent serial order of transactions before the new writes to this data item occur. Therefore, asynchronously replicated database systems can allow stale reads without giving up its serializability (or even one-copy serializability) guarantee. </p>\r\n<p>In a single-server system, there’s little motivation to read anything aside from the most recent value of a data item. In contrast, in a replicated system, network delays from synchronous replication are time-consuming and expensive. It is therefore tempting to do replication asynchronously, since reads can occur from asynchronous read-only replicas without violating serializability (as long as the replicated data becomes visible in the same order as the original).</p>\r\n<h2>The Causal Reverse</h2>\r\n<p>In contrast to the <em>stale read anomaly,</em> the <em>causal reverse anomaly</em> can happen in any distributed database system and is independent of how replication is performed (synchronous or asynchronous). In the causal reverse anomaly, a later write which was <strong>caused</strong> by an earlier write, time-travels to a point in the serial order prior to the earlier write. In general, these two writes can be to totally different data items. Reads that occur in the serial order between these two writes may observe the “effect” without the “cause”, which can lead to application bugs.</p>\r\n<p>For example, most banks do not exchange money between accounts in a single database transaction. Instead, money is removed from one account into bank-owned account in one transaction. A second transaction moves the money from the bank-owned account to the account intended as the destination for this transfer. The second transaction is <em>caused </em>by the first. If the first transaction didn’t succeed for any reason, the second transaction would never be issued.</p>\r\n<p>Let’s say that $1,000,000 is being transferred from account A (which currently has $1,000,000 and will have $0 left after this transfer) to account B (which currently has $0, and will have $1,000,000 after the transfer). Let’s say that account A and account B are owned by the same entity, and this entity wishes to get a sizable loan that requires $2,000,000 as a down payment. In order to see if this customer is eligible for the loan, the lender issues a read transaction that reads the values of accounts A and B and takes the sum of the balance of those two accounts. If this read transaction occurs in the serial order before the transfer of $1,000,000 from A to B, a total of $1,000,000 will be observed across accounts. If this read transaction occurs after the transfer of $1,000,000 from A to B, a total of $1,000,000 will still be observed across accounts. If this read transaction occurs between the two transactions involved in transfer of $1,000,000 from A to B that we described above, a total of $0 will be observed across accounts. In all three possible cases, the entity will be (correctly) denied the loan due to lack of funds necessary for the down payment. </p>\r\n<p>However, if a second transaction involved in the transfer (the one that adds $1,000,000 to account B) time-travels before the transaction that caused it to exist in the first place (the one that subtracts $1,000,000 from account A), it is possible for a read transaction that occurs between these two writes to (incorrectly) observe a balance across accounts of $2,000,000 and thereby allow the entity to secure the loan. Since the transfer was performed in two separate transactions, this example does not violate serializability. The equivalent serial order is: (1) the transaction that does the second part of the transfer (2) the read transaction and (3) the transaction that does the first part of the transfer. However, this example shows the potential for devastating bugs in application code if causative transactions are allowed to time-travel to a point in time before their cause. <strong></strong><br></p>\r\n<p>Causal reverse anomalies such as this are known to occur in partitioned database systems where transactions run on a particular partition receive a timestamp based on the local time at that partition. If such systems do not wait for the maximum clock skew bound (the maximum possible difference in local times across different partitions in the system) before committing a transaction, it is possible for a transaction to commit, and a later transaction to come along, that was caused by the earlier one (that started after the earlier one finished), and still receive an earlier timestamp than the earlier transaction. This enables a read to potentially see the write of the later transaction, but not the earlier one. </p>\r\n<p>In other words, if the bank example we’ve been discussing was implemented over a system that allows a causal reverse, the entity wishing to secure the loan could simply repeatedly make the loan request and then transfer money between accounts A and B until the causal reverse anomaly shows up, and the loan is approved. Obviously, a well-written application should be able to detect the repeated loan requests and prevent this hack from occurring. But in general, it is hard to predict all possible hacks and write defensive application code to prevent them. Furthermore, banks are not usually able to recruit elite application programmers, which leads to some <a href=\"https://www.theverge.com/2019/2/5/18212902/huaxia-bank-qin-qisheng-atm-loophole-hack-china\">mind-boggling vulnerabilities</a> in real-world applications.</p>\r\n<h3>Avoiding Time Travel Anomalies</h3>\r\n<p>All the anomalies that we’ve discussed so far --- the immortal write, the stale read, and the causal reverse --- all exploit the permissibility of time travel in the serializability guarantee, and thereby introduce bugs in application code. To avoid these bugs, the system needs to guarantee that transactions are not allowed to travel in time, in addition to guaranteeing serializability. As we mentioned above, single-server systems generally make this time-travel guarantee without advertising it, since the implementation of this guarantee is trivial on a single-server. In distributed and replicated database systems, this additional guarantee of “no time travel” on top of the other serializability guarantees is non-trivial, but has nonetheless been accomplished by several systems such as FaunaDB/Calvin, FoundationDB, and Google Spanner.&nbsp;</p>\r\n<blockquote>This highest level of correctness is called <a href=\"http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf\"><strong>strict serializability</strong></a>. </blockquote>\r\n<p>Strict serializability makes all of the guarantees of one-copy serializability that we discussed above. In addition, it guarantees that if a transaction X completed before transaction Y started (in real time) then X will be placed before Y in the serial order that the system guarantees equivalence to. </p>\r\n<h3>Classification of Serializable Systems</h3>\r\n<p>Systems that guarantee strict serializability eliminate all types of time travel anomalies. At the other end of the spectrum, systems that guarantee “only” one-copy serializability are vulnerable to all of the anomalies that we’ve discussed in this post (even though they are immune to the <a href=\"http://dbmsmusings.blogspot.com/2019/05/introduction-to-transaction-isolation.html\" target=\"_blank\" data-saferedirecturl=\"https://www.google.com/url?q=http://dbmsmusings.blogspot.com/2019/05/introduction-to-transaction-isolation.html&source=gmail&ust=1566340874646000&usg=AFQjCNHKUpwU5PmogPUrlfpyCAwor56oWA\">isolation anomalies we discussed in a previous post</a>). There also exist systems that guarantee a version of serializability between these two extremes. One example are <a href=\"https://cs.uwaterloo.ca/~kmsalem/pubs/DaudjeeICDE04.pdf\" target=\"_blank\" data-saferedirecturl=\"https://www.google.com/url?q=https://cs.uwaterloo.ca/~kmsalem/pubs/DaudjeeICDE04.pdf&source=gmail&ust=1566340874646000&usg=AFQjCNF6TqSGgKx-rrVljmDgwplL2fDMUQ\">“strong session serializable” systems</a> that guarantee strict serializability of transactions within the same session, but otherwise only one-copy serializability. Another example are \"strong write serializable\" systems that guarantee strict serializability for all transactions that insert or update data, but only one-copy serializability for read-only transactions. This isolation level is commonly implemented by read-only replica systems where all update transactions go to the master replica which processes them with strict serializability. These updates are asynchronously replicated to read-only replicas in the order they were processed at the master. Reads from the replicas may be stale, but they are still serializable. A third class of systems are \"strong partition serializable\" systems that guarantee strict serializability only on a per-partition basis. Data is divided into a number of disjoint partitions. Within each partition, transactions that access data within that partition are guaranteed to be strictly serializable. But otherwise, the system only guarantees one-copy serializability. This isolation level can be implemented by synchronously replicating writes within a partition, but avoiding coordination across partitions for disjoint writes. Now that we have given names to these different levels of serializability, we can summarize the anomalies to which they are vulnerable with a simple chart:\r\n</p>\r\n<div class=\"table-wrap\">\r\n<br><table><tbody><tr><td>System Guarantee</td><td>Immortal write</td><td>Stale read</td><td>Causal reverse</td></tr><tr><td>ONE COPY SERIALIZABLE</td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>STRONG SESSION SERIALIZABLE</td><td><span class=\"red\">Possible</span> (but not within same session)</td><td><span class=\"red\">Possible</span> (but not within same session)</td><td><span class=\"red\">Possible</span> (but not within same session)</td></tr><tr><td>STRONG WRITE SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"green\">Not Possible</span></td></tr><tr><td>STRONG<br>PARTITIONED SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>STRICT SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td></tr></tbody></table><br>\r\n</div>\r\n<p></p>\r\n<p>For readers who read my <a href=\"https://fauna.com/blog/introduction-to-transaction-isolation-levels\">previous post on isolation levels</a>, we can combine the isolation anomalies from that post with the time travel anomalies from this post to get a single table with all the anomalies we’ve discussed across the two posts:</p>\r\n<div class=\"table-wrap\">\r\n<br><table><tbody><tr><td>System Guarantee</td><td>Dirty read</td><td>Non-repeatable read</td><td>Phantom Read</td><td>Write Skew</td><td>Immortal write</td><td>Stale read</td><td>Causal reverse</td></tr><tr><td>READ UNCOMMITTED</td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>READ COMMITTED</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>REPEATABLE READ</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>SNAPSHOT ISOLATION</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>SERIALIZABLE / ONE COPY SERIALIZABLE / STRONG SESSION SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>STRONG WRITE SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td><td><span class=\"green\">Not Possible</span></td></tr><tr><td>STRONG<br>PARTITIONED SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"red\">Possible</span></td></tr><tr><td>STRICT SERIALIZABLE</td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td><td><span class=\"green\">Not Possible</span></td></tr></tbody></table><br>\r\n</div>\r\n<style>\r\n\r\ntable { background: white; }\r\ntable p { margin-bottom: 0; }\r\ntable tbody tr { border-bottom: 1px solid #dee2e6; }\r\ntable tbody tr:last-child { border-bottom: 0; }\r\ntable tbody td { border: 0; vertical-align: top; padding: 30px 10px; font-size: 13px; line-height: 1.3; }\r\ntable tbody tr:first-child td { vertical-align: bottom; font-weight: bold; }\r\ntable tbody td p { line-height: 1.3; }\r\n.green {color:#087f23;}\r\n.red {color:#ba000d;}\r\n\r\n@media screen and (max-width: 768px) {\r\n.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }\r\ntable { width:auto;}\r\ntable tbody td { font-size: 13px; }\r\n}\r\n@media screen and (min-width: 992px) and (max-width: 1400px) {\r\n.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }\r\ntable { width:auto;}\r\ntable tbody td { font-size: 13px; }\r\n}\r\n</style>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [
            "1816"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-06-18T08:10:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1829,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e0370f9c-a0de-438f-a63a-3009a520e063",
        "siteSettingsId": 1829,
        "fieldLayoutId": 4,
        "contentId": 1393,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Try FaunaDB's GraphQL API",
        "slug": "try-faunadbs-graphql-api",
        "uri": "blog/try-faunadbs-graphql-api",
        "dateCreated": "2019-06-17T11:30:01-07:00",
        "dateUpdated": "2019-08-21T14:41:34-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/try-faunadbs-graphql-api",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/try-faunadbs-graphql-api",
        "isCommunityPost": false,
        "blogBodyText": "<div class=\"admonitionblock tip\">\r\n<div class=\"icon\">Tip</div><br>\r\n<h3>The Fauna Cloud Console now has GraphQL Playground!</h3>\r\n<p>This blog was written before we added GraphQL Playground to our Cloud Console. Although this tutorial will still work, we recommend that you follow this tutorial series instead:</p>\r\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\r\n</div>\r\n<p>GraphQL is quickly becoming the default language for working with APIs and data, making it easy to stitch sources together, and enabling API consumers to move fast without breaking things. Fauna is proud to <a href=\"https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql\">announce our native GraphQL API</a>, available today in FaunaDB Cloud! Instructions to try it are just after the screenshot in this post.</p>\r\n<p>Now you can talk to the database in GraphQL. This means developers who want a backend data API for their app can get started just by importing a basic GraphQL schema, and FaunaDB will take care of creating the underling collections, indexes, and relationships. Advanced applications can use FaunaDB’s transaction-oriented FQL to extend GraphQL schemas. </p>\r\n<p>FaunaDB’s cloud database is fully-managed and free to get started with, so it’s effortless to add a GraphQL backend to your app. FaunaDB’s strongly consistent <a href=\"https://fauna.com/blog/faunadbs-official-jepsen-results\">ACID transactions</a> mean you don’t have to guess what’s in the database, cutting down on edge cases, and making it a great fit for everything from user profiles to game worlds to financial services workloads.</p>\r\n<p><strong>Instantly query FaunaDB from any GraphQL client</strong></p>\r\n<p>The easiest way to get a taste of GraphQL on FaunaDB is to launch <a href=\"https://www.graphqlbin.com/v2/new\">GraphQL Playground</a> and query the sample database we’ve prepared for you.</p>\r\n<figure><img src=\"{asset:1828:url}\" data-image=\"1828\"></figure>\r\n<p>To try it out, just configure GraphQL playground to connect to the FaunaDB Cloud GraphQL API endpoint: <a href=\"https://graphql.fauna.com/graphql\">https://graphql.fauna.com/graphql</a></p>\r\n<p>Authorize your client to the sample database by pasting the following into the <strong>HTTP Headers</strong> in the lower left corner:</p>\r\n<pre>{ \"Authorization\": \"Basic Zm5BRFFVdWNRb0FDQ1VpZDAxeXVIdWt2SnptaVY4STI4a2R6Y0p2UDo=\" }</pre>\r\n<p>Now that you are connected, paste this into the query box and run it:</p>\r\n<pre>query FindAListByID {\r\n  findListByID(id: \"234550211483009539\") {\r\n    title\r\n    todos {\r\n      data { \r\n        title \r\n        completed \r\n        _id\r\n      }\r\n    }\r\n  }\r\n}\r\n</pre>\r\n<p>You should see a handful of todo items pop up in the results area, like in the screenshot above.&nbsp;Under the hood the GraphQL engine is making use of FaunaDB joins to connect todos and lists, and as a user you don’t have to think about it.</p>\r\n<p>You can inspect the schema by clicking the tab on the right hand side. This blog post includes a read-only authorization key, so don’t worry, you can’t break anything. </p>\r\n<p>If you want to get serious, you’ll need your own database. <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">Part 1 in our Getting Started with GraphQL series</a> shows you how to set up a database and import your GraphQL schema. Once you have <a href=\"https://dashboard.fauna.com/accounts/register\">signed up for a FaunaDB Cloud account</a>, you'll be able to access GraphQL Playground directly within FaunaDB's cloud console. Enjoy!</p>\r\n<figure><img src=\"{asset:1830:url}\" data-image=\"1830\"></figure>\r\n<p></p>\r\n<style>\r\n.admonitionblock.tip{padding: 1rem 1.5rem;border-left:3px solid #00bfa5; background: #fff;margin-bottom:2rem;\r\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\r\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\r\nbody .description .admonitionblock h3{margin-top:0;}\r\n.admonitionblock.tip .icon {\r\nbackground: #00bfa5;\r\n    font-size: .73rem;\r\n    padding: .3rem .5rem;\r\n    height: 1.35rem;\r\n    line-height: 1;\r\n    font-weight: 500;\r\n    text-transform: uppercase;\r\nmargin-bottom:1.5rem;\r\ndisplay:inline-block;\r\n    color: #fff;\r\n}\r\n</style>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1831"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-06-18T07:54:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1822,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "79a7b9e5-d9dc-4aa3-9ea9-8349e495b88b",
        "siteSettingsId": 1822,
        "fieldLayoutId": 4,
        "contentId": 1386,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The world’s best serverless database, now with native GraphQL",
        "slug": "the-worlds-best-serverless-database-now-with-native-graphql",
        "uri": "blog/the-worlds-best-serverless-database-now-with-native-graphql",
        "dateCreated": "2019-06-14T14:34:18-07:00",
        "dateUpdated": "2020-05-20T14:06:32-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/the-worlds-best-serverless-database-now-with-native-graphql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/the-worlds-best-serverless-database-now-with-native-graphql",
        "isCommunityPost": false,
        "blogBodyText": "<p>GraphQL has revolutionized the way we think about APIs. Instead of working with extremely rigid REST endpoints, GraphQL allows developers to specify the shape of the data they need, without requiring changes to the backend components that provide that data. This enables teams to collaborate more smoothly -- so backend teams can focus on security and business logic, and front-end teams can focus on presentation and usability. In this way, GraphQL has emerged as a critical layer for universal database access.</p>\r\n<p>FaunaDB’s GraphQL API was first released in preview mode back in April on FaunaDB cloud. The GraphQL API has had a few incremental releases since then, with each one increasing user adoption from both partners and developers.</p>\r\n<h2>Ease of GraphQL, backed by the power of Calvin</h2>\r\n<p>FaunaDB's GraphQL API leverages the Calvin-inspired shared core to offer developers uniform access to transactional consistency, user authorization, data access, quality of service (QoS), and temporal storage. </p>\r\n<p><strong>Temporality:</strong> FaunaDB is the only database that provides built-in temporality support with no limits on data history. With per-query snapshots, any API (GraphQL and FQL) in FaunaDB can return data at any given time.</p>\r\n<p><strong>Consistency:</strong> FaunaDB offers the highest consistency levels for its transactions. These strong consistency guarantees are automatically applied to all APIs.</p>\r\n<p><strong>Authorization: </strong>Unlike most databases that control access at a table level, FaunaDB provides access control at the row (document) level. This fine-grained access control is applicable to all APIs, be it GraphQL or FQL. </p>\r\n<p><strong>Shared Data Access:</strong> In keeping with Fauna's dedication to modern polyglot development, data written by one API (e.g., GraphQL) can be read and modified by another API (e.g., FQL). In contrast, most other databases limit APIs to their specific datasets. </p>\r\n<p><strong>QoS:</strong> FaunaDB’s built-in prioritization policies for concurrent workloads are enforced at the database level or with access keys. All API access automatically adheres to these QoS definitions.</p>\r\n<h2>Highlights of FaunaDB's GraphQL API</h2>\r\n<p>FaunaDB's GraphQL API supports three general functions: Queries, Mutations, and Subscriptions. At this time, FaunaDB natively supports Queries and Mutations. Native support for subscriptions is in the roadmap; meanwhile, you can write a custom function to set up a subscription if necessary. (If you need help with this, reach out to us in <a href=\"https://community-invite.fauna.com/\">Community Slack</a>.)</p>\r\n<p>In the following section, we will cover highlights of this release.&nbsp;</p>\r\n<h3>GraphQL Support in FaunaDB Cloud Console</h3>\r\n<p>The FaunaDB Cloud Console is launching with a set of brand new features to make it easier for developers to interact with their data. </p>\r\n<h4>Importing the GraphQL schema definition file</h4>\r\n<p>To get started, simply import your GraphQL schema from the GraphQL tab in the Fauna web editor. If there’s a formatting issue with the uploaded schema, our import tool will expose the error and inform you of the specific issue:<br></p>\r\n<figure><img src=\"{asset:1825:url}\" data-image=\"1825\"></figure>\r\n<h4>Integrating the GraphQL Playground IDE </h4>\r\n<p>Long-time GraphQL fans will quickly notice that we’ve integrated Prisma’s <a href=\"https://github.com/prisma/graphql-playground\">GraphQL Playground IDE</a> directly into the Fauna Cloud Console. This allows us to provide a lot of out-of-the-box functionality in a GUI that most GraphQL developers are already comfortable using. Among other useful features, GraphQL Playground allows you to navigate your own auto-generated API documentation with ease using the Docs and Schema tabs, as seen below:</p>\r\n<figure><img src=\"{asset:1824:url}\" data-image=\"1824\"></figure>\r\n<p>More features offered by GraphQL Playground can be found on Prisma’s <a href=\"https://www.prisma.io/blog/introducing-graphql-playground-f1e0a018f05d\">blog post</a>.<br></p>\r\n<h3>Automatic Index Creation<br></h3>\r\n<p>When you import a GraphQL schema, FaunaDB uses the type definitions to generate the classes, and the query definitions to automatically generate the indexes.\r\n<br> </p>\r\n<figure><table> <tbody> <tr> <td><p>Schema Definition</p></td> <td><p>Objects generated</p></td> </tr> <tr> <td><pre>type Todo {\r\n   title: String!\r\n   completed: Boolean\r\n}</pre></td> <td><pre>Class Created\r\n{\"name\": \"Todo\",\r\n  \"history_days\": 30,\r\n  \"ttl_days\": null}</pre></td> </tr> <tr> <td><pre>type Query {\r\n   allTodos: [Todo!]\r\n   todosByCompletedFlag(completed: Boolean!): [Todo!]\r\n}</pre></td> <td><pre>Indexes Created:\r\n  1. Name: allTodos. A class index with no \r\nterms or values\r\n  2. Name: todosByCompletedFlag. An index \r\nwith the term completed flag but no values. </pre></td> </tr> </tbody> </table></figure>\r\n<p>FaunaDB is a schemaless database, so where does Fauna store all the metadata associated with the type and query definition? Fauna stores the schema definition as part of the object metadata using a \"gql\" tag. This information is visible in the object definition FQL tab (in the Cloud Console). </p>\r\n<figure><img src=\"{asset:1826:url}\" data-image=\"1826\"></figure>\r\n<h3>The @relation directive</h3>\r\n<p>GraphQL directives help with a variety of implementation related tasks. While the most common use is to create a custom derived field, the @relation directive in FaunaDB is also used to create relationships between two types. For example, in the schema definition below, each todo belongs to a specific list while a list can have many todos. This mimics the traditional primary key:foreign key relationship. FaunaDB automatically creates the required indexes while queries are implicitly executed across classes.</p>\r\n<pre>type Todo {\r\n    title: String!\r\n    completed: Boolean!\r\n    list: List\r\n}\r\n\r\ntype List {\r\n    title: String!\r\n    todos: [Todo] @relation\r\n}\r\n\r\ntype Query {\r\n    allTodos: [Todo!]\r\n    todosByCompletedFlag(completed: Boolean!): [Todo!]\r\n    allLists: [List!]\r\n}</pre>\r\n<h3>The @unique directive</h3>\r\n<p>The @unique directive allows adding constraints to the GraphQL schema. With this directive, FaunaDB automatically creates a unique index on the term on which it is specified.</p>\r\n<pre>type User {\r\n username: String! @unique\r\n}</pre>\r\n<p>Importing this schema will create a class called User and a unique index named unique_User_username. Trying to enter a duplicate username will result in an error. These directives are quite powerful as the developers can create database side objects with single keywords in their GraphL schema file.</p>\r\n<figure><img src=\"{asset:1827:url}\" height=\"400\" width=\"320\" data-image=\"1827\"><span id=\"selection-marker-start\" class=\"redactor-selection-marker\">﻿</span><span id=\"selection-marker-end\" class=\"redactor-selection-marker\">﻿</span></figure>\r\n<h3>Pagination support </h3>\r\n<p>Pagination support was recently added to the GraphQL service. Instead of fetching all records at the same time, pagination allows a developer to get 50 records at a time. High cardinality fields, such as `Query` object fields that return arrays or fields marked with the @relation directive, now return a `Page` object. </p>\r\n<p>As an example, let's take this simple schema definition file: </p>\r\n<pre>type Todo { \r\n  title: String \r\n}\r\n\r\ntype Query {\r\n  allTodos: [Todo!]\r\n}</pre>\r\n<p>The corresponding allTodos query would be:</p>\r\n<pre>{\r\n  allTodos(_size: 10, _cursor: $cursor) {\r\n    data: {\r\n      title\r\n    }\r\n    before    # backwards cursor\r\n    after     # forward cursor\r\n  }\r\n}</pre>\r\n<p>The cursor variable can be used to move across the pages of data.</p>\r\n<h3>User-defined Resolvers</h3>\r\n<p>User-defined resolvers are another feature we've added in response to developer feedback on the April beta release. The @resolver directive allows a developer to associate a field in the query object to a specific user-defined function in the case of complex business logic, as opposed to the 1:1 object mapping that FaunaDB automatically generates with document terms.</p>\r\n<pre>// User-defined function. Create this function using Fauna Shell\r\nCreateFunction({\r\n  name: \"say_hello\",\r\n  body: Query(Lambda([\"name\"],\r\n    Concat([\"Hello \", Var(\"name\")])\r\n  ))\r\n})\r\n\r\n// GraphQL Schema\r\ntype Query {\r\n  sayHello(name: String!): String! @resolver(name: \"say_hello\")\r\n}\r\n\r\n// Calling from the GraphQL API\r\n{\r\n  sayHello(name: \"Kate\")\r\n}\r\n\r\n// Response\r\n{\r\n  \"data\": {\r\n    \"sayHello\": \"Hello Kate\"\r\n  }</pre>\r\n<h2>Getting started with FaunaDB’s GraphQL API</h2>\r\n<p>Our official documentation is the best place to get started with FaunaDB's GraphQL API. We recommend completing the tutorials in the following order:</p>\r\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\r\n<p>We've also added a <a href=\"https://docs.fauna.com/fauna/current/reference/graphql/\">GraphQL Reference</a> section, where you can find additional details on topics like:\r\n</p>\r\n<ul>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/endpoints\" class=\"page\">Endpoints</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/directives/\" class=\"page\">Directives</a></li><li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/relations\" class=\"page\">Relations</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/functions\" class=\"page\">User-defined functions</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/troubleshooting\" class=\"page\">Troubleshooting</a></li></ul>\r\n<p> Over the last few months, we have also published a three-part series&nbsp;[<a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">Part 1</a>|<a href=\"https://fauna.com/blog/getting-started-with-graphql-part-2-relations\">Part 2</a>|<a href=\"https://fauna.com/blog/getting-started-with-graphql-part-3-the-unique-directive\">Part 3</a>]&nbsp;outlining how to get started with GraphQL in FaunaDB. You can start testing your GraphQL queries and mutations right away using the FaunaDB Cloud Console, Fauna Shell, or whatever other GraphQL platform you prefer.<br></p>\r\n<h2>Stay tuned for more...</h2>\r\n<p>Cloud developer happiness is our #1 priority here at Fauna, and GraphQL support is just the beginning. Over the next few months, stay tuned for several more big \"ease of use\" features that will help cement FaunaDB's place as the most developer-friendly database on the market. There has never been a better time to share Fauna with your development team and friends &#x1f604;</p>\r\n<p>We also welcome your feedback! Please email me at <a href=\"mailto:product@fauna.com\">product@fauna.com</a> and&nbsp;let us know of any other features that would make FaunaDB an obvious choice for your next project.</p>",
        "blogCategory": [
            "8",
            "1530",
            "1531",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1821"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "477",
        "postDate": "2019-06-12T15:22:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1803,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "280335b1-0397-40fd-a8ec-483468ffeb46",
        "siteSettingsId": 1803,
        "fieldLayoutId": 4,
        "contentId": 1367,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB Serverless Scheduling: Cooperative Scheduling with QoS",
        "slug": "serverless-scheduling-with-qos-based-multi-tenancy",
        "uri": "blog/serverless-scheduling-with-qos-based-multi-tenancy",
        "dateCreated": "2019-06-11T11:07:55-07:00",
        "dateUpdated": "2019-08-19T20:46:54-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/serverless-scheduling-with-qos-based-multi-tenancy",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/serverless-scheduling-with-qos-based-multi-tenancy",
        "isCommunityPost": false,
        "blogBodyText": "<p>While working on database infrastructure at Twitter some of the core features we developed early on were related to scaling the servers to keep up with the user growth as well as handling unexpected load increase due to runaway queries and malicious actors. We implemented scale-out NoSQL models, server timeouts and rate limits and optimized the system such that each cluster could handle millions of queries of a given type. However, despite being highly efficient in single workload settings, the clusters did not perform well in a shared workload environment as they lacked support for resource pools and co-operative scheduling.</p>\r\n<p>When we set out to build FaunaDB we observed this pattern across industries and use-cases as existing systems were attempting to solve this either through the system administrator manually assigning a number of servers for a specific workload (static isolation boundaries) or the system attempting to elastically scale its resources based on step functions of demand (auto-scaling groups). Both of these approaches have inherent flaws and neither of them generalizes very well across a wide variety of workloads. </p>\r\n<p>We found that the optimal way to achieve multi-tenancy in a shared data infrastructure was not through external container orchestration, but through building cooperative scheduling and QoS into the core itself.&nbsp;This resource management model is a good fit for&nbsp;serverless use cases because runtime requirement spikes can be absorbed by the database provider, while at the same time the provider can benefit from economies of scale. This reduces risk and cost for both the serverless user and the database provider.&nbsp;</p>\r\n<figure><img src=\"{asset:1804:url}\" data-image=\"1804\"></figure>\r\n<p>As a result, we built a database system that supports efficient and fair resource sharing, security through process isolation and allows teams to create suites of products and workloads under one organizational database hierarchy and seamlessly scale workloads without having to implement scaling logic in the orchestration layer or having to estimate resource utilization beforehand.</p>\r\n<h2>Current industry standards for multi-tenant system design</h2>\r\n<p>Achieving multi-tenancy through infrastructure sharing is an important optimization for applications and provides enterprises with several advantages such as reducing costs and more efficient resource utilization as well as long-term benefits such as ease of upgrades, better scalability and service support. As such most enterprise-level offerings have some support for distributed multi-tenancy. Multi-tenancy in most such systems takes one of the following approaches.</p>\r\n<h2>Competing access to shared containers</h2>\r\n<p>Manual provisioning or predictive provisioning does not account for unpredictable spikes which will often cause an outage of one or all the processes if enough buffer resource is not allocated. In cases where the shared system is tenant unaware, the workloads compete for resources with a goal of acquiring the maximum amount of resources in order to stay ahead of the competition and consequently if enough idle capacity is not present, all workloads will suffer including critical ones.</p>\r\n<p>Cloud-native solutions, seemingly solve these problems with a theoretical ability to provision and deallocate resources on the fly, however these solutions cannot scale stateful services fast enough as attempting to provision more resources further strains the existing nodes which are already running at capacity and in case of unpredictable spikes this can lead to cascading effects and subsequent downtime. In short, the worst time to undertake operational movement, such as expanding a cluster, is when you are over capacity.</p>\r\n<h2>Per-tenant dedicated containers</h2>\r\n<p>Some other database systems apply static isolation boundaries to workloads which require the application developer to preemptively predict how many resources each workload will consume and allocate containers for each tenant. In such systems however pools must pre-allocate idle capacity for expansion and containers must have the idle capacity to handle spikes as well. This approach does not work for the shared state as a state must be replicated leading to increase in storage costs while data consistency is not guaranteed. </p>\r\n<p>Furthermore, in such systems, spikes get exaggerated as workloads can only soak up their own idle capacity and not that of others and as such these systems are highly inelastic.</p>\r\n<h2>The FaunaDB approach- Cooperative scheduling with QoS</h2>\r\n<p>FaunaDB uses cooperative scheduling to avoid these pitfalls by allowing low priority workloads to use the available hardware at a steady-state consumption without interfering with higher priority workloads which can burst to higher consumption during periods of heavy usage. This architecture allows for greater efficiency of hardware utilization in the presence of diverse workloads in contrast to single-tenant systems which optimize for performance during single workload operations.</p>\r\n<h2>Operational challenges in providing QoS over shared data-sets</h2>\r\n<p>Ensuring quality of service for shared workloads at a scale comes with operational complexity and tradeoffs relating to resource utilization and security of the workloads. Listed below are the most significant challenges that a multi-tenant service provider must mitigate in order to provide QoS guarantees -</p>\r\n<ul>\r\n<li><strong>Ensuring a fair resource distribution among workloads </strong><br>\r\nWhen multiple workloads share the same physical infrastructure, a strategy of distribution of resources is needed which takes into account priority and fairness of distribution. First-come first-serve scheduling can lead to processes in the back of the queue getting resource-starved and particularly so during periods of high utilization. </li>\r\n<li><strong>Ensuring efficient context switching and predictable outcomes of batch operations.</strong><br>\r\nThe frequent context switching which comes from evicting workload processes and saving state at each such interval introduces several bottlenecks and can cause significant overhead and leave one or all of the workload in an inconsistent state.</li>\r\n<li><strong>Ensuring security and isolation of the workloads sharing the infrastructure</strong><br>\r\nMulti-tenant application security remains a primary concern amongst <a href=\"https://www.researchgate.net/publication/261342160_Multitenancy_-_Security_Risks_and_Countermeasures\">policy-makers</a>, as the past years have seen several breaches and data leakage in production and research settings causing data-loss to the enterprise and exposure of sensitive customer data.</li>\r\n</ul>\r\n<h2>Providing resource guarantees and prioritization</h2>\r\n<p>Ensuring fairness in resource sharing is one of the most important aspects of designing data infrastructure for resource-sharing patterns. For instance, let us consider a case where a production database, an analytics database, and a development database are running on the same cluster and the system needs to ensure that higher priority operations get more resources while not starving low priority resources during periods of high usage. FaunaDB provides simple configuration level constructs for setting priorities on the client or at the<a href=\"https://fauna.com/blog/secure-hierarchical-multi-tenancy-patterns\"> data layer</a> while also allowing for databases to be arranged in a hierarchical order and as a result,<a href=\"https://fauna.com/blog/secure-hierarchical-multi-tenancy-patterns\">&nbsp;a database may contain other databases</a> and QoS and security rules are inherited down the tree. This allows for easy to implement, efficient resource sharing down the database hierarchy. </p>\r\n<p>Resource scheduling in FaunaDB is based on<a href=\"https://www2.cs.duke.edu/courses/cps214/spring09/papers/drr.pdf\"> deficit round-robin</a> algorithm which provides a means of fair resource scheduling for weighted queues while requiring only a constant unit of work. Unlike most multi-tenant databases which have only a skin-deep implementation of query prioritization and drop priority context once a query is chosen for execution, query priority is a first-class citizen in FaunaDB and is propagated through the system throughout its query life-cycle.</p>\r\n<p>Each query propagates its priority to the node in which it executes and the node applies QoS limits based entirely on its local state i.e. if a query hits a node and the node has resources available for servicing that operation it is serviced immediately else prioritization applies based on local node QoS state. This approach prevents unnecessary round trips and the need for a global QoS policy and works really well in practice as long as the work units are small enough and there is eventual preemption for unresponsive queries.  </p>\r\n<p>Systems often focus on absolute benchmarks while ignoring overheads caused due to cumbersome multithreading models and context switch overheads. Through optimized eviction points such as I/O, compute and termination of loops, FaunaDB query-executor evaluates transactions from independent workloads as a series of parallelizable and interleaved subqueries. This also ensures that process eviction happens at granular and predictable barriers and that the overhead during context switching can be minimized.</p>\r\n<p>FaunaDB query processor can simultaneously run thousands of concurrent queries in a mixed workload setting and concurrency is implemented through cooperative threading with async coroutine execution which provides easy support for high-level concurrency without the need for frequent context-switching.</p>\r\n<h2>Security through isolation and access control</h2>\r\n<p>FaunaDB allows for fine-grained identity management and access control to be applied to workloads that share the same cluster. Through its high-level constructs of instance-object, class, and database, it allows for customizable role/access combinations to be created and provides different levels of granularity and visibility to any single tenant in a multi-tenant application group.</p>\r\n<p>Attribute-based security in FaunaDB is achieved through secrets that correspond to access keys which authenticate the user as having particular permissions. Upon creation, only the hash of the key is stored in the disk and the responsibility of storing the key is delegated to the user.</p>\r\n<figure><img src=\"{asset:1805:url}\" data-image=\"1805\"></figure>\r\n<p>FaunaDB’s identity management and row-level access control prevent data leakage and unauthorized access while the hierarchical database grouping structure also allows for access policy and QoS schemes to be easily passed down the databases tree allowing for authorized access for co-operative use-case patterns in a manner that is easy to understand and implement.</p>\r\n<figure><img src=\"{asset:1806:url}\" data-image=\"1806\"></figure>\r\n<h2>Conclusion</h2>\r\n<p>In this article, we saw some of the challenges faced by teams implementing multi-tenant data architectures and how FaunaDB solves those challenges through its QoS based infrastructure abstraction. To learn more about how resource-sharing works in FaunaDB as well as view shared-resource QoS benchmarks you can read more<a href=\"https://fauna.com/blog/prioritize-workloads-with-quality-of-service-api\"> here</a>.</p><p>Most virtualized data infrastructure exposed through IaaS and PaaS abstractions rely heavily on sharing underlying resources. Even in applications running on dedicated node abstraction of the public cloud, the infrastructure behind the abstraction is usually still sharing physical resources with other colocated applications. Teams often want to take advantage of multi-tenancy on shared infrastructure without having to deal with the complexity that comes with managing workloads on shared infrastructure. </p>\r\n<p>FaunaDB empowers its partners to create their own private serverless database clouds where they can enforce custom quality of service rules through the policy-based resource manager and run metered workloads with fair access to contentious resources. By making it easy for cooperative applications and databases to share infrastructure, FaunaDB&nbsp;allows many applications to coexist and share hardware through asynchronous coroutine execution and granular access control while also allowing enforcement of strict isolation rules to non-cooperative processes.</p>\r\n<p><br></p>",
        "blogCategory": [
            "1462",
            "1530"
        ],
        "mainBlogImage": [
            "1807"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-05-14T16:56:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1797,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "86d66fa2-f59b-4afd-9a7e-92c512d77717",
        "siteSettingsId": 1797,
        "fieldLayoutId": 4,
        "contentId": 1361,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introducing Cloud Console 1.3.0",
        "slug": "introducing-cloud-console-1-3-0",
        "uri": "blog/introducing-cloud-console-1-3-0",
        "dateCreated": "2019-05-14T15:28:18-07:00",
        "dateUpdated": "2020-08-21T05:56:20-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introducing-cloud-console-1-3-0",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introducing-cloud-console-1-3-0",
        "isCommunityPost": false,
        "blogBodyText": "<p>The FaunaDB team is very pleased to announce the availability of the latest version of the <a href=\"https://docs.fauna.com/fauna/current/release_notes/console\">Cloud Console</a>. This release brings multiple enhancements in usability, performance, and usage reporting. The section below summarizes some of the highlights of this release.</p>\r\n<h2>Usability Improvements</h2>\r\n<p>The main focus of this release is to improve the Cloud Console user flow to make it more intuitive and consistent across views. We hope this improves the learning curve in performing basic CRUD operations.</p>\r\n<h3>Refactored CRUD Workflows</h3>\r\n<p>This is the biggest user-facing change from the previous release. The resource creation and edit workflows are now simplified with dedicated screens. Be it a database, a class, or index, the most commonly used options for any resources are exposed to the user in the UI. So getting started with FaunaDB becomes very simple and straight-forward. </p>\r\n<figure><img src=\"{asset:1794:url}\" data-image=\"1794\"></figure>\r\n<h3>Improved Index Creation and Querying UI</h3>\r\n<p>When observing interaction with our previous index screens, we noticed some users were having difficulty with how to create and execute indexes. &nbsp;We implemented a two-fold improvement on how to define terms and values, coupled with a query interface enhanced to simplify and maximize UI layout and vertical space.</p>\r\n<figure><img src=\"{asset:1793:url}\" data-image=\"1793\"></figure>\r\n<h3>FQL Tab</h3>\r\n<p>We have also added an FQL tab that provides the Data portion of an equivalent FQL query for any object creation statements. Someone new to FaunaDB will find this feature very useful in learning FQL. As you go through the different CRUD workflows, you can toggle between the simple and FQL tabs to learn more about the equivalent FQL statements.</p>\r\n<figure><img src=\"{asset:1795:url}\" data-image=\"1795\"></figure>\r\n<h3>Improved Mobile Browsing</h3>\r\n<p>In refactoring the basic CRUD operation workflows, we've also improved mobile viewing so that you can perform all the same operations on smaller devices with greater ease. Overall navigation across the Console has been improved and made consistent across the various resource pages.</p>\r\n<h2>Performance Improvements </h2>\r\n<h3>Improved Class Index Creation Times </h3>\r\n<p>When you create a new class, you can now choose to create an empty class index at the same time. Just keep the class index checkbox selected and one will be ready right away! Normally, index creation in FaunaDB is an asynchronous operation competing with other tasks, so it can take a few minutes to get active. With this change, class indexes become active right away and can be used in queries from the get-go. Soon, we will improve the creation time for all other indexes as well. </p>\r\n<h3>Reduced Footprint Improves App Load Times</h3>\r\n<p>We've also streamlined dependencies to reduce the Console application footprint. This reduces browser load times so that your experience doesn't suffer due to high page load times--even at slow network speeds. &nbsp;</p>\r\n<h2>Improved Usage Reporting</h2>\r\n<p>A few months ago, we moved our cloud usage reporting from a point-based system to usage-based pricing. Users have since requested usage reporting to be made part of the console landing page so that they can have an accurate picture of their activity at any given time. This release adds granular usage reporting to the dashboard with options for you to view your total $USD usage over the last 7 days, the current billing period, and the previous billing period. &nbsp;After you surpass your free usage tier, you will also now  receive monthly invoices based on the new usage-based pricing. </p>\r\n<figure><img src=\"{asset:1796:url}\" data-image=\"1796\"></figure>\r\n<p></p>\r\n<p>We plan to introduce new features to the Cloud Console at a regular cadence. If there is something you’d like to see, please reach me at <a href=\"mailto:product@fauna.com\">product@fauna.com</a>. I’d love to talk to you.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "1800"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-05-03T20:55:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1792,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "4fc85dfc-38e2-4c5e-a7c2-63b059b2d727",
        "siteSettingsId": 1792,
        "fieldLayoutId": 4,
        "contentId": 1356,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Performance Preview of FaunaDB 2.7 with YCSB",
        "slug": "performance-preview-of-faunadb-2-7",
        "uri": "blog/performance-preview-of-faunadb-2-7",
        "dateCreated": "2019-05-03T18:48:48-07:00",
        "dateUpdated": "2019-05-29T20:27:13-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/performance-preview-of-faunadb-2-7",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/performance-preview-of-faunadb-2-7",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is a distributed operational database that guarantees data correctness, without operational complexity, at a global scale.&nbsp;At Fauna, horizontal and vertical scalability are both very important to us,&nbsp;since they improve not only&nbsp;performance, but also operational flexibility.</p>\r\n<p>One area we have been spending time on is multi-core scalability. Historically, many database systems could scale to faster cores, but not to more cores, due to contention on shared local resources like locks. But CPU&nbsp;parallelism, whether discrete cores or hyperthreads, is increasingly common in the cloud era and we need to make sure FaunaDB can take full advantage of it.&nbsp;</p>\r\n<p>This post compares FaunaDB 2.7 to 2.6. We are happy to report an approximately 25%&nbsp;across-the-board improvement in both read and write throughput&nbsp;regardless of core count, but in particular, up to 200% or more improvement in read throughput on higher core count machines.&nbsp;And of course, because of its <a href=\"{entry:1399:url}\">Calvin transaction protocol</a>, FaunaDB maintains these performance improvements even at global replication latencies.</p>\r\n<p>The below benchmarks capture a single dimension of FaunaDB’s overall performance profile; we look forward to exploring many other dimensions in&nbsp;subsequent blog posts.</p>\r\n<h2>Setup</h2>\r\n<p>The purpose of this test was to estimate how FaunaDB’s throughput responds to CPU parallelism. Three separate <a href=\"https://en.wikipedia.org/wiki/YCSB\">YCSB</a> workloads were executed on 3 different vCPU configurations on a cluster running on the Google Cloud Platform (GCP). The cluster configuration, a single replica with 3 nodes, remained constant throughout the tests. Only the number of vCPU cores on each node was changed before each run.<br></p>\r\n<table><tbody><tr><td><p><strong>Configuration</strong></p></td><td><p><strong>Description</strong></p></td></tr><tr><td><p>Cluster </p></td><td><p>1 Replica with 3 nodes</p></td></tr><tr><td><p>vCPUs</p></td><td><p>4, 8, 16 virtual cores @2.30 GHz</p></td></tr><tr><td><p>Memory</p></td><td><p>26 GB</p></td></tr><tr><td><p>Storage</p></td><td><p>2 TB of locally attached SSD</p></td></tr><tr><td><p>YCSB Workload</p></td><td><p>Workload A: 50% Read 50% Write</p><p>Workload B: 95% Read 5% Write</p><p>Workload C: 100% Read </p></td></tr><tr><td><p>Java</p></td><td><p>JDK 11</p></td></tr><tr><td><p>Hugepages</p></td><td><p>Enabled</p></td></tr><tr><td>OS Version</td><td>CentOS 7</td></tr></tbody></table>\r\n<p><br>For each test run, the following procedure was followed:</p>\r\n<ol><li>Setup the FaunaDB cluster with the specific node configuration</li><li>Initialize the FaunaDB cluster</li><li>Create a database and schema for the test</li><li>Populate the database with test data </li><li>Run the workload</li></ol>\r\n<p>Each scenario was run repeatedly, increasing the number of concurrent clients each time. At lower core counts, more client threads&nbsp;would eventually lead to errors when the servers became saturated. Even though additional&nbsp;throughput was likely available, only client concurrency levels that achieved 100% success rates have been included in the results.<br></p>\r\n<p>In each of the workload scenarios, the client threads either read or updated records each of size 1 KB in a single transaction. No other userspace processes were running on the servers.&nbsp;Each test lasted anywhere between 3 to 8 minutes. In the 4 and 8 core setups, the number of client threads ranged from 300 to 2,000. In the 16 core setup with more headroom available, the number of threads ranged from 300 to 4,000. </p>\r\n<h2>Observations</h2>\r\n<p>The following table enumerates the operations per second, the number of client threads,&nbsp;and the vCPU parallelism&nbsp;in the three workload scenarios tested.</p>\r\n<h3>Workload A (50% Read, 50% Update)</h3>\r\n<p>This workload simulates equal proportions of reads and writes. The throughput ranged from 3,000 to 15,500 operations per second&nbsp;at 4,000 client threads. At 2,000 client threads, throughput scaled effectively with cores, although not quite linearly. Writes by their nature are more contended than reads, so this is expected, although we will continue to improve it over time.<br></p>\r\n<table><tbody><tr><td><strong>Workload A&nbsp;&nbsp;<br>2.7</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>3,069</p></td><td><p>5 m 26 s</p></td><td><p>4,249</p></td><td><p>7 m 51 s</p></td><td><p>5,200</p></td><td><p>6 m 25 s</p></td></tr><tr><td><p>500</p></td><td><p>3,594</p></td><td><p>4 m 38 s</p></td><td><p>5,508</p></td><td><p>6 m 3 s</p></td><td><p>7,557</p></td><td><p>4 m 25 s</p></td></tr><tr><td><p>1,000</p></td><td><p>3,974</p></td><td><p>4 m 12 s</p></td><td><p>7,148</p></td><td><p>4 m 40 s</p></td><td><p>9,927</p></td><td><p>3 m 21 s</p></td></tr><tr><td><p>1,500</p></td><td><p>4,256</p></td><td><p>3 m 55 s</p></td><td><p>8,009</p></td><td><p>4 m 10 s</p></td><td><p>11,257</p></td><td><p>2 m 58 s</p></td></tr><tr><td><p>2,000</p></td><td><p>4,435</p></td><td><p>3 m 45 s</p></td><td><p>8,638</p></td><td><p>3 m 52 s</p></td><td><p>12,757</p></td><td><p>2 m 37 s</p></td></tr><tr><td><p>3,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>14,136</p></td><td><p>2 m 21 s</p></td></tr><tr><td><p>4,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>15,578</p></td><td><p>2 m 8 s<br></p></td></tr></tbody></table>\r\n<p><br>Charting the above throughput shows semi-linear scalability with increasing threads and CPU cores.</p>\r\n<figure><img src=\"{asset:1791:url}\" data-image=\"1791\"></figure>\r\n<p>Throughput improvement in 2.7 &nbsp;compared to 2.6.3 for Workload A ranges from 16 to 28% depending on the configuration, i.e. the number of the CPU cores in the server. The following table shows the throughput operations from the same workload running on release 2.6.3.</p>\r\n<table><tbody><tr><td><strong>Workload A&nbsp;<br>2.6.3</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>2,748</p></td><td><p>6 m 4 s</p></td><td><p>3,490</p></td><td><p>5m 17s</p></td><td><p>6,293</p></td><td><p>4m 49s</p></td></tr><tr><td><p>500</p></td><td><p>3,157</p></td><td><p>5 m 17 s</p></td><td><p>4,041</p></td><td><p>5m 3s</p></td><td><p>7,092</p></td><td><p>4m 32s</p></td></tr><tr><td><p>1,000</p></td><td><p>3,602</p></td><td><p>4 m 38 s</p></td><td><p>4,901</p></td><td><p>4m 58s</p></td><td><p>8,293</p></td><td><p>4m 03s</p></td></tr><tr><td><p>1,500</p></td><td><p>3,774</p></td><td><p>4 m 25 s</p></td><td><p>5,755</p></td><td><p>4m 49s</p></td><td><p>8,896</p></td><td><p>3m 59s</p></td></tr><tr><td><p>2,000</p></td><td><p>3,798</p></td><td><p>4m 18 s</p></td><td><p>5,777</p></td><td><p>4m 37s</p></td><td><p>9,923</p></td><td><p>3m 48s</p></td></tr></tbody></table>\r\n<h3>Workload B (95% Read, 5% Update)</h3>\r\n<p>This is a read-heavy workload with 5% writes. The maximum throughput was observed under 4,000 client threads was 31,000 operations per second. Here, due to the emphasis on reads, we start to see very close to linear scalability across cores.<br></p>\r\n<table><tbody><tr><td><strong>Workload B<br>2.7</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>7,120</p></td><td><p>7 m 1 s</p></td><td><p>13,209</p></td><td><p>5 m 3 s</p></td><td><p>17,492</p></td><td><p>3 m 49 s</p></td></tr><tr><td><p>500</p></td><td><p>7,547</p></td><td><p>6 m 38 s</p></td><td><p>15,100</p></td><td><p>4 m 25 s</p></td><td><p>21,320</p></td><td><p>3 m 8 s</p></td></tr><tr><td><p>1,000</p></td><td><p>8,210</p></td><td><p>6 m 5 s</p></td><td><p>16,391</p></td><td><p>4 m 4 s</p></td><td><p>26,556</p></td><td><p>2 m 31 s</p></td></tr><tr><td><p>1,500</p></td><td><p>8,637</p></td><td><p>5 m 47 s</p></td><td><p>16,997</p></td><td><p>3 m 55 s</p></td><td><p>28,632</p></td><td><p>2 m 20 s</p></td></tr><tr><td><p>2,000</p></td><td><p>8,675</p></td><td><p>5 m 46 s</p></td><td><p>17,332</p></td><td><p>3 m 51 s</p></td><td><p>29,241</p></td><td><p>2 m 17 s</p></td></tr><tr><td><p>3,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>30,288</p></td><td><p>2 m 12 s</p></td></tr><tr><td><p>4,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>30,830</p></td><td><p>2 m 10 s</p></td></tr></tbody></table>\r\n<p></p>\r\n<figure><img src=\"{asset:1790:url}\" data-image=\"1790\"></figure>\r\n<p></p>\r\n<p>Just as we have seen in Workload A, a quick glance into the 2.6.3 Workload B numbers, below, reveal a similar 25% improvement across the various tested configurations, but even more dramatic improvements on the 16&nbsp;core machines.<br></p>\r\n<table><tbody><tr><td><strong>Workload B<br>2.6.3</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>5,892</p></td><td><p>8 m 29 s</p></td><td><p>8,249</p></td><td><p>7 m 37 s</p></td><td><p>10,476</p></td><td><p>5 m 53 s</p></td></tr><tr><td><p>500</p></td><td><p>6,219</p></td><td><p>8 m 2 s</p></td><td><p>9,329</p></td><td><p>6 m 51 s</p></td><td><p>12,874</p></td><td><p>5 m 25 s</p></td></tr><tr><td><p>1,000</p></td><td><p>6,639</p></td><td><p>7 m 32 s</p></td><td><p>10,091</p></td><td><p>6 m 5 s</p></td><td><p>13,623</p></td><td><p>4 m 49 s</p></td></tr><tr><td><p>1,500</p></td><td><p>6,406</p></td><td><p>7 m 48 s</p></td><td><p>9,993</p></td><td><p>5 m 49 s</p></td><td><p>12,991</p></td><td><p>3 m 59 s</p></td></tr><tr><td><p>2,000</p></td><td><p>7,078</p></td><td><p>7 m 4 s</p></td><td><p>10,617</p></td><td><p>5 m 43 s</p></td><td><p>13,908</p></td><td><p>3 m 51 s</p></td></tr></tbody></table>\r\n<h3>Workload C (100% Read)</h3>\r\n<p>This is a read-only workload. The maximum throughput was 39,500 operations per second&nbsp;with 4,000 client threads. We see similar results compared to Workload B, with strong performance as cores increase.<br></p>\r\n<table><tbody><tr><td><strong>Workload C<br>2.7</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>9,166</p></td><td><p>18 m 11 s</p></td><td><p>20,944</p></td><td><p>7 m 51 s</p></td><td><p>27,205</p></td><td><p>6 m 8 s</p></td></tr><tr><td><p>500</p></td><td><p>9,829</p></td><td><p>16 m 57 s</p></td><td><p>22,831</p></td><td><p>6 m 3 s</p></td><td><p>33,550</p></td><td><p>4 m 58 s</p></td></tr><tr><td><p>1,000</p></td><td><p>10,484</p></td><td><p>15 m 54 s</p></td><td><p>24,236</p></td><td><p>4 m 40 s</p></td><td><p>37,077</p></td><td><p>4 m 30 s</p></td></tr><tr><td><p>1,500</p></td><td><p>10,555</p></td><td><p>15 m 47 s</p></td><td><p>24,022</p></td><td><p>4 m 10 s</p></td><td><p>39,054</p></td><td><p>4 m 16 s</p></td></tr><tr><td><p>2,000</p></td><td><p>10,814</p></td><td><p>15 m 25 s</p></td><td><p>25,473</p></td><td><p>3 m 52 s</p></td><td><p>39,255</p></td><td><p>4 m 15 s</p></td></tr><tr><td><p>3,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>39,515</p></td><td><p>4 m 13 s</p></td></tr><tr><td><p>4,000</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>39,532</p></td><td><p>4 m 13 s</p></td></tr></tbody></table>\r\n<p></p>\r\n<figure><img src=\"{asset:1789:url}\" data-image=\"1789\"></figure>\r\n<p>For the read-only Workload C, the improvement compared to 2.6.3 is been similar at lower core parallelism, but at 16 cores we see very substantial gains, where&nbsp;FaunaDB 2.7 is almost 250% faster.<br></p>\r\n<table><tbody><tr><td><strong>Workload C<br>2.6.3&nbsp;</strong></td><td colspan=\"2\"><p><strong>4 Cores</strong></p></td><td colspan=\"2\"><p><strong>8 Cores</strong></p></td><td colspan=\"2\"><p><strong>16 Cores</strong></p></td></tr><tr><td><p><strong>Threads</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td><td><p><strong>Ops/Sec</strong></p></td><td><p><strong>Avg RunTime</strong></p></td></tr><tr><td><p>300</p></td><td><p>5,863</p></td><td><p>28 m 26 s</p></td><td><p>8,795</p></td><td><p>16m 37s</p></td><td><p>11,961</p></td><td><p>13m 53s</p></td></tr><tr><td><p>500</p></td><td><p>6,786</p></td><td><p>24 m 34 s</p></td><td><p>10,111</p></td><td><p>15m 1s</p></td><td><p>14,155</p></td><td><p>12m 20s</p></td></tr><tr><td><p>1,000</p></td><td><p>6,979</p></td><td><p>23 m 53 s</p></td><td><p>10,399</p></td><td><p>14m 43s</p></td><td><p>13,519</p></td><td><p>12m 01s</p></td></tr><tr><td><p>1,500</p></td><td><p>7,380</p></td><td><p>22 m 35 s</p></td><td><p>10,996</p></td><td><p>14m 03s</p></td><td><p>15,614</p></td><td><p>11m 32s</p></td></tr><tr><td><p>2,000</p></td><td><p>7,982</p></td><td><p>20 m 53 s</p></td><td><p>12,053</p></td><td><p>13m 47s</p></td><td><p>16,392</p></td><td><p>11m 01s</p></td></tr></tbody></table>\r\n<p></p>\r\n<h2>Conclusions</h2>\r\n<p>From the above test results, we can conclude that:</p>\r\n<ul><li>A very modestly sized cluster of 3 commodity cloud&nbsp;compute instances is capable of meeting the throughput demands of most transactional workloads, while ensuring predictable performance.<br><br></li><li>FaunaDB distributes workloads evenly among all nodes within the replica and can meet increased throughput needs, especially for reads, simply by provisioning more cores.&nbsp;</li></ul>\r\n<p>FaunaDB operators can start small, and as applications grow, they can scale their cluster&nbsp;by adding new nodes to existing replicas or&nbsp;adding new replicas in neighboring zones or regions. But if&nbsp;availability goals are already met, with FaunaDB 2.7,&nbsp;operators can also&nbsp;simply increase available cores, which can be done without any service interruption due to FaunaDB's distributed nature.&nbsp;</p>\r\n<p>Of course, for geographically distributed workloads, we recommend distributing multiple replicas across regions. We look forward to benchmarking this aggressively in upcoming posts.<br></p>",
        "blogCategory": [],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2019-05-03T05:58:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1771,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b3f21339-4fc6-4f4e-b887-87e208b9bb58",
        "siteSettingsId": 1771,
        "fieldLayoutId": 4,
        "contentId": 1335,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Demystifying Database Systems, Part 1: An Introduction to Transaction Isolation Levels",
        "slug": "introduction-to-transaction-isolation-levels",
        "uri": "blog/introduction-to-transaction-isolation-levels",
        "dateCreated": "2019-04-24T14:06:40-07:00",
        "dateUpdated": "2019-08-19T21:11:45-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introduction-to-transaction-isolation-levels",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introduction-to-transaction-isolation-levels",
        "isCommunityPost": false,
        "blogBodyText": "<p>For decades, database systems have given their users multiple isolation levels to choose from, ranging from some flavor of “serializability” at the high end down to “read committed” or “read uncommitted” at the low end. These different isolation levels expose an application to markedly different types of concurrency bugs. Nonetheless, many database users stick with the default isolation level of whatever database system they are using, and do not bother to consider which isolation level is optimal for their application. This practice is dangerous—the vast majority of widely-used database systems—including Oracle, IBM DB2, Microsoft SQL Server, SAP HANA, MySQL, and PostgreSQL—do not guarantee any flavor of serializability by default. As we will detail below, isolation levels weaker than serializability can lead to concurrency bugs in an application and negative user experiences. It is very important that a database user is aware of the isolation level guaranteed by the database system, and what concurrency bugs may emerge as a result.</p>\r\n<blockquote>Many database users stick with the default isolation level...and do not bother to consider which isolation level is optimal for their application</blockquote>\r\n<p>In this post we give a tutorial on database isolation levels, the advantages that come with lower isolation levels, and the types of concurrency bugs that these lower levels may allow. Our main focus in this post is the difference between serializable isolation levels and lower levels that expose an application to specific types of concurrency anomalies. There are also important differences within the category for serializable isolation (e.g., “strict serializability” makes a different set of guarantees than “one-copy serializability”). </p>\r\n<p>However, in keeping with the title of this post as an “introduction” to database isolation levels, we will ignore the subtle differences within the class of serializable isolation, and focus on the commonality of all elements within this class—referring to this commonality as “serializable”. In a future, less introductory, post, we will investigate the category of serializable isolation in more detail.</p>\r\n<h3>What is an “Isolation Level”?</h3>\r\n<p><strong>Database isolation</strong> refers to the ability of a database to allow a transaction to execute as if there are no other concurrently running transactions (even though in reality there can be a large number of concurrently running transactions). The overarching goal is to prevent reads and writes of temporary, aborted, or otherwise incorrect data written by concurrent transactions.<br></p>\r\n<p>There is such a thing as perfect isolation (we will define this below). Unfortunately, perfection usually comes at a performance cost—in terms of transaction latency (how long before a transaction completes) or throughput (how many transactions per second can the system complete). Depending on how a particular system is architected, perfect isolation becomes easier or harder to achieve. In poorly designed systems, achieving perfection comes with a prohibitive performance cost, and users of such systems will be pushed to accept guarantees significantly short of perfection. However, even in well-designed systems, there is often a non-trivial performance benefit achieved by accepting guarantees short of perfection. Therefore, <strong>isolation levels</strong> came into existence: they provide the user of a system the ability to trade off isolation guarantees for improved performance.</p>\r\n<blockquote>The overarching goal is to prevent reads and writes of temporary, aborted, or otherwise incorrect data written by concurrent transactions</blockquote>\r\n<p>As we will see as we proceed in this discussion, there are many subtleties that lead to confusion when discussing isolation levels. This confusion is exacerbated by the existence of a SQL standard that <strong>fails to accurately define database isolation levels</strong> and database vendors that attach liberal and non-standard semantics to particular named isolation levels. Nonetheless, a database user is obligated to do due diligence on the different options provided by a particular system, and choose the right level for their application. </p>\r\n<h3>Perfect Isolation</h3>\r\n<p>Let’s begin our discussion of database isolation levels by providing a notion of what “perfect” isolation is. We defined <strong>isolation</strong> above as the ability of a database system to allow a transaction to execute as if there are no other concurrently running transactions (even though in reality that can be a large number of concurrently running transactions). What does it mean to be perfect in this regard? At first blush, it may appear that perfection is impossible. If two transactions both read and write the same data item, it is critical that they impact each other. If they ignore each other, then whichever transaction completes the write last could clobber the first transaction, resulting in the same final state as if it never ran. </p>\r\n<p>The database system was one of the first scalable concurrent systems, and has served as an archetype for many other concurrent systems developed subsequently. The database system community—many decades ago—developed an incredibly powerful (and yet perhaps underappreciated) mechanism for dealing with the complexity of implementing concurrent programs. </p>\r\n<p>The idea is as follows: human beings are fundamentally bad at reasoning about concurrency. It’s hard enough to write a bug-free non-concurrent program. But once you add concurrency, there are a near-infinite array of race conditions that could occur—if one thread reaches line 17 of a program before another thread reaches line 5, but after it reaches line 3, a problem could occur that would not exist under any other concurrent execution of the program. It is nearly impossible to consider all the different ways that program execution in different threads could overlap with each other, and how different types of overlap can lead to different final states.</p>\r\n<p>Instead, database systems provide a beautiful abstraction to the application developer—a “transaction”. A transaction may contain arbitrary code, but it is fundamentally single-threaded. </p>\r\n<p>An application developer only needs to focus on the code within a transaction—to ensure it is correct when there are no other concurrent processes running in the system. Given any starting state of the database, the code must not violate the semantics of the application. Ensuring correctness of code is non-trivial, but it is much easier to ensure the correctness of code when it is running by itself, than ensuring the correctness of code when it is running alongside other code that may attempt to read or write shared data. <br></p>\r\n<blockquote>A transaction may contain arbitrary code, but it is fundamentally single-threaded</blockquote>\r\n<p>If the application developer is able to ensure the correctness of their code when no other concurrent processes are running, a system that guarantees <strong>perfect</strong> isolation will ensure that the code remains correct even when there is other code running concurrently in the system that may read or write the same data. In other words, a database system enables a user to write code without concern for the complexity of potential concurrency, and yet still process that code concurrently without introducing new bugs or violations of application semantics. </p>\r\n<p>Implementing this level of perfection sounds difficult, but it’s actually fairly straightforward to achieve. We have already assumed that the code is correct when run without concurrency over <strong>any starting state</strong>. Therefore, if transaction code is run serially —&nbsp;one after the other—then the final state will also be correct. Thus, in order to achieve perfect isolation, all the system has to do is to ensure that when transactions are running concurrently, the final state is equivalent to a state of the system that would exist if they were run serially. There are several ways to achieve this—such as via locking, validation, or multi-versioning—that are out of scope for this article. The key point for our purposes is that we are defining “perfect isolation” as the ability of a system to run transactions in parallel, but in a way that is equivalent to as if they were running one after the other. In the <a href=\"http://jtc1sc32.org/doc/N2301-2350/32N2311T-text_for_ballot-CD_9075-2.pdf\">SQL standard</a>, this perfect isolation level is called <strong>serializability</strong>. </p>\r\n<p>Isolation levels in distributed systems get more complicated. Many distributed systems implement variations of the serializable isolation level, such as <strong>one copy-serializability (1SR),</strong> <strong>strict serializability (strict 1SR) </strong>or <strong>update serializability (US)</strong>. Of those,&nbsp;<a href=\"https://fauna.com/blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels\">“strict serializability” is the most perfect</a> of those serializable options. However, as we mentioned above, in order to focus the discussion in this post on core concepts behind database isolation, we will defer discussion of these more advanced concepts to a future post.<br></p>\r\n<h3>Anomalies in Concurrent Systems</h3>\r\n<p>The SQL standard defines several isolation levels below serializability. Furthermore, there are other isolation levels commonly found in commercial database systems—most notably snapshot isolation—which are not included in the SQL standard. Before we discuss these different levels of isolation, let’s discuss some well-known application bugs/anomalies that can occur at isolation levels below serializability. We will describe these bugs using a retail example.</p>\r\n<p>Let’s assume that whenever a customer purchases a widget, the following “Purchase” transaction is run:</p>\r\n<ol><li>Read old inventory</li><li>Write new inventory which is one less than what was read in step 1</li><li>Insert new order corresponding to this purchase into the orders table</li></ol>\r\n<p>If such Purchase transactions run serially, then all initial inventory will always be accounted for. If we started with 42 widgets, then at all times, the sum of all inventory remaining plus orders in the order table will be 42.</p>\r\n<p>But what if such transactions run concurrently at isolation levels below serializability?</p>\r\n<p>For example, let’s say that two transactions running concurrently read the same initial inventory (42), and then both attempt to write out the new inventory of one less than the value that they read (41) in addition to the new order. In such a case, the final state is an inventory of 41, yet there are two new orders in the orders table (for a total of 43 widgets accounted for). We created a widget out of nothing! Clearly, this is a bug. It is known as the <strong>lost-update anomaly</strong>. <br></p>\r\n<p>As another example, let’s say these same two transactions are running concurrently but this time the second transaction starts in between steps (2) and (3) of the first one. In this case, the second transaction reads the value of inventory after it has been decremented - i.e. it reads the value of 41 and decrements it to 40, and writes out the order. In the meantime, the first transaction aborted when writing out the order (e.g. because of a credit card decline). In such a case, during the abort process, the first transaction reverts back to the state of the database before it began (when inventory was 42). Therefore the final state is an inventory of 42, and one order written out (from the second transaction). Again, we created a widget out of nothing! This is known as the <strong>dirty-write anomaly</strong> (because the second transaction overwrote&nbsp;the value of the first transaction’s write before it decided whether it would commit or abort). </p>\r\n<p>As a third example, let’s say a separate transaction performs a read of both the inventory and the orders table, in order to make an accounting of all widgets that ever existed. If it runs between steps (2) and (3) of a Purchase transaction, it will see a temporary state of the database in which the widget has disappeared from the inventory, but has not yet appeared as an order. It will appear that a widget has been lost—another bug in our application. This is known as the <strong>dirty-read anomaly</strong>, since the accounting transaction was allowed to read the temporary (incomplete) state of the purchase transaction. </p>\r\n<p>As a fourth example, let’s say that a separate transaction checks the inventory and acquires some more widgets if there are fewer than 10 widgets left:</p>\r\n<ol><li><code>IF (READ(Inventory) = (10 OR 11 OR 12))</code><br>Ship some new widgets to restock inventory via standard shipping<br><br></li><li><code>IF (READ(Inventory) &lt; 10)</code><br>Ship some new widgets to restock inventory via express shipping</li></ol>\r\n<p>Note that this transaction reads the inventory twice. If the Purchase transaction runs in between step (1) and (3) of this transaction, then a different value of inventory will be read each time. If the initial inventory before the Purchase transaction ran was 10, this would lead to the same restock request to be made twice—once with standard shipping and once with express shipping. This is called the <strong>non-repeatable read anomaly.</strong></p>\r\n<p>As a fifth example, imagine a transaction that scans the orders table in order to calculate the maximum price of an order and then scans it again to find the average order price. In between these two scans, an extremely expensive order gets inserted that skews the average so much that it becomes higher than the maximum price found in the previous scan. This transaction returns an average price that is larger than the maximum price—a clear impossibility and a bug that would never happen in a serializable system. This bug is slightly different than the non-repeatable read anomaly since every value that the transaction read stayed the same between the two scans—the source of the bug is that <strong>additional records were inserted</strong> in between these two scans. This is called the <strong>phantom read anomaly</strong>.</p>\r\n<p>As a final example, assume that the application allows the price of the widget to change based on inventory. For example, many airlines increase ticket price as the inventory for a flight decreases. Assume that the application uses a formula to place a constraint on how these two variables interrelate—e.g. 10I + P &gt;= $500 (where I is inventory and P is price). Before allowing a purchase to succeed, the purchase transaction has to check both the inventory and price to make sure the above constraint is not violated. If the constraint will not be violated, the update of inventory by that Purchase transaction may proceed.</p>\r\n<p> Similarly, a separate transaction that implements special promotional discounts may check both the inventory and price to make sure that the constraint is not violated when updating the price as part of a promotion. If it will not be violated, the price can be updated.</p>\r\n<p>Now, imagine these two transactions are running at the same time—they both read the old value of I and P and independently decide that their updates of inventory and price respectively will not violate the constraint. They therefore proceed with their updates. Unfortunately, this may result in a new value of I and P that violates the constraint! If one had run before the other, the first one would have succeeded and the other would would have read the value of I and P after the first one finished and detected that their update would violate the constraint and therefore not proceed. But since they were running concurrently, they both see the old value and incorrectly decide that they can proceed with the update. This bug is called the <strong>write skew anomaly</strong> because it happens when two transactions read the same data but update disjoint subsets of the data that was read. </p>\r\n<h3>Definitions in the ISO SQL Standard</h3>\r\n<p>The SQL standard defines reduced isolation levels in terms of which of these anomalies are possible. In particular, it contains the following table:<br><br></p>\r\n<table><tbody><tr><td><p>Isolation Level</p></td><td><p>Dirty read</p></td><td><p>Non-repeatable read</p></td><td><p>Phantom Read</p></td></tr><tr><td><p>READ UNCOMMITTED</p></td><td><p>Possible</p></td><td><p>Possible</p></td><td><p>Possible</p></td></tr><tr><td><p>READ COMMITTED</p></td><td><p>Not Possible</p></td><td><p>Possible</p></td><td><p>Possible</p></td></tr><tr><td><p>REPEATABLE READ</p></td><td><p>Not Possible</p></td><td><p>Not Possible</p></td><td><p>Possible</p></td></tr><tr><td><p>SERIALIZABLE</p></td><td><p>Not Possible</p></td><td><p>Not Possible</p></td><td><p>Not Possible</p></td></tr></tbody></table>\r\n<p><br>There are many, many problems which how the SQL standard defines these isolation levels. Most of these problems were <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf\">already pointed out in 1995</a>, but inexplicably, revision after revision of the SQL standard have been released since that point without fixing these problems.</p>\r\n<p><strong>The first problem </strong>is that the standard only uses three types of anomalies to define each isolation level—dirty read, non-repeatable read, and phantom read. However, there are many types of concurrency bugs that can appear in practice—many more than just these three anomalies. In this post alone we have already described six unique types of anomalies. The SQL standard makes no mention about whether the READ UNCOMMITTED, READ COMMITTED, and REPEATABLE READ isolation levels are susceptible to the lost update anomaly, the dirty-write anomaly, or the write-skew anomaly. As a result, each commercial system is free to decide which of these other anomalies these reduced isolation levels are susceptible to—and in many cases these vulnerabilities are poorly documented, leading to confusion and unpredictable bugs for the application developer.</p>\r\n<blockquote>The SQL standard only uses three types of anomalies to define each isolation...however, there are many types of concurrency bugs that can appear in practice</blockquote>\r\n<p><strong>A second, related, problem</strong> is that using anomalies to define isolation levels only gives the end user a guarantee of what specific types of concurrency bugs are impossible. It does not give a precise definition of the potential database states that are viewable by any particular transaction. There are several improved and more precise definitions of isolation levels given in the academic literature. <a href=\"http://pmg.csail.mit.edu/papers/adya-phd.pdf\">Atul Adya’s PhD thesis</a> gives a precise definition of the SQL standard isolation levels based on how reads and writes from different transactions may be interleaved. However these definitions are given from the point of view of the system. The <a href=\"http://www.cs.cornell.edu/lorenzo/papers/Crooks17Seeing.pdf\">recent work by Natacha Crooks et. al</a> gives elegant and precise definitions from the point of view of the user. </p>\r\n<p><strong>A third problem</strong> is that the standard does not define, nor provide correctness constraints on one of the most popular reduced isolation levels used in practice: snapshot isolation (nor any of its many variants—<a href=\"http://www.news.cs.nyu.edu/~jinyang/pub/walter-sosp11.pdf\">PSI</a>, <a href=\"https://pages.lip6.fr/Marc.Shapiro/papers/NMSI-SRDS-2013.pdf\">NMSI</a>, <a href=\"http://www.bailis.org/papers/ramp-sigmod2014.pdf\">Read Atomic</a>, etc). By failing to provide a definition of snapshot isolation, differences in concurrency vulnerabilities allowed by snapshot isolation have emerged across systems. In general, snapshot isolation performs all reads of data as of a particular snapshot of the database state which contains only committed data. This snapshot remains constant throughout the lifetime of the transaction, so all reads are guaranteed to be repeatable (in addition to being only of committed data). Furthermore, concurrent transactions that write the same data detect conflicts with each other and typically resolve this conflict via aborting one of the conflicting transactions. This prevents the lost-update anomaly. However, conflicts are only detected if conflicting transactions write an overlapping set of data. If the write sets are disjoint, these conflicts will not be detected. Therefore snapshot isolation is vulnerable to the write skew anomaly. Some implementations are also vulnerable to the phantom read anomaly. </p>\r\n<p><strong>The fourth problem</strong> is that the SQL standard seemingly gives two different definitions of the SERIALIZABLE isolation level. First, it defines SERIALIZABLE correctly: that the final result must be equivalent to a result that could have occurred if there were no concurrency. But then, it presents the above table, which seems to imply that as long as an isolation level does not allow dirty reads, non-repeatable reads, or phantom reads, it may be called SERIALIZABLE. Oracle, has historically leveraged this ambiguity to justify calling its implementation of snapshot isolation “SERIALIZABLE”. To be honest, I think most people who read the ISO&nbsp;<a href=\"http://jtc1sc32.org/doc/N2301-2350/32N2311T-text_for_ballot-CD_9075-2.pdf\">SQL standard</a> would come away believing the more precise definition of SERIALIZABLE given earlier in the document (which is the correct one) is the intention of the authors of the document.</p>\r\n<p>Nonetheless, I guess Oracle’s lawyers have looked at it and determined that there is enough ambiguity in the document to legally justify their reliance on the other definition. (If any of my readers are aware of any real lawsuits that came from application developers who believed they were getting a SERIALIZABLE isolation level, but experienced write skew anomalies in practice, I would be curious to hear about them. Or if you are an application developer and this happened to you, I would also be curious to hear about it.)</p>\r\n<p>The bottom line is this: it is nearly impossible to give clear definitions of the actual isolation levels available to application developers, because vagueness and ambiguities in the SQL standard has led to semantic differences across implementations/systems. <br></p>\r\n<h3>What Isolation Level Should You Choose?</h3>\r\n<p>My advice to application programmers is the following: reduced isolation levels are dangerous. It is very hard to figure out which concurrency bugs may present themselves. If every system defined their isolation levels using the methodology of Crooks et. al. that I cited above, at least you would have a precise and formal definition of their associated guarantees. Unfortunately, the formalism of the Crooks paper is too advanced for most database users, so it is unlikely that database vendors will adopt these formalisms in their documentation any time soon. In the meantime, the definition of reduced isolation levels remain vague in practice and risky to use. </p>\r\n<blockquote>Reduced isolation levels are dangerous...the definition of reduced isolation levels remain vague in practice and risky to use</blockquote>\r\n<p>Furthermore, even if you could know exactly which concurrency bugs are possible for a particular isolation level, writing an application in a way that these bugs will not happen in practice (or if they do, that they will not cause negative experiences for the users of the application) is also very challenging. If your database system gives you a choice, the right choice is usually to avoid lower isolation levels than serializable isolation. For the vast majority of database systems, you actually have to go and change the defaults to accomplish this!.</p>\r\n<p>However, there are three caveats:</p>\r\n<ol><li> As I mentioned above, some systems use the word “SERIALIZABLE” to mean something weaker than true serializable isolation. &nbsp;Unfortunately, this means that simply choosing the SERIALIZABLE isolation level in your database system may not be sufficient to actually ensure serializability. You need to check the documentation to ensure that it defines SERIALIZABLE in the following way: that the visible state of the database is always equivalent to a state that could have occurred if there was no concurrency. Otherwise, your application will likely be vulnerable to the write-skew anomaly.<br><br></li><li>As mentioned above, serializable isolation level comes with a performance cost. Depending on the quality of the system architecture, the performance cost of serializability may be large or small. In a recent research paper that I wrote with Jose Faleiro and Joe Hellerstein, we <a href=\"http://www.vldb.org/pvldb/vol10/p613-faleiro.pdf\">showed</a> that in a well-designed system, the performance difference between SERIALIZABLE &nbsp;and READ COMMITTED can be negligible…and in some cases it is possible for the SERIALIZABLE isolation level to (surprisingly) outperform the READ COMMITTED isolation level. If you find that the cost of serializable isolation in your system is prohibitive, you should probably consider using a different database system earlier than you consider settling for a reduced isolation level.<br><br></li><li>In distributed systems, there are important anomalies that can (and do) emerge even within the class of serializable isolation levels. For such systems, it is important to understand the subtle differences between the elements of the class of serializable isolation (strict serializability is known to be the most safe). We will shed more light into this matter in a future post.</li></ol>\r\n<p></p>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [
            "1786"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-04-30T10:06:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1781,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "5ebc1b75-1fed-4a8a-9f7b-71b4a2254877",
        "siteSettingsId": 1781,
        "fieldLayoutId": 4,
        "contentId": 1345,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with GraphQL, Part 3: The Unique Directive",
        "slug": "getting-started-with-graphql-part-3-the-unique-directive",
        "uri": "blog/getting-started-with-graphql-part-3-the-unique-directive",
        "dateCreated": "2019-04-30T09:22:59-07:00",
        "dateUpdated": "2019-08-21T14:43:08-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-graphql-part-3-the-unique-directive",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-graphql-part-3-the-unique-directive",
        "isCommunityPost": false,
        "blogBodyText": "<div class=\"admonitionblock tip\">\r\n<div class=\"icon\">Tip</div><br>\r\n<h3>The Fauna Cloud Console now has GraphQL Playground!</h3>\r\n<p>This blog was written before we added GraphQL Playground to our Cloud Console. Although this tutorial will still work, we recommend that you follow this tutorial series instead:</p>\r\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\r\n</div>\r\n<p>In the previous two articles we explored how to <a href=\"{entry:1724:url}\">set up your development environment</a> and query your GraphQL schema, then <a href=\"{entry:1739:url}\">added relationships to your GraphQL queries</a>. In this article we’ll look at the <tt>@unique</tt> directive, which allows you to add constraints to your GraphQL schema.</p>\r\n<p>To define a simple user type with a uniqueness constraint on the username, you simply add the <tt>@unique</tt> directive to the <tt>username</tt> field, and FaunaDB handles creating the index to enforce this constraint.</p>\r\n<pre>type User {\r\n username: String! @unique\r\n}</pre>\r\n<p>Import this schema into FaunaDB by creating a database, provisioning a key secret, and uploading it via curl (for detailed instructions see <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">the first article in this series</a>). Alternatively, you can just reuse the secret from the previous article, and your schema will be safely extended with a User type:</p>\r\n<pre>curl -u &lt;key-secret&gt;: https://graphql.fauna.com/import --data-binary \"@&lt;graphql-schema-filename&gt;\"</pre>\r\n<p>Now you can inspect the schema via Fauna Shell or GraphQL Playground. Remember to configure the Authorization header in your GraphQL Playground for the current database.</p>\r\n<p>You can list the classes and indexes in your database by launching the shell and issuing this query. To launch the shell directly into your current database, use the <tt>--secret</tt> command line flag:<br></p>\r\n<pre>fauna shell --secret=&lt;key-secret&gt;</pre>\r\n<p>If you started with an empty database, you’ll only have one class and one index:</p>\r\n<pre>&gt; Paginate(Union(Classes(), Indexes()))</pre>\r\n<pre>{ data: [ Class(\"User\"), Index(\"unique_User_username\") ] }</pre>\r\n<p>Now let’s inspect the index with the following query in the Fauna Shell:</p>\r\n<pre>&gt; Get(Index(\"unique_User_username\"))</pre>\r\n<pre>{ ref: Index(\"unique_User_username\"),\r\n  ts: 1556578547300000,\r\n active: true,\r\n partitions: 1,\r\n name: 'unique_User_username',\r\n source: Class(\"User\"),\r\n data: { gql: {} },\r\n values: [],\r\n terms: [ { field: [ 'data', 'username' ] } ],\r\n unique: true }</pre>\r\n<p>We can see in this response that the FaunaDB index is maintaining a uniqueness constraint on the username field. Trying to create users with duplicate usernames will result in an error. We'll try doing that in GraphQL Playground in a moment.</p>\r\n<p>Now let’s switch to the GraphQL Playground (make sure you configure it with the correct Authorization header), and inspect the generated schema. Here are the relevant domain objects (inspect the schema yourself to see some additional boilerplate):<br></p>\r\n<pre>type Mutation {\r\n createUser(data: UserInput!): User!\r\n updateUser(\r\n   id: ID!\r\n   data: UserInput!\r\n ): User\r\n deleteUser(id: ID!): User\r\n}\r\n\r\ntype Query {\r\n findUserByID(id: ID!): User\r\n}\r\n\r\ntype User {\r\n _id: ID!\r\n _ts: Long!\r\n username: String!\r\n}\r\n\r\ninput UserInput {\r\n username: String!\r\n}</pre>\r\n<figure><img src=\"{asset:1782:url}\" data-image=\"1782\"></figure>\r\n<p><br>Create&nbsp;a new user by pasting the following query and pressing the \"Play\"&nbsp;button:</p>\r\n<pre>mutation CreateAUser {\r\n   createUser(data: {\r\n     username: \"Alice\"\r\n   }) {\r\n     username\r\n   }\r\n}</pre>\r\n<p>You should receive the following response:</p>\r\n<pre>{\r\n  \"data\": {\r\n    \"createUser\": {\r\n      \"username\": \"Alice\"\r\n    }\r\n  }\r\n}</pre>\r\n<p>Now, let's&nbsp;try adding a duplicate user. Simply&nbsp;click the \"Play\" button to run the same query again. This time, you should get an error:</p>\r\n<pre>{\r\n  \"errors\": {\r\n    \"message\": \"Instance is not unique.\",\r\n    \"extensions\": {\r\n      \"code\": \"instance not unique\"\r\n    }\r\n  }\r\n}</pre>\r\n<p>This tutorial has demonstrated how to apply uniqueness constraints to your data with the @unique directive.&nbsp;Stay tuned for more about GraphQL and FaunaDB, as we explore schema updates, access control, and custom schema elements.&nbsp;<br></p>\r\n<style>\r\n.admonitionblock.tip{padding: 1rem 1.5rem;border-left:3px solid #00bfa5; background: #fff;margin-bottom:2rem;\r\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\r\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\r\nbody .description .admonitionblock h3{margin-top:0;}\r\n.admonitionblock.tip .icon {\r\nbackground: #00bfa5;\r\n    font-size: .73rem;\r\n    padding: .3rem .5rem;\r\n    height: 1.35rem;\r\n    line-height: 1;\r\n    font-weight: 500;\r\n    text-transform: uppercase;\r\nmargin-bottom:1.5rem;\r\ndisplay:inline-block;\r\n    color: #fff;\r\n}\r\n</style>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1783"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-04-25T08:51:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1777,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "16c2e8f9-1d3c-48fe-8e99-7307a8c24407",
        "siteSettingsId": 1777,
        "fieldLayoutId": 4,
        "contentId": 1341,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Database Authority Andy Pavlo Joins Fauna as Technical Advisor",
        "slug": "database-authority-andy-pavlo-joins-fauna-as-technical-advisor",
        "uri": "blog/database-authority-andy-pavlo-joins-fauna-as-technical-advisor",
        "dateCreated": "2019-04-25T08:51:21-07:00",
        "dateUpdated": "2020-05-20T14:04:44-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/database-authority-andy-pavlo-joins-fauna-as-technical-advisor",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/database-authority-andy-pavlo-joins-fauna-as-technical-advisor",
        "isCommunityPost": false,
        "blogBodyText": "<p>Fauna is proud to announce that Dr. Andy Pavlo, Assistant Professor in the Computer Science Department at Carnegie Mellon University, has joined as a technical advisor. Andy’s deep expertise in database query processing systems will play an important role in the development of FaunaDB’s core capabilities. At Fauna, he joins notable database expert Daniel Abadi, Darnell-Kanal Professor of Computer Science at the University of Maryland, in helping build a globally distributed database optimized for cloud environments.</p>\r\n<p>FaunaDB is the first enterprise distributed database to leverage Professor Abadi’s work on the Calvin protocol. This protocol provides the mathematical basis for FaunaDB’s globally replicated ACID transactions, allowing FaunaDB clusters to operate correctly regardless of wall clock time. This means that FaunaDB is robust and correct, even in chaotic cloud deployments.</p>\r\n<p>According to a recent report by <a href=\"https://fauna.com/blog/faunadbs-official-jepsen-results\">Jepsen</a>, FaunaDB offers strict-serializability, the gold standard for transactional consistency. This isolation level makes FaunaDB suitable for mission critical workloads like financial ledgers, booking and hospitality markets, and customer identity management, where data integrity is paramount.</p>\r\n<p>Building on this foundation, FaunaDB is now adding a&nbsp;GraphQL API&nbsp;to interact with a shared set of data, while preserving all the core correctness, quality of service, and security guarantees that FaunaDB offers. We look forward to collaborating with Dr. Pavlo and leveraging his expertise in query processing to innovate at the database interface layer. Our goal is to deliver a database that serves different developer needs without sacrificing the foundational correctness, scale and reliability that are table stakes for an operational database. </p>\r\n<p>“I am impressed with the work Fauna has done in developing and delivering a distributed database based on Dr. Abadi’s groundbreaking research on Calvin,” said Pavlo. “FaunaDB combines the flexibility and agility of first-generation NoSQL systems with the strong transactional reliability guarantees of enterprise relational DBMSs. To be sure, this is not an easy engineering feat. Its use of Calvin’s deterministic concurrency control protocol to maintain consistency is unique to the industry and solves the notoriously difficult challenges of operating in multi-cloud environments.”</p>\r\n<p>Read the full press release <a href=\"https://www.prnewswire.com/news-releases/distinguished-database-authority-andy-pavlo-joins-fauna-as-technical-advisor-300838076.html\">here.</a></p>\r\n<p></p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [
            "1778"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-04-11T09:58:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1739,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "49307d46-ab2d-409d-bdfe-b4605477b953",
        "siteSettingsId": 1739,
        "fieldLayoutId": 4,
        "contentId": 1316,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with GraphQL, Part 2: Relations",
        "slug": "getting-started-with-graphql-part-2-relations",
        "uri": "blog/getting-started-with-graphql-part-2-relations",
        "dateCreated": "2019-04-10T11:41:27-07:00",
        "dateUpdated": "2020-02-24T09:32:03-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-graphql-part-2-relations",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-graphql-part-2-relations",
        "isCommunityPost": false,
        "blogBodyText": "<div class=\"admonitionblock tip\">\n<div class=\"icon\">Tip</div><br>\n<h3>The Fauna Cloud Console now has GraphQL Playground!</h3>\n<p>This blog was written before we added GraphQL Playground to our Cloud Console. Although this tutorial will still work, we recommend that you follow this tutorial series instead:</p>\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\n</div>\n<p>In the previous post, we explored <a href=\"{entry:1724:url}\">how to set up your development environment and query your GraphQL schema</a>. Assuming that you are already connected to your database via Fauna Shell and GraphQL Playground, this article focuses on adding relationships to your GraphQL queries.<br></p>\n<blockquote>FaunaDB’s GraphQL API automatically turns your schema definition into FaunaDB relations so that relational queries are&nbsp;easy.</blockquote>\n<p>Follow along on your workstation in&nbsp;GraphQL&nbsp;Playground to explore FaunaDB's native GraphQL API.</p>\n<h3>The @relation directive</h3>\n<p>The <tt>@relation</tt> directive connects an attribute to another GraphQL type, so that you can model your domain. By default, document references are embedded in the source object, but when the <tt>@relation</tt> directive is used, indexes are created and queried automatically. In this app, each todo belongs to a particular list. Also, we add a query to find all of the lists including their related todos. Note that FaunaDB’s GraphQL API takes care of connecting the List and Todo types, so the GraphQL you expect to write just works.</p>\n<pre>type Todo {\n    title: String!\n    completed: Boolean!\n    list: List\n}\n\ntype List {\n    title: String!\n    todos: [Todo] @relation\n}\n\ntype Query {\n    allTodos: [Todo!]\n    todosByCompletedFlag(completed: Boolean!): [Todo!]\n    allLists: [List!]\n}</pre>\n<p>Import this schema into FaunaDB by creating a database, provisioning a key secret, and uploading it via <tt>curl</tt> (for detailed instructions see the first article in this series). Alternatively, you can just reuse the secret from the previous article, and your schema will be safely upgraded:<br></p>\n<pre>curl -u &lt;key-secret&gt;: https://graphql.fauna.com/import --data-binary \"@&lt;graphql-schema-filename&gt;\"</pre>\n<p>Now you can inspect the schema via Fauna Shell or GraphQL Playground. Remember to configure the Authorization header in your GraphQL Playground for the current database.</p><figure><img src=\"{asset:6942:url}\" data-image=\"6942\"></figure><p><br></p>\n\n<p>The generated schema adds some new types for the <tt>@relation</tt> directive: <tt>ListTodosRelation</tt> and <tt>TodoListRelation</tt>. These types correspond to indexes or document references in FaunaDB and are transparently managed by the FaunaDB GraphQL API.</p>\n<pre>input ListTodosRelation {\n create: [TodoInput]\n connect: [ID]\n disconnect: [ID]\n}\n\ninput TodoListRelation {\n create: ListInput\n connect: ID\n disconnect: Boolean\n}</pre>\n<p>When you create an object, you can connect it to existing objects, or create new related&nbsp;objects. Additionally, when modifying an object you can disconnect related objects by id. Continue reading for examples.</p>\n<h3>Create a todo list</h3>\n<p>The first thing we need to do is create a list and add all of our existing todos to it. Let’s create the list via GraphQL:<br></p>\n<pre>mutation CreateAList {\n   createList(data: {\n     title: \"Build an awesome app!\"\n   }) {\n     title\n     _id\n   }\n}</pre>\n<p>Note the id from this result, as you'll need it in the Fauna Shell.</p>\n<h3>Migrate existing todos to the list</h3>\n<p>To migrate the existing todos, it’s easiest to use Fauna Shell and issue a bulk update using the id returned from the above <tt>CreateAList</tt> query:</p>\n<pre>$ fauna shell graphql # or whatever your database is named</pre>\n<p>In Fauna Shell issue this query (using your id):</p>\n<pre>&gt; Map(\n  Paginate(Match(Index(\"allTodos\"))), \n  todoID =&gt;\n    Update(todoID, { \n      data: { \n        list: Ref(Class(\"List\"), \"228661410636235268\") \n      }\n    }))\n</pre>\n<p>And the results should look something like:</p>\n<pre>{ data:\n   [ { ref: Ref(Class(\"Todo\"), \"228474048686850572\"),\n       ts: 1554334241180000,\n       data:\n        { title: 'Build an awesome app!',\n          completed: true,\n          list: Ref(Class(\"List\"), \"228661410636235268\") } },\n     { ref: Ref(Class(\"Todo\"), \"228474165045232141\"),\n       ts: 1554334241180000,\n       data:\n        { title: 'Connect the front end.',\n          completed: false,\n          list: Ref(Class(\"List\"), \"228661410636235268\") } },\n     { ref: Ref(Class(\"Todo\"), \"228474207247270411\"),\n       ts: 1554334241180000,\n       data:\n        { title: 'Implement RBAC for Access control.',\n          completed: false,\n          list: Ref(Class(\"List\"), \"228661410636235268\") } } ] }</pre>\n<h3>Query todos in a list</h3>\n<p>This query lists all of the todos in a particular list, as well as the list’s title:</p>\n<pre>query FindAListByID {\n    findListByID(id: \"228661410636235268\") {\n        title\n        todos {\n          data { title completed }\n        }\n    }\n}</pre>\n\n<figure><img src=\"{asset:6943:url}\" data-image=\"6943\"></figure><p><br></p><p>In the screenshot, we can see that all of the todo titles and their completed status have been returned as part of the result set. FaunaDB’s GraphQL API automatically turns your schema definition into FaunaDB relations so that relational queries are easy.</p>\n<h3>Move a todo to a new list</h3>\n<p>We can move one of our todo items from one list to another list. First create a new list using the <tt>CreateAList</tt> query above, and copy its id. Then choose a todo to update, and use its id together with the list id, in a query to set the <tt>connect</tt> field on the list attribute:</p>\n<pre>mutation UpdateATodo {\n    updateTodo(id: \"228474048686850572\", data: {\n        title: \"Build an awesome app!\"\n        completed: true,\n        list: { connect: \"228668866541126147\" }\n    }) {\n        title\n        completed\n    }\n}</pre>\n<p>Note the <tt>connect</tt> field which is supplied as part of the generated FaunaDB schema. This is how the FaunaDB GraphQL API manages references between documents.</p>\n<h3>Query all lists with todos</h3>\n<p>The last query is similar to the <tt>FindAListByID</tt> query above, except instead of finding one list, it finds them all. I like this query because it shows the power of relations in FaunaDB’s GraphQL API. Each list includes the todos, without the developer having to concern themselves with any underlying database design issues like joins or indexes. Here’s the query:<br></p>\n<pre>query FindAllLists {\n    allLists {\n      data { \n        _id\n        title\n        todos { \n          data {\n            title \n            completed \n          }\n        }\n      }\n    }\n}\n</pre>\n<p>You can see in the screenshot that our new list has some todo items assigned to it, and their title and completed state are inlined into the response. Thanks GraphQL, for making app development super simple!</p><figure><img src=\"{asset:6944:url}\" data-image=\"6944\"></figure>\n\n<h3>Create a list with some todos</h3>\n<p>Similarly to the <tt>connect</tt> field, the <tt>create</tt> field gives access to the underlying collection. So you can add a list together with a handful of todos in a single GraphQL mutation. Here is a query which does that:</p>\n<pre>mutation CreateListWithTodos {\n    createList(data: {\n        title: \"The Basics\",\n          todos: { create: [ \n            {completed: false, title: \"Water\"}, \n            {completed: false, title: \"Food\"},\n            {completed: false, title: \"Shelter\"}]},\n    }) {\n        _id\n        title\n        todos {\n          data {\n            title\n          }\n        }\n    }\n}</pre>\n<p>If you rerun your&nbsp;<tt>FindAllLists</tt> query,&nbsp;above, you’ll see the new list with its todos.&nbsp;</p>\n<h3>Add todos to an existing list using the create API</h3>\n<p>The create capability can also be used in the context of a mutation. One way to add new todos to a list (while changing its other fields if you like) looks like this:</p>\n<pre> mutation UpdateAList {\n    updateList(id: \"228833728394166795\", data: {\n        title: \"The Very Basics\"\n        todos: {\n          create : [{title : \"Fire\", completed: true}]\n        }\n    }) {\n        title\n        todos { \n            data {\n               title\n            }\n        }\n    }\n}</pre>\n<p>In <a href=\"{entry:1781:url}\">Part 3</a> of this series, we'll look at how to add constraints to our schema using the <tt>@unique</tt> directive.<br></p>\n<style>\n.admonitionblock.tip{padding: 1rem 1.5rem;border-left:3px solid #00bfa5; background: #fff;margin-bottom:2rem;\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\nbody .description .admonitionblock h3{margin-top:0;}\n.admonitionblock.tip .icon {\nbackground: #00bfa5;\n    font-size: .73rem;\n    padding: .3rem .5rem;\n    height: 1.35rem;\n    line-height: 1;\n    font-weight: 500;\n    text-transform: uppercase;\nmargin-bottom:1.5rem;\ndisplay:inline-block;\n    color: #fff;\n}\n</style>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1741"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-04-09T06:17:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1723,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "af772957-e862-46ff-8d95-0ff39bea1e0e",
        "siteSettingsId": 1723,
        "fieldLayoutId": 4,
        "contentId": 1300,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Announcing Native Polyglot APIs in FaunaDB: GraphQL Now, SQL Forthcoming",
        "slug": "new-platform-apis-graphql-cql-sql",
        "uri": "blog/new-platform-apis-graphql-cql-sql",
        "dateCreated": "2019-04-05T15:23:37-07:00",
        "dateUpdated": "2020-05-22T11:01:42-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/new-platform-apis-graphql-cql-sql",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/new-platform-apis-graphql-cql-sql",
        "isCommunityPost": false,
        "blogBodyText": "<p>Today’s applications require data to be modeled in various ways. Your favorite payment application not only supports real-time transactions, but also provides the ability to communicate with a network of friends. FaunaDB’s multi-model approach enables developers to model almost all types of data including relational, document, graph, and temporal paradigms. This has become very attractive to developers as they do not need to use multiple databases to address these different requirements within the same application. </p>\r\n<p>At Fauna, we also understand that our developers want to use the language/s of their choice to work with their different data models. FaunaDB’s proprietary query language (FQL) is intuitive, flexible, and typesafe. FQL interaction with the database is mediated by drivers that publish embedded DSLs for most popular application languages. But as Fauna expands its user base, more and more developers are requesting different dialects that are popular for specific application use cases. We have listened to our users and are announcing support for a set of new APIs that will empower our developers and boost their productivity.</p>\r\n<figure><img src=\"{asset:1745:url}\" data-image=\"1745\"></figure>\r\n<h2>The New APIs</h2>\r\n<p>Developers using FaunaDB will now be able to manipulate data directly using multiple languages such as GraphQL for JAMStack applications, as well as others such as&nbsp;SQL, which have historically been popular in the enterprise. These languages will be supported as native endpoints within FaunaDB, and will not require any additional software to be installed by the database client. With support for GraphQL and FQL, and later SQL, FaunaDB’s managed serverless cloud will become truly versatile. Serverless, multi-tenant, and multi-cloud, FaunaDB’s managed cloud will allow developers to use any of these languages to manipulate all their data, thereby unlocking new levels of productivity for serverless application development.</p>\r\n<h2>The Fauna Advantage</h2>\r\n<p>Unlike other systems that create API layers with different properties for each dialect, platform APIs in FaunaDB leverage common database capabilities to offer developers uniform access to transactional consistency, user authorization, data access, quality of service (QoS), and temporal storage. </p>\r\n<p><strong>Consistency</strong>: FaunaDB offers the <a href=\"https://fauna.com/blog/a-comparison-of-scalable-database-isolation-levels\">highest consistency levels</a> for its transactions. These strong consistency guarantees are automatically applied to all platform APIs.</p>\r\n<p><strong>Authorization</strong>: Unlike most databases that control access at a table level, FaunaDB provides access control at a row (document) level. This fine-grained access control is applicable to all APIs, be it GraphQL or SQL.</p>\r\n<p><strong>Shared Data Access</strong>: Data written by one API (e.g., GraphQL) can be read and modified by another API (e.g., SQL). This is a huge improvement on other database platforms that limit APIs to their specific datasets.</p>\r\n<p><strong>QoS</strong>: FaunaDB's built-in prioritization policies for concurrent workloads are enforced at the database level or with access keys. All API access automatically adheres to these QoS definitions.</p>\r\n<p><strong>Temporality</strong>: FaunaDB is the only database on the market that provides built-in temporality support with no limits on data history. With per query snapshots, any platform API (e.g., SQL) in FaunaDB can return data at any given point in time</p>\r\n<h2>GraphQL API</h2>\r\n<p>GraphQL allows developers to specify the shape of the data they need, without requiring changes to the backend components that provide that data. This enables teams to collaborate more smoothly so that backend teams can focus on security and business logic, and front-end teams can focus on presentation and usability. In this way, GraphQL has emerged as a critical layer for universal database access. </p>\r\n<p>With FaunaDB’s GraphQL API, developers no longer need to create a database schema or write time-consuming resolver functions; rather, a GraphQL schema definition file is all you need to get started. Once you import the schema definition in FaunaDB, all required database objects and functions are automatically generated. This automation tremendously boosts developer productivity as it significantly reduces the time spent in schema design and development. </p>\r\n<p>A preview of FaunaDB’s GraphQL API is immediately available in FaunaDB Cloud. If you want to start using it today, please follow the getting started instructions <a href=\"https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema\">here</a>.&nbsp;</p>\r\n<h2>The Road Ahead</h2>\r\n<p>While GraphQL is gaining popularity,&nbsp;SQL has long been the lingua franca for traditional enterprise applications. With digital transformation driving cloud adoption within the enterprise, application development continues to rely on SQL for a vast majority of enterprise workloads. However, despite this huge market need, very few cloud-centric SQL databases offer strong consistency and resilience guarantees in mainframe-like environments.<br></p>\r\n<p>FaunaDB’s support for SQL will enable enterprises and digital businesses alike to gain from the productivity of prevalent skills in SQL while leveraging cloud platforms, without sacrificing the transactional safety that enterprises need. The SQL API will be available in the second half of CY 2019. </p>\r\n<p>If you have feedback, or you would like to know more about our product, please reach me at <a href=\"mailto:product@fauna.com\">product@fauna.com</a>. I’d love to talk to you.</p>",
        "blogCategory": [
            "8",
            "1530",
            "1531",
            "1866"
        ],
        "mainBlogImage": [
            "1735"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-04-09T06:17:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1724,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "312e3e5b-336e-46ea-b1b0-c50bd48b8628",
        "siteSettingsId": 1724,
        "fieldLayoutId": 4,
        "contentId": 1301,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with GraphQL, Part 1: Importing and Querying Your Schema",
        "slug": "getting-started-with-graphql-part-1-importing-and-querying-your-schema",
        "uri": "blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema",
        "dateCreated": "2019-04-05T16:22:23-07:00",
        "dateUpdated": "2020-02-24T09:28:06-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-graphql-part-1-importing-and-querying-your-schema",
        "isCommunityPost": false,
        "blogBodyText": "<div class=\"admonitionblock tip\">\n<div class=\"icon\">Tip</div><br>\n<h3>The Fauna Cloud Console now has GraphQL Playground!</h3>\n<p>This blog was written before we added GraphQL Playground to our Cloud Console. Although this tutorial will still work, we recommend that you follow this tutorial series instead:</p>\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\n</div>\n<p>GraphQL is a common language that backend and frontend developers can use to specify the shape and content of the data they request. By decoupling backend development from frontend API requirements, GraphQL removes tons of friction from the development process and allows frontend developers to make progress without waiting on backend changes. Similarly, it allows backend developers to focus on the logical aspects of providing a data API, not the procedural headaches involved in formatting the data for the front end. Because it’s such a compelling technology, there is an abundance of GraphQL middleware, caches, developer tools, and other components.</p>\n<p>FaunaDB is proud to join this ecosystem with the beta release of our GraphQL API. This post shows you how to get started. Simply supply a GraphQL schema definition and FaunaDB is ready to handle your queries. Thanks especially to <a href=\"https://github.com/erickpintor\">Erick Pintor</a> and the engineering team for their hard work on this feature.</p>\n<p>This series will continue and cover more advanced features like relations. If you start writing code today, please join our <a href=\"https://publicslack.com/slacks/fauna-community/invites/new\">community Slack</a> and let us know how it’s going. These are beta features, so your feedback today would have a big impact on the production release.<br></p>\n<h2>Steps to get started</h2>\n<p>By following these steps, you’ll be up and running with the FaunaDB GraphQL API. It should take about 15 minutes, or less if you are already familiar with the tools.</p>\n<h3>1.&nbsp;GraphQL schema definition file</h3>\n<ol></ol>\n<p>First you need a GraphQL schema definition file. You’ll upload this file to our GraphQL endpoint URL. Here is a simple example:<br></p>\n<pre>type Todo {\n   title: String!\n   completed: Boolean\n}\ntype Query {\n   allTodos: [Todo!]\n   todosByCompletedFlag(completed: Boolean!): [Todo!]\n}</pre>\n<p>To follow along at home, put this in a file called <tt>schema.gql</tt>. We’ll use curl to upload it to the FaunaDB GraphQL import API.</p>\n<h3>2. Create a database using Fauna Shell and a provision a key</h3>\n<p>To create a database with <a href=\"https://docs.fauna.com/fauna/current/quickstart.html\">Fauna Shell</a>, and import your GraphQL schema, issue the following commands and copy the key secret (it doesn't matter what name you give your database, just be consistent):</p>\n<pre>$ fauna create-database graphql\ncreated database 'graphql'\n$ fauna create-key 'graphql'\ncreating key for database 'graphql' with role 'admin'\n created key for database 'graphql' with role 'admin'.\n secret: &lt;key-secret&gt;\n To access 'graphql' with this key, create a client using\n the driver library for your language of choice using\n the above secret.</pre>\n<p>Alternatively, you can create a database via <a href=\"https://dashboard.fauna.com/\">dashboard.fauna.com</a>, and provision a <tt>server</tt> secret. Copy this secret for the next command.</p><figure><img src=\"{asset:6936:url}\" data-image=\"6936\"></figure>\n\n<h3>3. Import the graphql file into FaunaDB’s GraphQL endpoint</h3>\n<p>The FaunaDB GraphQL API can import your schema definition, creating all of the classes and indexes as necessary. The key secret you just provisioned (followed by the <tt>:</tt> character) is sent as an HTTP header to scope the query to the database you just created. Run this command to import your schema:<br></p>\n<pre>$ curl -u &lt;key-secret&gt;: https://graphql.fauna.com/import --data-binary \"@schema.gql\"\nSchema imported successfully.\nUse the following HTTP header to connect to the FaunaDB GraphQL API:\n\n{ \"Authorization\": \"Basic &lt;encoded secret&gt;\" }</pre>\n<p>You can see in the result that the GraphQL schema was imported successfully. Looking at the database in the Shell, you can see the generated schema:</p>\n<pre>graphql&gt; Paginate(Union(Classes(), Indexes()))\n{ data:\n  [ Class(\"Todo\"),\n    Index(\"todosByCompletedFlag\"),\n    Index(\"allTodos\") ] }</pre>\n<p>You can see FaunaDB has generated a schema to support your GraphQL environment. The Fauna Shell can be useful for understanding the underlying indexes and classes, but everyday work can be done via GraphQL. Once we are connected via the GraphQL explorer, we can view the schema in GraphQL form.</p>\n<h3>4. Open GraphQL Playground</h3>\n<p>Open GraphQL Playground &nbsp;<a href=\"https://electronjs.org/apps/graphql-playground\">https://electronjs.org/apps/graphql-playground</a>, and connect it to <a href=\"https://graphql.fauna.com/graphql\">https://graphql.fauna.com/graphql</a>. </p>\n<p>You'll have to configure GraphQL Playground to send a base64 encoded \"Authorization\" header with your secret in it. This is provided as part of the response to importing a schema.</p>\n<p>In GraphQL Playground, configure the headers tab with:<br></p>\n<pre>{\n   \"Authorization\": \"Basic &lt;encoded-secret&gt;\"\n}</pre>\n<h3>5. Inspect the GraphQL schema</h3>\n<p>FaunaDB adds a few metadata attributes to the GraphQL schema that you provided, and you can see this in GraphQL Playground:<br></p>\n<pre>directive @collection(name: String!) on OBJECT\ndirective @index(name: String!) on FIELD\ndirective @embedded on OBJECT\ndirective @relation(relation: String) on FIELD\ndirective @unique(unique: String) on FIELD\nscalar Date\n\nscalar Long\n\ntype Mutation {\n  createTodo(data: TodoInput!): Todo!\n  updateTodo(\n    id: ID!\n    data: TodoInput!\n  ): Todo\n  deleteTodo(id: ID!): Todo\n}\n\ntype Query {\n  findTodoByID(id: ID!): Todo\n  todosByCompletedFlag(\n    _size: Int\n    _cursor: String\n    completed: Boolean!\n  ): TodoPage!\n  allTodos(\n    _size: Int\n    _cursor: String\n  ): TodoPage!\n}\n\nscalar Time\n\ntype Todo {\n  _id: ID!\n  _ts: Long!\n  title: String!\n  completed: Boolean\n}\n\ninput TodoInput {\n  title: String!\n  completed: Boolean\n}\n\ntype TodoPage {\n  data: [Todo]!\n  after: String\n  before: String\n}</pre>\n<p>One of my favorite things about GraphQL is first class support for inspecting the schema, and seeing exactly how FaunaDB models objects and inputs in GraphQL is a powerful aid in development.</p><figure><img src=\"{asset:6937:url}\" data-image=\"6937\"></figure>\n\n<h3>6. Run GraphQL Queries</h3>\n<p>Once connected, you can run queries like the following to create a todo:<br></p>\n<pre>mutation CreateATodo {\n   createTodo(data: {\n   title: \"Build an awesome app!\"\n   completed: false\n   }) {\n       title\n       completed\n   }\n}</pre>\n\n<figure><img src=\"{asset:6938:url}\" data-image=\"6938\"></figure><p><br></p><p>Once you have created a few todos, you can list them with this query:<br></p>\n<pre>query FindAllTodos {\n  allTodos {\n    data {\n      _id\n      title\n      completed\n    }\n  }\n}</pre>\n\n<figure><img src=\"{asset:6939:url}\" data-image=\"6939\"></figure><p><br></p><p>Or you can look up an individual todo by its ID. Take one of the IDs from the last query's result, and paste it in place of &lt;id&gt; in the following query:<br></p>\n<pre>query FindATodoByID {\n   findTodoByID(id: \"&lt;id&gt;\") {\n       title\n       completed\n   }\n}</pre>\n\n<figure><img src=\"{asset:6940:url}\" data-image=\"6940\"></figure><p><br></p><p>You can also update a todo by its ID. Just paste one of the IDs in place of &lt;id&gt;&nbsp;in this query:<br></p>\n<pre>mutation UpdateATodo {\n   updateTodo(id: \"&lt;id&gt;\", data: {\n       title: \"Build two awesome apps!\"\n       completed: true\n   }) {\n       title\n       completed\n   }\n}</pre>\n<p>Similarly, delete a todo, based on its id, with this query:<br></p>\n<pre>mutation DeleteATodo {\n   deleteTodo(id: \"&lt;id&gt;\") {\n       title\n   }\n}</pre>\n<p></p>\n<h3>Automatic index generation</h3>\n<p>When a parameter is defined with a flag, it can be used as an index term. For example, this query lists all todos that are completed:<br></p>\n<pre>query FindAllCompletedTodos {\n  todosByCompletedFlag(completed: true) {\n      data {\n        title\n      }\n  }\n}</pre>\n\n<figure><img src=\"{asset:6941:url}\" data-image=\"6941\"></figure><p><br></p><p>In <a href=\"{entry:1739:url}\">Part 2</a> of this series, we’ll look at relationships by expanding our example to include lists, so that we can query for all of the todos in a list, retrieving their title and other metadata in a compact GraphQL representation that’s right for your app.<br></p>\n<style>\n.admonitionblock.tip{padding: 1rem 1.5rem;border-left:3px solid #00bfa5; background: #fff;margin-bottom:2rem;\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\nbody .description .admonitionblock h3{margin-top:0;}\n.admonitionblock.tip .icon {\nbackground: #00bfa5;\n    font-size: .73rem;\n    padding: .3rem .5rem;\n    height: 1.35rem;\n    line-height: 1;\n    font-weight: 500;\n    text-transform: uppercase;\nmargin-bottom:1.5rem;\ndisplay:inline-block;\n    color: #fff;\n}\n</style>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1734"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-04-08T11:19:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1720,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "394db607-d038-433b-9a16-51ba3bf5fe42",
        "siteSettingsId": 1720,
        "fieldLayoutId": 4,
        "contentId": 1297,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Learning FQL, Part 3: Database Access Keys",
        "slug": "learning-fql-part-3-database-access-keys",
        "uri": "blog/learning-fql-part-3-database-access-keys",
        "dateCreated": "2019-03-26T11:53:11-07:00",
        "dateUpdated": "2019-04-08T11:19:40-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/learning-fql-part-3-database-access-keys",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/learning-fql-part-3-database-access-keys",
        "isCommunityPost": false,
        "blogBodyText": "<p>In the previous two posts in this series, we covered FaunaDB schema basics, and basic CRUD operations (create, read, update, and delete). Fauna Shell handles access control for you, so you haven’t had to do any key management. This article will cover key creation and use. The basic operations discussed here can be combined with FaunaDB’s <a href=\"https://fauna.com/blog/secure-hierarchical-multi-tenancy-patterns\">multi-tenancy patterns</a> to automate SaaS application provisioning, user account creation, and other application lifecycle steps.</p>\r\n<h2>Access Keys</h2>\r\n<p>FaunaDB <a href=\"https://docs.fauna.com/fauna/current/reference/security.html#access-keys\">keys correspond to a database and an access control level</a>. Admin keys can provision and manage nested databases, while server keys can manage the data in a particular database. There are also access tokens, which allow end users to authenticate connections from mobile devices and web clients.</p>\r\n<p>When you are embedding a key into application code for queries or schema management, you’ll use a server key. To manage keys for a particular child database, you must connect with an admin key connection that corresponds to the parent database. In this case, Fauna Shell is currently connected to the root database with an admin key, so we can create a server key for the child database, called \"my-database\", that we created in an earlier article.</p>\r\n<pre>CreateKey({ \r\n    database: Database(\"my-database\"),\r\n    role: \"server\" })</pre>\r\n<p>The return value contains the key secret:</p>\r\n<pre>=&gt; { ref: Ref(id=200669511526908416, class=Ref(id=keys)),\r\n  ts: 1527632209151020,\r\n  database: Ref(id=caledonia, class=Ref(id=databases)),\r\n  role: 'server',\r\n  secret: 'fnACyOvbh9ACAOtRQDQSefokn4iuMf9zn_FakPAx',\r\n  hashed_secret: '$2a$05$dszFRic/tkKzsG0rrUj3Fu7.nSoYuJL7BdnHXepx4XTzfcWtVEZRC' }</pre>\r\n<p>Once a key is created, its secret field is what you’ll store and use in your application configuration, and use to connect to the individual database to perform data operations. This server key can create classes and indexes, as well as run queries and modify data. If you want a key with other privileges, you can read about the other options for key roles in the <a href=\"https://docs.fauna.com/fauna/current/reference/security.html#access-keys\">access keys documentation</a>: admin, server, server-readonly, and client. The query to provision those types of keys will look like what you see above, with a different role specified.<br></p>\r\n<h2>Server Connection</h2>\r\n<p>By using the server key secret we just provisioned, we can connect to our database. If you are using Fauna Shell, this happens automatically, but you still need to know about it when you are writing code. The secret is used in code that creates client connections to FaunaDB. Since FaunaDB connections are stateless, there is no harm in maintaining multiple client objects in your application—for instance an admin client for provisioning new SaaS tenants, and a server client for each tenant, to interact with the data.</p>\r\n<pre>var serverClient = new faunadb.Client({\r\n  secret: process.env.FAUNADB_SERVER_SECRET\r\n});</pre>\r\n<p>Where FAUNADB_SERVER_SECRET looks something like the 'fnACyOvbh9ACAOtRQDQSefokn4iuMf9zn_FakPAx' as returned above. It’s up to you and your hosting provider, the best way to manage environment variables, but it is a best practice not to paste secrets into production code. </p>\r\n<p>This server client is what you’ll use for schema changes and data queries within the database my-database. If you are using Fauna Shell, you can connect to the database by running `fauna shell my-database`.</p>\r\n<p>The server client connection we created in the code above is useful for defining indexes and interacting with data in the database. It can’t be used to create new child databases. It should not be distributed to end users or user agents, because it has the capability to read the entire database as well as update or delete arbitrary data. </p>\r\n<p>For more details about <a href=\"https://docs.fauna.com/fauna/current/reference/security.html\">working with Access Keys check out the FaunaDB docs</a>. </p>\r\n<h2>Client Keys and Public Read Permissions</h2>\r\n<p>Sometimes you want to connect to the database as an untrusted user. Client keys connect to a particular database, but only give access to public resources, which must be explicitly tagged by developers. For example you might create an index with public data in the values, and allow anonymous users to query it. For this pattern you’d create an index that is publicly readable, and a client key that can read it.</p>\r\n<p>To create a publicly queryable index, you can set it’s “read” permission to “public” (in this case we’re in a shell session for my-databases, which you can reach with fauna shell my-database):</p>\r\n<pre>CreateIndex({\r\n    name: \"all_spell_titles\",\r\n    permissions: {\r\n        read: \"public\"\r\n    },\r\n    source: Class(\"spells\"),\r\n    values: [{ field: [\"data\", \"title\"] }] })</pre>\r\n<p>To create a key that can query this index, we just issue a query to create a client key, and share the resulting key secret with our client applications. Client keys can only read public resources.</p>\r\n<pre>CreateKey({ \r\n    database: Database(\"my-database\"),\r\n    role: \"client\" })</pre>\r\n<p>This pattern allows you to open certain data resources to queries from anonymous clients, without exposing the rest of the database. This can be useful if you plan to have mobile devices or web browsers query the database directly. It can also be helpful if you are offering data to partner companies.</p>\r\n<h2>Conclusion</h2>\r\n<p>These patterns give developers and administrators fine-grained control over data in FaunaDB. Whether you are provisioning databases for SaaS customers, business units, development teams, or projects, you can give just the right level of access to just those who need it, without worrying about impacts to the rest of the cluster.</p>\r\n<p>Admin keys can be used to manage the database tenancy tree and to provision other keys. Server keys can define the database schema and issue queries. Client keys can query public resources. These capabilities are enough to get started with FaunaDB access keys. </p>\r\n<p>More complex applications may require that users may only view and edit their own data, or perhaps multiple users collaborate on data, with different users working on different parts of the database. In a future article we’ll talk about how to authenticate users with tokens, and how to work with role-based application control. If you’d like to jump ahead, these patterns are illustrated in the application development series <a href=\"https://fauna.com/blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-2\">Building a Serverless JAMStack app with FaunaDB Cloud.</a> <br><br><br><br></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2019-03-15T07:30:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1692,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "2a76eff6-12a9-4e60-a7b6-f68047328b01",
        "siteSettingsId": 1692,
        "fieldLayoutId": 4,
        "contentId": 1277,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "A Comparison of Scalable Database Isolation Levels",
        "slug": "a-comparison-of-scalable-database-isolation-levels",
        "uri": "blog/a-comparison-of-scalable-database-isolation-levels",
        "dateCreated": "2019-03-14T08:45:15-07:00",
        "dateUpdated": "2019-11-01T17:00:59-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/a-comparison-of-scalable-database-isolation-levels",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/a-comparison-of-scalable-database-isolation-levels",
        "isCommunityPost": false,
        "blogBodyText": "<p>It is very difficult to find accurate information about the correctness and isolation levels offered by modern distributed databases and the operational conditions required to achieve them. Developers use different terms for the same thing, the meaning of terms varies or is ambiguous, and sometimes vendors themselves do not actually know.</p><p>At Fauna, we care a lot about accurately describing which guarantees different systems provide. This is our effort to centralize a description&nbsp;of which database does what. For consistency’s sake, we will use the terminology from Kyle Kingsbury’s <a href=\"https://jepsen.io/consistency\">explanation on the Jepsen site</a>. The chart is ranked by the maximum multi-partition isolation level offered.</p><p>The data&nbsp;is based on statements about&nbsp;isolation levels from vendor documentation, white papers,&nbsp;and developer commentary, exclusive of aspirational marketing statements.&nbsp;We have tried to be neutral in the characterization of the various systems' architectural properties.&nbsp;Whether the system implementations uphold these guarantees&nbsp;is&nbsp;<a href=\"https://jepsen.io/analyses\">addressed elsewhere</a>.&nbsp;If you haven't already, please see FaunaDB's own <a href=\"https://fauna.com/blog/faunadbs-official-jepsen-results\">Jepsen results</a>&nbsp;for confirmation that FaunaDB upholds its guarantees.</p><h2>Before we BEGIN</h2><p>In discussing transactional isolation, we frequently encounter&nbsp;the \"worse is better\" argument, which essentially goes:</p><ul><li>This database does what it does</li><li>Implementing better isolation&nbsp;in the <em>database</em> is&nbsp;impossible or&nbsp;has unacceptable tradeoffs</li><li>Implementing better isolation in the&nbsp;<em>application&nbsp;</em>is simple and useful</li></ul><p>This argument also goes by \"it's not a bug, it's a feature\".&nbsp;</p><p>The pretense of low maximum&nbsp;isolation levels,&nbsp;eventual consistency, or&nbsp;<a href=\"http://christophermeiklejohn.com/erlang/lasp/2019/03/08/monotonicity.html\">CRDTs</a>&nbsp;is that application developers are ready and willing to work through every failure and recovery condition of their distributed dataflow. But in practice, moving beyond “works on my machine” correctness testing&nbsp;requires an extraordinary level of investment that product teams simply can&nbsp;not do.&nbsp;</p><p>In my experience,&nbsp;the implications of different isolation levels are&nbsp;<a href=\"https://fauna.com/blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels\">very subtle</a>.&nbsp;Pushing the burden to application developers—especially when there are a lot of distinct applications, like in a microservices architecture—is tremendously detrimental to productivity.&nbsp;And although&nbsp;tunable consistency increases flexibility, it&nbsp;cannot be&nbsp;used to paper over an isolation level that is&nbsp;<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlConfigConsistency.html\">fundamentally too weak to effectively compose</a>.&nbsp;</p><p>After all, /dev/null is serializable, but not very useful as a database.&nbsp;</p><h2>Distributed Databases</h2><p>Distributed databases present a unified topology and&nbsp;do not require operator management of replication, although some, like the Percolator systems, do require management of special nodes.</p><div class=\"table-wrap\"><table><tbody><tr><td></td><td><p><strong>Maximum isolation level</strong></p></td><td><p><strong>Default isolation level</strong></p></td><td><p><strong>Minimum isolation level</strong></p></td><td><p><strong>Consensus<br>architecture</strong></p></td><td><p><strong>Limitations<br></strong></p></td></tr><tr><td><p><strong>FaunaDB</strong></p></td><td><a href=\"https://jepsen.io/consistency/models/strict-serializable\">Strict serializability<br><br></a></td><td><a href=\"https://jepsen.io/consistency/models/snapshot-isolation\">Snapshot</a><br><br>Read/write transactions without indexes are always strictly serializable.</td><td>Snapshot</td><td>Calvin <a href=\"#references\"><sup>1</sup></a> with optimistic concurrency control<br><br>Writes must coordinate on local log leaders. Reads can be served from any replica.</td><td></td></tr><tr><td><p><strong>Google Cloud Spanner&nbsp;<a href=\"#references\"><sup>4</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/strict-serializable\">Strict serializability</a> (and \"external consistency\")</td><td>Strict serializability</td><td>Snapshot (called “bounded staleness”)</td><td>Spanner <a href=\"#references\"><sup>2</sup></a><br><br>Writes must coordinate on partition leaders which may be remote. Reads can be served from any replica.</td><td></td></tr><tr><td><p><strong>FoundationDB&nbsp;<a href=\"#references\"><sup>5&nbsp;6</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/strict-serializable\">Strict serializability</a></td><td>Strict serializability</td><td>Snapshot</td><td>Modified Percolator <a href=\"#references\"><sup>3</sup></a><br><br>All queries must coordinate on the timestamp oracle.<br><br>Conflict resolution is deferred until commit.</td><td></td></tr><tr><td><p><strong>CockroachDB&nbsp;<a href=\"#references\"><sup>7&nbsp;8</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/serializable\">Serializable</a></td><td>Serializable</td><td>Serializable</td><td>Modified Spanner <br><br>All queries must coordinate on the partition leaders for their respective keys.</td><td>Transactions with shared keys are mutually serializable, but transactions with disjoint keys can suffer “causal reversal”. <br><br>Isolation is violated under clock skew.</td></tr><tr><td><p><strong>Yugabyte&nbsp;<a href=\"#references\"><sup>9</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/snapshot-isolation\">Snapshot</a></td><td>Snapshot</td><td>Snapshot</td><td>Spanner</td><td>Isolation is violated under clock skew.</td></tr><tr><td><p><strong>TiDB&nbsp;<a href=\"#references\"><sup>10</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/repeatable-read\">Repeatable read</a></td><td>Repeatable read</td><td>Repeatable read</td><td>Percolator <br><br>All queries must coordinate on the timestamp oracle.</td><td></td></tr><tr><td><p><strong>DynamoDB</strong></p></td><td><a href=\" https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">Strong partition serializability</a></td><td><a href=\"https://jepsen.io/consistency/models/read-committed\">Read committed</a></td><td>Read committed</td><td>Paxos <br><br>Multi-partition two-phase commit offers limited serializability support.<br></td><td>Multi-partition transactions limited to 10 primary keys with explicit read dependencies.<br><br>Indexes are not serializable.<br><br>Isolation is violated if there are non-transactional queries to the same keys or if global tables are used.</td></tr><tr><td><p><strong>CosmosDB&nbsp;<a href=\"#references\"><sup>11&nbsp;12</sup></a></strong></p></td><td><a href=\" https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">Strong partition serializability</a></td><td>Linearizable for single-region, snapshot for multi-region</td><td>Read uncommitted</td><td>Paxos</td><td>Multi-partition transactions are not supported.</td></tr><tr><td><p><strong>Cassandra&nbsp;<a href=\"#references\"><sup>13</sup></a></strong></p></td><td><a href=\" https://fauna.com/blog/demystifying-database-systems-correctness-anomalies-under-serializable-isolation\">Strong partition serializability</a></td><td><a href=\"https://jepsen.io/consistency/models/read-uncommitted\">Read uncommitted</a> (aka “eventual consistency)</td><td>Read uncommitted</td><td>Single-decree Paxos</td><td>Multi-partition transactions are not supported.<br><br>Isolation is violated if there are non-transactional queries to the same keys, or if global secondary indexes are used.</td></tr><tr><td><p><strong>MongoDB&nbsp;<a href=\"#references\"><sup>14</sup></a></strong></p></td><td><a href=\"https://jepsen.io/consistency/models/writes-follow-reads\">Session causality</a></td><td>Read uncommitted</td><td>Read uncommitted</td><td>Sharded, semi-synchronous replication with automated failover</td><td>Multi-partition transactions are not supported.<br><br>Isolation is violated during partitions and shard leader election.</td></tr></tbody></table><br></div><h2>Replicated Databases</h2><p>Replicated databases require operator management of primaries and secondaries and the associated replication links. Asynchronous replication can improve availability and scale read capacity,&nbsp;but does not offer any distributed&nbsp;consistency guarantees.&nbsp;Semi-synchronous replication further improves availability, but does not improve distributed isolation.</p><p>This is the traditional RDBMS scale-out model.</p><div class=\"table-wrap\"><table><tbody><tr><td></td><td><p><strong>Maximum isolation level</strong></p></td><td><p><strong>Default isolation level</strong></p></td><td><p><strong>Minimum isolation level</strong></p></td><td><p><strong>Replication architecture</strong></p></td><td><p><strong>Limitations</strong></p></td></tr><tr><td><p><strong>Oracle</strong></p></td><td><a href=\"https://jepsen.io/consistency/models/snapshot-isolation\">Snapshot</a></td><td>Snapshot</td><td>Read committed</td><td>Asynchronous replication</td><td>Oracle's SERIALIZABLE isolation is not serializable, but is actually snapshot isolation with write conflict detection. This allows write skew anomalies.</td></tr><tr><td><p><strong>MySQL</strong></p></td><td><a href=\"https://jepsen.io/consistency/models/serializable\">Serializable</a>, primary node only</td><td><a href=\"https://jepsen.io/consistency/models/repeatable-read\">Repeatable read</a>, primary node only</td><td>Read uncommitted</td><td>Semi-synchronous replication</td><td></td></tr><tr><td><p><strong>PostgreSQL</strong></p></td><td><a href=\"https://jepsen.io/consistency/models/serializable\">Serializable</a>, primary node only</td><td><a href=\"https://jepsen.io/consistency/models/read-committed\">Read committed</a></td><td>Read committed</td><td>Semi-synchronous replication</td><td></td></tr></tbody></table><br></div><h2>Conclusion</h2><p>A good&nbsp;way to think about isolation is in terms of the breadth of potential anomalies. The lower the isolation level, the more types of anomalies can occur, and the harder it is to reason about application behavior both at steady-state and under faults.&nbsp;At Fauna, we encourage you to think critically about whether your current databases really guarantee the level of transactional isolation you need.<br></p><h2 id=\"references\">References</h2><ol class=\"fn\"><li><a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin: Fast Distributed Transactionsfor Partitioned Database Systems</a></li><li><a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">Spanner: Google’s Globally-Distributed Database</a></li><li><a href=\"https://ai.google/research/pubs/pub36726\">Large-scale Incremental Processing Using Distributed Transactions and Notifications</a></li><li><a href=\"https://cloud.google.com/spanner/docs/true-time-external-consistency\">Cloud Spanner: TrueTime and external consistency</a></li><li><a href=\"https://apple.github.io/foundationdb/consistency.html\">FoundationDB Consistency</a></li><li><a href=\"https://arxiv.org/pdf/1901.04452.pdf\">FoundationDB Record Layer:A Multi-Tenant Structured Datastore</a></li><li><a href=\"https://www.cockroachlabs.com/blog/consistency-model/\">CockroachDB's Consistency Model</a><br></li><li><a href=\"https://jepsen.io/analyses/cockroachdb-beta-20160829\">Jepsen CockroachDB beta-20160829</a></li><li><a href=\"https://docs.yugabyte.com/latest/architecture/transactions/isolation-levels/\">YugaByte Isolation Levels</a></li><li><a href=\"https://github.com/pingcap/docs/blob/8dd9b3e/sql/transaction-isolation.md\">TiDB Transaction Isolation Levels</a></li><li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\">Consistency levels in Azure Cosmos DB</a></li><li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/database-transactions-optimistic-concurrency\">Transactions and optimistic concurrency control</a></li><li><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlTransactionsDiffer.html#dmlTransactionsDiffer__isolation\">How are Cassandra transactions different from RDBMS transactions?</a></li><li><a href=\"https://jepsen.io/analyses/mongodb-3-6-4\">Jepsen MongoDB 3.6.4</a></li></ol><p><br><br></p><style>table { background: white; }table p { margin-bottom: 0; }table tbody tr { border-bottom: 1px solid #dee2e6; }table tbody tr:last-child { border-bottom: 0; }table tbody td { border: 0; vertical-align: top; padding: 30px 10px; font-size: 13px; line-height: 1.3; }table tbody tr:first-child td { vertical-align: bottom; }table tbody td p { line-height: 1.3; }table tbody td:nth-child(5) {  min-width: 200px;  width: ;}table tbody td:nth-child(6) {  min-width: 200px;  width: ;}@media screen and (max-width: 768px) {.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }table { width:auto;}table tbody td { font-size: 13px; }}@media screen and (min-width: 992px) and (max-width: 1400px) {.table-wrap{ overflow-x: scroll; background: white; padding: 0 20px; }table { width:auto;}table tbody td { font-size: 13px; }}</style>",
        "blogCategory": [
            "1462"
        ],
        "mainBlogImage": [
            "1694"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-03-06T10:20:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1686,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "1079721e-2126-445a-91a7-7cdd3613c436",
        "siteSettingsId": 1686,
        "fieldLayoutId": 4,
        "contentId": 1272,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Learning FQL, Part 2: Create, Read, Update, and Delete Operations (CRUD)",
        "slug": "learning-fql-part-2-create-read-update-and-delete-operations-crud",
        "uri": "blog/learning-fql-part-2-create-read-update-and-delete-operations-crud",
        "dateCreated": "2019-03-06T10:16:39-08:00",
        "dateUpdated": "2019-03-11T18:26:23-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/learning-fql-part-2-create-read-update-and-delete-operations-crud",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/learning-fql-part-2-create-read-update-and-delete-operations-crud",
        "isCommunityPost": false,
        "blogBodyText": "<p>The most common database queries are used simply to manipulate individual data items. For instance a recipe application allows users to save a new recipe, read an existing recipe, make changes to a recipe, or remove an unwanted recipe. Of course, most applications also include complex queries, such as listing all my favorite recipes, or adding a flag to all the recipes that include ingredients which contain gluten – which will be the focus of other articles in this series. </p>\r\n<p>This article is about basic CRUD operations. There is some setup and context that you need before you can save documents to the database. For instance, you’ll need to <a href=\"https://app.fauna.com/documentation/gettingstarted\">define a schema</a> to tell FaunaDB which types of documents you’ll be dealing with. You’ll also need to ensure your code can <a href=\"https://app.fauna.com/tutorials/hello\">connect to the database</a>. Those prerequisites are outside the scope of this article, as here we’ll focus on the common runtime operations on documents: create, read, update, and delete. Check out <a href=\"{entry:1676:url}\">the first article in the series</a> for help setting up your database schema.</p>\r\n<h3>Create</h3>\r\n<p>Once you’ve <a href=\"https://app.fauna.com/tutorials/hello\">setup your FaunaDB client driver</a> and <a href=\"https://app.fauna.com/documentation/gettingstarted\">created a database in FaunaDB</a>, you are ready to create a document. In Fauna Shell, your query might look like this:</p>\r\n<pre>Create(Class(\"recipes\"), {\r\n    data : {\r\n        title : \"Lentil Soup\",\r\n        ingredients : [\"carrots\", \"lentils\", \"tomatoes\", \"onions\", \"ginger\", \"garlic\", \"olive oil\"],\r\n        description : \"...\"\r\n}})\r\n\r\n#=&gt; { \r\n    ref: Ref(Class(\"recipes\"), \"226111113652077056\"), \r\n    ts: 1551895211820000, \r\n    data: { title : \"Lentil Soup\" ... } }</pre>\r\n<p><br><br><br></p>\r\n<p>This query references the “recipes” class, to tell the database which kind of document to create. Indexes and other schema objects can work on classes of documents. It is common for each application to define several classes. You can learn more about <a href=\"https://app.fauna.com/documentation/crud-examples\">setting up your schema</a> in the FaunaDB documentation.</p>\r\n<h3>Read</h3>\r\n<p>You can see in the above query that the application data is all contained in the data field. FaunaDB makes system fields available as well, so everything you store will be in the data field. One of the system fields is <a href=\"https://app.fauna.com/documentation/reference/queryapi#ref\">the ref, which is a reference</a> to the object itself. You can acquire a ref via an index query, or from the result of a CRUD operation. We can use this reference in other queries. Let’s run the query we defined above, and use the reference returned to issue a read.</p>\r\n<p>The FQL we use to fetch a document is simple, in this case we use the ref value returned from the above Create query. Your return value will have a different id.</p>\r\n<pre>Get(Ref(Class(\"recipes\"), \"226111113652077056\"))</pre>\r\n<p>In the below JavaScript example, soupRef is a reference obtained from running the above query. This example uses JavaScript in addition to FQL, in order to show you how refs can be reused for future queries in your code:</p>\r\n<pre>dbClient.query(makeRecipeQuery).then((createResult) =&gt; {\r\n    const soupRef = createResult.ref\r\n    const readSoupQuery = q.Get(soupRef)\r\n    dbClient.query(readSoupQuery).then((soupRecipe) =&gt; {\r\n        console.log(soupRecipe.data.title) // “Lentil Soup”\r\n    })\r\n})</pre>\r\n<p>The actual read query <tt>Get(soupRef)</tt> is so simple, that it makes a good opportunity to show how data flows through an asynchronous JavaScript example.</p>\r\n<p><a href=\"https://app.fauna.com/documentation/reference/queryapi#get\">The Get function</a> can also be embedded in more complex queries, for instance it is common to iterate over the results of an index match and use Get to load the associated documents. A future article in this series will discuss patterns for bulk operations.</p>\r\n<h3>Update</h3>\r\n<p><a href=\"https://app.fauna.com/documentation/reference/queryapi#update\">Updating a document</a> we loaded from database is done by directly manipulating it and saving it back. Extending on the above example:</p>\r\n<pre>Update(Ref(Class(\"recipes\"), \"226111113652077056\"), { \r\n    data : {\r\n        description: “A flavorful take on the hearty classic.”\r\n}})</pre>\r\n<p>In this query, we only have to specify the data field we wish to change. Any unspecified data fields will not be modified. If you want to remove a field, you can replace it with a null value.<br></p>\r\n<h3>Delete</h3>\r\n<p>The new description has been added to the soup recipe. But then our uncle Robert calls and insists the recipe is a family secret and can’t be shared! So to delete it, we issue a query using the same reference we used before:</p>\r\n<pre>Delete(Ref(Class(\"recipes\"), \"226111113652077056\"))</pre>\r\n<p>You can read the full <a href=\"https://app.fauna.com/documentation/reference/queryapi#delete\">documentation for Delete</a> here. It is also a common pattern to iterate over an index Match result set and delete all of the documents that it references. Look forward to exploring bulk create, read, update and delete patterns in a future article in this series.</p>\r\n<h3>Putting it Together</h3>\r\n<p>Composability is a key design goal of FQL. Composable queries are easy to build programmatically, and the patterns you learn can be reused and nested. Compare-and-swap or check-and-set (CAS) is a common pattern built by combining the functions described in this post. Essentially, a CAS update is an update that only succeeds if no other process has changed the data since it was read. This allows the application to alert the user that someone else is editing a document, instead of the users discovering that they have clobbered each other’s work.</p>\r\n<p>To implement CAS, first the application reads a document, then makes changes to it, and saves them back with an instruction to only save if the document hasn’t been touched in the meantime. In FQL it looks like this: (assume <tt>1551895211820000</tt> came from the read the application made to populate the UI, and was submitted with the user as part of a form request.)</p>\r\n<pre>If(Equals(1551895211820000, Select(\"ts\", Get(Ref(Class(\"recipes\"), \"226111113652077056\")))),\r\n    Update(Ref(Class(\"recipes\"), \"226111113652077056\"), { data : { title : \"Best Lentil Soup\" } }),\r\n    \"Too late, someone else changed the record since you read it.\")</pre>\r\n<p>This query combines Get, and Update, along with control flow operators like If, logical operators like Equals, and data addressing with Select. If the document’s timestamp has changed since it was loaded by the user, the query fails with an error saying the update is too late. The CRUD operations we are familiar with, and the other operators are designed to be familiar from other languages. Select allows you to pull a particular field from a document. In this case we used the timestamp, so any other change to the document (which automatically updates its timestamp) results in a CAS failure, but if you Select the field you plan to update, you can scope your CAS to the title alone. This allows you to do a conditional update on one field, for instance changing the title even if someone else has changed the ingredients in the meantime.</p>\r\n<p>In the next part of this series we'll cover database Access Keys. Here is&nbsp;a preview of the kinds of <a href=\"{entry:515:url}\">patterns you can implement with FaunaDB database access control</a>.<br><br><br></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "1685"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2019-03-05T04:22:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1677,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "65742991-d11b-48ae-9a10-be985f3f83e4",
        "siteSettingsId": 1677,
        "fieldLayoutId": 4,
        "contentId": 1263,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB's Official Jepsen Results",
        "slug": "faunadbs-official-jepsen-results",
        "uri": "blog/faunadbs-official-jepsen-results",
        "dateCreated": "2019-03-01T04:22:50-08:00",
        "dateUpdated": "2020-03-04T20:18:44-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadbs-official-jepsen-results",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadbs-official-jepsen-results",
        "isCommunityPost": false,
        "blogBodyText": "<p>I am pleased to present, along with Kyle Kingsbury of <a href=\"http://jepsen.io/\">Jepsen.io</a>, the <a href=\"https://www2.fauna.com/jepsen\">official Jepsen results</a> for FaunaDB version 2.5.4 and 2.6.0.&nbsp;</p>\n<p>Our team at Fauna worked extensively with Kyle for three months on one of the most thorough Jepsen tests of all time. Our mandate for him was not merely to test the basic properties of the system, but rather to poke into the dark corners and exhaustively validate that FaunaDB is architecturally sound, correctly implemented, and ready for enterprise workloads in the cloud.</p>\n<p>We’re excited to report that FaunaDB passed the core tests right away:</p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">FaunaDB’s core operations on single instances in 2.5.5 appeared solid: in our tests, we were able to reliably create, read, update, and delete records transactionally at snapshot, serializable, and strict serializable isolation. Acknowledged instance updates were never lost.</blockquote>\n<p>It now passes additional tests, covering features like indexes and temporality:&nbsp;</p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">By 2.6.0-rc10, Fauna had addressed almost all issues we identified; some minor work around availability and schema changes is still in progress.</blockquote>\n<p>Additionally, it offers the highest possible level of correctness:<em></em><em></em><em><em></em><br></em></p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">We expect to observe snapshot isolation at a minimum, and where desired, we can promote SI or serializable transactions to strict serializability: the gold standard for concurrent systems.</blockquote>\n<p>It is self-operating:</p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">Many consensus systems rely on fixed node membership, which is cumbersome for operators. FaunaDB is designed to support online addition and removal of nodes with appropriate backpressure.</blockquote>\n<p>And its architecture is sound:</p>\n<p></p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">FaunaDB is based on peer-reviewed research into transactional systems, combining Calvin’s cross-shard transactional protocol with Raft’s consensus system for individual shards. We believe FaunaDB’s approach is fundamentally sound...Calvin-based systems like FaunaDB could play an important future role in the distributed database landscape.</blockquote>\n<p>In consultation with Kyle, we’ve fixed many known issues and newly discovered&nbsp;bugs, made API improvements, and expanded our documentation. Kyle has extended the Jepsen suite itself with new tests specifically inspired by FaunaDB. We have also incorporated the extended Jepsen test suite into our internal QA, to help ensure that we never backtrack on the level of reliability we intend to provide.</p>\n<h2>What Is Jepsen?</h2>\n<p>Kyle describes Jepsen as “an effort to improve the safety of distributed databases.” It is an open source software verification suite born out of industry frustration with the unsubstantiated claims made by database vendors at the dawn of the cloud era. Jepsen is now widely regarded as the critical test that any distributed system must pass before it is considered mature.</p>\n<figure data-widget-code=\"%3Cimg%20style=%22width:%2033%25%22%20src=%22https://fauna.com/assets/site/design-assets/FaunaDB_2.5.4_Jepsen.png#asset:1679:url%22%20data-image=%221679%22%3E\"><a href=\"https://www2.fauna.com/jepsen\" target=\"true\"><img style=\"width: 33%\" src=\"{asset:1679:url}\" data-image=\"1679\" alt=\"FaunaDB 2.5.4 Jepsen Analysis\" title=\"FaunaDB 2.5.4 Jepsen Analysis\"></a></figure>\n<p>Those familiar with Jepsen reports will note that no other database tested has met the stringent reliability levels that FaunaDB has now met. The FaunaDB report also contains a lovely, extensive description of FaunaDB’s architecture, and I encourage you to read it in its entirety.</p>\n<h2>Why Test FaunaDB?</h2>\n<p>When we started building FaunaDB, our objective was to deliver a cloud-native database that offered both transactional consistency and global scalability. For that reason, we chose <a href=\"http://cs-www.cs.yale.edu/homes/dna/papers/calvin-sigmod12.pdf\">Calvin</a> as the basis for underlying transaction protocol.</p>\n<p>Other distributed, transactional databases use the first-generation <a href=\"https://blog.octo.com/en/my-reading-of-percolator-architecture-a-google-search-engine-component/\">Google Percolator model</a>, which cannot scale transactions across datacenters, or the second-generation <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">Google Spanner model</a>, which <a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">requires atomic hardware clocks</a> and a specialized operational environment. FaunaDB is the only production database to use the third-generation <a href=\"http://cs-www.cs.yale.edu/homes/dna/papers/calvin-sigmod12.pdf\">Calvin</a> protocol.</p>\n<p>By designing for global correctness up front, FaunaDB offers mainframe-like capabilities even in the chaos of a multi-cloud deployment. <a href=\"https://fauna.com/blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels\">Externally consistent, multi-partition distributed transactions</a> were widely believed to be impossible in a software-only solution until FaunaDB showed the way. We are proud to see our architecture validated in Kyle’s analysis.</p>\n<h2>Summary of Correctness Tests</h2>\n<p>Jepsen’s correctness tests exercised FaunaDB under a wide variety of fault conditions and administrative actions to simulate the unreliable operating conditions of the public cloud, including:</p>\n<ul><li>Individual process crashes</li><li>Individual process restarts</li><li>Rapid multi-process crashes</li><li>Rapid multi-process restarts</li><li>Small and large forward jumps in clock skew</li><li>Small and large backwards jumps in clock skew</li><li>Rapidly strobing clocks</li><li>While undergoing log topological change</li><li>While undergoing replica topological change</li></ul>\n<p>The testing validated that FaunaDB meets its expected isolation levels, avoids anomalies present in other databases, and maintains ACID semantics at all times. Additionally, the process of updating and running the Jepsen suite itself&nbsp;provided extensive verification of the general liveness, availability, and durability properties of FaunaDB, and let to numerous improvements.</p>\n<h2>Ongoing Work</h2>\n<p>FaunaDB <a href=\"https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol\">does not depend on clock synchronization</a> or a central clock oracle to maintain correctness, as the Jepsen analysis shows. Databases that rely on synchronized clocks can enter a state of ambiguous, irrecoverable data corruption if clocks skew beyond tolerance. FaunaDB never corrupts data, regardless of skew.</p>\n<p>FaunaDB versions 2.6 and earlier do partially rely on clocks to maintain liveness—the ability to process new transactions. Jepsen testing uncovered an issue where clock skews many seconds long, chaotically introduced across multiple nodes, can create cluster pauses until the skews are resolved. This operational scenario is rare in practice.</p>\n<p>However, as Kyle notes, FaunaDB’s architecture makes it possible to maintain complete availability and liveness even with extreme clock skew. This is an implementation detail of FaunaDB rather than an architectural limitation. We look forward to proving it in an upcoming release.</p>\n<h2>Conclusion</h2>\n<p>Since no amount of bug fixing can save the wrong architecture, we are gratified that the Jepsen report is highly complimentary of that of FaunaDB, and that the report validates that the issues found during testing were rapidly fixed:</p>\n<blockquote style=\"font-size: 1em; margin: 20px auto;\">The bugs that we’ve found appear to be implementation problems, and Fauna has shown a commitment to fixing these bugs as quickly as possible.</blockquote>\n<p>We look forward to working with Kyle and the Jepsen team in the future as we make further improvements to FaunaDB’s architecture and implementation. In the meantime, go read the <a href=\"https://www2.fauna.com/jepsen\">full report</a>!<br></p>",
        "blogCategory": [
            "8",
            "1461",
            "1531"
        ],
        "mainBlogImage": [
            "6966"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-02-26T08:10:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1676,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "82983b8a-c425-4bf9-8db1-87747b9663a1",
        "siteSettingsId": 1676,
        "fieldLayoutId": 4,
        "contentId": 1262,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Learning FQL, Part 1: FaunaDB Schema Objects",
        "slug": "learning-fql-part-1-faunadb-schema-objects",
        "uri": "blog/learning-fql-part-1-faunadb-schema-objects",
        "dateCreated": "2019-02-26T08:03:52-08:00",
        "dateUpdated": "2019-03-26T11:49:05-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/learning-fql-part-1-faunadb-schema-objects",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/learning-fql-part-1-faunadb-schema-objects",
        "isCommunityPost": false,
        "blogBodyText": "<p>This series introduces the FaunaDB Query Language, giving examples of patterns and APIs that you’ll interact with as you use the database. This is the first article in the series, and discusses the primitive objects that you’ll work with in FaunaDB. You can find a <a href=\"https://app.fauna.com/documentation/reference/glossary\">comprehensive glossary of FaunaDB terms</a> in the documentation. This article focuses on introducing you to the API objects that you’ll work with most.</p>\r\n<p>We’ll explore the FaunaDB data model in the order that you are likely to encounter it. Databases, Classes (a.k.a. collections), Instances (a.k.a. documents), and Indexes are used by most applications. This article also describes the steps that you will follow to create and query these database objects.</p>\r\n<p>There are additional objects we'll cover in a future article. Access keys are how you control which processes and users can interact with which databases and documents.&nbsp;User-defined functions are important for applications with additional security constraints. Authorization Tokens are useful for providing data APIs to mobile and web clients. These advanced features are important for certain applications, so we’ll cover them in another blog post. This post focuses on the most common objects you’ll encounter.</p>\r\n<h2>Shell Connection</h2>\r\n<p>After you set up a FaunaDB cluster, or create a FaunaDB Cloud account, you can use <a href=\"https://app.fauna.com/documentation/gettingstarted\">Fauna Shell to connect to your account and create databases</a>. </p>\r\n<pre>fauna cloud-login \r\nfauna create-database my-database \r\nfauna shell my-database</pre>\r\n<h2>Databases</h2>\r\n<p>Databases contain the structures that your application works with. FaunaDB databases can also contain other databases, giving you the ability to organize a tree of nested databases, each of which inherits access control and quality-of-service prioritization from its parent. Read more about <a href=\"https://fauna.com/blog/secure-hierarchical-multi-tenancy-patterns\">arranging a tree of databases for shared services, SaaS, DBaaS, or multi-tenant workloads</a> to control both security and priority according to your business needs. To avoid confusion and to enable fine-grained security,<strong> your root database should only contain other databases</strong>. That way, you can use key secrets which correspond to particular applications, instead of configuring your code with root access.</p>\r\n<p>You’ve already created a database in the shell command above, and connected to it. You can create more databases inside my-database by issuing a query like:</p>\r\n<pre>CreateDatabase({name : \"nested-inside-my-database\"}) </pre>\r\n<p>This creates &nbsp;a new database nested inside of my-database. If you wanted to create another sibling database, located at the root of your account, you could either use the command line interface as you did above to create my-database, or you could launch fauna shell without a database name, in which case it connects to the root context. With a root shell connection, you can issue another query like the above to create a peer database:</p>\r\n<pre>CreateDatabase({name : \"another-top-level-database\"}) </pre>\r\n<p>You can see example database creation queries in more programming languages in <a href=\"https://app.fauna.com/documentation/crud-examples#create-a-database\">the CRUD examples doc</a>. </p>\r\n<p>When creating a database, you have the option to specify an optional priority. Should the underlying FaunaDB cluster become resource-constrained, due to either a hardware failure or a traffic spike, the highest priority databases will be impacted last. This allows you to run development and production workloads on the same cluster, without worry that less important queries can impact performance in production.</p>\r\n<h2>Classes (a.k.a. Collections)</h2>\r\n<p>Related documents in FaunaDB are stored within classes (also known as collections), which are similar to tables in relational databases, except that the different items in the class are not all required to have the same fields. Typically, classes correspond to types in your application, for example, blog posts, authors, comments, shopping carts, line items, and invoices. Creating a class is easy as you don’t have to specify constraints or field names. In Fauna Shell, the same query looks like this:</p>\r\n<pre>CreateClass({ name: \"spells\" })</pre>\r\n<p>You can <a href=\"https://app.fauna.com/documentation/crud-examples#create-a-class\">see this query in other languages</a> in the docs. </p>\r\n<p>Classes are the container for documents (also known as instances). Classes are also the scope for indexes. Since documents contain our data, we can create some documents first, and then define an index on the class so we can query them. Alternatively, we could create an index first, and then add data to the class—it’s just a matter of taste.</p>\r\n<h2>Documents (a.k.a. Instances)</h2>\r\n<p>Documents contain your application data, which can be stored in fields with types such as string, number, boolean, date, null, etc. Data can also be structured into objects and arrays, which can be nested. Anything that can be represented in JSON can be stored in FaunaDB, as well as richer data types. Here’s an example query (formatted for Fauna Shell) that creates a document:</p>\r\n<pre>Create(Class(\"spells\"), {        \r\n    data : {\r\n        title : \"Invisibility\",    \r\n        ingredients : [\"cauldron\", \"crystal\", \"newt\", \"stardust\"],                           description : \"...\" }\r\n})</pre>\r\n<p>This document contains a title, ingredients, and a description. Note that these fields are all presented within the <tt>data</tt> top level field. When retrieving a document, you’ll see other top level fields like <tt>ref</tt> and <tt>ts</tt> which track metadata. These fields will also be in the response returned from the Create query above. The most important thing to know is that the <tt>ref</tt> is how you can load the document in other queries. For instance, if you index the documents by ingredients, the <tt>ref</tt> for the above document would appear in the result set for “stardust,” and from there you can load the original document. Additionally, if you want to link to this document from another, you can store this document’s ref somewhere in the other document’s data field.</p>\r\n<p>Stay tuned for the next post in this series to learn about creating, reading, updating, and deleting documents.</p>\r\n<h2>Indexes</h2>\r\n<p>FaunaDB’s indexes give you powerful options when it comes to finding and sorting your data, as well as reading historical changes. Documents can be indexed by <strong>term</strong> for scalable lookup. To query an index by term, you must provide an exact match, and multiple documents can be found in the same term. Easy examples are tags, or the ingredients in the spell document above. Looking up all the spells that require “stardust” would be as simple as using stardust as a term in an index query. Term indexes use O(1) lookups, so they stay fast no matter how many distinct terms are included in your set, making them good for usernames, product ids, and even <a href=\"https://app.fauna.com/documentation/reference/queryapi#ngram\">ngrams for raw text</a> search.</p>\r\n<p>Documents may also be sorted by a <strong>value</strong> within a particular term. For instance, the documents within each tag or ingredient set can be sorted by title or publication date. Pagination across the sorted values is designed to be efficient. These indexes are what you would use to query for recent articles by a particular author, or the contents of a user’s inbox sorted by arrival time. If your query can be satisfied by exact lookup instead of range queries, you are likely better off working with terms. For example, if you want to load users by ZIP code, you are better off indexing ZIP code as a term than as a value.</p>\r\n<p>Here is an index definition allowing us to list all of the spells with a given ingredient, sorted by their title:</p>\r\n<pre>CreateIndex({ \r\n    name: \"spells_by_ingredient\",\r\n    source: Class(\"spells\"),\r\n    terms: [{ field: [\"data\", \"ingredients\"] }],\r\n    values: [{ field: [\"data\", \"title\"] }] \r\n})</pre>\r\n<p>To query this for all the spells using stardust, we use the <tt>Match</tt> function to find the corresponding term entry, and then <tt>Paginate</tt> over the result set. Here is the query formatted for Fauna Shell:</p>\r\n<pre>Paginate(Match(Index(\"spells_by_ingredient\"), \"stardust\"))</pre>\r\n<p>Most indexes define a term, which limits the amount of data processed by the <tt>Match</tt> function. If your index does not define a term, then all documents are indexed under the null term, and sorted according to their ref, which is used as the default value. This can be useful in development as it makes listing all members of a class easy, but it can be a performance hog in production where, generally speaking, you are better off using terms in your indexes, to prevent any one set of values growing too large.</p>\r\n<h2>Conclusion</h2>\r\n<p>These objects (databases, classes, documents, and indexes) are involved in &nbsp;the bulk of your work with FaunaDB. If you’d like to learn more about them, the <a href=\"https://app.fauna.com/documentation/reference/instances\">FaunaDB reference guide</a> gives you more detail. The next post in this series will dive into working with documents in detail. Future articles in this series will cover data modeling questions, bulk operations, index queries, working with temporal events, and pagination.</p>\r\n<p></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1787",
        "postDate": "2019-02-21T11:13:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1673,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9fc3032d-112e-4499-9417-72e56976f29e",
        "siteSettingsId": 1673,
        "fieldLayoutId": 4,
        "contentId": 1259,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Serializability vs “Strict” Serializability: The Dirty Secret of Database Isolation Levels",
        "slug": "serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels",
        "uri": "blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels",
        "dateCreated": "2019-02-20T14:32:11-08:00",
        "dateUpdated": "2019-08-19T21:01:52-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/serializability-vs-strict-serializability-the-dirty-secret-of-database-isolation-levels",
        "isCommunityPost": false,
        "blogBodyText": "<p><br>For many years “serializability” was referred to as the “<a href=\"https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf\">gold standard</a>” of database isolation levels. It was the highest isolation level offered in the vast majority of commercial database systems (some highly widely-deployed systems could <a href=\"https://blog.dbi-services.com/oracle-serializable-is-not-serializable/\">not even offer</a> an isolation level as high as serializability). In this post, we show how serializability is not and never was a “gold standard” for database systems. In fact, strict serializability has always been the gold standard. As we will see, because the implementation of serializability in legacy database systems has provided key strict guarantees, the difference between \"serializability\" and \"strict serializability\" has been mostly ignored. However, in the modern world of cloud-centric distributed systems, the difference in guarantees is significant.</p>\r\n<p>This post first explains the concept of isolation in database systems&nbsp;and then illustrates the important differences between these two isolation levels, and correctness bugs introduced by “only” serializable systems that application programmers need to be aware of.</p>\r\n<h1>Background: What is “Isolation”</h1>\r\n<p>Assume we have a ticket sales application allows people to purchase tickets to a limited supply event. The application sends transactions of the following form to the data store (when a customer attempts to purchase a ticket):</p>\r\n<pre> IF inventory &gt; 0 THEN\r\n     Inventory = inventory -1;\r\n     Process(order)\r\n     RETURN SUCCESS\r\nELSE\r\n     RETURN FAIL\r\n</pre>\r\n<p>If the data store supports ACID transactions, the above code will be processed atomically --- either the entire order will be processed and the inventory decremented, or neither. But it is impossible for anything else to happen.</p>\r\n<p>If there is only one transaction running in the system, it is clear that there will not be any problems running the above code. If there is enough inventory, the order will be processed. Otherwise, it will not.</p>\r\n<p>Problems typically only occur under concurrency: multiple customers attempt to purchase a ticket at the same time. If there are more customers attempting to buy tickets than there is inventory, the ideal final outcome is that all of the inventory will be sold, and no more. Indeed, overselling inventory for limited supply events make even cause legal liability for the application developers.</p>\r\n<p>If the above code is allowed to run without system-level guarantees, there is a clear “race condition” in our code. Two parallel threads may run the first line at the same time and both see that inventory is “1”. Both threads thus pass the IF condition and process the order, which results in the negative outcome of an oversale of the inventory.</p>\r\n<p>The system level guarantees that prevent these negative outcomes in the face of concurrent requests fall under the I of ACID ---- “Isolation”.</p>\r\n<p>The gold standard in isolation is “serializability”. A system that guarantees serializability is able to process transactions concurrently, but guarantees that the final result is equivalent to what would have happened if each transaction was processed individually, one after other (as if there were no concurrency). This is an incredibly powerful guarantee that has withstood the test of time (5 decades) in terms of enabling robust and bug-free applications to be built on top of it.</p>\r\n<p>What makes serializable isolation so powerful is that the application developer doesn’t have to reason about concurrency at all. The developer just has to focus on the correctness of individual transactions in isolation. As long as each individual transaction cannot violate the semantics of an application, the developer is ensured that running many of them concurrently will also not violate the semantics of the application. In our example, we just have to ensure the correctness of the 6 lines of code we showed above. As long as they are correct when processed over any starting state of the database, the application will remain correct in the face of concurrency.</p>\r\n<h1>The Dirty Secret of Serializability: It’s Not as \"Safe\" as it Seems</h1>\r\n<p>Despite the incredible power of serializability, there do exist stronger guarantees, and databases that guarantee “only” serializability are actually prone to other types of bugs that are only tangentially related to concurrency.</p>\r\n<p>The first limitation of serializability is that it does not constrain how the equivalent serial order of transactions is picked. Given a set of concurrently executing transactions, the system guarantees that they will be processed equivalently to a serial order, but it doesn’t guarantee any particular serial order. As a result, two replicas that are given an identical set of concurrent transactions to process may end up in very different final states, because, they chose to process the transactions in different equivalent serial orders. Therefore, replication of databases that guarantee “only” serializability cannot occur by simply replicating the input and having each replica process the input simultaneously. Instead, one replica must process the workload first, and a detailed set of state changes generated from that initial processing is replicated (thereby increasing the amount of data that must be sent over the network, and causing other replicas to lag behind the primary).</p>\r\n<p>The second (but related) limitation of serializability is that the serial order that a serializable system picks to be equivalent to doesn’t have to be at all related to the order of transactions that were submitted to the system. A transaction Y that was submitted after transaction X may still be processed in an equivalent serial order with Y before X. This is true even if Y was submitted many weeks after X completed --- it is still theoretically possible for a serializable system to bring Y back in time, and process it over a state of the system before X existed (as long as no committed transaction read the same data that Y writes). Technically speaking, every read-only transaction could return the empty-set (which was the initial state of the database) and not be in violation of the serializability guarantee --- this is legal since the system can bring read-only transactions “back in time” and get a slot in the serial order before any transactions that write to the database.</p>\r\n<p>It is unlikely any user would use a database that always returned the empty-set for every read-only query. So how did serializability become the “gold standard”?</p>\r\n<p>In the old days, databases would only run on a single machine. It was therefore trivial for database systems to make stronger correctness guarantees than serializability. In fact, it was so trivial that vendors never bothered advertising this stronger guarantee. Nonetheless, this stronger guarantee has a name:  <a href=\"http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf\" target=\"-blank\">Strict Serializability</a>.</p>\r\n<h1>Strict Serializability is Much Safer</h1>\r\n<p>Strict serializability adds a simple extra constraint on top of serializability. If transaction Y starts after transaction X completes (note that this means that X and Y are, by definition, not concurrent), then a system that guarantees strict serializability guarantees both (1) final state is equivalent to processing transactions in a serial order and (2) X must be before Y in that serial order. Therefore, if I were to submit a transaction to a system that guarantees strict serializability, I know that any reads of that transaction will reflect the state of the database resulting from committed transactions (at least) up to the point in time when my transaction was submitted. The transaction may not see writes from concurrently submitted transactions, but at least it will see all the writes that completed before it began.</p>\r\n<p>When the database sits on a single machine, guaranteeing strict serializability usually involves no increased effort relative to guaranteeing serializability. Before committing a transaction, every existing system (the one exception being Dr. Daniel Abadi’s <a href=\"http://www.cs.umd.edu/~abadi/papers/lazy-xacts.pdf\">“lazy transactions” paper </a>from 2014) must perform the writes involved in that transaction. For a transaction that comes along later, it is usually more work to ignore these writes (and not see them) than it is to see them. Therefore, just about every serializable system running on a single machine also guaranteed strict serializability. This resulted in the state of the industry where documentation of database systems would indicate that they guarantee “serializability” but in reality, they were guaranteeing “strict serializability”.</p>\r\n<p>The “strict serializability” guarantee eliminates the possibility of the system returning stale/empty data that we discussed in the previous section. [It does not, however, eliminate the problem of replica divergence that we discussed in that section. We will come back to that issue in a future post.]</p>\r\n<p>To summarize thus far: in reality, “strict serializability” is the gold standard isolation level in database systems. However, as long as most databases were running on a single machine, there was no perceivable difference in systems that guaranteed “serializability” and systems that guaranteed “strict serializability”. Therefore, colloquially, “serializability” became known as the gold standard and the semantic inaccuracy of this colloquialism never mattered.</p>\r\n<h1>Distributed Systems Make Serializability&nbsp;Dangerous</h1>\r\n<p>In a distributed system, the jump from serializability to strict serializability is no longer trivial. Take the simple example of our ticket sales application. Let’s say that the ticket inventory is <strong>replicated</strong> across two machines --- A and B ---- both of which are allowed to process transactions. A customer issues a transaction to machine A that decrements the inventory. After this transaction completes, a different transaction is issued to machine B that reads the current inventory. On a single machine, the second transaction would certainly read the write of the first transaction. But now that they are running on separate machines, if the system does not guarantee <strong>consistent reads</strong> across machines, it is very much possible that the second transaction would return a stale value (e.g. this could happen if replication was asynchronous and the write from the first transaction has not yet been replicated from A to B). This stale read would not be a violation of serializability. It is equivalent to a serial order of the second transaction prior to the first. But since the second transaction&nbsp;was submitted after the first one completed, it is certainly a violation of strict serializability.</p>\r\n<p>Therefore, when data is replicated across machines, there is a huge and practical difference between serializability and strict serializability. It is very important for the end user to be aware of this difference before selecting a database system to use. Distributed systems that fail to guarantee strict serializability are prone to many different types of bugs.</p>\r\n<p>The difference between serializability and strict serializability is much more than the “stale read” bug from my previous example. Let’s look at another bug that may emerge, called a “causal reverse”.</p>\r\n<p>A bank gives you “free checking” if the total amount of money you have deposited with them (across all your accounts) exceeds $5000. Alice has exactly $5000 and needs to write a check to her child’s babysitter, so she transfers some additional money into her savings account. After getting the assurance that the transfer was successful and the money is in her savings account, she writes the check from her checking account. When she gets her statement, she sees the penalty for not keeping her balance across all accounts above $5000.</p>\r\n<p>So what happened? A violation of strict serializability. The two transactions --- the addition to her savings account and subtraction from her checking account got reordered so that the checking account subtraction happened before the savings account addition, even though the subtraction was submitted after the addition completed. Such reordering is totally possible in a system that only guarantees serializability, but impossible in a system that guarantees strict serializability. Transaction reordering of this kind is very common in this type of example where the transactions access disjoint data (checking balance vs savings balance) if the disjoint data are located on different machines in a distributed system. Once the transactions got reordered, Alice’s balance across both accounts temporarily dipped below $5000, which caused the penalty to be assessed. If the bank had built their app on top of a system that guarantees strict serializability, they would not have to deal with the angry phone call from Alice.</p>\r\n<h1>FaunaDB: Strict Serializability for All Transactions</h1>\r\n<p>There are very few distributed database systems that claim to guarantee an isolation level of strict serializability. Even fewer can actually achieve this guarantee with no small print. FaunaDB is one of those systems.</p>\r\n<p>FaunaDB now supports the configuration to make all transactions --- both read-write and read-only transactions --- run with strictly serializable isolation.</p>\r\n<p>FaunaDB’s guarantee of strict serializability is also the most robust on the market. Since the definition of strict serializability is based on real-time (as we discussed above: transactions submitted after the completion of other transactions in real time must respect their respective ordering), other vendors use local clocks on the different machines in the distributed system in order to enforce the guarantee. Unfortunately, approaches based on this method are only as good as the time synchronization protocol across machines and are prone to corner case violations when the synchronization algorithm temporarily exceeds assumptions on maximum clock skew. Much ink has been spilled how FaunaDB is able to <a href=\"https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol\">guarantee consistency without clock synchronization</a>, and why this approach is <a href=\"https://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">better than the approach taken by Spanner and Spanner derivative systems</a>. FaunaDB’s approach to achieving strict serializability also spans to multi-region environments and is similar to how it achieves consistency, through its scalable, distributed unified log. We will go into deeper technical detail in a future post.</p>\r\n<p>The bottom line is that if FaunaDB is configured to be strictly serializable for all transactions, then stale reads will never happen. Every read issued by an application is guaranteed to reflect the writes of transactions that have completed prior to when the read was issued. This is true, no matter how much skew exists across the local clocks of the machines in the FaunaDB deployment. Even if one machine thinks that the year is currently 1492, any reads served by that machine will reflect all committed writes. Furthermore, transactions in FaunaDB never get reordered ahead of completed transactions --- even if they touch completely disjoint sets of data that are located on different machines. Therefore, the bank transaction reordering problem we saw earlier is guaranteed to never occur in FaunaDB.</p>\r\n<p>FaunaDB’s support for strict serializability thus makes it the safest distributed (scalable) database system on that market.<br></p>",
        "blogCategory": [
            "8",
            "1462",
            "1465"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-02-13T15:35:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1671,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "fb31f273-0fb1-4d9e-8ec4-d2e7d8867574",
        "siteSettingsId": 1671,
        "fieldLayoutId": 4,
        "contentId": 1257,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Announcing the New Cloud Console",
        "slug": "announcing-the-new-cloud-console",
        "uri": "blog/announcing-the-new-cloud-console",
        "dateCreated": "2019-02-12T19:47:46-08:00",
        "dateUpdated": "2019-06-18T16:10:58-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-new-cloud-console",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-new-cloud-console",
        "isCommunityPost": false,
        "blogBodyText": "<p>The FaunaDB team is pleased to announce the <a href=\"https://app.fauna.com/documentation/releasenotes/console-1.0.0\">availability</a> of the <a href=\"https://dashboard.fauna.com/\">Cloud Console</a>, a tool that significantly improves the experience of interacting with database objects in FaunaDB Cloud. The new Cloud Console essentially comes with two distinct elements: an authentication service and a web application written in React with Redux. The Cloud Console is built on a robust framework and it replaces the prior dashboard that was limited to basic CRUD functionality. &nbsp;The following section summarizes some of the highlights of this Cloud Console release.</p>\r\n<h3>Integration of Github for User Authentication</h3>\r\n<p>Asking for basic user information like name, email, etc. during registration may look trivial, but still slows down developer productivity. With the new authentication system, you can now use your GitHub credentials to sign up with FaunaDB Cloud and alleviate the need to remember yet another password. </p>\r\n<figure><img src=\"{asset:1667:url}\" data-image=\"1667\"></figure>\r\n<p>We also plan to expand the signup integration with Google, Twitter, Netlify, and other services in the near future.</p>\r\n<h3>New usage-based metrics reporting</h3>\r\n<p>Last August, we announced moving to a utility-based <a href=\"https://fauna.com/pricing\">pricing model</a> built on the following four usage attributes: </p>\r\n<ol><li>Read operations</li><li>Write operations</li><li>Outbound data transfer</li><li>Storage consumed</li></ol>\r\n<p>The Cloud Console marks the transition to this new pricing/usage model. The new reporting allows database users to not only track usage at the account level, but to also drill down to verify the usage of an individual child database. This makes tracking usage at a tenant level very simple. For your SaaS applications, you can now track each individual tenant down to the penny. </p>\r\n<figure><img src=\"{asset:1670:url}\" data-image=\"1670\"></figure>\r\n<h3>Improved Usability </h3>\r\n<p>The overall usability on object creation, data manipulation, and data retrieval has greatly improved. Unlike the previous dashboard, the Cloud Console allows its users to manipulate each document in its native form. Users will really like this close integration where they can create classes, indexes, and keys, all in few mouse clicks.<br></p>\r\n<figure><img src=\"{asset:1668:url}\" data-image=\"1668\"></figure>\r\n<h3>Upgrade your Cloud Plan</h3>\r\n<p>The FaunaDB Cloud Free plan is free forever, provided that you do not cross the predefined daily limits. The daily limits are sufficient to run a simple application with a small to medium-sized user base. For serious applications, it is recommended that you use the Serverless plan. The Serverless plan uses utility-based pricing on a pay-per-use basis. You are only charged for the amount of usage beyond the free daily limits. You can start with the Free plan and then use our self-service upgrade to choose the pricing plan that suits your needs as your business grows. </p>\r\n<figure><img src=\"{asset:1669:url}\" data-image=\"1669\"></figure>\r\n<p>For a more in-depth overview watch our getting started video on the cloud console.&nbsp;It covers all the interesting new features that we have packed in this new release.</p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/S8B7B3q8X2Q\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<h3>Roadmap</h3>\r\n<p>While we are very happy to announce the general availability of the Cloud Console, we would also like to provide a glimpse of what is slated for the next few releases. We are going to make the Cloud Console available for on-premises usage. That is, you will be able to download the binaries and run the Cloud Console code locally in your data center. We will also add direct data manipulation capabilities, bringing the power of Fauna Shell to the Cloud Console. You will be able to write and save user-defined functions directly through the Cloud Console. We also plan to bring bulk data import/export capabilities to the Cloud Console in the near future.</p>\r\n<p>We plan to introduce new features to the cloud console&nbsp;at a regular cadence. If there are some you’d like to see, please reach me at&nbsp;<a href=\"mailto:product@fauna.com\">product@fauna.com</a>. I’d love to talk to&nbsp;you.</p>",
        "blogCategory": [
            "8",
            "1530"
        ],
        "mainBlogImage": [
            "1672"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-02-08T17:48:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1664,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e76fada1-da21-4d94-98ed-be57044255b9",
        "siteSettingsId": 1664,
        "fieldLayoutId": 4,
        "contentId": 1250,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Preventing Lost Paychecks: Lessons from the Wells Fargo Datacenter Failure",
        "slug": "preventing-lost-paychecks-lessons-from-the-wells-fargo-datacenter-failure",
        "uri": "blog/preventing-lost-paychecks-lessons-from-the-wells-fargo-datacenter-failure",
        "dateCreated": "2019-02-08T17:46:16-08:00",
        "dateUpdated": "2019-08-19T21:11:05-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/preventing-lost-paychecks-lessons-from-the-wells-fargo-datacenter-failure",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/preventing-lost-paychecks-lessons-from-the-wells-fargo-datacenter-failure",
        "isCommunityPost": false,
        "blogBodyText": "<p>Recently, Wells Fargo was in the <a href=\"https://np.reddit.com/r/sysadmin/comments/ao4g2y/wells_fargo_is_down_declining_transactions_and_no/\">news</a> because of a datacenter outage. Various reports suggested that they had power interruption and fires within the facility. The outage impacted debit card transactions and online banking. However, this was just day one. News from day two revealed that people were reporting <a href=\"https://www.usatoday.com/story/money/2019/02/08/wells-fargo-outage-customers-report-issues-paychecks-deposits/2810562002/\">missing</a> paychecks. Direct deposits were not showing up in their accounts.</p>\r\n<p>If you are a bank, this scenario is the one you meticulously plan to prevent because the damage to your reputation is far more costly than the damage caused by any individual event. You simply cannot lose your customer’s hard earned money!</p>\r\n<p>If you’re wondering how a bank in this day and age can still be vulnerable to multi-day outages and, more importantly, how they can lose data, we think the answer may lie in often the most overlooked component of IT infrastructure—the database.</p>\r\n<p>It is the database behind the banking system that holds your paycheck information. Once written to the database, that information should never be lost, no matter what the disruption. But as this news suggests, today’s banking systems remain vulnerable to data loss, and I blame the database.</p>\r\n<h2>Databases are at Odds with Resiliency</h2>\r\n<p>The industry as a whole has been making strides to build resilient systems. Private datacenters and public cloud providers with multiple availability zones offer the options to create multi-node (or distributed) deployments. Application architectures have also evolved at even faster pace to take advantage of these distributed options. In theory, should a zone erupt in fire, your systems should work, almost, until you hit the database.</p>\r\n<p>Unfortunately, most modern databases still make resilience extremely hard to achieve. Legacy transactional databases often employ a centralized architecture. Failover systems are expensive to implement (often requiring specialized hardware and networks) and even then, come with many lines of fineprint. They are also hard to configure and maintain, even within the same datacenter, much less in multi-datacenter scenarios. Moreover, they simply weren’t designed for the Cloud. If you’re a bank adopting Cloud, you’re out of luck—you’re still bottlenecked on the database.<br></p>\r\n<h2>Replatforming Your Information Systems</h2>\r\n<p>New global transactional databases like FaunaDB and Google Spanner offer hope for building new systems that are more resilient to disasters. They are designed to operate as globally distributed clusters while offering the same data consistency and isolation guarantees of Oracle and the RDBMS movement. Here are some capabilities to look for in your new database.</p>\r\n<h3>Effortless Highly Available Architecture</h3>\r\n<p>Look for databases that are built to operate distributed from the ground up. Whether you host the database in your own datacenters, leverage public clouds, or some hybrid mechanism, employ a database that was designed for dispersion of nodes with ease. No matter how you partition your compute resources, no matter your virtualization or container strategy, your database must work with it effortlessly. It should do so without requiring explicit manual work to replicate or partition data, adding additional hardware or software, or spending excessive man hours in configuration.</p>\r\n<h3>Multi-region ACID Transactions</h3>\r\n<p>Watch out for distributed databases that offer “single-region” or “multi-document” ACID. Constraining ACID in any form is not conforming to ACID at all. You’re back in the same scenario—a regional failure could cause data loss.</p>\r\n<p>To ensure consistency and durability, it isn’t sufficient that your database merely support the ACID primitives of RDBMS systems. More importantly, your database must offer these capabilities in highly partitioned environments to allow for writes to be committed across the entire cluster in real-time. Such “multi-region” ACID capabilities ensure that your applications should not have to be built with the deployment topology in mind.</p>\r\n<p>No matter how you deploy your database nodes, your transactions should just work. Should portions of your infrastructure fail, the database as a whole should continue to offer the same ACID guarantees</p>\r\n<h3>Multi-cloud Deployments</h3>\r\n<p>If you plan to leverage public cloud options, relying on a multi-vendor strategy greatly diminishes your disaster risk. Probabilistically speaking, there is a much lower chance that both or all three of your cloud vendors will fail across all their availability zones simultaneously. So pick a database that gives you the option to operate across multiple clouds without any retrofitting to your application or infrastructure. Specifically, ensure that your database is not susceptible to divergence in underlying hardware and network characteristics, or clock skew.</p>\r\n<h3>Resistance to Chaos</h3>\r\n<p>All environments have inherent chaos, and moving to a distributed environment increases your exposure to such chaos. Individual nodes can go out, daily devops procedures can shutdown or restart your VMs, and other minor disruptions and intermittent noise in the network all have the potential to corrupt the state of your database. As long as the database cluster has a quorum of nodes in operational condition, it must continue to operate without impacting your applications. Your database must be resilient to chaos.</p>\r\n<h3>Easy Distributed Backup & Recovery</h3>\r\n<p>The administrative overhead of operating distributed systems often becomes prohibitive in adopting a distributed approach to your data. Therefore, one of the requirements to adopt such a distributed database is to ensure that it is easy to make node backups, and that your node recovery time is within acceptable bounds. More importantly, your database should allow for easy automation of daily tasks using your existing devops frameworks.</p>\r\n<h2>FaunaDB Enables Resilience</h2>\r\n<p>At its core,&nbsp;FaunaDB is a globally distributed database built to solve the challenges of modern-day planet-scale web and mobile applications. Inspired by <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>, FaunaDB is a CP database that is also highly available. As a developer,&nbsp;you can use FaunaDB (Serverless Cloud), the world’s first multi-cloud serverless database, with nodes on both AWS and GCP. Partners can opt for FaunaDB&nbsp;Managed Serverless, or operate their own serverless data clouds powered by FaunaDB.&nbsp;</p>\r\n<p>Some of the salient points of FaunaDB’s architecture include:</p>\r\n<ul><li><strong>Globally Distributed, Multi-Cloud</strong>: FaunaDB is architected to achieve global distribution of data to place it close to your users. Completely independent from underlying infrastructure to execute distributed transactions, FaunaDB operates multi-cloud environments to deliver the multiple levels of data redundancy and low-latency access to data. You can read more about its patent-pending platform-agnostic transaction protocol in this <a href=\"https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol\">article</a>.</li></ul>\r\n<ul><li><strong>100% ACID & Strict Serializability</strong>: FaunaDB offers multi-region ACID transactions and provides the strongest levels of transaction isolation in its class. Data once committed is never lost. Be they paycheck deposits or hotel reservations, records are durable irrespective of your partitioning strategy. Concurrent reads and writes adhere to the order in which they were accepted, no matter the chaos conditions such as failure or clock skew. This <a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">article</a> by Dr. Daniel Abadi compares Calvin to approaches like Spanner and other derivatives of Spanner.</li></ul>\r\n<ul><li><strong>High Availability & Scalability</strong>: Partners operating FaunaDB will find that&nbsp;FaunaDB scales horizontally on commodity hardware. Deploy a new node, and go. There are no extra hardware or network parameters to tweak. Check out this <a href=\"https://www.youtube.com/watch?v=_Xujgy-agFM\">demo</a> to see FaunaDB transactions under a datacenter failure.</li></ul>\r\n<ul><li><strong>Effortless Ops</strong>: While users of FaunaDB (Serverless Cloud) never have to worry about operations, FaunaDB is&nbsp;simple to roll out in partner environments. We use transactional cluster configuration commands to ensure that you have to perform minimal manual housekeeping. The database should not be an impediment to operations. It should blend in and just work. You have your business to worry about. With each release, our objective is to handle operational automation within the database itself. Here’s <a href=\"https://youtu.be/zD89al4kMRA\">how</a> we think of operational simplicity for the modern distributed datacenter.</li></ul>\r\n<h2>What Can We Learn from Wells? &nbsp;</h2>\r\n<p>If you’re a bank or a financial services company, or any business that operates financial transactions (payments, billing, reservations, e-commerce etc.), upgrading your data-plane to support your disaster scenarios is no longer a luxury. To thrive in a digital economy, and to compete in the digital business era, you must transform your systems to withstand unpredictable situations. While downtime is acceptable, data loss is possibly the worse outcome. You lose not only revenue but also your reputation. <br><br></p>",
        "blogCategory": [
            "3"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-02-06T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1652,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "89e62f5a-7e4a-4a5c-8197-de86acdedaee",
        "siteSettingsId": 1652,
        "fieldLayoutId": 4,
        "contentId": 1238,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FQL: Boosting Developer Productivity with String Functions",
        "slug": "boosting-developer-productivity-with-string-functions",
        "uri": "blog/boosting-developer-productivity-with-string-functions",
        "dateCreated": "2019-01-30T13:01:04-08:00",
        "dateUpdated": "2019-06-18T16:11:06-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/boosting-developer-productivity-with-string-functions",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/boosting-developer-productivity-with-string-functions",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB <a href=\"https://app.fauna.com/documentation/releasenotes/2.6.0\">2.6</a> introduces new capabilities for string and math functions in Fauna Query Language (FQL). String functions are built-in database functions for string manipulations that developers can leverage out of the box in their code. As a result, developers can use these functions within the database, much closer to the data, instead of on the client side. This delivers gains in both developer productivity and application performance.</p>\r\n<h2>Design Considerations in FQL String Functions</h2>\r\n<p>While traditional databases assume certain defaults, programming languages have evolved to a certain set of standards in how string functions are used by developers. Since one of the goals for FQL is to closely mimic a developer’s programming experience, we aimed to comply with three key best practices.</p>\r\n<h3>1. String Encoding</h3>\r\n<ol></ol>\r\n<p>Strings are UTF-8 encoded in Fauna, so we can include any properly encoded data in them.</p>\r\n<p>नमस्ते &nbsp;Здравствуйте Hello - are all valid values and each string function within Fauna should use the same ground rules to calculate the length or position of a specific character.</p>\r\n<p>From the user’s point of view, the minimal unit of a string is a character. \"A\" is a character, and so is \"€\". The user expects the same behavior irrespective of whether a string is composed of single-byte characters, multi-byte characters, or a mixture of both.</p>\r\n<p>For example:</p>\r\n<p>SubString('Hello World',3,2) should return 'lo' &nbsp;and similarly SubString('Здравствуйте',2,2) &nbsp;should return ра. In essence, the user always interacts with a string character by character irrespective of the characterset that translates to the behavior of calculating lengths using unicode complete characters, or code points when it comes to multi-byte characters.</p>\r\n<p>In the future, we plan to provide a separate set of string functions that will allow users to manipulate strings as bytes.</p>\r\n<h3>2. Position of Character</h3>\r\n<p>There is a difference between how most databases assign (or calculate) the position of a character and how programming languages treat positions of characters. For most databases, like Oracle, the substring function works like the following: SUBSTR(String, starting_position, string_length), which translates to</p>\r\n<pre>SQL&gt; select substr('Hello World',3,5) from dual;\r\nSUBST\r\n-----\r\nllo W</pre>\r\n<p>A programming language like Python also requires the start_position and the end_position:<br></p>\r\n<pre>&gt;&gt;&gt; a=\"Hello World\"\r\n&gt;&gt;&gt; a[3:8]\r\n'lo Wo'</pre>\r\n<p>As we can see, the output of the above two is different. Python, and other programming languages assign the first character a position of ‘0’, whereas databases treat the first character as 1.</p>\r\n<p>Since FaunaDB is looking to integrate with most programming languages and make the developer experience as seamlessly as possible, we decided to use Zero as the starting index position.</p>\r\n<h3>3. Sign of the Starting position</h3>\r\n<p>For functions that substring or find a specific character in a given string when the starting position is marked as negative, we should look at the string in the reverse direction. In that case, the last digit will be assigned a position of 0.</p>\r\n<h2>String Functions in FaunaDB 2.6</h2>\r\n<p>In this section, I will highlight a few interesting ones.</p>\r\n<table><tbody><tr><td><p>Function Name</p></td><td><p>Description</p></td><td><p>Example</p></td></tr><tr><td><p>CaseFold</p></td><td><p>The casefold function returns a normalized string</p></td><td data-gramm_id=\"6e658085-f90d-bfe1-3506-e2c738df5195\" data-gramm=\"true\" spellcheck=\"false\" data-gramm_editor=\"true\"><p>test&gt; Casefold(\"San FrancIsCo\")<br>&gt; 'san francisco'</p></td></tr><tr><td><p>FindStr</p></td><td data-gramm_id=\"08059106-382c-6a78-e1f1-eb0e5d0894d6\" data-gramm=\"true\" spellcheck=\"false\" data-gramm_editor=\"true\"><p>The FindStr function returns the offset position of a string in another string, or -1 if the string is not found.</p></td><td data-gramm_id=\"285cf883-e546-1081-ab16-5e204bdbdbf7\" data-gramm=\"true\" spellcheck=\"false\" data-gramm_editor=\"true\"><p></p><p>test&gt; FindStr(\"San#Francisco\",\"#\")</p>\r\n<p>&gt; 3</p>\r\n<p>Note: The index assumes a starting position of 0.</p><p></p></td></tr><tr><td><p>FindStrRegex</p></td><td><p>The FindStrRegex function returns an array of objects which contains the start position, the end position, and the data which was matched. This function supports regular expressions unlike the FindStr function</p></td><td><p>test&gt; FindStrRegex(\"San Francisco\",\"an\")</p><p>[ { start: 1, end: 2, data: 'an' },</p><p>&nbsp;{ start: 6, end: 7, data: 'an' } ]</p><br></td></tr><tr><td><p>Length</p></td><td><p>The length function returns the length of a string.</p></td><td><p>test&gt; Length(\"San Francisco\")</p><p>&gt; 13</p><br></td></tr><tr><td><p>NGram</p></td><td><p>The NGram function tokenizes the input terms into n-grams of the given sizes.</p><p>Any non-string terms are returned unmodified.</p></td><td><p>test&gt; [NGram(\"Fauna\"),NGram(\"Fauna\",3,4)]</p><p>[ [ 'F', 'Fa', 'a', 'au', 'u', 'un', 'n', 'na', 'a' ],</p><p>&nbsp;[ 'Fau', 'Faun', 'aun', 'auna', 'una' ] ]</p></td></tr><tr><td><p>SubString</p></td><td><p>The SubString function returns a portion of the “value” string beginning at the character “start” position for “length” characters long.</p></td><td><p>test&gt; SubString(\"San Francisco\",1,2)</p><p>'an'</p><br></td></tr></tbody></table>\r\n<h2><br><br>Conclusion</h2>\r\n<p>As you can see from these examples, the availability of these string functions in FQL helps developers write more efficient code closer to the data. Executing functions within the database also ensure global data correctness and improves the overall robustness of your app. A complete list of string functions along with their detailed description, purpose, and examples are available in the <a href=\"https://app.fauna.com/documentation/reference/queryapi#string-functions\">FaunaDB documentation</a>.</p>\r\n<p>We plan to introduce new data processing functions in FQL at a regular cadence. If there are some you’d like to see, please reach me at <a href=\"mailto:product@fauna.com\">product@fauna.com</a>. I’d love to talk to you.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-02-04T10:16:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1651,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "77c8e128-446e-48ae-bf0f-6c2dc7621aff",
        "siteSettingsId": 1651,
        "fieldLayoutId": 4,
        "contentId": 1237,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introducing Automated Log Topology in FaunaDB 2.6",
        "slug": "automated-log-topology",
        "uri": "blog/automated-log-topology",
        "dateCreated": "2019-01-30T12:56:14-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/automated-log-topology",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/automated-log-topology",
        "isCommunityPost": false,
        "blogBodyText": "<p>One of the most exciting features of the FaunaDB <a href=\"https://app.fauna.com/documentation/releasenotes/2.6.0\">2.6 release</a> is the automated log topology. Prior to this release, setting up the log topology was a manual process where an enterprise operator had to decide on a log topology upfront, and then repeat that in the <a href=\"https://app.fauna.com/documentation/howto/operations/newclustersetup\">FaunaDB configuration file</a> on each node within and across a replica. The operator also had to be educated on how to associate a log segment with a node and ensure consistency in configuration across all replicas in the cluster. Updating this configuration was not straightforward either. In this release, all the inefficiencies around the log setup go away. It is now completely automated and transparent to the operator. In the sections below, we discuss how log topology is set up and how transaction logging works in general.</p>\r\n<h2><strong>Transactions in FaunaDB</strong></h2>\r\n<p>FaunaDB guarantees fully ACID distributed transactions and offers strictly serializable operations across replicas. This means that the system can process many transactions in parallel, but the final result is equivalent to processing them in serial order. FaunaDB’s Calvin inspired transaction protocol decides the serial order prior to performing any writes to the database. Since transaction application is deterministic in FaunaDB, the system has to ensure every data node that applies writes to itself sees the same transactions in the same order. Each batch of transactions in an epoch is inserted into the transaction log. The FaunaDB execution engine ensures that the final result of processing every batch of transactions is equivalent to processing each transaction individually and in a serialized order. For more details on how transactions are processed, refer to the blog post on “<a href=\"https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol\">Fauna Transaction Protocol</a>”.</p>\r\n<p>Each transaction in the log is associated with a unique timestamp that approximates real time. Unlike other globally consistent databases, real-time is not a central component of FaunaDB’s protocol. FaunaDB does not rely on global synchronization of clocks across servers. The notion of “before” and “after” is entirely dependent on the order in which transactions appear in the global log. This is important as log segments can skew a bit from one another with regard to their last committed epoch. This skew is inherent in a distributed system as the various data nodes in the cluster can be exposed to different loads and interruptions. To resolve this and preserve determinism, data nodes within a replica ensure that they have read transaction batches for the same not-yet-applied epoch from all log segments that participated in the consensus for that epoch. The transaction batches are then merged in the order of the numbered log segments and applied.&nbsp;</p>\r\n<h2>Transaction Log Architecture</h2>\r\n<p>In FaunaDB, transactions are durably written to a distributed log before they are applied. Each replica in the Fauna cluster can contain a copy of the log. A typical cluster will contain at least three replicas with copies of the log. A Raft-inspired protocol is used to achieve consensus among the log replicas. Odd numbers of replicas allow the consensus algorithm to achieve a quorum in the face of log replica disruption or failure.</p>\r\n<p>Each replica of the log can contain one or more log segments. In these cases, there will be a one-to-one relationship between the log segments across the replicas. A very typical deployment would be a cluster with three replicas each containing three nodes. Each of the three replicas would contain a copy of the log and each log would be divided into three log segments as depicted below.</p>\r\n<p></p>\r\n<figure><img src=\"{asset:1661:url}\" data-image=\"1661\"></figure>\r\n<p>In clusters where some replicas do not contain copies of the log or have unbalanced node counts, there may be nodes that do not contain any log segments. The diagram below provides an idea of how diverse such a configuration can be. In this example, we have a cluster with four replicas. Only three of the replicas contain copies of the log. In those replicas, each contains three log segments. This is the maximum number of log segments possible as Region 1 and 3 only have three nodes thereby capping the maximum log segments to three.</p>\r\n<p></p>\r\n<figure><img src=\"{asset:1662:url}\" data-image=\"1662\"></figure>\r\n<p>Transactions in Fauna are not written to the log individually. Coordinators send transactions to log nodes which are then batched together in-memory and written to a log segment. This process occurs every 10 ms and we call it an epoch. The log nodes within a single segment achieve consensus on their contents every epoch. This is referred to as “commit” in Raft. At the point of the log consensus, all transactions are deterministic and durable.</p>\r\n<p>In addition to the transaction logs, the system also maintains a global log at the cluster level to maintain the log topology state. The information in this topology state includes the currently active log segment IDs and the assignments of hosts that participate as log nodes in those segments, and the current epoch position for each segment.</p>\r\n<h2>Log Segment Status<br></h2>\r\n<p>As illustrated above, the nodes in a cluster need to be aware of the current state of the log topology to make sure the correct batches are applied for each epoch. FaunaDB keeps an independent Raft log that maintains the state of the log topology. Logs segments can be in one of several different states. When a log segment is first created, it is Uninitialized. The first log node in the list of its members will take it upon itself to initialize the segment's Raft log. Other members will try to join it. If the membership of the segment changes during this initialization process, it will be started over. Only once all initially known members have joined will the initializing node register it as Started in the log topology Raft state and assigned a start epoch. The start epoch is in the logical future so that when the data nodes reach that point they can all observe its existence in a consistent manner. Once the start epoch rolls around, the log nodes are free to start committing batches into the segment log.</p>\r\n<p>When a segment needs to be closed, it is registered in topology Raft log as Closed with a record of its ending epoch. When the coordinators notice that the segment is in a closed state, they will cease to send it transactions. The log node itself will refuse any transactions sent to it and will also reject any transactions it has pending in memory. The leader for the segment will keep committing batches (which at this point will be empty) until its end epoch rolls around.</p>\r\n<h2>Log Segment Membership</h2>\r\n<p>While there'd be many ways to assign nodes to segments, the algorithm that FaunaDB internally follows can be imagined as orthogonally cross-cutting the replicas with segments. If we have three replicas (A, B, C) with five nodes in each (A1-A5, B1-B5 and C1-C5), then the system will allocate five segments, with each segment having a node in every replica: segment 1 will have A1, B1, C1 nodes, segment 2 will have A2, B2, C2 nodes, up to segment 5 having nodes A5, B5, C5. Segments are enlarged when a new replica is added (if we add a replica D with nodes D1-D5, then segment 1 will become A1, B1, C1, D1, etc.), and new segments are added when all replicas are enlarged: if we add nodes A6, B6, C6, D6 to every replica, then a new segment 6 will be created with them as members.</p>\r\n<p>When a log segment is closed, it is closed on all its remaining nodes in other replicas too. Those nodes are freed up and can later participate in new log segments. As discussed above, a node does not have to have an active log segment to continue its roles as a coordinator and data node.</p>\r\n<h3>Replica Types</h3>\r\n<p>Every FaunaDB replica has a type describing the level of functionality its nodes will provide. The three types, in increasing order of functionality, are named compute, data, and data+log. <br><br></p>\r\n<ul><li><strong>Compute</strong> - nodes in compute replicas do not store data, nor do they participate in the distributed transaction log. They can receive queries from clients and execute them, relying on nodes in other replicas for data storage and the transaction log.</li><li><strong>Data</strong> - nodes in data replicas execute queries from clients just as compute replicas do, but in addition, they also store data. The full data set is partitioned between nodes in a single replica, and the replica as a whole will contain the full data set.</li><li><strong>Data+Log</strong> - nodes in data+log replicas have all the functionality of a data replica, but in addition, they also participate in the distributed transaction log.</li></ul>\r\n<p>The type of the replica can be set any number of times during its lifetime, as long as the cluster at all times has at least one data+log replica. When the cluster is initialized, the replica of the first node is by default set to be of the data+log type, while all additional replicas start out as compute replicas and need to be explicitly set to a different type in order to participate in either the data or log replication.<br>When a replica type is changed from data (or data+log) to compute, and the topology change passes safety checks to ensure it won’t lose data or compromise availability, it will immediately discard its full data set and if it is later reclassified as data, it will need to re-acquire data from other replicas, which can take some time. Data replicas that are acquiring data become gradually available for both reads and writes (and are thus useful data members of the cluster) as their data acquisition process progresses.</p>\r\n<p>It is recommended that the cluster should have at least 3 data+log replicas in order to ensure the cluster's ability to process writes in the event that one replica goes down. Write latency is affected by the physical location of the nodes which replicate the transaction log. Writes originating closer to the nodes replicating the transaction log will have lower latency than writes originating from farther away.</p>\r\n<h2>Useful commands for log management</h2>\r\n<ul><li>To review the configuration of the cluster issue the following command</li></ul>\r\n<pre>$faunadb-admin status</pre>\r\n<ul><li>To set a replica to ‘data+log’ issue the command using fauna-admin on any of the nodes in that replica</li></ul>\r\n<pre>$faunadb-admin update-replica data+log replica2</pre>\r\n<ul><li>You may also specify multiple replicas in the same command</li></ul>\r\n<pre>$faunadb-admin update-replica data+log replica2 replica3</pre>\r\n<ul><li>In order to change a replica to a non-log role simply update the replica using the following command</li></ul>\r\n<pre>$faunadb-admin update-replica compute replica2\r\n</pre>\r\n<ul></ul>\r\n<ul></ul>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2019-01-31T11:19:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1641,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "0b5e05bd-139d-4f7e-8143-39e95c8ecace",
        "siteSettingsId": 1641,
        "fieldLayoutId": 4,
        "contentId": 1227,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Announcing the availability of FaunaDB Enterprise 2.6.0",
        "slug": "announcing-the-availability-of-faunadb-enterprise-2-6-0",
        "uri": "blog/announcing-the-availability-of-faunadb-enterprise-2-6-0",
        "dateCreated": "2019-01-28T14:06:46-08:00",
        "dateUpdated": "2019-09-24T10:53:31-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/announcing-the-availability-of-faunadb-enterprise-2-6-0",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/announcing-the-availability-of-faunadb-enterprise-2-6-0",
        "isCommunityPost": false,
        "blogBodyText": "<p>Releases for agile dev/ops companies like Fauna are a regular occurrence. We sometimes do press releases about the code release, especially if it is a major (x.0) release. But each release, even a point release, is special in its own way so we will now be posting a blog about each new version as it is released. We will also blog about some key features added within the release to provide additional detail about those features, like what it might mean to our customers and how they can take full advantage of each new feature.</p>\r\n\r\n<p>For v2.6, we are adding a few such key new features and other improvements:</p>\r\n<ul><li><strong>Automated log topology</strong> - FaunaDB is based on the Calvin distributed transaction protocol and, thus, the log is a very important component for our database and we treat it very special. Before v2.6, you needed to figure out your log topology ahead of time and have your database operator implement it. One of our key drivers at Fauna is operational simplicity so, in that light, we automated the log setup process. Now, the database operator just needs to mark a replica as a log and FaunaDB automatically does what is needed (creating log segments across that replica) to make it work.</li></ul>\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp;This is a significant new feature and will post a blog dedicated to it in the near future.</p>\r\n<p> </p>\r\n<ul><li><strong>Improved Developer productivity</strong> – Not a single feature but rather a category of features. In addition to striving for higher <a href=\"https://youtu.be/zD89al4kMRA\">operational simplicity</a>, Fauna does everything we can to make application developers’ lives easier and more productive. In v2.6, we have added a bunch of new math and string manipulation functions that application developers can now call within the database rather than using only their host language functions. This provides the advantage of not needing to have data shuttled from the database to the client -- saving time as well as coding effort.&nbsp;&nbsp;</li></ul>\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; A blog on this will be coming soon as well.</p>\r\n<p> </p>\r\n<ul><li><strong>Security</strong> – And, as security is a primary concern of ours, we have improved the security of using our database by adding encryption in motion with TLS implementation for all communication across the wire, so the data and the commands can’t be snooped.</li></ul>\r\n<p>A complete list of bug fixes, enhancements is available in the <a href=\"https://app.fauna.com/documentation/releasenotes/2.6.0\">2.6 Release Notes</a>.&nbsp;</p>\r\n<p>The release can be downloaded from <a href=\"https://app.fauna.com/releases/enterprise/2.6.0\">https://app.fauna.com/releases/enterprise/2.6.0</a>. If you are running your application on the FaunaDB Cloud, you are already up and running on this version. And if you aren’t running on FaunaDB Cloud yet but would like to try it out (for free), you can&nbsp;<a href=\"https://app.fauna.com/sign-up\">sign-up here</a>. If you are running v2.5.8, you can do a rolling upgrade to v2.6 to be able to take advantage of the new features.</p>\r\n<p></p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-01-29T12:18:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1646,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b88c8126-ca5c-49d7-8e7c-b0a83194d72f",
        "siteSettingsId": 1646,
        "fieldLayoutId": 4,
        "contentId": 1232,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Building a Serverless JAMStack app with FaunaDB: Part 2",
        "slug": "building-a-serverless-jamstack-app-with-faunadb-cloud-part-2",
        "uri": "blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-2",
        "dateCreated": "2019-01-29T12:16:28-08:00",
        "dateUpdated": "2019-08-21T14:44:46-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-2",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-2",
        "isCommunityPost": false,
        "blogBodyText": "<p>In my experience with enterprise application development, the pattern I’ll illustrate in this series fits a wide range of collaborative apps. Most apps have some form of login, and once users are identified, they’re able to participate in a particular set of shared workspaces, depending on who they are. For example, players and coaches can collaborate with each other on game plans in one workspace, while coaches make roster decisions in a separate workspace only they can access. This same pattern repeats itself across application domains, from photo sharing to options trading.</p>\r\n<p>The <a href=\"https://fauna.com/blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-1\">previous post</a> was about where we are today in the development process of an example application template. With one click, the grunt work of your new application is done, and your valuable domain-specific code ready to write.</p>\r\n<blockquote>The end result is an application template embodying best practices, like React hooks and Netlify Identity for user management, that offers user-level, role-based data access control with minimal development effort.&nbsp;</blockquote>\r\n<p>The highlighted purple area in the diagram below corresponds to the main collaborative business objects of the application. This is where the code lives that can set your application apart from the competition. Whether it’s a simple discussion board or a complex point-of-sale system for a large chain retailer, most of the vertical specific logic will live in the heart of the application, the collaborative documents. And the more cleanly we encapsulate that area, the more flexibility we’ll have in adapting the application to different operational requirements. For instance swapping out the identity component, or the cloud provider, shouldn’t need to impact the business logic. <br></p>\r\n<figure><img src=\"{asset:1644:url}\" data-image=\"1644\"></figure>\r\n<p>Outside of the collaborative documents, most of the complexity doesn’t differentiate the application, so application developers try to minimize it. It’s boilerplate, and the real value is in the collaborative documents, whether they represent travel plans or market positions. Developers understand the value of a starter kit. As a database company, Fauna is in a unique position to offer starter kits that include the fundamental building blocks for an application data model. </p>\r\n<p>I’ve been building applications along these lines since pre 1.0 Ruby on Rails. Here I’ll show you how I'm implementing&nbsp;this model for a FaunaDB Netlify JAMStack app.</p>\r\n<p>First of all, you should know there’s a ton of boilerplate that Netlify handles for you. Below is a diagram that visualizes <strong>more stuff we don’t have to worry about</strong> when writing the app. Also included in the platform (but not the diagram) are operational concerns like cloud hosting, CDN, DNS, security certificates, etc. A developer building application features that work with collaborative documents, has leverage over so much infrastructure, so anytime we remove friction the effects are cumulative, and the productivity boosts are tangible. In this case by including identity as part of a starter kit, developers can start out with an authentication and identity system that is easy to use as-is or customize. Improvements made in the starter kit implementation are shared across all the apps the use it.<br></p>\r\n<figure><img src=\"{asset:1645:url}\" data-image=\"1645\"></figure>\r\n<p>Given the interplay between deployment-specific application configuration, identity, and initializing the database schema, one of the challenges that emerges is how to encapsulate application lifecycle changes so that the collaborative document schema can depend on the user model, without introducing complexity and brittleness. In short, how can we package the whole thing up so your app just pulls in a couple of modules to do the boilerplate, while maintaining complete flexibility for your business logic?</p>\r\n<p>One solution is to maintain the user and identity components in a separate module, perhaps as part of a <tt>faunadb-user</tt> package and a <tt>faunadb-netlify</tt> package, that can be reused. On the identity front, we are most of the way there, with the only task being the splitting of the schema management into identity and application components. They’ll share a namespace, so a system of setup hooks may be on the horizon. This allows the boilerplate components of the app to be managed independently of the application-specific code.</p>\r\n<p>The end result is an application template embodying best practices, like React hooks and Netlify Identity for user management, that offers user-level, role-based data access control with minimal development effort. The secure data API means both web and mobile clients can connect to FaunaDB. Working in a code environment like this, adding valuable new features becomes the focus of the developers’ attention.</p>\r\n<p>This article is an introduction to a long-term project. My current next step is to add tests to the database schema setup code, and then to add access control tests. This will make future work on the application safer and easier. For an update on the code-level current status of the project, check out <a href=\"https://fauna.com/blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-1\">the previous post in this series.</a><br></p>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866",
            "5603"
        ],
        "mainBlogImage": [
            "1645"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2019-01-25T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1675,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "67380c77-4438-4871-9d2a-882317363959",
        "siteSettingsId": 1675,
        "fieldLayoutId": 4,
        "contentId": 1261,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "It’s Time to Move on from Two Phase Commit",
        "slug": "its-time-to-move-on-from-two-phase-commit",
        "uri": "blog/its-time-to-move-on-from-two-phase-commit",
        "dateCreated": "2019-02-22T15:01:32-08:00",
        "dateUpdated": "2019-02-26T09:52:58-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/its-time-to-move-on-from-two-phase-commit",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/its-time-to-move-on-from-two-phase-commit",
        "isCommunityPost": false,
        "blogBodyText": "<p>The two-phase commit protocol (2PC) has been <a href=\"https://dl.acm.org/citation.cfm?id=7266\">used in enterprise software systems for over three decades</a>. It has been an an incredibly impactful protocol for ensuring atomicity and durability of transactions that access data in multiple partitions or shards. It is used everywhere --- both in older “venerable” distributed systems, database systems, and file systems such as Oracle, IBM DB2, PostgreSQL, and Microsoft TxF (transactional NTFS), and in younger “millennial” systems such as MariaDB, TokuDB, VoltDB, Cloud Spanner, Apache Flink, Apache Kafka, and Azure SQL Database. If your system supports ACID transactions across shards/partitions/databases, there’s a high probability that it is running 2PC (or some variant thereof) under the covers. [Sometimes it’s even “over the covers” --- older versions of MongoDB <a href=\"https://docs.mongodb.com/v3.4/tutorial/perform-two-phase-commits/\">required users to implement 2PC for multi-document transactions in application code</a>.]</p>\r\n<p>In this post, we will first describe 2PC: how it works and what problems it solves. Then, we will show some major issues with 2PC and how modern systems attempt to get around these issues. Unfortunately, these attempted solutions cause other problems to emerge. In the end, I will make the case that the next generation of distributed systems should avoid 2PC, and how this is possible.<br></p>\r\n<h3>Overview of the 2PC protocol</h3>\r\n<p><br>There are many variants of 2PC, but the basic protocol works as follows:&nbsp;</p><p>Background assumption: The work entailed by a transaction has already been divided across all of the shards/partitions that store data accessed by that transaction. We will refer to the effort performed at each shard as being performed by the “worker” for that shard. Each worker is able to start working on its responsibilities for a given transaction independently of each other. The 2PC protocol begins at the end of transaction processing, when the transaction is ready to “commit”. It is initiated by a single, coordinator machine (which may be one of the workers involved in that transaction).<br><br></p><p>The basic flow of the 2PC protocol is shown in the figure below. [The protocol begins at the top of the figure and then proceeds in a downward direction.]<br><br><br><br></p>\r\n<figure><img height=\"218\" src=\"https://lh6.googleusercontent.com/F7giuNAtrYsZz1ZYZPOuqXnnYYfUTiXq-IU-wKkiRbnKqhnEehTA1cIwdDTIQ5MylDjbsKhbjYLyZJkt-HFQQ50BaMtS2Vel_vr0II5vbjl2K6UX3tjQTnOb75ICgeo7IQDmIyeG\" width=\"400\" data-image=\"f5e05q1c6fcx\"></figure>\r\n<p><br><br>Phase 1: A coordinator asks each worker whether they have successfully completed their responsibilities for that transaction and are ready to commit. Each worker responds ‘yes’ or ‘no’.</p><p>Phase 2: The coordinator counts all the responses. If every worker responded ‘yes’, then the transaction will commit. Otherwise, it will abort. The coordinator sends a message to each worker with the final commit decision and receives an acknowledgement back.</p><p>This mechanism ensures the atomicity property of transactions: either the entire transaction will be reflected in the final state of the system, or none of it. If even just a single worker cannot commit, then the entire transaction will be aborted. In other words: each worker has “veto-power” for a transaction.&nbsp;</p><p>It also ensures transaction durability. Each worker ensures that all of the writes of a transaction have been durably written to storage prior to responding ‘yes’ in phase 1. This gives the coordinator freedom to make a final decision about a transaction without concern for the fact that a worker may fail after voting ‘yes’. [In this post, we are being purposefully vague when using the term “durable writes” --- this term can either refer to writing to local non-volatile storage or, alternatively, replicating the writes to enough locations for it to be considered “durable”.]</p><p>In addition to durably writing the writes that are directly required by the transaction, the protocol itself requires additional writes that must be made durable before it can proceed. For example, a worker has veto power until the point it votes ‘yes’ in phase 1. After that point, it cannot change its vote. But what if it crashes right after voting ‘yes’? When it recovers it might not know that it voted ‘yes’, and still think it has veto power and go ahead and abort the transaction. To prevent this, it must write its vote durably before sending the ‘yes’ vote back to the coordinator. [In addition to this example, in standard 2PC, there are two other writes that are made durable prior to sending messages that are part of the protocol.]<br><br></p>\r\n<h3>The problems with 2PC</h3>\r\n<p><br>There are two major problems with 2PC. The first is well known, and discussed in every reputable textbook that presents 2PC. The second is much less well known, but a major problem nonetheless.</p><p>The well-known problem is referred to as the “blocking problem”. This happens when every worker has voted ‘yes’, but the coordinator fails before sending a message with the final decision to at least one worker. The reason why this is a problem is that by voting ‘yes’, each worker has removed its power to veto the transaction. However, the coordinator still has absolute power to decide the final state of a transaction. If the coordinator fails before sending a message with the final decision to at least one worker, the workers cannot get together to make a decision amongst themselves --- they can’t abort because maybe the coordinator decided to commit before it failed, and they can’t commit because maybe the coordinator decided to abort before it failed. Thus, they have to block --- wait until the coordinator recovers --- in order to find out the final decision. In the meantime, they cannot process transactions that conflict with the stalled transaction since the final outcome of the writes of that transaction are yet to be determined.&nbsp;</p><p>There are two categories of work-arounds to the blocking problem. The first category of work-around modifies the core protocol in order to eliminate the blocking problem. Unfortunately, these modifications reduce the performance --- typically by adding an extra round of communication --- and thus are rarely used in practice. The second category keeps the protocol in tact but reduces the probability of the types of coordinator failure than can lead to the blocking program --- for example, by running 2PC over replica consensus protocols and ensuring that important state for the protocol is replicated at all times. Unfortunately, once again, these work-arounds reduce performance, since the protocol requires that these replica consensus rounds occur sequentially, and thus they may add significant latency to the protocol.&nbsp;</p><p>The lesser-known problem is what I call the “cloggage problem”. 2PC occurs after transaction is processed, and thus necessarily increases the latency of the transaction by an amount equal to the time it takes to run the protocol. This latency increase alone can already be an issue for many applications, but a potentially larger issue is that worker nodes do not know the final outcome of a transaction until mid-way through the second phase. Until they know the final outcome, they have to be prepared for the possibility that it might abort, and thus they typically prevent conflicting transactions from making progress until they are certain that the transaction will commit. These blocked transactions in turn block other transactions from running, and so on, until 2PC completes and all of the blocked transactions can resume. &nbsp;This cloggage further increases the average transaction latency and also decreases transactional throughput.</p><p>To summarize the problems we discussed above: 2PC poisons a system along four dimensions: <strong>latency </strong>(the time of the protocol plus the stall time of conflicting transactions), <strong>throughput </strong>(because it prevents conflicting transactions from running during the protocol), <strong>scalability </strong>(the larger the system, the more likely transactions become multi-partition and have to pay the throughput and latency costs of 2PC), and <strong>availability </strong>(the blocking problem we discussed above). &nbsp;<em>Nobody likes 2PC, but for decades, people have assumed that it is a necessary evil.</em><br></p>\r\n<h3>It’s time to move on</h3>\r\n<p><br>For over three decades, we’ve been stuck with two-phase commit in sharded systems. People are aware of the performance, scalability, and availability problems it introduces, but nonetheless continue on, with no obvious better alternative.</p><p>The truth is, if we would just architect our systems differently, the need for 2PC would vanish. There have been some attempts to accomplish this --- both in academia (such as <a href=\"https://cs.uwaterloo.ca/~kdaudjee/courses/cs848/papers/non2PC.pdf\" target=\"_blank\">this SIGMOD 2016 paper</a>) and industry. However, these attempts typically work by avoiding multi-sharded transactions in the first place, such as by repartitioning data in advance of a transaction so that it is no longer multi-sharded. Unfortunately, this repartitioning reduces performance of the system in other ways.</p><p>What I am calling for is a deeper type of change in the way we architect distributed systems. I insist that systems should still be able to process multi-sharded transactions --- with all the ACID guarantees and what that entails --- such as atomicity and durability --- but with much simpler and faster commit protocols.&nbsp;</p><p><strong>It all comes down to a fundamental assumption that has been present in our systems for decades: a transaction may abort at any time and for any reason.</strong> Even if I run the same transaction on the same initial system state … if I run it at 2:00PM it may commit, but at 3:00 it may abort.</p><p>There are several reasons why most architects believe we need this assumption. First, a machine may fail at anytime --- including in the middle of a transaction. Upon recovery, it is generally impossible to recreate all of the state of that transaction that was in volatile memory prior to the failure. As a result, it is seemingly impossible to pick up where the transaction left off prior to the failure. Therefore, the system aborts all transactions that were in progress at the time of the failure. Since a failure can occur at any time, this means that a transaction may abort at any time.</p><p>Second, most concurrency control protocols require the ability to abort a transaction at any time. Optimistic protocols perform a “validation” phase after processing a transaction. If validation fails, the transaction aborts. &nbsp;Pessimistic protocols typically use locks to prevent concurrency anomalies. This use of locks may lead to deadlock, which is resolved by aborting (at least) one of the deadlocked transactions. Since deadlock may be discovered at any time, the transaction needs to retain the ability to abort at any time.</p><p>If you look carefully at the two-phase commit protocol, you will see that this arbitrary potential to abort a transaction is the primary source of complexity and latency in the protocol. Workers cannot easily tell each other whether they will commit or not, because they might still fail after this point (before the transaction is committed) and want to abort this transaction during recovery. Therefore, they have to wait until the end of transaction processing (when all important state is made durable) and proceed in the necessary two phases: in the first phase, each worker publically relinquishes its control to abort a transaction, and only then can the second phase occur in which a final decision is made and disseminated.</p><p>In my opinion we need to <strong>remove veto power from workers </strong>and <strong>architect systems in which the system does not have freedom to abort a transaction whenever it wants</strong> during its execution. Only logic within a transaction should be allowed to cause a transaction to abort. If it is theoretically possible to commit a transaction given an current state of the database, that transaction must commit, no matter what types of failures occur. Furthermore, there must not be race conditions relative to other concurrently running transactions that can affect the final commit/abort state of a transaction.</p><p>Removing abort flexibility sounds hard. We’ll discuss soon how to accomplish this. But first let’s observe how the commit protocol changes if transactions don’t have abort flexibility. <br></p>\r\n<h3>What a commit protocol looks like when transactions can’t abort arbitrarily</h3>\r\n<p><br>Let’s look at two examples:</p><p>In the first example, assume that the worker for the shard that stores the value for variable X is assigned a single task for a transaction: change the value of X to 42. Assume (for now) that there are no integrity constraints or triggers defined on X (which may prevent the system from setting X to 42). In such a case, that worker is never given the power to be able to abort the transaction. No matter what happens, that worker must change X to 42. If that worker fails, it must change X to 42 after it recovers. Since it never has any power to abort, there is no need to check with that worker during the commit protocol to see if it will commit.&nbsp;</p><p>In the second example, assume that the worker for the shard that stores the value for variables Y and Z is assigned two tasks for a transaction: subtract 1 from the previous value of Y and set Z to the new value of Y. Furthermore, assume that there is an integrity constraint on Y that states that Y can never go below 0 (e.g., if it represents the inventory of an item in a retail application). Therefore, this worker has to run the equivalent of the following code:</p><pre>          &nbsp;&nbsp;IF (Y &gt; 0)\r\n     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subtract 1 from Y\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ELSE\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ABORT the transaction\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Z = Y</pre><p>This worker must be given the power to abort the transaction since this required by the logic of the application. However, this power is limited. Only if the initial value of Y was 0 can this worker abort the transaction. Otherwise, it has no choice but to commit. Therefore, it doesn’t have to wait until it has completed the transaction code before knowing whether it will commit or not. On the contrary: as soon as it has finished executing the first line of code in the transaction, it already knows its final commit/abort decision. This implies that the commit protocol will be able to start much earlier relative to 2PC.</p><p>Let’s now combine these two examples into a single example in which a transaction is being performed by two workers --- one of them is doing the work described in the first example, and the other one doing the work described in the second example. Since we are guaranteeing atomicity, the first worker cannot simply blindly set X to 42. Rather, it’s own work must also be dependent on the value of Y. In effect, it’s transaction code becomes:<br> <br></p><pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;temp = Do_Remote_Read(Y)\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (temp &gt; 0)\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = 42<br><br></pre><p>Note that if the first worker’s code is written in this way, the code for the other worker can be simplified to just:&nbsp;</p><pre> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IF (Y &gt; 0)\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subtract 1 from Y\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Z = Y<br></pre><p>By writing the transaction code in this way, we have removed explicit abort logic from both workers. Instead, both workers have if statements that check for the constraint that would have caused the original transaction to abort. If the original transaction would have aborted, both workers end up doing nothing. Otherwise, both workers change the values of their local state as required by the transaction logic.&nbsp;</p><p>The important thing to note at this point is that <strong>the need for a commit protocol has been totally eliminated in the above code</strong>. The system is not allowed to abort a transaction for any reason other than conditional logic defined by application code on a given state of the data. And all workers condition their writes on this same conditional logic so that they can all independently decide to “do nothing” in those situations where a transaction cannot complete as a result of current system state. Thus, all possibility of a transaction abort has been removed, and there is no need for any kind of distributed protocol at the end of transaction processing to make a combined final decision about the transaction. All of the problems of 2PC have been eliminated. There is no blocking problem because there is no coordinator. And there is no cloggage problem, because all necessary checks are overlapped with transaction processing instead of after it completes.&nbsp;</p><p>Moreover, as long as the system is not allowed to abort a transaction for any reason other than the conditional application logic based on input data state, it is always possible to rewrite any transaction as we did above in order to replace abort logic in the code with if statements that conditionally check the abort conditions. Furthermore, it is possible to accomplish this without actually rewriting application code. [The details of how to do this are out of scope for this post, but to summarize at a high level: shards can set special system-owned boolean flags when they have completed any conditional logic that could cause an abort, and it is these boolean flags that are remotely read from other shards.]&nbsp;</p><p>In essence: there are two types of aborts that are possible in transaction processing systems: (1) Those that are caused by the state of the data and (2) Those that are caused by the system itself (e.g. failures or deadlocks). Category (1) can always be written in terms of conditional logic on the data as we did above. So if you can eliminate category (2) aborts, the commit protocol can be eliminated.<br>So now, all we have to do is explain how to eliminate category (2) aborts.<br></p>\r\n<h3>Removing system-induced aborts</h3>\r\n<p><br>I have spent almost an entire decade designing systems that do not allow system-induced aborts. Examples of such systems are <a href=\"http://www.cs.umd.edu/~abadi/papers/calvin-sigmod12.pdf\">Calvin</a>, <a href=\"http://www.cs.umd.edu/~abadi/papers/calvinfs.pdf\">CalvinFS</a>, <a href=\"http://www.cs.umd.edu/~abadi/papers/orthrus-sigmod16.pdf\">Orthrus</a>, <a href=\"http://www.cs.umd.edu/~abadi/papers/early-write-visibility.pdf\">PVW</a>, and a <a href=\"http://www.cs.umd.edu/~abadi/papers/lazy-xacts.pdf\">system that processes transactions lazily</a>. The impetus for this feature came from the first of these projects --- Calvin --- because of its status of being a deterministic database system. A deterministic database guarantees that there is only one possible final state of the data in the database given a defined set of input requests. It is therefore possible to send the same input to two distinct replicas of the system and be certain that the replicas will process this input independently and end up in the same final state, without any possibility of divergence.&nbsp;</p><p>System-induced aborts such as system failure or concurrency control race conditions are fundamentally nondeterministic events. It is very possible that one replica will fail or enter a race condition while the other replica will not. If these nondeterministic events were allowed to result in an a transaction to abort, then one replica may abort a transaction while the other one would commit --- a fundamental violation of the deterministic guarantee. Therefore, we had to design Calvin in a way that failures and race conditions cannot result in a transaction to abort. For concurrency control, Calvin used pessimistic locking with a deadlock avoidance technique that ensured that the system would never get into a situation where it had to abort a transaction due to deadlock. In the face of a system failure, Calvin did not pick up a transaction exactly where it left off (because of the loss of volatile memory during the failure). Nonetheless, it was able to bring the processing of that transaction to completion without having to abort it. It accomplished this via restarting the transaction from the same original input.&nbsp;</p><p>Neither of these solutions --- neither deadlock avoidance nor transaction restart upon a failure --- are limited to being used in deterministic database systems. [Transaction restart gets a little tricky in nondeterministic systems if some of the volatile state associated with a transaction that was lost during a failure was observed by other machines that did not fail. But there are simple ways to solve this problem that are out of scope for this post.] Indeed, some of the other systems I linked to above are nondeterministic systems. Once we realized the power that comes with removing system-level aborts, we built this feature into every system we built after the Calvin project --- even the nondeterministic systems.<br></p>\r\n<h3>Conclusion</h3>\r\n<p><br>I see very little benefit in system architects making continued use of 2PC in sharded systems moving forward. I believe that removing system-induced aborts and rewriting state-induced aborts is the better way forward. Deterministic database systems such as Calvin or <a href=\"https://fauna.com/\">FaunaDB </a>&nbsp;always remove system-induced aborts anyway, and thus usually can avoid 2PC as we described above. But it is a huge waste to limit this benefit to only deterministic databases. It is not hard to remove system-induced aborts from nondeterministic systems. Recent projects have shown that it is even possible to remove system-induced aborts in systems that use concurrency control techniques other than pessimistic concurrency control. For example, both the PVW and the lazy transaction processing systems we linked to above use a variant of multi-versioned concurrency control. And FaunaDB uses a variant of optimistic concurrency control.&nbsp;</p><p>In my opinion there is very little excuse to continue with antiquated assumptions regarding the need for system-induced aborts in the system. In the old days when systems ran on single machines, such assumptions were justifiable. However, in modern times, where many systems need to scale to multiple machines that can fail independently of each other, these assumptions require expensive coordination and commit protocols such as 2PC. The performance problems of 2PC has been a major force behind the rise of non-ACID compliant systems that give up important guarantees in order to achieve better scalability, availability, and performance. 2PC is just too slow --- it increases the latency of all transactions --- not just by the length of the protocol itself, but also by preventing transactions that access the same set of data from running concurrently. 2PC also limits scalability (by reducing concurrency) and availability (the blocking problem we discussed above). The way forward is clear: we need to reconsider antiquated assumptions when designing our systems and say “good-bye” to two phase commit!</p>",
        "blogCategory": [],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-01-23T11:51:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1608,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "dbf0148a-b4ab-46b2-9846-c3ad9bd483aa",
        "siteSettingsId": 1608,
        "fieldLayoutId": 4,
        "contentId": 1209,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Building a Serverless JAMStack app with FaunaDB: Part 1",
        "slug": "building-a-serverless-jamstack-app-with-faunadb-cloud-part-1",
        "uri": "blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-1",
        "dateCreated": "2019-01-23T11:49:47-08:00",
        "dateUpdated": "2019-08-21T14:38:05-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-1",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/building-a-serverless-jamstack-app-with-faunadb-cloud-part-1",
        "isCommunityPost": false,
        "blogBodyText": "<p>I’ve written a handful of articles about this code and the access control pattern it uses in previous blogs. Moving forward I’ll group articles related to this code example as a series: Building a Serverless JAMStack App with FaunaDB. This app focuses on real-world usage of FaunaDB, including database schemas and access control. Access control can eat up a lot of time in the early phases of an app, so by making a template available, you can jump directly into writing code that works with your business objects, instead of spending energy implementing login.</p>\r\n<blockquote>You can start with the simple instructions in the&nbsp;<a href=\"https://github.com/fauna/netlify-faunadb-todomvc\">FaunaDB Netlify template app</a>, and begin writing winning code right away.</blockquote>\r\n<p><a href=\"http://todomvc.com/\">TodoMVC</a> is a classic single page example app that manages todo lists. I’ve extended it with user login, and multiple todo lists, and plan to add list sharing in a future version. This schema can provide the basis for a broad swath of interactive applications, so once you understand the data model, you can use FaunaDB for your app. In a hackday situation, you can start with the simple instructions in the&nbsp;<a href=\"https://github.com/fauna/netlify-faunadb-todomvc\">FaunaDB Netlify template app</a>, and begin writing winning code right away. </p>\r\n<p>I’ve been collaborating with Netlify engineers to build JAMStack examples and had the opportunity to work with developers at Free Code Camp at Github in San Francisco. This experience reinforced the importance of easy on-ramps and clear starting points. You can catch up with some of the backstories of this code with this video from a <a href=\"http://fauna.com/blog/freecodecamp-with-netlify\">FreeCodeCamp event hosted by Netlify</a> or <a href=\"https://github.com/fauna/netlify-faunadb-todomvc\">start with the steps in this “running” section here.</a><a href=\"https://github.com/fauna/netlify-faunadb-todomvc\"></a>&nbsp;</p>\r\n<blockquote>Thanks to Shawn Wang for simplifying the FaunaDB / Netlify TodoMVC example app, and upgrading it to use React hooks.</blockquote>\r\n<p>Sharing code with the community opens the door for contributions, and we are lucky to have a major one from <a href=\"https://www.swyx.io/\">Shawn Wang</a> who’s simplified the FaunaDB TodoMVC example app which you might have seen in a past demo. Most importantly, he also upgraded it to work with the latest React best practices like hooks. A fresh take on this app, with the same database logic underlying the new code, offers a chance for me to do some additional refactoring.</p>\r\n<p>You can see a screencast of his work here:</p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/7OB_IjTTcwg\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<p>If you want to learn more, read the excellent <a href=\"https://reactjs.org/docs/hooks-intro.html\">React Hooks documentation</a> and <a href=\"https://github.com/sw-yx/hooks\">Shawn’s hook repo.</a></p>\r\n<p>I’m picking up from this code, with the goal of creating a reusable app template that demonstrates FaunaDB’s access control, and makes it super easy to get started coding. I’ll write more about the design goal of an example template app in the next post in this series. While I’m starting in the JAMStack, I expect my schema to be accessed from multiple runtimes including mobile and web. Also, I would like to make the schema more generic and easily reusable, which is mostly a matter of renaming some classes and functions.</p>\r\n<p>To that end, I plan on refactoring the database usage apart from the identity concerns and bringing the schema setup logic into the database module. Once the FaunaDB code is fully encapsulated, I’ll be able to write tests that exercise the schema management code as well as the business logic. With those tests in place, the app will be in a position to illustrate best practices for full-stack development with FaunaDB. I’m currently in the middle of making this change, and almost to the point where I can begin writing tests. If you have a test framework recommendation, <a href=\"https://twitter.com/jchris\">reach out to me on Twitter.</a></p>\r\n<p>The data model for an app that manages multiple lists, and allows you to share those lists with other users, is strikingly generic. With a little encapsulation, I hope to be able to provide a hackathon friendly app starting point that can scale to social media success, or enterprise line-of-business complexity, all with today’s best practices. You can watch the progress of development in <a href=\"https://github.com/jchris/netlify-fauna-todo/tree/extractBootstrap\">my fork of the repo here.</a><br></p>\r\n<p>Throughout this series, we will be leveraging FaunaDB Cloud as the serverless database backend for this JAMStack app. Netlify will auto-provision your FaunaDB environment, so you don’t have to actually do anything to get started. If you want to get access directly to the FaunaDB Cloud, you can also <a href=\"https://fauna.com/sign-up\">sign up for free</a>.<br></p>\r\n<h2>Other Dev News</h2>\r\n<p>In other news, I was able to spend some time diving into the React Native ecosystem recently, and I found that adding FaunaDB to an app created by the React Native Starter Kit was completely easy. Also, the development environment has come a long way since I last checked in—tools like <a href=\"https://expo.io/\">Expo</a> and <a href=\"https://yarnpkg.com/en/\">Yarn</a> mean it is mature. Writing JavaScript with auto-reload on the phone has become so easy I think I’ll use it for teaching my kids to code.</p>\r\n<p>Also I was recently the guest on the <a href=\"https://blog.couchbase.com/ndp-episode-26-chris-anderson-acid-raft-faunadb/\">NoSQL Podcast</a>, talking about FaunaDB consistency.</p>\r\n<p></p>",
        "blogCategory": [
            "8",
            "10",
            "1530",
            "1866"
        ],
        "mainBlogImage": [
            "1607"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-01-16T14:11:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1603,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "da98af70-6cf6-469d-8195-a2c1a3ee8464",
        "siteSettingsId": 1603,
        "fieldLayoutId": 4,
        "contentId": 1204,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Fauna Blog Technical Highlights of 2018",
        "slug": "2018-in-review",
        "uri": "blog/2018-in-review",
        "dateCreated": "2019-01-15T15:13:15-08:00",
        "dateUpdated": "2019-08-19T20:59:28-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/2018-in-review",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/2018-in-review",
        "isCommunityPost": false,
        "blogBodyText": "<p>Going into a new year, it’s important to review where you’ve come from. In 2017, Fauna established itself in the serverless ecosystem. In 2018, FaunaDB’s relational capabilities brought in <a href=\"https://www.datanami.com/2018/12/12/has-faunadb-cracked-the-code-for-global-transactionality/\">high-value workloads like distributed ledger</a> and <a href=\"https://fauna.com/assets/site/pdfs/Fauna_Casestudy_Nextdoor_091218.pdf\">social notification targeting</a>. In 2019, we are building on these accomplishments, with industry-defining features in the works. This post is a review of 2018 blog content, with a focus on technical content and our global ACID transaction capabilities.</p>\r\n<blockquote>Distinguish Calvin's clockless architecture from Spanner-style clock-dependence.<br></blockquote>\r\n<p>The next section is a deep dive into the technical details of FaunaDB's distributed transaction protocol, so the following video is a good background on the topic:</p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/m6M7_Ehl09g\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<h2>Calvin vs Spanner</h2>\r\n<p>One of Fauna’s big themes in 2018 was distinguishing our Calvin-based clockless architecture from Spanner-style clock-dependent consistency. There were a handful of blog posts and webcasts on the topic, including <a href=\"https://www2.fauna.com/wcspannercalvin\">our webcast on Spanner vs. Calvin - Comparing consensus protocols in strongly consistent database systems, with Professor Daniel Abadi</a>.</p>\r\n<p>On September 21, Daniel J. Abadi, Professor of Computer Science University of Maryland College Park wrote a controversial blog post, entitled \"<a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">NewSQL database systems are failing to guarantee consistency, and I blame Spanner</a>,\" that received hundreds of comments and <a href=\"https://news.ycombinator.com/item?id=18039489\">much debate on Hacker News</a> and the post itself. In the webcast, Daniel Abadi goes into deeper technical detail on the two major approaches to guaranteeing consistency in geo-replicated database systems. </p>\r\n<p>In a follow up post, <a href=\"https://fauna.com/blog/partitioned-consensus-and-its-impact-on-spanners-latency\">Partitioned consensus and its impact on Spanner’s latency</a>, Professor Abadi dives into the discussion around latency and the differences between unified-consensus systems and partitioned-consensus systems.</p>\r\n<blockquote>The core protocol maintains consistency across geographic distances with only a single round of consensus.</blockquote>\r\n<p>If you are interested in the architecture behind FaunaDB’s implementation of the Calvin protocol, we also shared some technical content about the distributed storage engine. In <a href=\"https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol\">Consistency without Clocks: The FaunaDB Distributed Transaction Protocol</a>, Matt Freels describes how transactions are implemented in FaunaDB. The post describes the deterministic transaction log and scalable storage, and then explains how the core protocol maintains consistency across geographic distances with only a single round of consensus. </p>\r\n<p>Chris Anderson discusses FaunaDB’s deterministic consistency in a follow-up webcast: <a href=\"https://www2.fauna.com/wcconsistencywoclocks\">Consistency without clocks—database correctness at scale</a>. Chris discusses how consistency is managed in FaunaDB and analyzes its architectural advantages over Google Spanner and Spanner derivatives, such as Cockroach and Yugabyte.</p>\r\n<p>See FaunaDB’s robustness under failure scenarios in this 3-minute video screencast by Cary Bourgeois:<br></p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/_Xujgy-agFM\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<p>For the technical background, read <a href=\"https://blog.fauna.com/demonstrating-transactional-correctness-in-failure-situations\">Demonstrating Transactional Correctness in Failure Situations</a>.</p>\r\n<h2>Engineering Blog Posts</h2>\r\n<p>Aside from discussion of our Calvin-based architecture, here are a few developer highlights from 2018:</p>\r\n<p>If you are coming from a relational database background, <a href=\"https://fauna.com/blog/faunadb-a-guide-for-relational-users\">FaunaDB: a guide for relational users</a> will give you the concepts you need to succeed with FaunaDB.</p>\r\n<p>Lowering friction for common tasks gives developers time to implement more valuable things. Read <a href=\"https://fauna.com/blog/introducing-endpoints\">Introducing Endpoints</a> to see how you can connect the Fauna Shell to any FaunaDB installation, whether it is in the cloud or on your own workstation.</p>\r\n<blockquote>Setting up FaunaDB is really simple and can be done within a few minutes.</blockquote>\r\n<p>Speaking of on-premise installations, <a href=\"https://fauna.com/blog/setting-up-a-new-fauna-cluster-using-docker\">Setting up a new Fauna Cluster using Docker</a> is a great place to start. When Fauna’s new Director of Products embarked on setting up his first FaunaDB cluster, on his second week on the job, he imagined it would take hours. Leave aside Oracle RAC — even Cassandra and Mongo gave him a hard time. Turns out that setting up FaunaDB is really simple and can be done within a few minutes. See how he did it.</p>\r\n<p>If you are writing accounting code, you might enjoy this <a href=\"https://blog.fauna.com/tutorial-how-to-create-and-query-a-ledger-with-faunadb\">tutorial on how to create and query a ledger with FaunaDB.</a></p>\r\n<p>On the backend, our 4-part <a href=\"https://blog.fauna.com/getting-started-with-faunadb-using-go\">Getting Started with Fauna Using Go</a> series is a powerful way to learn the database.<br></p>\r\n<p>Fauna sponsored a two-day FreeCodeCamp JAMStack Hackathon event, including a livestream interview from Github’s video studio. We learned so much by hacking alongside database technology users. Learn the JAMStack in minutes with this video from <a href=\"http://fauna.com/blog/freecodecamp-with-netlify\">FreeCodeCamp with Netlify</a> or <a href=\"https://github.com/fauna/netlify-faunadb-todomvc\">start with the steps in this “running” section here.</a></p>\r\n<blockquote>Breezeworks uses FaunaDB history to add event streams and audit logging for their SaaS customers.</blockquote>\r\n<p>One of the easiest ways to get started with Fauna is by teeing writes from your existing database. Read <a href=\"https://fauna.com/blog/serverless-change-capture-for-ruby-on-rails\">Serverless change capture for Ruby on Rails</a> to learn about how Breezeworks uses FaunaDB history to add event streams and audit logging for their SaaS customers.</p>\r\n<p>In this article, we cover intelligent batching, <a href=\"https://fauna.com/blog/efficient-graphql-resolvers-for-faunadb\">a GraphQL performance technique for FaunaDB</a>. &nbsp;The lessons might be useful no matter which database you use.</p>\r\n<p>If you are skeptical about running relational workloads in a cloud native database, read about how <a href=\"https://fauna.com/blog/relational-nosql-is-an-option\">relational NoSQL is something you can have today in your database</a>. The underlying storage format of the data isn’t what matters—it is how you can access it.<br></p>",
        "blogCategory": [
            "10",
            "1462",
            "1530",
            "1531",
            "1866"
        ],
        "mainBlogImage": [
            "1604"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2019-01-08T09:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 6432,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "dd78f14a-dccf-4af6-a274-7981847b4554",
        "siteSettingsId": 6432,
        "fieldLayoutId": 4,
        "contentId": 2211,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Back to the Future with Relational NoSQL",
        "slug": "back-to-the-future-with-relational-nosql",
        "uri": "blog/back-to-the-future-with-relational-nosql",
        "dateCreated": "2019-11-15T08:52:06-08:00",
        "dateUpdated": "2019-11-18T09:45:32-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/back-to-the-future-with-relational-nosql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/back-to-the-future-with-relational-nosql",
        "isCommunityPost": false,
        "blogBodyText": "<p>The database renaissance began with the NoSQL movement, broadly, around 2008 as the technical demands of a new wave of consumer internet companies operating at huge scale (Twitter, Facebook, Google, Amazon) outstripped the capabilities of the legacy RDBMS. Additionally, the price structures of RDMBS vendors like Oracle were not supportive of the wildly smaller unit economics of these new companies, especially the ad-supported ones. What to do?</p>\n<p>Initially, companies turned to custom, distributed sharding services that used legacy databases as storage engines, especially MySQL and InnoDB. Distributed caches like Memcache and later Redis became ubiquitous. However, these systems were not self-operating. They still required DBA intervention to recover from failures; they were not self-sharding; they were not self-healing.</p>\n<p>I was at Twitter at the time running the software infrastructure team, and it seemed obvious to us that it was possible to build a system that could transparently deliver scale and flexibility, so, along with others in the industry, we set about to do it as open-source, initially investing heavily in Apache Cassandra, Hadoop, and other storage platforms. And when your business requires scale, anything that get in its way must be abandoned.</p>\n<p>The commercial vendors of these systems preached that nobody needed or wanted the traditional features of the RDBMS that their systems could not deliver, which is obviously untrue. Now every business requires scale—but what did we give up to get it with NoSQL? And can we get it back?</p>\n<h2>Practical effects of eventual consistency</h2>\n<p>The CAP theorem can be effectively summarized as: what happens when the network partitions? Does a system maintain consistency (correctness), and temporarily pause, or does it maintain availability, and make a best-effort to keep working? In practice, though, consistency and availability are choices along an envelope of potential trade-offs, and the real-world story is more nuanced.</p>\n<p>The first generation of NoSQL systems were eventually consistent, claiming that after a partition event they will reconcile conflicts in a variable but finite period of time, probabilistically voting on what the data is supposed to be. Few real-world eventually consistent systems are actually this sophisticated; instead, they use a simplistic “last-write-wins” based on the local system time. Even if they were smarter, this is not a useful guarantee for application building. It requires the systems designer to import a whole host of conflict-resolution functionality into the application layer.</p>\n<h3>Example</h3>\n<p>An obvious transactional failure pattern is as follows:</p>\n<ol><li>Bob deposits $200 via an ATM.</li><li>A database shard accepts an “eventually consistent” deposit to Bob. His new balance of $200 is queued.</li><li>Bob deposits $100 via a mobile app. This update goes to a different shard and replicates to the rest of the cluster, but not to the original shard.</li><li>The network heals and replication proceeds.</li><li>Because it happened later in physical time, Bob’s balance on the remaining shard is updated to $100 as per step 3.</li></ol>\n<p>Bob has now lost $200 in cash. Eventual consistency guaranteed availability. We can deposit money all day long! But did not guarantee correctness.</p>\n<h2>CRDTs and blockchains</h2>\n<p>A variety of schemes have been proposed to solve this problem, specifically conflict-free replicated data types (CRDTs), which effectively close over all intermediate states so that the final value can be rebuilt correctly. This would record Bob’s transactions as debits and credits rather than final balances. Construction of the final balance can be performed in the database or in the application layer. The degenerate case is a blockchain, which records all transactions for all time on all nodes in the cluster.</p>\n<p>However, in practice this doesn’t solve the problem at all, because a database is not a closed system. The whole point of reading and writing any data at all is to coordinate external effects. If Bob withdrew cash from an ATM, the money is gone, even if later the CRDT reveals that he didn’t have enough money at the time.</p>\n<p>This is not at all a theoretical problem. Deliberately inducing withdrawal races at ATMs is a widely reported type of fraud. Thus, databases need “external consistency”, colloquially known as ACID.</p>\n<h2>The legacy solution</h2>\n<p>Legacy databases (typically the centralized RDBMS) solve this problem by being undistributed. Ultimately, a single machine with a single disk—and for practical purposes, a single mutex on updates to that disk—is the source of truth. Updates to the state of the world are serialized, meaning that they happen in a single, deterministic order—and are externally consistent—they correspond to the order that occurred in physical time. Any scalability comes from vertical scaling of that single machine.</p>\n<p>This model is easy to implement but fails to meet a number of critical guarantees, specifically ones related to availability.</p>\n<p>Thus, we got the rise of primary/follower replicated systems, where the primary node could be manually replaced by a DBA with a follower node that asynchronously replicated the state of the primary. This isn’t great, but unlike an eventually-consistent distributed system, the scope of the inconsistency is known: it is exactly what might have happened between the last transaction replicated to the follower, and the externally-visible failure of the primary.</p>\n<p>This is effectively the world the consumer internet sharding systems were operating in:</p>\n<ul><li>they could perform transactions within a shard (a single machine)</li><li>they could manually or semi-manually fail over to a backup shard</li><li>they could not transact across multiple shards at the same time</li></ul>\n<p>For Twitter and Facebook this was more or less fine, but undesirable. For example, it’s confusing to get a notification on a phone about a message you can’t yet read on the website, but it’s not a catastrophe. Product features that required transactionality—username assignment, for example—but not as much scale remained in the legacy RDBMS.</p>\n<p>But as products became more complex the downsides of a lack of external consistency became increasingly severe.</p>\n<h2>Google Spanner</h2>\n<p><a href=\"https://cloud.google.com/spanner/\">Spanner</a> is Google’s solution to both of these problems. Initially available only within Google’s internal infrastructure, it is now available as a managed product on Google Cloud.</p>\n<p>It does two things:</p>\n<ul><li>Multi-shard transactions are implemented by a two-phase prepare/commit algorithm—essentially equivalent to Tuxedo, the 1980s transaction monitoring protocol.</li><li>Instead of relying on HA or mainframe-class hardware to maintain availability, shard failover on commodity hardware is automated via Paxos.</li></ul>\n<p>This approach works well up to a point. It guarantees serializability—updates to each individual shard are guaranteed to happen in real-time order. But it does not guarantee external consistency, or coordination of real time across shards. To solve this final problem, Spanner does one more thing:</p>\n<ul><li>Physical atomic clock hardware synchronizes the system time on all shards within very small error bounds.</li></ul>\n<p>Now the transaction coordinator can essentially say, “Hey shards, I’m doing a transaction at real time T, if you do not see any other updates also arrive within error bounds of our shared view of time, you know that it is unconflicted.” This induces a small delay in every externally-consistent read and write as every shard must wait out the clock ambiguity window.</p>\n<p>This is great for Google, barring the latency impact. They have the resources to build and manage customized atomic clock hardware and bounded-latency networks. However, there are a variety of new database systems that implement similar protocols without atomic clocks, but always at a cost.</p>\n<h2>Spanner over NTP</h2>\n<p>Databases that rely on NTP clock synchronization have a much longer ambiguity window—hundreds of milliseconds. In practice, these systems forgo the wait entirely, and fall back to guaranteeing single-record linearizability without external consistency. This can lead to similar cross-row, double-debit effects.</p>\n<p>They also do not offer fast externally serializable reads, but often read from the last known Paxos leader. This can violate serializability as well because the leader may not know that it has been deposed and will happily serve a stale value during the election window, which is typically on the order of multiple seconds. Preventing this window requires inducing a pause.</p>\n<p>Finally, if the clocks happen to desynchronize—something Google works mightily to prevent, because in the cloud, all kinds of events unrelated to the clock itself, like VM stalls, can cause this to happen—even the serializability guarantees on writes are lost.</p>\n<h2>Google Percolator</h2>\n<p>Another class of databases query a single physical clock (a “clock oracle” as described in the Google Percolator <a href=\"https://ai.google/research/pubs/pub36726\">paper</a>) have an ambiguity window equivalent to the roundtrip internet latency to that shared clock, which is even worse, and suffer from an obvious single point of failure.</p>\n<p>This model is equivalent to the multiprocessor RDBMS—which also uses a single physical clock, because it’s a single machine—but the system bus is replaced by the network. In practice, these systems give up multi-region scale out and are confined to a single datacenter.</p>\n<p>It is clear that without physical clock synchronization, distributed external consistency is impossible...or is it?</p>\n<h2>Avoiding physical clocks with Calvin</h2>\n<p>What if you could construct a logical clock oracle for all your transactions, that does not rely on any single physical machine, and could be widely distributed? This is essentially what <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a> does. Calvin was developed by <a href=\"http://www.cs.umd.edu/~abadi/\">Daniel Abadi</a>’s lab at Yale University.</p>\n<p>John Calvin was a French Protestant reformer who believed that the eventual destiny of every soul—either heaven or hell—was predetermined by God before the beginning of the world.</p>\n<p>This is how Calvin works as well. The order of transactions is decided by preprocessing—a subset of nodes act as partitioned oracles and assign an externally consistent order to all incoming transactions in a series of pipelined batches. The batches are then committed, 10ms at a time, to a distributed write-ahead log that can be implemented in Paxos or Raft.</p>\n<p>This preprocessing has a number of benefits:</p>\n<ul><li>It can be accomplished in a single network round-trip for the commit to the distributed log—no two-phase locking on the replicas.</li><li>Its operational topology is distinct from the replica topology, which reduces long tail latency.</li><li>Serializable reads require no coordination and do not have to wait out an ambiguity window. Instead, they scale out globally just like an eventually consistent system.</li></ul>\n<p>It necessarily makes some trade-offs, however:</p>\n<ul><li>Since transactions are committed in batches, write latency cannot fall below the time to wait for the next commit, which is 5ms on average, and can be as high as 10ms.</li></ul>\n<p><a href=\"http://www.fauna.com/\">FaunaDB</a>, which implements this model with a variety of performance improvements, is a Relational NoSQL system, not a NewSQL system, although there is nothing that specifically precludes FaunaDB from eventually implementing a SQL interface.</p>\n<h2>On CAP</h2>\n<p>Although it is true that no CP system can maintain liveness during a completely arbitrary partition event, in practice, we see that real-world availability of systems like Spanner and FaunaDB in the cloud is equivalent to the availability of AP systems. Faults beyond 99.9% availability are just as likely to come from implementation problems as from hardware and network failures, and the formal consistency models of the CP systems make them easier to verify under faults than AP systems.</p>\n<p>For example, a five datacenter FaunaDB cluster will tolerate the loss of two entire datacenters without losing liveness at all. Also, CP systems like FaunaDB can typically maintain availability for reads at a lower consistency level (like snapshot isolation) even during partition events, which is equivalent or better than the consistency level offered by AP systems all the time.</p>\n<p>Theoretically speaking, moving from 99.999% availability for writes to 99.9999% (a difference of a few minutes a year) is not worth it, especially when the cost of accepting writes during that pause is permanent data inconsistency.</p>\n<h2>Conclusion</h2>\n<p>Distributed transactions are one of the hardest problems in computer science. We are lucky to be living in a world where these systems are becoming available off-the-shelf. Any kind of bounded consistency is better than eventual consistency—even putting aside all the other problems with legacy NoSQL, like durability, and the legacy RDBMS, like operational overhead.</p>\n<p>However, the holy grail of distributed, external consistency without physical clocks still eludes us—except for Calvin. FaunaDB is the only production-ready implementation of the Calvin-style transaction protocol. I encourage you to be mindful of the consistency guarantees your database systems provide, and also to give FaunaDB a try.</p>",
        "blogCategory": [
            "1465",
            "1530"
        ],
        "mainBlogImage": [
            "6429"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2019-01-03T13:46:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1601,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "dc133e78-46d9-4b19-b74b-effd509b77bc",
        "siteSettingsId": 1601,
        "fieldLayoutId": 4,
        "contentId": 1203,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Serverless Change Capture for Ruby on Rails",
        "slug": "serverless-change-capture-for-ruby-on-rails",
        "uri": "blog/serverless-change-capture-for-ruby-on-rails",
        "dateCreated": "2019-01-03T12:36:24-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/serverless-change-capture-for-ruby-on-rails",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/serverless-change-capture-for-ruby-on-rails",
        "isCommunityPost": false,
        "blogBodyText": "<p>A common problem in business applications is change capture, which enables use cases like audit logging, history browsing, and value attribution for content optimization. In finance, audit logs are useful for regulatory compliance. In content and gaming industries, they can be useful to pinpoint which changes increase audience engagement. For enterprise SaaS provider <a href=\"https://www.breezeworks.com/features/scheduling/\">Breezeworks</a>, a service industry team and customer management platform, end users often want to know when an appointment or estimate was changed, and by whom. The combined feed of changes to important business objects like job schedules and equipment availability also offers Breezeworks power users an organization-wide pulse of their company, so they can stay ahead of problems before they impact the job site.</p>\r\n<blockquote>“The technology just works; it’s basically set and forget from our perspective.” <br>- Adam Block, CTO & Co-founder of Breezeworks</blockquote>\r\n<p>Adding change tracking to an existing application has the potential to be a complex and expensive undertaking, but when I spoke to Adam Block, CTO & Co-founder of Breezeworks, about how they use FaunaDB, at first he didn’t think his use case was impressive because it was too easy. When I asked him about his use case and pain points, he told me users often want to know when an appointment or estimate was changed, and who made the change. They accomplish this by adding an <a href=\"https://guides.rubyonrails.org/active_record_callbacks.html\">after_commit hook to their Ruby on Rails ActiveRecord</a> base model, which maintains a copy of the data in FaunaDB Cloud. </p>\r\n<figure><img src=\"{asset:1600:url}\" data-image=\"1600\"></figure>\r\n<p>“Teeing” writes to a new database is often part of a database evaluation process, so readers may be familiar with the pattern so far. Essentially, it means that everytime the application server writes to the SQL database, it also writes the same information to FaunaDB. Using the snapshot retention capabilities that FaunaDB offers out of the box gives Breezeworks a powerful API which powers user-facing history and activity stream views. Teeing this data to FaunaDB, and using its built-in history features, is simpler than modifying the SQL database schema to keep multiple versions of records around. </p>\r\n<p>Trying to do this with a single database schema that must represent both the application’s important business objects, as well as their historical changes, can add complexity that impacts every query. Instead, Breezeworks keeps their SQL database focussed on representing the business objects, and uses FaunaDB for change capture. Breezeworks’ application servers use Ruby on Rails with ActiveRecord, and the models connect to PostgreSQL to store the current state of business objects. Each time a model writes a record to PostgresSQL, it also updates the object in FaunaDB. This means their SQL queries can remain blissfully unaware of historical versions and all the complexity that comes with it. FaunaDB has built-in change tracking and event feeds so, when users browse history, not only are the queries efficient, but they are also easy to write.</p>\r\n<p>Breezeworks calls the plugin they developed in-house to write history “FaunaDB-Tee”, after <a href=\"https://en.wikipedia.org/wiki/Tee_(command)\">the unix command.</a> The pattern is so simple that implementing it yourself in another language or framework wouldn’t be much work. The core idea is to add a callback to your existing database driver that duplicates write operations to FaunaDB. You can query this duplicate data set with FaunaDB’s built in <a href=\"https://app.fauna.com/tutorials/temporal\">temporal history and snapshot retention features</a>, including the ability to request an event feed for any query. Fauna Query Language (FQL) makes it easy to offer users historical views and change feeds of important business objects, filtered for specific projects and roles.</p>\r\n<blockquote>Fauna Query Language (FQL) makes it easy to offer users historical views and change feeds of important business objects, filtered for specific projects and roles.</blockquote>\r\n<p>Change capture benefits Breezeworks in ways they couldn’t have predicted when implementing it. Customer support depends on it to help users recover from mistakes. Users can see who in their organization made a change, so they know who to talk to about it. Managers can use the combined event feed to see when equipment shortages are holding up work, or which jobs are falling behind. And having historical views of estimates and schedules is important in case of disputes.</p>\r\n<p>By teeing your data to FaunaDB Cloud, you can get serverless change capture, and enable historical queries as well as event feeds, without impacting your existing database. Change capture doesn’t have to be a complex addition to your application database; it can run smoothly alongside it and deliver tons of business value.</p>",
        "blogCategory": [
            "3",
            "1530"
        ],
        "mainBlogImage": [
            "1600"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2018-12-20T07:59:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1595,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "dbe5411f-7a30-4ec0-bef5-c27be3e8922f",
        "siteSettingsId": 1595,
        "fieldLayoutId": 4,
        "contentId": 1197,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB: A Guide for Relational Users",
        "slug": "faunadb-a-guide-for-relational-users",
        "uri": "blog/faunadb-a-guide-for-relational-users",
        "dateCreated": "2018-12-18T14:52:21-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-a-guide-for-relational-users",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-a-guide-for-relational-users",
        "isCommunityPost": false,
        "blogBodyText": "<p>We are pleased to announce a new white paper to follow up on Fauna CEO Evan Weaver's recent modern history of NoSQL database solutions (<a href=\"https://www.infoq.com/articles/relational-nosql-fauna\">Back to the future with Relational NoSQL </a>published in <a href=\"https://www.infoq.com\">InfoQ</a>). This new white paper is now available for download <a href=\"https://www2.fauna.com/whitepaper-relationalusers\">here</a>.</p>\r\n<p>The white paper first looks into <a href=\"https://en.wikipedia.org/wiki/Codd%27s_12_rules\">E. Codd's 12 rules</a> for relational databases and how they are applied in the context of commercial relational databases today. The paper then dives into how relational databases became increasingly insufficient as the internet was adopted from academic to commercial purposes.</p>\r\n<p>The deluge of data led to the need for a horizontal scale that the relational databases failed to provide. This ushered in the era of NoSQL databases that for the last decade or so has outpaced the adoption of relational systems.</p>\r\n<figure><img src=\"{asset:1597:url}\" data-image=\"1597\"></figure>\r\n<p>However, wider adoption of NoSQL across mission-critical systems has been limited for a variety of reasons, such as inefficient operational readiness and lack of support for transactions. When it comes to capturing transactions in a billing or a reservation system, enterprises still rely on relational databases even though they are fully aware of the shortcomings in the technology.</p>\r\n<p><strong>So what should the modern day database look like? </strong></p>\r\n<p>The answer may be different depending on who you ask. But they are all looking for a single solution to their problems.<br></p>\r\n<figure><img src=\"{asset:1596:url}\" data-image=\"1596\"></figure>\r\n<p>We need a database product that marries the best of relational and NoSQL solutions in the same system. We need strongly consistent transactions in globally distributed systems.</p>\r\n<p>The white paper then briefly examines some of the features that take FaunaDB closer to relational systems on top of a very strong NoSQL foundation. It concludes with a technical example explaining how to map a well-known relational model (the emp & dept tables in the SCOTT schema found in Oracle databases) into FaunaDB.</p>\r\n<p>It is meant to be an easy read explaining how FaunaDB is bringing the best of both relational and NoSQL to the same place.</p>\r\n<p>Download it <a href=\"https://www2.fauna.com/whitepaper-relationalusers\">here</a>.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [
            "1599"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1383",
        "postDate": "2018-12-17T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1589,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d9111fe3-6dd2-45d1-aecf-39ad37a54e9a",
        "siteSettingsId": 1589,
        "fieldLayoutId": 4,
        "contentId": 1191,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introducing Endpoints",
        "slug": "introducing-endpoints",
        "uri": "blog/introducing-endpoints",
        "dateCreated": "2018-12-14T10:51:46-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introducing-endpoints",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introducing-endpoints",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://fauna.com/blog/introducing-fauna-shell\"></a>When we introduced the <a href=\"https://fauna.com/blog/introducing-fauna-shell\">Fauna Shell</a>, one of our goals was to make FaunaDB easier to use. In that article, we showed you how to get started with FaunaDB by using our <a href=\"https://fauna.com/faunadb/serverless-cloud\">serverless cloud</a>&nbsp;offering. Of course, we want to offer more, transforming it into the tool that makes your day-to-day usage of the database easier. In this blogpost, we show how to connect the shell to different FaunaDB instances. Enter endpoints.</p>\r\n<h2>Endpoints</h2>\r\n<p>To connect to different FaunaDB instances, we introduced the idea of endpoints into the Fauna Shell. We wanted them to be easy to manage—adding, listing, or removing endpoints had to be a frictionless task.</p>\r\n<p>So, let’s say that you have two FaunaDB instances running: one in localhost used for development, and one instance in your private cloud. Let's create our first endpoint:<br></p>\r\n<figure><img src=\"{asset:1590:url}\" data-image=\"1590\"></figure>\r\n<pre>$ fauna add-endpoint \"https://localhost:443\"\r\nEndpoint Key: ****************************************\r\nEndpoint Alias [localhost]: localhost\r\n</pre>\r\n<p>After running the <code>add-endpoint</code>, you will be prompted for both a database key for that endpoint and an endpoint alias. It should be something that lets you easily reference the endpoint, as we will see later (the default is the endpoint hostname).&nbsp;<br></p>\r\n<p>Now you can list endpoints and see that your endpoint was created successfully:</p>\r\n<pre>$ fauna list-endpoints\r\nlocalhost *\r\n</pre>\r\n<p>Let's add a new endpoint:<br></p>\r\n<figure><img src=\"{asset:1591:url}\" data-image=\"1591\"></figure>\r\n<pre>$ fauna add-endpoint \"https://cluster-us-east.example.com:443\"\r\nEndpoint Key: ****************************************\r\nEndpoint Alias [cluster-us-east.example.com]: us-east\r\n</pre>\r\n<p>As you can see in this case, we changed the endpoint alias to have a shorter name, since we want to remember that this FaunaDB instance is running in our <code>us-east</code> cluster. Let's list endpoints again:</p>\r\n<pre>$ fauna list-endpoints\r\nlocalhost *\r\nus-east\r\n</pre>\r\n<p>There's our newly created endpoint <code>us-east</code>. You may have noticed that there's a star <code>*</code> next to the <code>localhost</code> endpoint name. That's a way to tell which one is the default endpoint used by the Fauna Shell when connecting to FaunaDB.</p>\r\n<h2>Connecting to a different FaunaDB instance&nbsp;</h2>\r\n<p>\r\nIf you want the shell to connect to the <code>us-east</code> endpoint instead of using your <code>localhost</code> database, you can do that by setting <code>us-east</code> as the default endpoint by running this command:\r\n<br></p>\r\n<figure><img src=\"{asset:1592:url}\" data-image=\"1592\"></figure>\r\n<pre>$ fauna default-endpoint us-east</pre>\r\n<p>Let's list endpoints to make sure our change took effect:</p>\r\n<pre>$ fauna list-endpoints\r\nlocalhost\r\nus-east * \r\n</pre>\r\n<h2>What about the Cloud endpoint?</h2>\r\n<p>If you followed the steps in the Introducing <a href=\"https://fauna.com/blog/introducing-fauna-shell\">Fauna Shell</a>&nbsp;blog post, you may have ended up with a <code>cloud</code> endpoint as well:</p>\r\n<pre>$ fauna list-endpoints\r\ncloud\r\nlocalhost \r\nus-east *\r\n</pre>\r\n<p>\r\nThat's because the <code>cloud-login</code> command showcased in the <a href=\"https://fauna.com/blog/introducing-fauna-shell\">previous blogpost</a> created a <code>cloud</code> endpoint for us.</p>\r\n<h2>Deleting an endpoint</h2>\r\n<p>Let's say that we decommission our <code>us-east</code> cluster and decide to run all our business out of Fauna Cloud. Then, we need to delete the <code>us-east</code> endpoint from our configuration.</p>\r\n<p>First, we'll set the <code>cloud</code> endpoint as the default one:<br></p>\r\n<figure><img src=\"{asset:1594:url}\" data-image=\"1594\"></figure>\r\n<pre>$ fauna default-endpoint cloud\r\n$ fauna delete-endpoint 'us-east'</pre>\r\n<p>Now, let's list endpoints again to see the results of running the previous commands:</p>\r\n<pre>$ fauna list-endpoints\r\ncloud *\r\nlocalhost\r\n</pre>\r\n<p>As you can see, Fauna Shell makes it easy to manage endpoints that connect to FaunaDB. Now, as a developer you might be wondering where is the endpoint configuration stored?</p>\r\n<h2>What's happening behind the scenes?</h2>\r\n<p>When we were designing the shell, we tried to keep the solution as simple as possible. That's why, when it came to storing this kind of configuration information, we decided to simply keep it inside an <code>ini</code> file in your home folder. Try checking the contents of the file <code>~/.fauna-shell</code>. (On Windows, that file will be in your home folder.)</p>\r\n<p>Here's a sample <code>~/.fauna-shell</code>:</p>\r\n<pre>default=cloud\r\n\r\n[localhost]\r\ndomain=localhost\r\nport=8443\r\nscheme=http\r\nsecret=the_secret\r\n\r\n[cloud]\r\ndomain=db.fauna.com\r\nscheme=https\r\nsecret=FAUNA_SECRET_KEY\r\n\r\n[us-east]\r\ndomain=cluster-us-east.example.com\r\nport=443\r\nscheme=https\r\nsecret=OTHER_FAUNA_SECRET\r\n</pre>\r\n<p>By keeping things simple, we got a config file that fulfills its purpose while providing us with a very readable format (should our users try to connect to different endpoints).</p>\r\n<h2>Conclusion</h2>\r\n<p>In this blog post, we learned how to access different FaunaDB server instances from the Fauna Shell by declaring endpoints for each of them. This becomes quite useful when we want to run queries against different databases and servers. We can see that the shell comes packed with a handful of commands that will ease your day to day work with FaunaDB.</p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [
            "1588"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2018-12-14T07:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1579,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "f9f000a5-375f-42ad-9da3-ea99ef9712f1",
        "siteSettingsId": 1579,
        "fieldLayoutId": 4,
        "contentId": 1181,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Partitioned Consensus and Its Impact on Spanner’s Latency",
        "slug": "partitioned-consensus-and-its-impact-on-spanners-latency",
        "uri": "blog/partitioned-consensus-and-its-impact-on-spanners-latency",
        "dateCreated": "2018-12-12T17:46:31-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/partitioned-consensus-and-its-impact-on-spanners-latency",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/partitioned-consensus-and-its-impact-on-spanners-latency",
        "isCommunityPost": false,
        "blogBodyText": "<p><em></em><em><a href=\"http://cs-www.cs.yale.edu/homes/dna/\">Daniel J. Abadi</a>&nbsp;is the Darnell-Kanal Professor of Computer Science University of Maryland, College Park, was previously&nbsp;a professor at Yale University. His research focuses on database management systems architecture. He received a Ph.D. from MIT and a M.Phil from Cambridge.&nbsp;<em>This post is cross-posted on his&nbsp;<a href=\"https://dbmsmusings.blogspot.com/2018/12/partitioned-consensus-and-its-impact-on.html\">blog</a>.</em></em></p>\r\n<p>In a <a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">post that I published in September</a>, I described two primary approaches for performing consensus in distributed systems, and how the choice of approach affects the consistency guarantees of the system. In particular, consensus can either be <strong>unified</strong>, such that all writes in the system participate in the same distributed consensus protocol, or it can be <strong>partitioned</strong>, such that different subsets of the data participate in distinct consensus protocols.&nbsp;</p>\r\n<p>The primary downside of partitioned consensus was that achieving global consistency is much more complicated. Consistency guarantees require that requests submitted after previous requests complete will never “go back in time” and view a state of the system that existed prior to the completed request. Such guarantees are hard to enforce in partitioned consensus systems since different partitions operate independently from each other: Consistency requires a notion of “before” and “after”—even for events on separate machines or separate partitions. For partitions that operate completely independently, the most natural way to define “before” and “after” is to use real time—the time on the clocks of the different partitions. However, clocks tend to skew at the millisecond granularity, and keeping clocks in sync is nontrivial. We discussed how Google has a hardware solution that aids in clock synchronization, while other solutions attempt to use software-only clock synchronization algorithms.&nbsp;</p>\r\n<p>In contrast, unified consensus results in a global order of all requests. This global order can be used to implement the notion of “before” and “after” without having to rely on local clock time, which entirely avoids the need for clock synchronization. This results in stronger consistency guarantees: unified consensus systems can guarantee consistency at all times, while partitioned consensus systems can only guarantee consistency if the clock skew stays within an expected bound. For software-only implementations, it is hard to avoid occasionally violating the maximum clock skew bound assumption, and the violations themselves may not be discoverable. Therefore, unified consensus is the safer option.</p>\r\n<p>The post led to several interesting debates, most of which are beyond the scope of this post. However, there was one interesting debate I’d like to explore more deeply in this post. In particular, the question arose: Are there any fundamental <strong>latency </strong>differences between unified-consensus systems and partitioned-consensus systems? When I read the comments to my post (both on the post itself and also external discussion boards), I noticed that there appears to be a general assumption amongst my readers that unified consensus systems must have higher latency than partitioned consensus systems. One reader even accused me of purposely avoiding discussing latency in that post in order to cover up a disadvantage of unified consensus systems.&nbsp;</p>\r\n<p>In this post, I want to clear up some of the misconceptions and inaccurate assumptions around these latency tradeoffs, and present a deeper (and technical) analysis on how these different approaches to consensus have surprisingly broad consequences on transaction latency. We will analyze the latency tradeoff from three perspectives: (1) Latency for write transactions, (2) Latency for linearizable read-only transactions and (3) Latency for serializable snapshot transactions.<br></p>\r\n<h3>Latency for Write Transactions</h3>\r\n<p> The debate around write transactions is quite interesting since valid arguments can be made for both sides.&nbsp;</p>\r\n<p>The partitioned-consensus side points out two latency downsides of unified consensus: (1) As mentioned in <a href=\"http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html\">my previous post</a>, in order to avoid scalability bottlenecks, unified consensus algorithms perform consensus batch-at-a-time instead of on individual requests. They, therefore, have to pay the additional latency of collecting transactions into batches prior to running consensus. In the original Calvin paper, batch windows were 10ms (so the average latency would be 5ms); however, we have subsequently halved the batch window latency in my labs at Yale/UMD. <a href=\"http://www.fauna.com/\">FaunaDB</a> (which uses Calvin’s unified consensus approach) also limits the batch window to 10ms. (2) For unified consensus, there will usually be one extra network hop to get the request to the participant of the consensus protocol for its local region. This extra hop is local, and therefore can be assumed to take single-digit milliseconds. &nbsp;If you combine latency sources (1) and (2), the extra latency incurred by the preparation stage for unified consensus is approximately 10-15ms.&nbsp;</p>\r\n<p>On the other hand, the unified-consensus side points out that multi-partition atomic write transactions require two-phase commit for partitioned-consensus systems. For example, let’s say that a transaction writes data in two partitions: A and B. In a partitioned-consensus system, the write that occurred in each partition achieves consensus separately. It is very possible that the consensus in partition A succeeds while in B it fails. If the system guarantees atomicity for transactions, then the whole transaction must fail, which requires coordination across the partitions—usually via two-phase commit. Two-phase commit can result in significant availability outages (if the coordinator fails at the wrong time) unless it runs on top of consensus protocols.&nbsp;</p>\r\n<p>Thus Spanner and Spanner-derivative systems all run two-phase commit over partitioned consensus for multi-partition transactions. The latency cost of the Raft/Paxos protocol itself (once it gets started) is the same for unified and partitioned consensus, but two-phase commit requires <strong>two rounds of consensus</strong> to commit such transactions. A single round of consensus may take between 5ms and 200ms, depending on how geographically disperse the deployment is. Since Spanner requires two sequential rounds, the minimum transaction latency is double that—between 10ms for single-region deployments to 400ms for geographically disperse deployments.&nbsp;</p>\r\n<p>In practice, this two-phase commit also has an additional issue: a transaction cannot commit until all partitions vote to commit. A simple majority is not sufficient—rather every partition must vote to commit. A single slow partition (for example, a partition undergoing leader election) stalls the entire transaction. This increases the observed long tail latency in proportion to transaction complexity.<br>In contrast, unified consensus systems such as Calvin and its derivatives such as FaunaDB do not require two-phase commit. (A discussion of how to avoid two-phase commit in a unified consensus system can be found in <a href=\"http://www.vldb.org/pvldb/vol7/p821-ren.pdf\">this VLDB paper</a>. FaunaDB’s approach is slightly different, but the end result is the same: no two-phase commit.)&nbsp;As a result, unified consensus systems such as Calvin and FaunaDB only require one round of consensus to commit all transactions—even transactions that access data on many different machines.&nbsp;</p>\r\n<p>The bottom line is that the better latency option between unified or partitioned consensus for write transactions is somewhat workload dependent. Unified consensus increases latency for all transactions by a little, but partitioned consensus can increase latency by a lot more for multi-partition transitions. For most applications, it is impossible to avoid multi-partition transactions. For example, many applications allow transactional interactions between arbitrary users (payments between users, exchanges between users, “friendship” status updates between users, gaming interactions, etc.). Although it is possible to group users into partitions such that many of their interactions will be with other users within that partition (e.g. partition by a user’s location), as long as arbitrary interactions are allowed, there will always be interactions between users in different partitions. These multi-partition interactions are much slower in partitioned-consensus systems. Thus, for most workloads, unified-consensus is the better latency option for write transactions.<br></p>\r\n<h3>Latency for Linearizable Read-Only Transactions</h3>\r\n<p>Linearizable read-only transactions are generally sent to the consensus leader’s region and performed (or at least receive a timestamp) there [other options exist, but this is what Spanner and other systems mentioned in my previous post do]. In unified-consensus, there is only one leader region for the whole system. Linearizable read transactions that initiate from near this region will be processed with low latency, while transactions that initiate from farther away will observe correspondingly higher latency.&nbsp;</p>\r\n<p>Meanwhile, in partitioned-consensus, there is one leader per partition, and these leaders can be located in different regions. The partitioned-consensus supporters argue that this can lead to lower latency in an array of common scenarios. An application developer can specify a location-based partitioning algorithm. All data that is usually accessed from region X should be placed in the same partition, with a consensus leader located in region X. All data that is usually accessed from region Y should be placed in the same partition, with a consensus leader located in region Y. In doing so, a larger number of read-only transactions will observe lower latency.&nbsp;</p>\r\n<p>The downside of this approach is that it breaks the abstraction of the consensus protocol as a separate component of the system—now the consensus protocol and data storage layer become much more intertwined, increasing the monolithicity of the system. Furthermore, consensus protocols run leader elections after a lease expires, and would have to reduce the democratic nature of this protocol in order to ensure the leader remains in the closest possible region. Finally, it increases complexity and reduces the flexibility of the partitioning protocol. As far as I know, the most well-known example of a partitioned-consensus system—Spanner —does not take advantage of this potential optimization for these reasons.&nbsp;</p>\r\n<p>Consequently, although in theory, there is a potential latency advantage for partitioned-consensus systems for linearizable read-only transactions, in practice this advantage is not realized.</p>\r\n<p>On the other hand, there is a fundamental latency advantage for unified-consensus systems in the presence of multi-partitioned transactions. A multi-partition transaction in a partitioned-consensus system must involve more than one leader. The leaders of each partition accessed by the read transaction must communicate with each other in order to figure out a timestamp at which this linearizable read can be safely performed (see sections 4.1.4 and 4.2.2 of <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">the Spanner paper</a>).&nbsp;Alternatively, a timestamp sufficiently into the future (at the end of the clock skew uncertainty bound) can be chosen at which to perform the read.&nbsp;</p>\r\n<p>Either way—whether it is communication across leaders (that may be located in different geographic regions) or whether it is waiting until the end of the clock skew uncertainty bound—multi-partition reads pay an additional latency cost in order to ensure linearizability. In contrast, unified consensus systems have only a single leader region, and can perform linearizable reads across the servers in this region, without any communication with other regions or waiting for clock skew uncertainty windows to close. <br></p>\r\n<h3>Latency for Serializable Snapshot Read-Only Transactions</h3>\r\n<p>Many applications—even if they require linearizable write transactions—do not require linearizable read-only transactions, and instead are content to read from a recent snapshot of the database state. However, such snapshot reads must be serializable—they should reflect the database state as of a particular transaction in the serial order, and none of the writes from transactions after that transaction. <br>Recall that transactions may write data on different machines or&nbsp;partitions.&nbsp;</p>\r\n<p>For example, a transaction, T, may write data on partition A and partition B. A serializable snapshot that includes data from both partition A and partition B must therefore include both of T’s writes to those partitions, or neither. In particular, it should include both of T’s writes if the snapshot is as of a point in time “after” T, and neither of T’s writes if the snapshot is as of a point in time “before” T. Note that this notion of “point in time” must exist—even across partitions.Therefore, once again, there needs to be a global notion of “before” and “after”—even for writes across different machines. As long as this notion of “before” and “after” exists, such snapshot reads can be sent to any replica to be processed there, and do not require any consensus or interaction with consensus leader regions. This is critically important to support low latency reads in a geographically disperse deployment.&nbsp;</p>\r\n<p>As mentioned in the introductory paragraph of this post, both unified- and partitioned-consensus systems are capable of generating global notions of “before” and “after”, and thus both types of systems are able to achieve the performance advantage of being able to perform these reads from any replica. However, as we mentioned above, unified-consensus systems can achieve this global notion of “before” and “after” without any clock synchronization, while partitioned-consensus systems use clock synchronization. Thus, unified-consensus can always achieve correct serializable snapshot reads, while partitioned-consensus can only achieve the same result if the maximum clock skew bound assumption is not violated. <br></p>\r\n<h3>Conclusion</h3>\r\n<p>The latency debate between unified vs. partitioned consensus is an intricate one. However, it is clear that multi-partition transactions exacerbate the disadvantages of partitioned-consensus transactions in (at least) three dimensions:<br></p>\r\n<ol><li>\r\nMulti-partition transactions require two-phase commit on top of the consensus protocol in partitioned-consensus systems. &nbsp;In many deployments, consensus across replicas is the latency bottleneck. By requiring two-phase commit on top of consensus, partitioned-consensus systems result in (approximately) double the latency (relative to unified-consensus) in such deployments, and higher long tail latency.\r\n</li><li>\r\nMulti-partition transactions require communication across leaders or waiting out clock skew uncertainty bounds for linearizable transactions—even for linearizable read-only transactions. \r\n</li><li>\r\nPartitioned-consensus systems require clock synchronization in order to achieve low latency snapshot reads (in addition to all linearizable operations). Any incorrect assumptions of the maximum clock skew across machines may result in serializability violations (and thus incorrect results being returned).</li></ol>\r\n<p>As we discussed above, it is usually impossible to avoid multi-partition transactions in most real-world applications. Furthermore, as an application scales, the number of partitions must increase, and thus the probability of multi-partition transactions is also likely to increase. Therefore, the disadvantages of partitioned-consensus systems relative to unified-consensus systems accentuate as the application scales. <br></p>",
        "blogCategory": [
            "1462"
        ],
        "mainBlogImage": [
            "1586"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-11-30T16:41:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1533,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "2422dbcb-0e59-4115-8baa-7bb768706a42",
        "siteSettingsId": 1533,
        "fieldLayoutId": 4,
        "contentId": 1170,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Efficient GraphQL Resolvers for FaunaDB",
        "slug": "efficient-graphql-resolvers-for-faunadb",
        "uri": "blog/efficient-graphql-resolvers-for-faunadb",
        "dateCreated": "2018-11-30T15:56:20-08:00",
        "dateUpdated": "2019-07-11T10:28:41-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/efficient-graphql-resolvers-for-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/efficient-graphql-resolvers-for-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "\r\n<div class=\"admonitionblock\">\r\n<div class=\"icon\">Deprecated blog</div><br>\r\n<h3>FaunaDB now has native GraphQL!</h3>\r\n<p>FaunaDB now has native GraphQL, so this blog has been deprecated. We recommend following these tutorials instead:</p>\r\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\r\n<p>You can also check out the <a href=\"https://docs.fauna.com/fauna/current/reference/graphql/\">GraphQL Reference</a> section in our docs to learn more about:</p>\r\n<ul>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/endpoints\" class=\"page\">Endpoints</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/directives/\" class=\"page\">Directives</a></li><li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/relations\" class=\"page\">Relations</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/functions\" class=\"page\">User-defined functions</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/troubleshooting\" class=\"page\">Troubleshooting</a></li></ul>\r\n</div>\r\n<p>GraphQL is an API specification that standardizes how clients request data from servers, It is rapidly becoming a popular way to specify the data that backend services should provide to front end clients. It is independent of any underlying data model, and is routinely used to query relational, document, and graph databases, as well as other backend microservices such as search and analytics. This article covers a GraphQL performance technique for FaunaDB, and the lessons might be useful no matter which database you use.</p>\r\n<blockquote>Front end developers can add features and optimize the data they request, without making cumbersome change requests of backend API developers.</blockquote>\r\n<p>A GraphQL-based system decouples backend data layout from client requirements. This allows clients to specify the shape of data they are looking for, simplifying the process of creating new features. Instead of requesting a backend API change each time the front-end requires a new piece of data, the front end engineers can simply add the new field to the GraphQL query specification. If that field is already supported by the GraphQL resolver, then no changes are needed on the backend. Front end developers can add features and optimize the data they request, without making cumbersome change requests of backend API developers.</p>\r\n<p>One consequence of this architecture is that the backend server must resolve the GraphQL query, converting it to requests for backend microservices. For instance an app client might request a blog post, it’s comments, and it’s commenter’s avatars. If the posts, comments, and avatars are all in different systems, GraphQL will issue requests in parallel when possible, and stitch the results back together. This hides the complexity of microservices from API users.</p>\r\n<blockquote>FaunaDB’s query language makes it easy to combine complex query fragments before sending them to the database.</blockquote>\r\n<p>However, for applications which are querying data already stored in a single database, breaking the query apart with resolvers and issuing multiple queries can create unnecessary bottlenecks. <a href=\"https://blog.apollographql.com/optimizing-your-graphql-request-waterfalls-7c3f3360b051\">Intelligent batching is the answer.</a> FaunaDB’s query language makes it easy to combine complex query fragments before sending them to the database. This means your GraphQL resolvers can compose query fragments into the optimal FaunaDB query. This leads to improved performance for users, while freeing developers to iterate on GraphQL queries.</p>\r\n<figure><a href=\"https://shiftx.com/\"><img src=\"{asset:1534:url}\" data-image=\"1534\" alt=\"Identify and Improve Business Inefficiencies\" title=\"Identify and Improve Business Inefficiencies\"></a><figcaption>ShiftX Screenshot</figcaption></figure>\r\n<p>The <a href=\"https://www2.fauna.com/cs_shiftx\">early success ShiftX experienced with FaunaDB</a> inspired them to build a deeper GraphQL integration, which they are in the process of extracting into an open source library. To give a preview of how this looks for them, here is an example GraphQL request to load a process graph, which is made of edges and different types of nodes, along with associated issues:     &nbsp;&nbsp;&nbsp;timestamp</p>\r\n<pre>      \r\n   query ReadProcess($id: ProcessId! ) {\r\n    readProcess(id: $id) {\r\n      id\r\n      name\r\n      owner {\r\n        id\r\n        email\r\n      }\r\n      nodes {\r\n        id\r\n        type\r\n        narrative\r\n        issueCount\r\n        ... on ActionNode {\r\n          actor {\r\n            actorId\r\n          }\r\n        }\r\n        ... on TransactionNode {\r\n          initiator {\r\n            actorId\r\n          }\r\n          recipient {\r\n            actorId\r\n          }\r\n          channel {\r\n            channelId\r\n          }\r\n        }\r\n        ... on InteractionNode {\r\n          initiators {\r\n            actorId\r\n          }\r\n          recipients {\r\n            actorId\r\n          }\r\n          channel {\r\n            channelId\r\n          }\r\n        }\r\n      }\r\n      edges {\r\n        id\r\n        originId\r\n        targetId\r\n      }\r\n      issues {\r\n        page {\r\n          id\r\n          title\r\n          description\r\n          severity\r\n          status\r\n          nodeId\r\n          created {\r\n            timestamp\r\n          }\r\n        }\r\n      }\r\n      updated {\r\n        timestamp\r\n      }\r\n      created {\r\n        timestamp\r\n      }\r\n    }\r\n  }\r\n</pre>\r\n<p>After the resolvers have parsed and combined it, a single query is issued to FaunaDB. It’s a little long to show here, but logically it is doing things like paginating over indexes of nodes and edges, and selecting the fields needed by GraphQL.</p>\r\n<p>FaunaDB’s query language makes combining fragments easy, so GraphQL resolvers can compose queries programmatically. It’s only once the query reaches the top of the GraphQL tree that it needs to be sent to the database for processing. One query round trip is faster and easier to deal with.</p>\r\n<p>If you are interested in GraphQL and FaunaDB, you can <a href=\"https://fauna.com/blog/graphql-faunadb\">start with this simple tutorial</a>. Watch this space for more GraphQL content, and more details on how ShiftX is using FaunaDB.</p>\r\n\r\n<style>\r\n.admonitionblock{padding: 1rem 1.5rem;border-left:3px solid #d32f2f; background: #fff;margin-bottom:2rem;\r\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\r\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\r\nbody .description .admonitionblock h3{margin-top:0;}\r\n.admonitionblock .icon {\r\nbackground: #d32f2f;\r\n    font-size: .73rem;\r\n    padding: .3rem .5rem;\r\n    height: 1.35rem;\r\n    line-height: 1;\r\n    font-weight: 500;\r\n    text-transform: uppercase;\r\nmargin-bottom:1.5rem;\r\ndisplay:inline-block;\r\n    color: #fff;\r\n}\r\n</style>",
        "blogCategory": [
            "3"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-11-15T10:53:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1476,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "919e0ff7-809f-434f-aee4-9b3fe35b6e5d",
        "siteSettingsId": 1476,
        "fieldLayoutId": 4,
        "contentId": 1134,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "QCon and the Future of Serverless",
        "slug": "qcon-and-the-future-of-serverless",
        "uri": "blog/qcon-and-the-future-of-serverless",
        "dateCreated": "2018-11-13T16:14:33-08:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/qcon-and-the-future-of-serverless",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/qcon-and-the-future-of-serverless",
        "isCommunityPost": false,
        "blogBodyText": "<p>Last week I had the pleasure of <a href=\"https://qconsf.com/sf2018/presentation/deterministic-global-database-consistency\">exhibiting and speaking at QConSF</a>. The event went smoothly and I even got to play with a Magic Leap headset. There were all kinds of talks on foundational technologies like MySQL and NPM, and a crowd ready to receive them. There were a handful of database vendors on hand, but our booth was the most fun. (The conference staff shirts were sponsored by Fauna, here are the QCon volunteers below.)<br></p>\r\n<figure><img src=\"{asset:1485:url}\" data-image=\"1485\"></figure>\r\n<p>My talk, <a href=\"https://www.slideshare.net/JChrisAnderson/qconsf-faunadb-deterministic-transactions\">about consistency without clocks, comparing Google Spanner with FaunaDB’s approach</a>, and making the argument that our deterministic transactions are more suited to real world deployments, had a strong audience for the end of the day. Tough questions were asked about the determinacy of raft consensus itself.</p>\r\n<h2>The future of Serverless</h2>\r\n<p>Being in the city gives me a chance to connect with local luminaries, and the conversations kept coming around to function-as-a-service runtimes and business models. As the creator of one of the <a href=\"http://docs.couchdb.org/en/2.2.0/api/ddoc/render.html\">original serverless APIs</a>, I’ve watched this wave for almost a decade. It was fun to get plugged into the latest changes, and see a consensus emerging about what serverless means and where it is going.</p>\r\n<p>Lambda already serves web applications, and it’s already a simpler choice for a lot of workloads that would have gone to Hadoop in years past. As the platform expands, especially with edge functionality like Cloudflare Workers, applications and frameworks are taking advantage of the new landscape.</p>\r\n<p>Talking to Brian LeRoux and Ryan Block from <a href=\"http://begin.com\">Begin</a>, a picture emerges of small functions, loosely joined. Lightweight functions can run faster and closer to the user than monolithic apps, and most importantly, depend on the smallest environment possible, lowering the surface area for features, bugs, and security updates. Small functions are faster, easier to develop, and safer to deploy.</p>\r\n<p>Begin offers an opinionated approach to serverless application architecture that is in alignment with performance best practices. Their model means developers can transfer skills across an organization’s codebase, without restricting what an individual function can accomplish. You can read my <a href=\"https://fauna.com/blog/using-faunadb-with-begin-com\">hello world with FaunaDB and Begin here.</a></p>\r\n<p>Further afield, talking to Mikeal Rogers from Protocol Labs, a picture emerges of an interplanetary NPM, where WebAssembly modules are addressed by content hash, and used to manipulate other web data. In this world, service providers compete on price and performance, while developers will have robust access to all the code ever published. Modules that are well used will acquire reputation, and the security landscape will become easier to audit.</p>\r\n<p>Anywhere serverless goes, FaunaDB will be there, with our globally replicated ACID transactions. Read <a href=\"https://fauna.com/blog/serve-your-faunadb-single-page-app-from-ipfs\">a tutorial here about hosting your FaunaDB app on IPFS.</a></p>\r\n<p></p>",
        "blogCategory": [
            "1530"
        ],
        "mainBlogImage": [
            "1475"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "473",
        "postDate": "2018-11-15T04:35:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1466,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "ec1d414d-a818-4536-b763-6590967a2499",
        "siteSettingsId": 1466,
        "fieldLayoutId": 4,
        "contentId": 1124,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Fauna News: \"State of the Enterprise Database 2018\" Survey Results",
        "slug": "enterprise-database-survey-2018-results",
        "uri": "blog/enterprise-database-survey-2018-results",
        "dateCreated": "2018-11-13T08:06:55-08:00",
        "dateUpdated": "2019-09-24T10:53:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/enterprise-database-survey-2018-results",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/enterprise-database-survey-2018-results",
        "isCommunityPost": false,
        "blogBodyText": "<p><strong></strong></p>\r\n<p><strong>Fauna’s “State of the Enterprise Database 2018” Report Reveals that Cloud Adoption and &nbsp;Security Concerns are Key Considerations in Choosing Enterprise Datastores</strong></p>\r\n<p></p>\r\n<p><em>In Light of these Desires, Findings from Recent User Survey Demonstrate that a Majority of Businesses are Beholden to Legacy Technology and are Limited in their Potential for Global Growth</em></p>\r\n<p></p>\r\n<p><strong>SAN FRANCISCO, Calif</strong>. – <strong>November 15, 2018 – </strong><a href=\"http://www.fauna.com/\">Fauna</a>, provider of the leading relational NoSQL database, announced it has released a new report, “State of the Enterprise Database 2018,” that draws findings from a recent survey of more than 400 database managers and decision makers. Among the key findings, survey respondents across all industries ranked security as the most important aspect in choosing a database solution. Specifically, security was found to be roughly 13 times more important than cost when choosing a datastore, and stored secrets and end-to-end encryption were identified as the most important aspects.</p>\r\n<p></p>\r\n<p>Within specific industry sectors, the results show that retail and financial services companies are most concerned with multi-region availability, whereas financial services companies care most about multi-primary operations, and high availability is the key consideration for retail businesses. When it comes to the cloud, approximately 55% of survey takers said they deploy datastores to the cloud while 45% deploy to a data center they own or outsource to a third party. The data also reveals that enterprises are more likely to use the Google Cloud Platform, while small- and medium- sized businesses (SMBs) are more likely to deploy to a managed data center.</p>\r\n<p></p>\r\n<p>The survey also shows that while half of database managers and decision makers believe open source datastores are a better choice, <em>less than a third</em> are using them today. Additionally, legacy SQL databases are still in use across most enterprise environments today, while a small percentage of respondents – roughly 11% – said they have deployed NoSQL datastores.</p>\r\n<p></p>\r\n<p>“The data revealed in our survey is somewhat surprising, given the current business landscape and the growing digital transformation movement,” said Evan Weaver, CEO of Fauna. “As companies across all industries move forward with their digital strategies and offer customer-facing platforms and applications to deliver a more engaging experience, the amount of transactional data being generated is exploding. Yet the vast majority of companies are still sitting on legacy systems and seem hesitant to invest in new technologies that will address current and future business requirements.</p>\r\n<p></p>\r\n<p>“At the same time, database administrators have valid concerns about security, and there is also a growing movement to put datastores in the cloud,” he continued. “We see the industry at a critical crossroads, where businesses are competing for revenue and global customer growth while relying on outdated technologies that will drastically limit their success. This is the very reason we started Fauna – to offer enterprises a reliable, secure, relational NoSQL database that can scale to meet future demands, and also provide the cost and performance benefits they need to modernize their database architectures and establish stronger brand loyalty.”</p>\r\n<p></p>\r\n<p>In today’s Digital Age, businesses require availability, consistency, scalability,&nbsp;and security in their database architectures in order to maintain a competitive advantage. The primary difference between FaunaDB and other transactional databases is its ability to meet all of these requirements while also supporting multi-cloud environments, combining established technologies into a single system that delivers on three important business values: productivity, safety,&nbsp;and agility.</p>\r\n<p></p>\r\n<p>The Fauna survey was conducted in August 2018. The companies that participated in the survey employ an average of 1,000 people, manage hundreds of terabytes of data, and earn hundreds of millions of dollars in annual revenue. The full results and survey report are available at <a href=\"http://www2.fauna.com/ebook2018\">www2.fauna.com/ebook2018</a>.</p>\r\n<p><br><br></p>\r\n<p><strong>About Fauna</strong></p>\r\n<p>Fauna, Inc., founded by ex-Twitter engineers, is the company behind FaunaDB, a relational NoSQL database. FaunaDB is a mission-critical component for companies that need to deliver products at global scale, increase infrastructure utilization, decrease operational overhead, and improve time to market. Fauna is the only database of its kind that brings together established technologies into a single system that delivers on three critical business values: productivity, safety,&nbsp;and agility. Customers using cloud and on-premise deployments of FaunaDB include industry leaders in data-rich industries such as fintech, retail, ecommerce, and mobile gaming. For more information visit <a href=\"http://www.fauna.com/\">fauna.com </a>or follow us at <a href=\"http://www.twitter.com/fauna\">@fauna</a><a href=\"http://www.fauna.com/\">.</a></p>\r\n<p></p>\r\n<p><strong>Media Contact:</strong></p>\r\n<p><strong>Joe Volat, Milestone PR</strong></p>\r\n<p><strong>info@fauna.com | <span id=\"gc-number-0\" class=\"gc-cs-link\" title=\"Call with Google Voice\">(855) 432-8623</span></strong></p>\r\n<p><br><br></p>\r\n<p><strong></strong></p>",
        "blogCategory": [
            "1531"
        ],
        "mainBlogImage": [
            "1478"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-11-13T11:22:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1467,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d18d5922-bf74-4595-885f-d87fcc362752",
        "siteSettingsId": 1467,
        "fieldLayoutId": 4,
        "contentId": 1125,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "FreeCodeCamp with Netlify",
        "slug": "freecodecamp-with-netlify",
        "uri": "blog/freecodecamp-with-netlify",
        "dateCreated": "2018-11-13T10:13:24-08:00",
        "dateUpdated": "2019-08-19T20:59:41-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/freecodecamp-with-netlify",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/freecodecamp-with-netlify",
        "isCommunityPost": false,
        "blogBodyText": "<p>The weekend of November 3rd and 4th I had the opportunity to help facilitate the two day <a href=\"https://hackathon.freecodecamp.org/\">FreeCodeCamp JAMStack Hackathon</a> event, with prizes given by sponsors like Fauna and Formspree, as well as overall prizes and live coverage on a livestream interview from Github’s video studio.</p>\r\n<p>You can see my initial presentation on the livestream archive here, if you want to join in,&nbsp;the instructions are<a href=\"https://github.com/fauna/netlify-faunadb-todomvc\">&nbsp;just a few steps long in this README</a>.</p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/KinskoYXSHM?start=6180\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<p></p>\r\n<p>Many of the onsite teams succeeded with FaunaDB, and a handful of the remote teams as well. One lesson we learned is to have a stronger online support presence for remote attendees of events like this in the future. That said, we got some great remote submissions. Check out this video of <a href=\"https://friendsoverjam.netlify.com/\">the FriendsOverJAM app:</a></p>\r\n<p></p>\r\n<figure><iframe style=\"width: 500px; height: 281px;\" src=\"//www.youtube.com/embed/h5YhYnZcbJk\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<p></p>\r\n<p>There were 28 onsite teams, of which 14 reported using Fauna. Of those, about half built applications that made serious use of the database. After looking through everyone’s code, we awarded prizes to three teams, based on having the most interesting queries:</p>\r\n<h3><a href=\"https://captionthis.netlify.com/\">Caption This</a></h3>\r\n<p>You can add captions to animated GIFs and the best captions are upvoted. <a href=\"https://smartea-pants.netlify.com/\">Code for the Netlify functions based implementation is on Github.<br><br></a></p>\r\n<h3><a href=\"https://smartea-pants.netlify.com/\">SmarTea Pants</a><br></h3>\r\n<figure><img src=\"{asset:1468:url}\" data-image=\"1468\"></figure>\r\n<p></p>\r\n<p>Jeopardy study app, <a href=\"https://github.com/CodeFay/smarteapants/blob/master/functions-src/getQuestions.js\">code is here.</a></p>\r\n<h3>Face to Face</h3>\r\n<p>You can find <a href=\"https://github.com/jimmy-guzman/quiche-friends\">the code for this app here.</a><br></p>\r\n<h3><a href=\"https://keen-boyd-38d83f.netlify.com/\">Nu-rd</a></h3>\r\n<p>This uses FaunaDB queries to index math word problems by the relevant operators, so that <a href=\"https://github.com/amychan331/nu-rd/blob/master/functions/read-query-data.js\">numeric questions can be used to look up related word problems</a>.<br></p>\r\n<h2>Conclusion</h2>\r\n<p>It was thrilling to see how skilled the recent code school graduates are. The contestants all demonstrated an ability to cut through configuration errors etc, and create features that matter, as fast as possible. There was also an emphasis on user stories and planning, that gave these applications a completeness I hadn’t expected.</p>\r\n<p>Here are the winners from Caption This&nbsp;preparing for the interview in the Github video room.</p>\r\n<figure><img src=\"{asset:1469:url}\" data-image=\"1469\"></figure>\r\n<p><br>If you want to join in yourself, the instructions are<a href=\"https://github.com/fauna/netlify-faunadb-todomvc\"> just a few steps long in this README</a>, and will automatically provision a FaunaDB&nbsp;connection for you, so you'll be up and running even faster than in the video at the top of this blog.</p>",
        "blogCategory": [
            "10",
            "1530",
            "1531",
            "1866"
        ],
        "mainBlogImage": [
            "1471"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "473",
        "postDate": "2018-11-02T11:46:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1450,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e272af49-b245-4357-b792-25a178f1afd3",
        "siteSettingsId": 1450,
        "fieldLayoutId": 4,
        "contentId": 1109,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Relational NoSQL: Yes, that is an Option",
        "slug": "relational-nosql-is-an-option",
        "uri": "blog/relational-nosql-is-an-option",
        "dateCreated": "2018-10-26T07:25:20-07:00",
        "dateUpdated": "2019-08-21T14:36:20-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/relational-nosql-is-an-option",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/relational-nosql-is-an-option",
        "isCommunityPost": false,
        "blogBodyText": "<figure><img src=\"{asset:1452:url}\" data-image=\"1452\"></figure>\r\n<p>Recently, I had a conversation with someone who knows databases and database management systems very well. I had asked him for his technical opinion about FaunaDB and his comments provoked a very lively discussion. His main reservation about FaunaDB was that it is a NoSQL database and he prefers SQL DBMSs because he likes to work with data relationally.</p>\r\n<p>We went on to discuss what it means for a database to be relationally correct and also have the flexibility of NoSQL. I told my database friend that FaunaDB is a real example of a “Relational NoSQL” database and that it puts the focus back on data integrity through a modern database approach unlike that of other databases. I used the picture above to help illustrate where FaunaDB fits in a data management spectrum.<br><br>As my friend was intrigued by the picture and the possibility of “Relational NoSQL”, I wanted to explore the idea of Relational NoSQL in this blog.</p>\r\n<h3>Why Do People Think Relational = SQL?</h3>\r\n<p>Tradition can be very powerful and there is a strong sense of tradition in the field of data management. For quite a few years, I was one of the people telling others that they needed to use a relational database management system that supported SQL. SQL was familiar -- people in the data management field learned SQL early in their careers and could always count on it for accessing data. They used SQL for their applications and for their ad hoc queries. Many business intelligence tools used SQL to gather data out of databases for their pretty reports and graphs. And SQL gives many novices the ability to do powerful complex analytics on huge datasets without the need for deep programming skills.</p>\r\n<h3>SQL Can Be Good or Bad</h3>\r\n<p>But there can be downsides to SQL for some use cases, too. SQL can actually be a hindrance to consistently high application performance (and low latency). It is a declarative language so in using it, you relinquish some of the control over the execution flow to the interpreter or compiler. There are some applications where you want to know exactly how your code will behave and SQL doesn’t enable that.</p>\r\n<p>Talk to the systems engineers of any relational database management system company and they will tell you about that one person in the inner bowels of the company who is their SQL god or goddess, the go-to person who can look at any problem customer or demo SQL code and tell you what the execution plan will be, exactly what the code will do, and why it isn’t doing what you want it to do. It is that uncertainty and unpredictability that causes grief for application developers looking for certain and predictable high performance.</p>\r\n<h3>Which is Better? SQL or NoSQL? Both and Neither</h3>\r\n<p>I’m a believer in the “horses for courses” approach to databases. There are some use cases where a SQL-based RDBMS will work perfectly just like there are other use cases which are picture-perfect for a NoSQL DBMS. Likewise, the different flavors of NoSQL (document stores, key-value stores, and graph databases) each have their great fitting use cases as well.</p>\r\n<p>Of course, there can be too much of a good thing. If you take fit-for-purpose too far, you end up in the land of <strong>polyglot persistence</strong> where you may have a different database for every application or even a bunch of different databases for a single application. That itself creates all sorts of other problems like data silos, spaghetti system architectures, and ETL complications and it tends to make both application development and database operations very non-agile.</p>\r\n<p>The interesting thing to me is that <strong>people seem to conflate SQL and Relational</strong>. A similar association happened with Hadoop and Big Data – these have become almost synonymous so that anyone who thinks they have a Big Data problem naturally first thinks they need to use Hadoop to solve it. (No…Hadoop might help or, as many companies have discovered when they’ve tried that, it might not). And when SQL became synonymous with relational, NoSQL by default became non-relational.</p>\r\n<p><strong>Relational is a type of data model.&nbsp;</strong>It is a way of organizing data and looking at data -- seeing how it relates to other data.<strong></strong></p>\r\n<p><strong>SQL</strong> (as a query language)<strong> is a type of interface</strong> – it is a way of interacting with a data model, typically a relational data model.</p>\r\n<p>I wonder whether some (or most) people who talk highly of SQL are actually more fans of the relational data model underneath SQL.</p>\r\n<p>I think it is possible that many of these people just like working with their data relationally. They like normalizing their data – it feels “clean”. They like being able to relate different groups of data through mechanisms like joins. Relational gives you that ability to keep data separate but operate on it together with other data. It gives you the ability to eat your cake and have it, too.</p>\r\n<p>How did SQL come to mean relational, and NoSQL to be dismissed as non-relational? I have seen a pattern in recent database blogs, videos, and other media posts that encourages that kind of thinking: the debate about SQL vs NoSQL often includes the word “Relational” in the discussions. It always seems to come down to “you have a choice: you can have relational (and SQL) or you can have NoSQL.”</p>\r\n<p>When relational databases were first created, SQL became the first, most popular language for interfacing with them. There are people out there who think SQL was actually a poor choice for the dominant language for the then new relational database management systems. There were alternatives, like ISBL (Information Systems Base Language), and some people believe the inferior option (SQL) became the winner in that contest.</p>\r\n<p>It seems like the popular thinking is that tables are the main manifestation of relations so anything that stores data in tables must be relational and if data isn’t stored in tables, it can’t be relational. I think that is way too restrictive of a use of the term relational.</p>\r\n<p>I think, too, that some of the early NoSQL platform providers have been at least partially responsible for the conflating of the terms and concepts of SQL and relational. Like the Cassandra community. Cassandra can’t do relational. Cassandra doesn’t support SQL (it has its own version of a query language, CQL). You read through Cassandra documentation and you see the two concepts intertwined. Prominently shown on <a href=\"https://www.datastax.com/relational-database-to-nosql\">one of DataStax’s web pages</a>: “Relational databases (RDBMS) have struggled to keep up with the wave of modernization, leading to the rise of NoSQL as the most viable database option for online Web and mobile applications”. DataStax goes on to offer several pieces of collateral that will instruct you on how to “Implement a Migration Strategy from Relational to NoSQL”.</p>\r\n<p><strong>Why does that have to be a choice? Why do you need to choose between NoSQL and Relational? It doesn’t. You don’t.</strong></p>\r\n<p>A NoSQL database can be relational or, perhaps more precisely, a NoSQL DBMS can support a relational view of data and relational access of data. Some multi-model database management systems, such as FaunaDB, do exactly that. On the other hand, many NoSQL databases today support SQL-like declarative languages but they aren’t relational -- supporting SQL won't make a database relational.</p>\r\n<p>FaunaDB is a document store, but it also supports using graph data modeling or taking a temporal view or, as we have said, treating data relationally. FaunaDB doesn’t store data in tables but it still provides you with the ability to create relationships within your data.</p>\r\n<p>Here is a Fauna multi-model code snippet from our architecture web page which illustrates graph queries, multi-level joins, indexes, and temporality – all within a single query:</p>\r\n<figure><img src=\"{asset:1215:url}\" data-image=\"1215\"></figure>\r\n<p>Try that with a polyglot persistence architecture.</p>\r\n<p>Is there a need for Relational NoSQL? We believe there is -- we see it in our own customers and we hear it in our daily conversation with prospects. We have just as many of both who have tested and rejected Cassandra as who have tested and rejected Postgres. They wanted the performance and scalability of Cassandra but also needed the relational data interface of Postgres. In FaunaDB, they found a happy combination of performance, scalability and a relational data interface.</p>\r\n<h3>Conclusion</h3>\r\n<p>Relational NoSQL is not only a real thing, it is something you can have today in your database. The underlying storage format of the data isn’t what matters, it is how you can access it. Multi-model databases like FaunaDB provide different interfaces into the same data, letting application developers mix and match those interfaces, even within the same query. We will show you details of FaunaDB’s Relational NoSQL capability, including how it compares to Relational SQL, in upcoming papers and blogs.</p>\r\n<p><br><br></p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "477",
        "postDate": "2018-10-19T07:57:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1399,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d2dd1995-48ba-47dc-a682-b6dde5303c98",
        "siteSettingsId": 1399,
        "fieldLayoutId": 4,
        "contentId": 1060,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Consistency without Clocks: The FaunaDB Distributed Transaction Protocol",
        "slug": "consistency-without-clocks-faunadb-transaction-protocol",
        "uri": "blog/consistency-without-clocks-faunadb-transaction-protocol",
        "dateCreated": "2018-10-09T06:15:45-07:00",
        "dateUpdated": "2019-10-21T11:45:02-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/consistency-without-clocks-faunadb-transaction-protocol",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol",
        "isCommunityPost": false,
        "blogBodyText": "<blockquote><p style=\"margin-left: 45px; margin-right: 45px\"><em><strong>Transactions are hard. Distributed transactions are harder. Distributed transactions over the WAN are final boss hardness. - <a href=\"https://twitter.com/andy_pavlo/status/1051974710710407176\">Andy Pavlo</a></strong></em></p></blockquote><p>FaunaDB is a distributed database platform that supports strictly serializable, externally consistent transactions. Unlike <a href=\"https://ai.google/research/pubs/pub39966\">Google Spanner</a> or similar systems, FaunaDB does not rely on physical clock synchronization to maintain consistency. Also, unlike <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36726.pdf\">Google Percolator</a>, <a href=\"https://www.foundationdb.org/\">FoundationDB</a>, or similar systems, FaunaDB places no constraints on replica distance and is practical to deploy at global internet latencies.</p><p>This post describes how read-write and read only&nbsp;transactions are implemented in FaunaDB. We will start with some background and then explain how the core protocol maintains consistency across geographic distances. We then discuss the performance implications of FaunaDB’s architecture.</p><h3>Background</h3><p>FaunaDB is more specifically a <em>Relational NoSQL</em> database platform. The term \"NoSQL\" refers only to the interface; FaunaDB currently supports an execution-transparent, procedural interface instead of declarative SQL.</p><p>The term \"relational\" refers to the data model, but FaunaDB also supports graph and document models in addition to relational. It also invokes the customary guarantees of the RDBMS:</p><ul><li>ACID transactions with up to&nbsp;serializable isolation</li><li>Linearizable, consistent operations across replicas</li></ul><p>Unlike the legacy RDBMS, FaunaDB maintains these guarantees even when geographically distributed.</p><h3>Serializable Isolation</h3><p>Serializable isolation means that the system can process many transactions in parallel, but the final result is equivalent to processing them one after another. For most database systems, the order is not determined in advance. Instead, transactions are run in parallel, and some variant of locking is used to ensure that the final result is equivalent to some serial order.</p><h3>Pre-Processing in FaunaDB</h3><p>FaunaDB’s protocol, which was inspired by <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>, decides on this serial order prior to performing any writes to the database. For each batch of parallel read-write transactions, they are inserted into a distributed, write-ahead transaction log and the FaunaDB execution engine ensures that the final result of processing this batch of transactions is equivalent to as if they were processed one-by-one in the order they appeared in this pre-generated log.</p><figure><img src=\"{asset:1441:url}\" data-image=\"1441\"></figure><p>Each transaction in the log is associated with a real time. However, unlike other distributed databases, real time is not a central component of FaunaDB’s protocol, and FaunaDB does not rely on global synchronization of clocks across servers. Rough correspondence between \"FaunaDB time\" and real time is merely an affordance for the developer and not an operational constraint. Instead, the notion of \"before\" and \"after\" is entirely dependent on the order in which transactions appear in the distributed log.</p><p>FaunaDB can take a \"snapshot\" as of any point in the distributed log which includes the writes of all transactions that appear in the log before that point, and none after it. For example, a snapshot at T4 would contain transactions T0-T4, but none afterwards.</p><p>In order to quickly generate a snapshot at an arbitrary point in the log, FaunaDB keeps around multiple versions of each record. Each version is annotated with the timestamp—the transaction identifier in the transaction log—that wrote that version:</p><figure><img src=\"{asset:1439:url}\" data-image=\"1439\"></figure><p>To read a snapshot as of a particular transaction in the log, say for example T3, the latest version of each record earlier than or equal to T3 is read. In the example of the record associated with customer 2 above, there are two potential versions to read: one written at T1 and the other at T4. Since T1 is the latest of these two options less than or equal to T3, that record is the one that is read in this example.</p><p></p><p>FaunaDB is a scalable system and it partitions large tables over multiple nodes. For example, let’s say we are deploying a retail application with two relational tables: the \"customer\" table (which was shown above) that indicates the customer id and store credit of all customers, and a \"widget\" table that shows the price of each widget for sale by that application and how many are left in stock. These two tables can be horizontally partitioned over an arbitrary number of nodes, as shown below:</p><figure><img src=\"{asset:1440:url}\" data-image=\"1440\"></figure><p>FaunaDB also replicates data, potentially over large geographic distances:</p><figure><img src=\"{asset:1436:url}\" data-image=\"1436\"></figure><p>Clients can send transactions to any replica — both read-only transactions and transactions that update data in the database. However, there is only one transaction log for the entire FaunaDB deployment. Replicas must achieve consensus for how to insert new transactions into the log. FaunaDB uses an optimized Raft implementation to achieve consensus.</p><h3>Summary</h3><p>In FaunaDB, data is both partitioned and replicated across machines. Each partition contains multiple records (rows), and each record may have many versions associated with it. Each version is stored separately and is annotated with the transaction identifier that wrote that version. When a FaunaDB transaction needs to read data, it chooses a snapshot and reads the correct version based on which snapshot was chosen.</p><p></p><h3>The FaunaDB Distributed Transaction Protocol</h3><p>We will now describe the core FaunaDB transaction protocol. We will explain it through an example, by tracing the lifetime of a couple of example transactions that are submitted to the system. Let’s continue with the same example application that we discussed above: a retail application with two tables — one table providing information about the widgets being sold, and one table providing information about customers and how much store credit they have.</p><p>Let’s take the specific example where there is a widget being sold of which there is only one remaining (widget 3). Let’s assume that two customers attempt to buy it at approximately the same time. Further, let’s assume that these two customers are interacting with different replicas of the data — one customer with the replica in San Francisco, and the other with the replica in Washington, DC.</p><p>In other words, the following transaction is submitted to the replica in San Francisco:</p><figure><img src=\"{asset:1448:url}\" data-image=\"1448\"></figure><p>And a very similar transaction is submitted to the replica in Washington, DC (the only difference is that a different customer is trying to purchase the same widget):</p><figure><img src=\"{asset:1449:url}\" data-image=\"1449\"></figure><p></p><p><grammarly-btn></grammarly-btn></p><div class=\"_1BN1N Kzi1t _2DJZN\" style=\"z-index: 2; transform: translate(899px, 5031.22px);\"><div class=\"_1HjH7\"><div title=\"Protected by Grammarly\" class=\"_3qe6h\"> </div></div></div><p>The server within a replica that receives the transaction request becomes the \"coordinator\" of that request. In the figure below, we see that the two transactions (for which we gave the pseudocode above) arrived at particular machines in San Francisco and Washington, DC respectively. Those machines become the coordinators of those two transactions.</p><figure><img src=\"{asset:1437:url}\" data-image=\"1437\"></figure><p>The coordinator executes the transaction code. In most cases, it will not have all of the relevant data locally, and thus will have to read data from nearby servers within the same replica that have the required partitions of data that are involved in the transaction request. It chooses a recent snapshot time (this choice can be arbitrary) and makes requests to the nearby servers to read data as of that snapshot. In our example, let’s assume that the coordinator for each of our two competing transactions chooses to read as of T9 (the most recent transaction in the distributed transaction log):</p><figure><img src=\"{asset:1438:url}\" data-image=\"1438\"></figure><p>The transaction running in San Francisco is for customer 2 trying to buy widget 3. Therefore, the coordinator reads the two relevant records (for customer 2 and widget 3) as of snapshot T9. The transaction running in Washington, DC is for customer 6 trying to buy widget 3. Therefore, the coordinator reads the two relevant records (for customer 6 and widget 3) as of snapshot T9. The figure above shows the correct version of the records being sent from the machines where they are stored to the coordinator.</p><p>Recall that our example transaction performs some checks, and if they succeed, the transaction proceeds with updating the data (in particular, reducing the inventory of the widget and the store credit of the customer that bought the widget). The coordinator does not yet perform these writes. Instead, it just buffers them locally, keeping a record of which records it wants to write and what the new values should be:</p><figure><img src=\"{asset:1433:url}\" data-image=\"1433\"></figure><p>After completing all the transaction logic and buffering all writes, the coordinator is now ready to initiate the commit process. To do this, the coordinator attempts to insert this transaction into the distributed log (for scalability, this insertion process happens in batches, and the log itself is replicated and partitioned). The Raft protocol ensures that all replicas achieve consensus on the order in which batches of transactions (from any replica) are inserted into the distributed log. After being inserted into the log, the relative position of that transaction in the distributed log becomes the transaction identifier. The actual log entry contains the newly determined identifier, along with a record of all the reads and buffered writes that were performed by that transaction’s coordinator:</p><figure><img src=\"{asset:1434:url}\" data-image=\"1434\"></figure><p>In our example, the transaction that was submitted to San Francisco got inserted into the log first (with an identifier of T10, and the one submitted to Washington, DC second (with an identifier of T11). The reads and writes of each of those two transactions (shown within the red rectangle) were included in the log entry when those two transactions were appended into the distributed log.</p><p><strong>Inserting a transaction into the distributed transaction log is the <em>only</em> part of the FaunaDB protocol that requires consensus across replicas.</strong> This is a distinctive feature of FaunaDB—other geo-replicated systems require at least two rounds of global consensus.</p><p>Each replica independently reads from the distributed transaction log and attempts to commit each transaction in the log. Remember that each log entry contains all of the reads and writes performed by the coordinator of that transaction when it was originally processed. Furthermore, remember that the coordinator chose a snapshot at which to perform the reads prior to the transaction being inserted into the distributed log and receiving a distributed transaction identifier.</p><p>In order to properly guarantee global serializability, the correct snapshot at which to perform the reads for that transaction is the location of that transaction in the distributed log. Therefore, the snapshot at which the coordinator had performed the reads was earlier than the correct snapshot that would guarantee global serializability. To prevent serializability violations, each replica must perform the reads again, to see if the values changed between the snapshot at which they were originally read and the correct snapshot as of the transaction’s identifier in the distributed log:</p><figure><img src=\"{asset:1435:url}\" data-image=\"1435\"></figure><p>In the case of T10, the original reads were the same as the reads as of the correct snapshot. Therefore, the transaction can commit:</p><figure><img src=\"{asset:1431:url}\" data-image=\"1431\"></figure><p>Note that the replicas perform this check independently from each other, reading from their local copy of the data. They will always come to the same conclusion about whether the reads changed or not. This is because each replica sees the same distributed transaction log, and therefore will always agree on the value of a snapshot as of a particular point in the log.</p><p>If the reads did not change from the original snapshot, the transaction can commit. To commit a transaction, the buffered writes are appended to the core tables, annotated with the transaction identifier given to that transaction. In the figure above, the new version of customer 2 and widget 3 are shown as inserted into the correct customer and widget partitions respectively, annotated with T10 — the transaction that wrote this new version.</p><p>In the case of T11, the original reads are different from the reads of the correct snapshot:</p><figure><img src=\"{asset:1432:url}\" data-image=\"1432\"></figure><p>This is because the original reads were performed as of snapshot T9, but T10 appears before T11 in the distributed log, which updated the data (widget 3) that T11 reads. Therefore, the original read of widget 3 performed by the coordinator was incorrect, and the transaction needs to be aborted and restarted:</p><figure><img src=\"{asset:1430:url}\" data-image=\"1430\"></figure><p>Each replica will independently figure out that the original read was incorrect, and thus independently decide to abort the transaction, without requiring any further coordination. When T11 gets restarted with the correct read set, it will see the inventory of widget 3 is 0, and thus customer 6 will be correctly notified that the purchase cannot proceed.</p><p></p><h3>Multi-Region Consistency</h3><p>The final result of our example from the previous section is that when two customers try to purchase the last item in the inventory at the same time, FaunaDB is able to correctly ensure that only one of these purchases succeeds. This is true even though the two customers trying to purchase the same item submit their respective transactions to different replicas of the database, and even though these regions are geographically distant from each other.&nbsp;</p><p>This ability to prevent this duplicate purchase stems from FaunaDB’s consistency guarantee: once a transaction commits, it is guaranteed that any subsequent read-write&nbsp;transaction—no matter which replica is processing it—will read all data that was written by the earlier transaction.&nbsp;<strong>Other NoSQL systems, and even most SQL systems, cannot guarantee global replica consistency. </strong>They allow replicas to temporarily diverge, and the result of a transaction may vary depending on which replica it is sent to.</p><p>For example, in an eventually consistent database system, the transaction that was submitted to the San Francisco replica and the transaction that was submitted to the Washington DC replica may both proceed and commit at their respective data centers, thereby potentially allowing multiple customers to believe that they have purchased the last item in the inventory. It is only at a later point, after the replicas eventually become consistent, that the system finds out that the same item was sold twice.</p><p>In such a scenario the application must take measures to repair the potential damage caused by this inconsistency. This dramatically increases the complexity of application development, and in many circumstances, for example, microservice environments where many separate services must interact, adding consistency in the application layer is not realistically possible.</p><p>In contrast, FaunaDB guarantees global consistency through deterministic processing of transactions in the order they appear in the global input log. Every read-write&nbsp;transaction, no matter where it originated from, gets written to the same log of input transactions where the order of transactions in this log is agreed upon through the Raft consensus protocol. The order of transactions in this log implies a linear order of global operations that is enforced by each replica and ensures that replicas remain consistent.</p><h3>Summary</h3><p>To summarize the overall FaunaDB protocol, each read-write&nbsp;transaction proceeds in three phases:</p><ul><li>The first phase is a speculative phase in which reads are performed as of a recent snapshot, and writes are buffered. <br><br></li><li>Next, a consensus protocol is used (Raft) to insert the transaction into a distributed log, which results in the transaction receiving a global transaction identifier that specifies its equivalent serial order relative to all other transactions that are being processed by the system. This is the only point at which global consensus is required.<br><br></li><li>Finally, a check begins in each replica which verifies the speculative work. If that speculative work did not result in potential violations of serializability guarantees, then the work becomes permanent and the buffered writes written back to the database. Otherwise, the transaction is aborted and restarted.</li></ul><ul></ul><h3>Performance Implications</h3><p>In FaunaDB, only a single round of global consensus is required for even the most complicated of transactions. Furthermore, the consensus protocol is only being used for inserting transactions into a distributed log. For every other part of the protocol, replicas can proceed completely independently from each other. For example, serializable reads can be performed with no consensus whatsoever. This leads to several important performance implications:</p><ol><li><strong>Transactions that update data only go through a single round of global consensus. </strong>Most other consistent database systems require at least two rounds of consensus. When data is geographically dispersed, consensus can be the dominant cost in a transaction. In such scenarios, FaunaDB is approximately half the latency of other database systems that require two rounds of consensus.<br><br></li><li><strong>FaunaDB does not require clock synchronization or bounds on clock skew uncertainty across machines in a deployment. </strong>Thus, FaunaDB’s architecture does not experience the latency cost of delaying transactions by clock skew uncertainty, a cost that is present in many other consistent database systems. Other systems are also subject to desynchronization, where consistency guarantees abandoned (potentially without detection) because the operator was not able to keep clock skew within bounds.<br><br></li><li>By having a single distributed log, <strong>FaunaDB has a global notion of \"FaunaDB time\" that is agreed upon by every node in the system</strong>. A \"timestamp\" in FaunaDB is a logical concept and is simply the location in the distributed log. As a result,<strong> </strong>it is trivial for any node to serve data at a particular snapshot in time. The snapshot is a particular point between transactions in the distributed log, and a node that serves data as of this snapshot simply has to ensure that the snapshot includes all modifications by transactions prior to this location in the log, and not any of the modifications by transactions after this point. Consequently, any replica that has processed a sufficient prefix of the log can serve snapshot reads as of that point in time. <br><br></li><li>Finally, by running on a transactionally complete read snapshot, read-only transactions are guaranteed to be serializable in FaunaDB. <strong>FaunaDB supports serializable snapshot reads with no consensus or locking, so they complete with local datacenter latency.</strong></li></ol><p></p><h3>Conclusion</h3><p>FaunaDB is an elegant, software-only solution for achieving global ACID transactions, with complete guarantees of serializability and consistency. <strong>FaunaDB requires no clock synchronization, no specialized hardware, and no atomic clocks.</strong></p><p><strong></strong>The beauty of the FaunaDB transaction protocol is its simplicity. FaunaDB’s unique design enables a number of performance gains, allowing it to provide the usability advantages of serializable and consistent systems, while approaching the performance of systems that fail to make these strong guarantees. Serializable reads, for example, have the same scalability, throughput, and latency profile of an eventually-consistent system like <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a>, and can be scaled independently from writes.</p><p>There is no other publicly available system in the world like FaunaDB. We encourage you to <a href=\"https://app.fauna.com/sign-up\">give it a try</a>, and to consider the implication of similar techniques in your own engineering work.</p><p></p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "6351"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-10-04T12:08:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1398,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "c19cabe5-1c0d-4b4a-9590-4d5f3a77009b",
        "siteSettingsId": 1398,
        "fieldLayoutId": 4,
        "contentId": 1059,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Serve your FaunaDB Single Page App from IPFS",
        "slug": "serve-your-faunadb-single-page-app-from-ipfs",
        "uri": "blog/serve-your-faunadb-single-page-app-from-ipfs",
        "dateCreated": "2018-10-04T12:03:09-07:00",
        "dateUpdated": "2020-02-24T09:16:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/serve-your-faunadb-single-page-app-from-ipfs",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/serve-your-faunadb-single-page-app-from-ipfs",
        "isCommunityPost": false,
        "blogBodyText": "<p>IPFS is a distributed file system for the web. It uses content-addressing so that popular content is served by more peers, instead of overwhelming a single source. This also means content is validated by clients and can be received safely from any source. IPFS resources can be provided to web clients from any gateway, and it’s simple to configure DNS so that users don’t even know IPFS is the underlying technology. </p>\n<p>This article will show you how I ran <a href=\"https://github.com/fauna/todomvc-fauna-spa\">my favorite Fauna single page app</a> on <a href=\"https://ipfs.io/\">IPFS</a>. It was easy. The hardest part was the bit about relative paths in my <a href=\"https://github.com/facebook/create-react-app\">create react app</a> build configuration, which this post will make easy for you. Follow along with these steps, and you’ll be set up in no time.</p>\n<p>As a result of combining FaunaDB’s global availability and serverless pricing with IPFS peer-to-peer storage, this application model gives robust worldwide access at low cost. Apps deployed using FaunaDB and IPFS will be able to scale beyond millions of users, without requiring code or architecture changes.</p>\n<h3>Install the IPFS tools</h3>\n<p>Before you start developing, it’s worth it to install the IPFS tools, which include the option to run a local gateway daemon. <a href=\"https://dist.ipfs.io/#go-ipfs\">Download the latest go-ipfs package here</a> and <a href=\"https://docs.ipfs.io/introduction/install/\">follow these instructions for your platform.</a></p>\n<h3>Clone the app code</h3>\n<p>To obtain the latest code, run:</p>\n<pre>git clone https://github.com/fauna/todomvc-fauna-spa\ncd todomvc-fauna-spa/\nnpm install</pre>\n<p>We won’t cover the application logic in this post, except to say that it uses FaunaDB’s security APIs for user identity and access control. This allows fine-grained control over the database privileges that backend processes can operate with, and enables single page and mobile applications to connect directly to the database. </p>\n<h3>Test your app with live code changes</h3>\n<p>If you are familiar with <a href=\"https://github.com/facebook/create-react-app\">create react app</a>, you are probably already playing around with the app by using<strong> npm start</strong>, which starts a development server locally and allows you to see code changes reflected in the browser in real-time for quick iteration. Thank create react app, and the tools it’s built on!</p>\n<p>When you run it, you should see something like this:</p><figure><img src=\"{asset:6931:url}\" data-image=\"6931\"></figure>\n\n<p>Any changes you make will be reflected immediately on the development server. You won’t need to make changes to the code as part of this article, but live reload is one of the most powerful features of create react app’s development environment, so take a moment to appreciate it.</p>\n<h3>Configure your app for relative paths</h3>\n<p>If you are running the example app above, this step has already been done for you. I’m mentioning it because you’ll need to do it yourself if you start a fresh app.</p>\n<p>The default build output from create react app’s <strong>npm run build</strong> contains URL paths based on the pattern <strong>{homepage}/static/assets/…</strong> <strong></strong>by setting <strong>“homepage” : “.”</strong> in your package.json you are able to set a relative build page. (If you are on an older versions of create react app, this will be <strong>\"homepage\": \"./\"</strong>, instead.)</p>\n<p>You can tell you got it right when you open your <strong>build/index.html</strong> file and the links to css and js bundles start with <strong>./static/</strong> instead of <strong>/static/</strong> or <strong>http://example.com/static/...</strong> create a static build suitable to serverless hosts like <a href=\"https://fauna.com/blog/survive-cloud-vendor-crashes-with-netlify-and-faunadb\">Netlify</a>, and <a href=\"https://animal-exchange.neocities.org/\">Neocities</a>, and in this case IPFS. The same build configuration should work in all cases, so feel free to check in your <strong>“homepage” : “.”</strong> change to package.json.</p>\n<h3>Deploy it </h3>\n<p>Now that you have run <strong>npm run build</strong> and validated that <strong>build/index.html</strong>contains the correct URL paths, you can add the build to IPFS by running <strong>ipfs add -r build</strong> which will recursively add the site to IPFS, with output like this:\n</p>\n<pre>added Qmb3qYy89Q9d2HV3S26d4JD3TGGUJXQBJePbepKuX77zWP build/asset-manifest.json\nadded Qmci3NaiZpMDoGMAG5NRcWCfRTekxdGtUj69fjdwJMQv2W build/favicon.ico\nadded QmdvJ2NyPQKebmJo9dR5PB1TL8tL3puEBjN73pi8soQRf9 build/index.html\nadded QmVDLWb9FKR3MzoT11XmpBoiV79jsL3F8qozRxonAeKATX build/static/css/main.64cc0607.css\nadded QmbF2KAreFn41VVzYKqZttH7qoStjvdgo26xbK8NgfFJKy build/static/css/main.64cc0607.css.map\nadded QmUvAnr8Z7RomrZx5fSR2gBq5mi6EDgsMEWgr4MLPkXozr build/static/js/main.ec74f5df.js\nadded QmUmw6CBVbndEUqamTwGnFJst2qyEQoUoAiBPDMHxmiHoA build/static/js/main.ec74f5df.js.map\nadded QmdD1cVeEkckCntAiRHHQqJxnQHnYVadWP5T2z9tZzZ9Ng build/static/css\nadded Qmd3T4mqKYSDKsRGNfxaowtxQ6nA2Zi96nHFUbbuagh2KB build/static/js\nadded Qma1ajJXuWPotQ3TuEMmBNN1bp9ccz4FyCChCLAHCqCjYH build/static\nadded QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A build\n 2.41 MiB / 2.41 MiB [========================================================================================]  99.98%</pre>\n<p><br>Now you can access your app on the web via a gateway. Your local IPFS daemon will make it available at</p>\n<p><a href=\"http://localhost:8080/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/\">http://localhost:8080/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/</a> and you can also find it publicly via <a href=\"http://ipfs.io/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/\">http://ipfs.io/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/</a></p>\n<h3>Pin your files</h3>\n<p>Just adding your files isn’t quite enough. You also need to pin them somewhere or else they won’t stay on the network long. If your IPFS daemon is running on a workstation that doesn’t shut down, pinning to your local workstation should be fine. If you are like me, your laptop is offline at least when you’re asleep. So I pinned my content using <a href=\"https://www.eternum.io/\">https://www.eternum.io/</a> which allows you to buy IPFS storage at affordable rates. To pin content to your local IPFS daemon, run:</p>\n<p><strong>ipfs pin add -r /ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A</strong></p>\n<p>The hash in this command will be the same as in the last line of build output above, and the <strong>-r</strong> option pins all the files recursively. Now anyone who requests your content from a public gateway will be able to see it.</p>\n<h3>Set up DNS</h3>\n<p>A URL like <a href=\"http://ipfs.io/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/\">http://ipfs.io/ipfs/QmRdrpAVPu1SHDseh71G8bKCSkPwZ8SF3NkUX1PfSYCt8A/</a> is not very human-friendly. Luckily, IPFS provides a clean way to link a DNS name to the URL. Basically, you use your DNS provider to point your subdomain at a public gateway, and add a specific TXT record with metadata that tells the IPFS gateway which piece of content to serve for that name. </p>\n<p>Additionally, the IPFS naming feature called IPNS can be used to alias your content to your IPFS cryptographic identifier. This allows you to point DNS to your identifier, and have it resolve to the content hash you choose. This means you can update your application’s codebase without making DNS changes, just by publishing an updated to IPNS.</p>\n<p>I won’t duplicate the instructions here, but if you are putting your app on the public web, you’ll want to follow them. Here is an article about <a href=\"https://medium.com/textileio/the-definitive-guide-to-publishing-content-on-ipfs-ipns-dfe751f1e8d0\">how to publish content using your IPNS identifier.</a> And here is an article about <a href=\"https://blog.cloudflare.com/distributed-web-gateway/\">how to use DNS to serve your content from Cloudflare’s CDN.</a></p>\n<h3>The final result</h3>\n<p>If you do the work to configure DNS, your site will live at a normal URL, and your users will be none the wiser. Here is the human readable URL at which you can see my copy of the app running: <a href=\"http://todomvc-fauna.edgemob.org/\">http://todomvc-fauna.edgemob.org/</a></p>\n<p>The IPFS distributed filesystem is a good mix for FaunaDB, because both are globally available with serverless pricing models. FaunaDB’s ACID transactions and multi-region consistency contrast with the IPFS gradual distribution model. Because they have different strengths, they are useful together. For instance, FaunaDB can be used to track account status and profile data, while IPFS could be used for social media image uploads. This combination offers the best of both worlds, so developers can dip a toe in the waters of the distributed web and still benefit from convenient cloud database features.</p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [
            "1397"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1383",
        "postDate": "2018-10-03T13:40:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1396,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "645922ea-b010-406c-adc3-ab512f74c205",
        "siteSettingsId": 1396,
        "fieldLayoutId": 4,
        "contentId": 1057,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introducing Fauna Shell",
        "slug": "introducing-fauna-shell",
        "uri": "blog/introducing-fauna-shell",
        "dateCreated": "2018-10-03T13:00:07-07:00",
        "dateUpdated": "2020-02-24T09:23:50-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introducing-fauna-shell",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introducing-fauna-shell",
        "isCommunityPost": false,
        "blogBodyText": "<p>Back in April, our team got together with the goal of assessing the current status of FaunaDB to try to see which areas we wanted to improve next. This group was a mix of people who had been at Fauna since the beginning, and people like me who had just joined the company.<br></p>\n<p>As a new user, I struggled with how to get started using FaunaDB, so I raised the issue to the team. During our brainstorming sessions, we concluded that we needed to improve a couple of areas to provide the user with a blissful experience. One of them was overall documentation: information was usually there, somewhere, but it was hard to find for a new user. Since then, our team has been working hard on improving FaunaDB’s <a href=\"https://app.fauna.com/documentation/gettingstarted\">documentation</a>.&nbsp;At&nbsp;the same time, we wanted to give the user a straightforward path to running their first FaunaDB query. The team knew we had <a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwn\">great technology built into the database</a>, but we needed to help users start experimenting with it as fast as possible.</p>\n<p>An interactive shell was the answer to that, a tool where users could issue their FaunaDB queries while getting results from the database right away. And so the project <a href=\"https://github.com/fauna/fauna-shell\">Fauna Shell</a> was born.</p>\n<p>After you install the Fauna Shell, you can login from there to your Fauna Cloud <a href=\"https://app.fauna.com/sign-up\">account</a>&nbsp;directly from the command line:</p><figure><img src=\"{asset:6935:url}\" data-image=\"6935\"></figure><p>First, install the shell.&nbsp;If you're on a PC, you can install it with <a href=\"https://www.npmjs.com/get-npm\">npm</a>:</p>\n<pre>$ npm install -g fauna-shell</pre>\n<p>Alternatively, if you're on a&nbsp;Mac, you can install it with <a href=\"https://brew.sh/\">homebrew</a>:</p>\n<pre>$ brew install fauna-shell</pre>\n<p>Second, log in with your Fauna credentials:<br></p>\n<pre>$ fauna cloud-login\n\nEmail: email@example.com\nPassword: **********</pre>\n<p>After those two steps, you are ready to start using your FaunaDB instance, for example by creating a database:<br></p>\n<pre>$ fauna create-database my_app\n</pre>\n<p>And then start playing with it from the interactive shell:</p>\n<pre>$ fauna shell my_app\nStarting shell for database my_app\nConnected to https://db.fauna.com\nType Ctrl+D or .exit to exit the shell\nmy_app&gt;</pre>\n<p>Once in the shell, you will be able to start typing queries and receiving feedback right away:<br></p>\n<pre>my_app&gt; CreateClass({ name: \"posts\" })\n{ ref: Class(\"posts\"),\n ts: 1533753878043481,\n history_days: 30,\n name: \"posts\" }</pre>\n<h2>User-centered design</h2>\n<p>This might seem like a small thing, but it's great when a user wants to get their feet wet with FaunaDB as fast as possible. The shell brings up an environment where instant feedback can guide the user while learning how to use FaunaDB.</p>\n<p>Another important advantage of the Fauna Shell is that users don't need to setup a whole development environment to start using FaunaDB. In the past, there was the need to pick a programming language that had a FaunaDB client. Then, the user would have to install that client and learn the proper import directives that would let them use that client. From then on, they would have to write a query, compile or execute the program, see the results, and try again. With the Fauna Shell, we removed all those steps: The user writes queries and sees their results immediately.</p>\n<h2>Removing Obstacles</h2>\n<p>Something I enjoyed from the process of developing the Fauna Shell was how it was driven with a focus on providing the user with a pleasant experience. We wanted to remove obstacles. As mentioned above, one of them was the need for setting up a development environment just to use FaunaDB.</p>\n<p>After discussions with Matt our CTO, and Summer our UX Lead, we came to the conclusion that Javascript would be the language that would host the shell. On one side, it provided an interactive environment, and on the other side, due to the web's popularity, almost every developer has worked with Javascript at least once during their professional lives.</p>\n<p>Using our Javascript in plain node.js, we would write queries like this: <tt>query(q.CreateClass({ name: \"posts\" }))</tt>. Basically a <tt>CreateClass</tt> directive would be passed to the query method, and that method would execute the query for the user. During the prototyping stages of the Fauna Shell, we used to write queries that way, but the more we typed those queries, the more we saw something was out. Every time we wanted to execute a <em>query</em>, we had to wrap it inside a <tt>query()</tt> function call. That sounded less than ideal. We decided to bring into scope the whole <a href=\"http://fauna.github.io/faunadb-js/module-query.html\">query module</a>, removing that requirement.</p>\n<p>By removing the need to wrap queries all the time and the need to scope function calls to the <tt>q.</tt> module, &nbsp;we left the user with a more ergonomic experience where they could focus on typing FQL expressions directly. The code shown above just became <tt>CreateClass({ name: \"posts\" })</tt>, letting the user focus on what’s important to them, for example, creating a class called <em>posts</em>.</p>\n<h2>Small Details Matter</h2>\n<p>Another thing we noticed as we worked with the shell was that the output we were receiving after sending queries to the database was far from ideal. For example, if you submitted the query <tt>CreateClass({ name: \"posts\" })</tt>, you would receive something like this:<br></p>\n<pre>{ ref: Ref(id=posts, class=Ref(id=classes)),\n ts: 1527349751848648,\n history_days: 30,\n name: 'posts' }</pre>\n<p>As we kept seeing those results, and we tried to use them in future queries, we noticed that they couldn't be copy-pasted right back into the shell to be used in future queries. Our colleague Marrony <a href=\"https://github.com/fauna/faunadb-js/pull/164\">worked</a> on improving our <a href=\"https://github.com/fauna/faunadb-js\">faunadb-js driver</a> so results would be pretty printed in a way that would allow for easy copy-paste:</p>\n<pre>{ ref: Class(\"posts\"),\n ts: 1532624109799742,\n history_days: 30,\n name: 'posts' }</pre>\n<p>While these examples all seem like small details, they help users have a nicer and more natural experience while using the Fauna Shell to learn how FaunaDB works. It's all about removing obstacles.</p>\n<p>Finally, once we were ready to launch, we noticed that the shell provided no information to the user about to which server they were connected to, nor about which database they were using.</p>\n<pre>$ fauna shell my_app\nStarting shell for database my_app\nConnected to https://db.fauna.com:443\nCtrl+D or .exit to exit the shell\n\nmy_app&gt;</pre>\n<p>Again, just a small detail that greatly improves the usability of the shell, providing users with the right context about where their queries are being sent. Nobody wants to suddenly delete all their information in the production database!</p>\n<h2>Conclusion</h2>\n<p>In this article, we not only wanted to give you a quick intro to the Fauna Shell, but also to provide you with some insight into how we develop products here at Fauna. We try to put ourselves in the shoes of our users and from that point try to see where our tools need to improve.</p>\n<p>Also, even though in this case I was the lead for the Fauna Shell project, the whole work was a team effort: Our sales engineers helped us identify where the Fauna Shell was falling short. This feedback was looped between our UX lead, our CTO, and me so we could retrofit it into the project. Also, other colleagues stepped in and helped by improving other aspects of the product, like our JS driver, so at the end we could deliver a polished experience for our users while organically improving other areas of the system.</p>\n<p>Of course, this is just the start of the road for the Fauna Shell project. We’re improving it further every day, and can't wait to hear from you about features you would like to see next.</p>\n<p><a href=\"https://db.fauna.com:443</p><p>Type\"><br></a></p>",
        "blogCategory": [
            "8",
            "10",
            "1462"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2018-09-27T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1389,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "867f829a-9205-4a8d-ace8-5dcbcb99bed9",
        "siteSettingsId": 1389,
        "fieldLayoutId": 4,
        "contentId": 1053,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB: Single Node Free for Commercial Use",
        "slug": "faunadb-single-node-free-for-commercial-use",
        "uri": "blog/faunadb-single-node-free-for-commercial-use",
        "dateCreated": "2018-09-27T08:48:08-07:00",
        "dateUpdated": "2019-08-21T14:35:49-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-single-node-free-for-commercial-use",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-single-node-free-for-commercial-use",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is not open source, but we want to make it as easy as possible for everybody to try it out, build real apps, and learn what makes our architecture and database so special.</p>\r\n<p>Recently, we updated our Serverless Cloud pricing to offer up to three months of free usage as well as daily free tiers. We've seen tremendous uptake of FaunaDB Cloud, with fintech, social, retail, gaming,&nbsp;and crypto applications being built on it.</p>\r\n<p>Cloud isn’t for everybody though -- maybe you need to deploy in specific data centers, run your own distributed tests, or see how simple operating FaunaDB really is. So, in order to improve the experience for our on-premises developers and operators, FaunaDB is now available as a free, full-featured download for single node (unclustered, single process) in any environment and for non-commercial usage.</p>\r\n<p>Specifically:</p>\r\n<ul><li>Any single-node use is free, even if used commercially -- for example, on a developer's laptop, or in a single VM in your cloud or&nbsp;data center, without replication or clustering.<br></li><li>Any other use is free for 90 days, regardless of cluster size or commercial purpose. But if you need more than time to explore, we will be happy to extend your trial period.</li><li>Any&nbsp;non-commercial use is free in all configurations.</li></ul>\r\n<p>You can download the latest release&nbsp;<a href=\"https://www2.fauna.com/downloads\">here</a>, and read about pricing details <a href=\"https://fauna.com/faundb/pricing\">here</a>.<br></p>\r\n<p>Please review <a href=\"http://www2.fauna.com/enteula\">license</a> and contact us at <a href=\"mailto:priority@fauna.com\">priority@fauna.com</a> or <a href=\"https://twitter.com/fauna\">@fauna</a> for any questions.</p>\r\n<p>Evan Weaver<br>Founder & CEO</p>",
        "blogCategory": [
            "8",
            "1531"
        ],
        "mainBlogImage": [
            "1391"
        ],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2018-09-21T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1674,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "cc4e8576-3824-40e2-a2ef-1c7407bd86e4",
        "siteSettingsId": 1674,
        "fieldLayoutId": 4,
        "contentId": 1260,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "NewSQL database systems are failing to guarantee consistency, and I blame Spanner (Spanner vs. Calvin, Part 2)",
        "slug": "newsql-database-systems-are-failing-to-guarantee-consistency-and-i-blame-spanner-spanner-vs-calvin-part-2",
        "uri": "blog/newsql-database-systems-are-failing-to-guarantee-consistency-and-i-blame-spanner-spanner-vs-calvin-part-2",
        "dateCreated": "2019-02-22T15:00:20-08:00",
        "dateUpdated": "2019-08-19T21:02:10-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/newsql-database-systems-are-failing-to-guarantee-consistency-and-i-blame-spanner-spanner-vs-calvin-part-2",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/newsql-database-systems-are-failing-to-guarantee-consistency-and-i-blame-spanner-spanner-vs-calvin-part-2",
        "isCommunityPost": false,
        "blogBodyText": "<p><em>[TL;DR I <a href=\"http://dbmsmusings.blogspot.com/2017/04/distributed-consistency-at-scale.html\" target=\"_blank\">wrote a post in 2017</a> that discussed Spanner vs. Calvin that focused on performance differences. This post discusses another very important distinction between the two systems: the subtle differences in consistency guarantees between Spanner (and Spanner-derivative systems) vs. Calvin.]</em><br><br>The CAP theorem famously states that it is impossible to guarantee both consistency and availability in the event of a network partition. Since network partitions are always theoretically possible in a scalable, distributed system, the architects of modern scalable database systems fractured into two camps: those that prioritized availability (the NoSQL camp) and those that prioritized consistency (the NewSQL camp). For a while, the NoSQL camp was clearly the more dominant of the two --- in an “always-on” world, downtime is unacceptable, and developers were forced into handling the reduced consistency levels of scalable NoSQL systems. [Side note: NoSQL is a broad umbrella that contains many different systems with different features and innovations. When this post uses the term “NoSQL”, we are referring to the subset of the umbrella that is known for building scalable systems that prioritize availability over consistency, such as <a href=\"http://cassandra.apache.org/\" target=\"_blank\">Cassandra</a>, <a href=\"https://aws.amazon.com/dynamodb/\" target=\"_blank\">DynamoDB </a>(default settings), <a href=\"http://www.project-voldemort.com/\" target=\"_blank\">Voldemort</a>, <a href=\"http://couchdb.apache.org/\" target=\"_blank\">CouchDB</a>, <a href=\"http://basho.com/products/riak-kv/\" target=\"_blank\">Riak</a>, and multi-region deployments of <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\" target=\"_blank\">Azure CosmosDB</a>.]</p>\r\n<p>Over the past decade, application developers have discovered that it is extremely difficult to build bug-free applications over database systems that do not guarantee consistency. This has led to a surprising shift in momentum, with many of the more recently released systems claiming to guarantee consistency (and be CP from CAP). Included in this list of newer systems are: <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\" target=\"_blank\">Spanner </a>(and its <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\" target=\"_blank\">Cloud Spanner</a> counterpart), <a href=\"https://fauna.com/\" target=\"_blank\">FaunaDB</a>, <a href=\"https://www.cockroachlabs.com/\" target=\"_blank\">CockroachDB</a>, and <a href=\"https://yugabyte.com/\" target=\"_blank\">YugaByte</a>. In this post, we will look more deeply into the consistency claims of these four systems (along with similar systems) and note that while some do indeed guarantee consistency, way too many of them fail to completely guarantee consistency. We will trace the failure to guarantee consistency to a controversial design decision made by Spanner that has been tragically and imperfectly emulated in other systems. </p>\r\n<h2>WHAT IS CONSISTENCY ANYWAY?</h2>\r\n<p>Consistency, also known as “atomic consistency” or “linearizability”, guarantees that once a write completes, all future reads will reflect that value of the write. For example, let’s say that we have a variable called X, whose value is currently 4. If we run the following code:</p>\r\n<pre>X = 10;\r\nY = X + 8;</pre>\r\n<p>In a consistent system, there is only one possible value for Y after running this code (assuming the second statement is run after the first statement completes): 18. Everybody who has completed an “Introduction to Programming” course understands how this works, and relies on this guarantee when writing code.</p>\r\n<p>In a system that does not guarantee consistency, the value of Y after running this code is also probably 18. But there’s a chance it might be 12 (since the original value of X was 4). Even if the system returns an explicit message: “I have completed the X = 10 statement”, it is nonetheless still a possibility that the subsequent read of X will reflect the old value (4) and Y will end up as 12. Consequently, the application developer has to be aware of the non-zero possibility that Y is not 18, and must deal with all possible values of Y in subsequent code. This is MUCH more complicated, and beyond the intellectual capabilities of a non-trivial subset of application developers.</p>\r\n<p>[Side note: Another name for \"consistency\" is \"strong consistency\". This alternate name was coined in order to distinguish the full consistency guarantee from weaker consistency levels that also use the word \"consistency\" in their name (despite not providing the complete consistency guarantee). Indeed, some of these weaker consistency levels, such as \"causal consistency\", \"session consistency\", and \"bounded staleness consistency\" provide useful guarantees that somewhat reduce complexity for application developers. Nonetheless, the best way to avoid the existence of corner case bugs in an application is to build it on top of a system that guarantees complete, strong consistency.]  </p>\r\n<h2>WHY GIVE UP ON CONSISTENCY?</h2>\r\n<p>Consistency is a basic staple, a guarantee that is extremely hard to live without. So why do most NoSQL systems fail to guarantee consistency? They blame the CAP theorem. (For example,<a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\"> the Amazon Dynamo paper</a>, which inspired many widely used NoSQL systems, such as Cassandra, DynamoDB, and Riak, mention the availability vs. consistency tradeoff in the first paragraph of the section that discussed their “Design Considerations”, which lead to their famous “eventually consistent” architecture.) It is very hard, but not impossible, to build applications over systems that do not guarantee consistency. But the CAP theorem says that it is impossible for a system that guarantees consistency to guarantee 100% availability in the presence of a network partition. So if you can only choose one, it makes sense to choose availability. As we said above, once the system fails to guarantee consistency, &nbsp;developing applications on top of it without ugly corner case bugs is extremely challenging, and generally requires highly-skilled application developers that are able to handle the intellectual rigors of such development environments. Nonetheless, such skilled developers do exist, and this is the only way to avoid the impossibility proof from the CAP theorem of 100% availability.</p>\r\n<p>The reasoning of the previous paragraph, although perhaps well-thought out and convincing, is fundamentally flawed. The CAP theorem lives in a theoretical world where there is such a thing as 100% availability. In the real world, there is no such thing as 100% availability. Highly available systems are defined in terms of ‘9s’. Are you 99.9% available? Or 99.99% available? The more 9s, the better. Availability is fundamentally a pursuit in imperfection. No system can <strong>guarantee</strong> availability.</p>\r\n<p>This fact has significant ramifications when considering the availability vs. consistency tradeoff that was purported by the CAP theorem. It is not the case that if we guarantee consistency, we have to give up the guarantee of availability. We never had a guarantee of availability in the first place! Rather, guaranteeing consistency causes a <strong>reduction</strong> to our already imperfect availability.&nbsp;</p>\r\n<p>Therefore: the question becomes: how much availability is lost when we guarantee consistency? In practice, the answer is very little. Systems that guarantee consistency only experience a necessary reduction in availability in the event of a network partition. As networks become more redundant, partitions become an increasingly rare event. And even if there is a partition, it is still possible for the majority partition to be available. Only the minority partition must become unavailable. Therefore, for the reduction in availability to be perceived, there must be both a network partition, and also clients that are able to communicate with the nodes in the minority partition (and not the majority partition). This combination of events is typically rarer than other causes of system unavailability. Consequently, the real world impact of guaranteeing consistency on availability is often negligible. It is very possible to have a system that guarantees consistency and achieves high availability at the same time.</p>\r\n<p>[Side note: I have <a href=\"http://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf\" target=\"_blank\">written </a>extensively about these <a href=\"http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html\" target=\"_blank\">issues with the CAP theorem</a>. I believe the <a href=\"https://en.wikipedia.org/wiki/PACELC_theorem\" target=\"_blank\">PACELC theorem</a> is better able to summarize consistency tradeoffs in distributed systems.] </p>\r\n<h2>THE GLORIOUS RETURN OF CONSISTENT NEWSQL SYSTEMS</h2>\r\n<p>The argument above actually results in 3 distinct reasons for modern systems to be CP from CAP, instead of AP (i.e. choose consistency over availability):</p>\r\n<p>(1) &nbsp;&nbsp;&nbsp;Systems that fail to guarantee consistency result in complex, expensive, and often buggy application code.</p>\r\n<p>(2) &nbsp;&nbsp;&nbsp;The reduction of availability that is caused by the guarantee of consistency is minute, and hardly noticeable for many deployments.</p>\r\n<p>(3) &nbsp;&nbsp;&nbsp;The CAP theorem is fundamentally asymmetrical. CP systems can <strong>guarantee</strong> consistency. AP systems <strong>do not guarantee</strong> availability (no system can guarantee 100% availability). Thus only one side of the CAP theorem opens the door for any useful guarantees.</p>\r\n<p>I believe that the above three points is what has caused the amazing renaissance of distributed, transactional database systems --- many of which have become commercially available in the past few years --- &nbsp;that choose to be CP from CAP instead of AP. There is still certainly a place for AP systems, and their associated NoSQL implementations. But for most developers, building on top of a CP system is a safer bet.&nbsp;</p>\r\n<p>However, when I say that CP systems are the safer bet, I intend to refer to CP systems that <em><strong>actually</strong></em> guarantee consistency. Unfortunately, way too many of these modern NewSQL systems fail to guarantee consistency, despite their claims to the contrary. And once the guarantee is removed, the corner case bugs, complexity, and costs return. </p>\r\n<h2>SPANNER IS THE SOURCE OF THE PROBLEM</h2>\r\n<p> I have <a href=\"http://dbmsmusings.blogspot.com/2011/12/replication-and-latency-consistency.html\">discussed in previous posts</a> that there are many ways to guarantee consistency in distributed systems. The most popular mechanism, which guarantees consistency at minimal cost to availability, is to use the Paxos or Raft consensus protocols to enforce consistency across multiple replicas of the data. At a simplified level, these protocols work via a majority voting mechanism. Any change to the data requires a majority of replicas to agree to the change. This allows the minority of replicas to be down or unavailable and the system can nonetheless continue to read or write data.</p>\r\n<p>Most NewSQL systems use consensus protocols to enforce consistency. However, they differ in a significant way in <strong>how</strong> they use these protocols. I divide NewSQL systems into two categories along this dimension: The first category, as embodied in systems such as Calvin (which came out of my research group) and FaunaDB, uses a single, global consensus protocol per database. Every transaction participates in the same global protocol. The second category, as embodied in systems such as Spanner, CockroachDB, and YugaByte, partitions the data into ‘shards’, and applies a separate consensus protocol per shard.</p>\r\n<p>The main downside of the first category is scalability. A server can process a fixed number of messages per second. If every transaction in the system participates in the same consensus protocol, the same set of servers vote on every transaction. Since voting requires communication, the number of votes per second is limited by the number of messages each server can handle. This limits the total amount of transactions per second that the system can handle.</p>\r\n<p>Calvin and FaunaDB get around this downside via batching. Rather than voting on each transaction individually, they vote on batches of transactions. Each server batches all transactions that it receives over a fixed time period (e.g., 10 ms), and then initiates a vote on that entire batch at once. With 10ms batches, Calvin was able to achieve a throughput of over 500,000 transactions per second. For comparison, Amazon.com and NASDAQ likely process no more than 10,000 orders/trades per second even during peak workloads <em>[Update: there has been some discussion about these numbers from my readers. The number for NASDAQ might be closer to 100,000 orders per second. I have not seen anybody dispute the 10,000 orders per second number from Amazon.com, but readers have pointed out that they issue more than 10,000 writes to the database per second. However, this blog post is focused on strictly serializable transactions rather than individual write operations. For Calvin's 500,000 transactions per second number, each transaction included many write operations.]</em>&nbsp;</p>\r\n<p> The main downside of the second category is that by localizing consensus on a per-shard basis, it becomes nontrivial to guarantee consistency in the presence of transactions that touch data in multiple shards. The quintessential example is the case of someone performing a sequence of two actions on a photo-sharing application (1) Removing her parents from having permission to see her photos (2) Posting her photos from spring break. Even though there was a clear sequence of these actions from the vantage point of the user, if the permissions data and the photo data are located in separate shards, and the shards perform consensus separately, there is a risk that the parents will nonetheless be able to see the user’s recently uploaded photos.</p>\r\n<p>Spanner famously got around this downside with their TrueTime API. All transactions receive a timestamp which is based on the actual (wall-clock) current time. This enables there to be a concept of “before” and “after” for two different transactions, even those that are processed by completely disjoint set of servers. The transaction with a lower timestamp is “before” the transaction with a higher timestamp. Obviously, there may be a small amount of skew across the clocks of the different servers. Therefore, Spanner utilizes the concept of an “uncertainty” window which is based on the maximum possible time skew across the clocks on the servers in the system. After completing their writes, transactions wait until after this uncertainty window has passed before they allow any client to see the data that they wrote.</p>\r\n<p>Spanner thus faces a potentially uncomfortable tradeoff. It is desirable that the uncertainty window should be as small as possible, since as it gets larger, the latency of transactions increases, and the overall concurrency of the system decreases. On the other hand, it needs to 100% sure that clock skew <strong>never</strong> gets larger than the uncertainty window (since otherwise the guarantee of consistency would no longer exist), and thus larger windows are safer than smaller ones.</p>\r\n<p>Spanner handles this tradeoff with a specialized hardware solution that uses both GPS and atomic clocks to ensure a minimal clock skew across servers. This solution allows the system to keep the uncertainty window relatively narrow while at the same time keeping the probability of incorrect uncertainty window estimates (and corresponding consistency violations) to be extremely small. Indeed, the probability is so small that Spanner’s architects feel comfortable claiming that Spanner “guarantees” consistency.</p>\r\n<p>[It is worth noting at this point that systems that use global consensus avoid this problem entirely. If every transaction goes through the same protocol, then a natural order of all transactions emerges --- the order is simply the order in which transactions were voted on during the protocol. When batches are used instead of transactions, it is the batches that are ordered during the protocol, and transactions are globally ordered by combining their batch identifier with their sequence number within the batch. There is no need for clock time to be used in order to create a notion of before or after. Instead, the consensus protocol itself can be used to elegantly create a global order.] </p>\r\n<h2>SPANNER DERIVATIVES</h2>\r\n<p><br>Spanner is a beautiful and innovative system. It was also invented by Google and widely used there. Either because of the former or latter (or both), it has been extremely influential, and many systems (e.g., CockroachDB and YugaByte) have been inspired by the architectural decisions by Spanner. Unfortunately, &nbsp;these derivative systems are software-only, which implies that they have inherited only the software innovations without the hardware and infrastructure upon which Spanner relies at Google. In light of Spanner’s decision to have separate consensus protocols per shard, software-only derivatives are extremely dangerous. Like Spanner, these systems rely on real-world time in order to enforce consistency --- CockroachDB on<a href=\"https://cse.buffalo.edu/tech-reports/2014-04.pdf\"> HLC (hybrid logical clocks)</a> and YugaByte on<a href=\"http://users.ece.utexas.edu/~garg/pdslab/david/hybrid-time-tech-report-01.pdf\"> Hybrid Time</a>. Like Spanner, these systems rely on knowing the maximum clock skew across servers in order to avoid consistency violations. But unlike Spanner, these systems lack hardware and infrastructure support for minimizing and measuring clock skew uncertainty.<br><br></p>\r\n<p>CockroachDB, to its credit, has<a href=\"https://www.cockroachlabs.com/blog/living-without-atomic-clocks/\"> acknowledged</a> that by only incorporating Spanner’s software innovations, the system cannot guarantee CAP consistency (which, as mentioned above, is linearizability).&nbsp;</p>\r\n<p>YugaByte, however, continues to claim a guarantee of consistency [Edit for clarification: YugaByte only makes this claim for single key operations; however, YugaByte also relies on time synchronization for reading correct snapshots for transactions running under snapshot isolation.]. I would advise people to be wary of these claims which are based on assumptions of maximum clock skew. YugaByte, by virtue of its Spanner roots, will run into consistency violations when the local clock on a server suddenly jumps beyond the skew uncertainty window. This can happen under a variety of scenarios such as when a VM that is running YugaByte freezes or migrates to a different machine. Even without sudden jumps, YugaByte’s free edition relies on the user to set the assumptions about maximum clock skew. Any mistaken assumptions on behalf of the user can result in consistency violations.</p>\r\n<p>In contrast to CockroachDB and YugaByte, FaunaDB was inspired by Calvin instead of Spanner. [Historical note: the Calvin and Spanner papers were both published in 2012]. FaunaDB therefore has a single, elegant, global consensus protocol, and needs no small print regarding clock skew assumptions. Consequently, FaunaDB is able to <strong>guarantee</strong> consistency of transactions that modify <em>any </em>data in the database without concern for the corner case violations that can plague software-only derivatives of Spanner-style systems.</p>\r\n<p>There are other differences between Calvin-style systems and Spanner-style systems <a href=\"http://dbmsmusings.blogspot.com/2017/04/distributed-consistency-at-scale.html\">that I’ve talked about in the past</a>. In this post we focused on perhaps the most consequential difference: global consensus vs. partitioned consensus. As with any architectural decision, there are tradeoffs between these two options. For the vast majority of applications, exceeding 500,000 transactions a second is beyond their wildest dreams. If so, then the decision is clear. Global consensus is probably the better choice. &nbsp;<br>[Editor's note: Daniel Abadi is an advisor at FaunaDB.]</p>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "601",
        "postDate": "2018-09-20T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1363,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "45f19ac3-f6fe-4f87-b132-bb8a50e5d796",
        "siteSettingsId": 1363,
        "fieldLayoutId": 4,
        "contentId": 1028,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Fauna News: Nextdoor Chooses FaunaDB Serverless Cloud to Support Global Growth",
        "slug": "fauna-news-nextdoor-chooses-faunadb-serverless-cloud-to-support-global-growth",
        "uri": "blog/fauna-news-nextdoor-chooses-faunadb-serverless-cloud-to-support-global-growth",
        "dateCreated": "2018-09-20T07:19:37-07:00",
        "dateUpdated": "2020-08-18T11:40:59-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/fauna-news-nextdoor-chooses-faunadb-serverless-cloud-to-support-global-growth",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/fauna-news-nextdoor-chooses-faunadb-serverless-cloud-to-support-global-growth",
        "isCommunityPost": false,
        "blogBodyText": "<p>SAN FRANCISCO,&nbsp;Sept. 20, 2018&nbsp;/PRNewswire/ --&nbsp;<a href=\"http://www.fauna.com/\" rel=\"nofollow\" target=\"_blank\">Fauna</a>, provider of the leading relational NoSQL database, announced today that social network and neighborhood conversations provider&nbsp;<a href=\"http://www.nextdoor.com/\" rel=\"nofollow\" target=\"_blank\">Nextdoor</a>&nbsp;has chosen FaunaDB to meet increasing requirements for scalability, security and flexibility as the company experiences significant global growth.</p>\r\n<p>In particular, Nextdoor is seeing a steady increase in the number of globally distributed users of its mobile app, prompting the company to find a database solution capable of scaling to meet growing traffic demands while maintaining high availability to regions around the globe.</p>\r\n<p>Nextdoor wanted a data platform designed to grow with its business, and with the full suite of platform features FaunaDB provides, Nextdoor is able to expand its installation to support more applications and use cases. FaunaDB was designed from the ground up as a cloud-native, horizontally scalable database that delivers robust data management capabilities without sacrificing relational features.</p>\r\n<p>\"FaunaDB is a unique piece of software. We were able to achieve the transactional requirements of our workload and the scalability and performance levels to support our massive online community of users without compromising developer productivity,\" said&nbsp;Prakash Janakiraman, Co-founder and Chief Architect of Nextdoor. \"Neighbors often use Nextdoor in times of emergency to communicate with others, and for that reason Nextdoor must always be available. FaunaDB offers rock-solid infrastructure and was able to support our availability requirements from day one.\"</p>\r\n<p>A highly popular neighborhood community platform, Nextdoor provides a useful channel for local government services to connect with residents and broadcast emergency and other alerts to users in particular areas. Compiling lists of users based on group membership is a daunting task that created significant performance and operational challenges for Nextdoor's existing Postgres SQL database. The company needed a more globally scalable solution for creating a new groups subsystem in order to minimize the impact on the Nextdoor mobile app performance. But it didn't want sacrifice its needs for transactionality and performance.</p>\r\n<p>Nextdoor's requirements for scale without compromising mission-critical features such as strong data consistency and robust security were key factors in choosing Fauna. With its unique multi-region ACID transactions, data committed in FaunaDB is correct and complete and always available across all regions, even in the face of disasters. Additionally, FaunaDB features include multi-tenancy and object level security which means Nextdoor can easily add new workloads while continuing to grow the groups subsystem, while also meeting requirements for privacy and authentication.</p>\r\n<p>\"We built FaunaDB to give companies the safety, security, and reliability found in relational databases with the scalability and performance of NoSQL databases, which is critical to any business serving customers online,\" said&nbsp;Evan Weaver, CEO of Fauna. \"Similar to the transformation major social networking platforms underwent early on to support exponential growth in a short amount of time, businesses must be able to support high volumes of globally distributed mission-critical transactions – without making tradeoffs on critical enterprise capabilities such as scalability, security and development flexibility. Existing database architectures were never designed to meet those needs. Fauna is changing that.\"</p>\r\n<p>Fauna recently raised&nbsp;$25M&nbsp;through its Series A round of financing, led by&nbsp;Daniel Gwak&nbsp;at Point72 Ventures, with participation from GV (formerly Google Ventures), Costanoa Ventures, Afore Capital, and others. Existing investors also participated in the round, including CRV, Data Collective, Quest Venture Partners, the Webb Investment Network, and Ulu Ventures. This is the largest Series A financing round for an OLTP database company.</p>\r\n<p><strong>About Nextdoor.com, Inc.<br></strong>Nextdoor (nextdoor.com) is the free and private social network for neighborhoods available on Web, iOS, and Android. On Nextdoor, neighbors create private online communities where they get to know one another, ask questions, and exchange advice and recommendations. More than 180,000 neighborhoods across&nbsp;the United States,&nbsp;Germany,&nbsp;France,&nbsp;United Kingdom, and&nbsp;the Netherlands&nbsp;are using Nextdoor to build stronger and safer places to call home.</p>\r\n<p>Headquartered in&nbsp;San Francisco, Calif., Nextdoor is a privately-held company with the backing of prominent investors, including Benchmark, Greylock Partners, Tiger Global Management,&nbsp;Kleiner Perkins Caufield&nbsp;& Byers, and others.</p>\r\n<p>For additional information and images: nextdoor.com/newsroom</p>\r\n<p><strong>About Fauna<br></strong>Fauna, Inc., founded by ex-Twitter engineers, is the company behind FaunaDB, a relational NoSQL database. FaunaDB is a mission-critical component for companies that need to deliver products at global scale, increase infrastructure utilization, decrease operational overhead, and improve time to market. Fauna is the only database of its kind that brings together established technologies into a single system that delivers on three critical business values: productivity, safety and agility. Customers using cloud and on-premise deployments of FaunaDB include industry leaders in data-rich industries such as fintech, retail, ecommerce, and mobile gaming. For more information visit&nbsp;<a href=\"http://www.fauna.com/\" rel=\"nofollow\" target=\"_blank\">fauna.com</a>&nbsp;or follow us at&nbsp;<a href=\"http://www.twitter.com/fauna\" rel=\"nofollow\" target=\"_blank\">@fauna</a><a href=\"http://www.fauna.com/\" rel=\"nofollow\" target=\"_blank\">.</a></p>\r\n<p>SOURCE Fauna, Inc.</p>\r\n<h4>Related Links</h4>\r\n<p><a title=\"Link to http://www.fauna.com\" href=\"http://www.fauna.com/\" rel=\"nofollow\" target=\"_blank\">http://www.fauna.com</a></p>",
        "blogCategory": [
            "3",
            "1531"
        ],
        "mainBlogImage": [
            "1362"
        ],
        "bodyText": "<p>SAN FRANCISCO, Sept. 20, 2018 /PRNewswire/ -- <a href=\"http://www.fauna.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Fauna</a>, provider of the leading relational NoSQL database, announced today that social network and neighborhood conversations provider <a href=\"http://www.nextdoor.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Nextdoor</a> has chosen FaunaDB to meet increasing requirements for scalability, security and flexibility as the company experiences significant global growth.</p>\n<p>In particular, Nextdoor is seeing a steady increase in the number of globally distributed users of its mobile app, prompting the company to find a database solution capable of scaling to meet growing traffic demands while maintaining high availability to regions around the globe.</p>\n<p>Nextdoor wanted a data platform designed to grow with its business, and with the full suite of platform features FaunaDB provides, Nextdoor is able to expand its installation to support more applications and use cases. FaunaDB was designed from the ground up as a cloud-native, horizontally scalable database that delivers robust data management capabilities without sacrificing relational features.</p>\n<p>\"FaunaDB is a unique piece of software. We were able to achieve the transactional requirements of our workload and the scalability and performance levels to support our massive online community of users without compromising developer productivity,\" said Prakash Janakiraman, Co-founder and Chief Architect of Nextdoor. \"Neighbors often use Nextdoor in times of emergency to communicate with others, and for that reason Nextdoor must always be available. FaunaDB offers rock-solid infrastructure and was able to support our availability requirements from day one.\"</p>\n<p>A highly popular neighborhood community platform, Nextdoor provides a useful channel for local government services to connect with residents and broadcast emergency and other alerts to users in particular areas. Compiling lists of users based on group membership is a daunting task that created significant performance and operational challenges for Nextdoor's existing Postgres SQL database. The company needed a more globally scalable solution for creating a new groups subsystem in order to minimize the impact on the Nextdoor mobile app performance. But it didn't want sacrifice its needs for transactionality and performance.</p>\n<p>Nextdoor's requirements for scale without compromising mission-critical features such as strong data consistency and robust security were key factors in choosing Fauna. With its unique multi-region ACID transactions, data committed in FaunaDB is correct and complete and always available across all regions, even in the face of disasters. Additionally, FaunaDB features include multi-tenancy and object level security which means Nextdoor can easily add new workloads while continuing to grow the groups subsystem, while also meeting requirements for privacy and authentication.</p>\n<p>\"We built FaunaDB to give companies the safety, security, and reliability found in relational databases with the scalability and performance of NoSQL databases, which is critical to any business serving customers online,\" said Evan Weaver, CEO of Fauna. \"Similar to the transformation major social networking platforms underwent early on to support exponential growth in a short amount of time, businesses must be able to support high volumes of globally distributed mission-critical transactions – without making tradeoffs on critical enterprise capabilities such as scalability, security and development flexibility. Existing database architectures were never designed to meet those needs. Fauna is changing that.\"</p>\n<p>Fauna recently raised $25M through its Series A round of financing, led by Daniel Gwak at Point72 Ventures, with participation from GV (formerly Google Ventures), Costanoa Ventures, Afore Capital, and others. Existing investors also participated in the round, including CRV, Data Collective, Quest Venture Partners, the Webb Investment Network, and Ulu Ventures. This is the largest Series A financing round for an OLTP database company.</p>\n<p><strong>About Nextdoor.com, Inc.<br /></strong>Nextdoor (nextdoor.com) is the free and private social network for neighborhoods available on Web, iOS, and Android. On Nextdoor, neighbors create private online communities where they get to know one another, ask questions, and exchange advice and recommendations. More than 180,000 neighborhoods across the United States, Germany, France, United Kingdom, and the Netherlands are using Nextdoor to build stronger and safer places to call home.</p>\n<p>Headquartered in San Francisco, Calif., Nextdoor is a privately-held company with the backing of prominent investors, including Benchmark, Greylock Partners, Tiger Global Management, Kleiner Perkins Caufield &amp; Byers, and others.</p>\n<p>For additional information and images: nextdoor.com/newsroom</p>\n<p><strong>About Fauna<br /></strong>Fauna, Inc., founded by ex-Twitter engineers, is the company behind FaunaDB, a relational NoSQL database. FaunaDB is a mission-critical component for companies that need to deliver products at global scale, increase infrastructure utilization, decrease operational overhead, and improve time to market. Fauna is the only database of its kind that brings together established technologies into a single system that delivers on three critical business values: productivity, safety and agility. Customers using cloud and on-premise deployments of FaunaDB include industry leaders in data-rich industries such as fintech, retail, ecommerce, and mobile gaming. For more information visit <a href=\"http://www.fauna.com/\" target=\"_blank\" rel=\"noreferrer noopener\">fauna.com</a> or follow us at <a href=\"http://www.twitter.com/fauna\" target=\"_blank\" rel=\"noreferrer noopener\">@fauna</a><a href=\"http://www.fauna.com/\" target=\"_blank\" rel=\"noreferrer noopener\">.</a></p>\n<p>SOURCE Fauna, Inc.</p>\n<h4>Related Links</h4>\n<p><a title=\"Link to http://www.fauna.com\" href=\"http://www.fauna.com/\" target=\"_blank\" rel=\"noreferrer noopener\">http://www.fauna.com</a></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "1370",
        "postDate": "2018-09-12T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1371,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "5f70d22c-bd9d-49d3-9a7e-06caa0722655",
        "siteSettingsId": 1371,
        "fieldLayoutId": 4,
        "contentId": 1035,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Setting up a new local FaunaDB cluster using Docker",
        "slug": "setting-up-a-new-fauna-cluster-using-docker",
        "uri": "blog/setting-up-a-new-fauna-cluster-using-docker",
        "dateCreated": "2018-09-20T09:54:47-07:00",
        "dateUpdated": "2019-08-21T14:35:14-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/setting-up-a-new-fauna-cluster-using-docker",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/setting-up-a-new-fauna-cluster-using-docker",
        "isCommunityPost": false,
        "blogBodyText": "<p>Every Oracle DBA will have a RAC setup story to tell. It can range from the time it took, the patches they had to install midway, or not being able to connect to the database for no apparent reason. Having lived through these experiences over the years, the very idea of setting up a database cluster makes me think that it cannot be very straightforward. So when I embarked on setting up my first FaunaDB cluster on my second week on the job, I was a bit apprehensive and thought that it would take hours. Leave aside Oracle RAC-- even Cassandra and Mongo gave me a hard time. But it turned out that setting up FaunaDB is really simple and can be done within a few minutes. My DBA friends, are you listening ?!!!</p>\r\n<p>As with most new databases these days, I decided to setup a cluster on my laptop using docker. Docker provides a nice way to keep things very simple and clean. My first step was to pull the FaunaDB container image from docker hub.</p>\r\n<pre>$ docker pull fauna/faunadb:latest\r\nlatest: Pulling from fauna/faunadb\r\nbe8881be8156: Pull complete\r\n60f08eedb1d2: Pull complete\r\nd4f58360b842: Pull complete\r\n6a391283a674: Pull complete\r\ne5fae5985ac7: Pull complete\r\neb00faac30ba: Pull complete\r\n90e011c54f88: Pull complete\r\n6529b57b5cf9: Pull complete\r\n71a47929723d: Pull complete\r\n3e67826a23e5: Pull complete\r\nc62cc3207452: Pull complete\r\nDigest: sha256:15fcf6e1daf31447fd8762c4d925d268f7138623f3e448602\r\na33a3ba9efb9168\r\nStatus: Downloaded newer image for fauna/faunadb:latest\r\n$ docker image list|grep -i fauna\r\nfauna/faunadb    latest    d2f23397fcce   9 days ago   327MB\r\nDebadityas-MacBook-Pro:~ deba$\r\n</pre>\r\n<p>Now that the image is available, we can straightaway use it to start my first node of the cluster.</p>\r\n<pre>$ docker run -d --rm --name faunadb -p 8443:8443 fauna/faunadb\r\n47a75e91096149d9607d660eb81b29f87ed32659cc473dc70466976f2f590c4e\r\n</pre>\r\n<p>If you intend to write a lot of data and want it to persist it between container shutdowns, you will want to map a local disk as a volume inside the container. Further, if you want to access the logs from your host OS, you will want to map the log folder. You can do all of this using the command below.</p>\r\n<pre>$ docker run --rm --name faunadb -p 8443:8443 \\\r\n    -v &lt;host-directory or=\"\" named-volume=\"\"&gt;:/var/lib/faunadb \\\r\n    -v &lt;host-directory&gt;:/var/log/faunadb \\\r\n    fauna/faunadb:&lt;version&gt;&lt;/version&gt;&lt;/host-directory&gt;&lt;/host-directory&gt;</pre>\r\n<p>Once the container is up and running, then you can check the status of this node. We will log into the container and check the status with the admin tool:</p>\r\n<pre>$ docker exec -it 47a /bin/bash\r\nroot@47a75e910961:/faunadb# cd /faunadb/enterprise/\r\nroot@47a75e910961:/faunadb/enterprise# bin/faunadb-admin --key secret status\r\nNo configuration file specified; loading defaults...\r\nDatacenter: NoDc\r\n================\r\nStatus  State  WorkerID  Address     Owns    Goal    HostID\r\nup      live   512       172.17.0.2  100.0%  100.0%\r\n6bc66c3b-8a67-40a4-a06c-fdbe3bd281ad\r\n</pre>\r\n<p>Once the first node is up and running, open another terminal window and join two new nodes. Make sure to note the ip/address assigned to the first node.</p>\r\n<pre># Add the 2nd Node\r\n$ docker run -d --rm --name faunadb2 -p 8444:8443 fauna/faunadb --join\r\n172.17.0.2\r\n1e56362fdfede5f884abec8a5b9bc8050db013b483498a0fed107120f7458d71\r\n# Add the 3rd Node\r\n$ docker run -d --rm --name faunadb3 -p 8445:8443 fauna/faunadb --join\r\n172.17.0.2\r\n921594173d98af5042b1f77312d321543cd3484926f754c22397452f433d8dea\r\n$ docker ps\r\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                   NAMES\r\n921594173d98        fauna/faunadb       \"faunadb-entrypoint.…\"   4 minutes ago       Up 4 minutes        7500-7501/tcp, 0.0.0.0:8445-&gt;8443/tcp   faunadb3\r\n1e56362fdfed        fauna/faunadb       \"faunadb-entrypoint.…\"   5 minutes ago       Up 5 minutes        7500-7501/tcp, 0.0.0.0:8444-&gt;8443/tcp   faunadb2\r\n47a75e910961        fauna/faunadb       \"faunadb-entrypoint.…\"   6 minutes ago       Up 6 minutes        7500-7501/tcp, 0.0.0.0:8443-&gt;8443/tcp   faunadb\r\n</pre>\r\n<p>After starting the two nodes, we can now check the status of the cluster.</p>\r\n<pre>root@47a75e910961:/faunadb/enterprise# bin/faunadb-admin --key secret status\r\nNo configuration file specified; loading defaults...\r\nDatacenter: NoDc\r\n================\r\nStatus  State  WorkerID  Address     Owns   Goal   HostID\r\nup      live   512       172.17.0.2  40.2%  33.6%\r\n6bc66c3b-8a67-40a4-a06c-fdbe3bd281ad\r\nup      live   513       172.17.0.3  42.6%  35.4%\r\ncb2f3ff4-dda7-47c5-ade4-fef9e96ff146\r\nup      live   514       172.17.0.4  17.2%  31.1%\r\n44ecee55-b1a4-4eba-af40-0dda3e1daadc\r\n</pre>\r\n<p>So the cluster, a single replica with multiple nodes, is up and running. But keep in mind this has only been installed on my laptop to play with the database. Real FaunaDB clusters are installed across globally distributed data centers. If you want to get a feel for that architecture, here is a <a href=\"https://blog.fauna.com/introduction-to-faunadb-clusters\">great post</a> by John Miller.<br></p>\r\n<p>Once the database is setup, we want to spin up the dashboard tool. For setting up the dashboard tool, you can refer to its Git repository <a href=\"https://github.com/fauna/dashboard\">here</a>.</p>\r\n<p>I have already cloned the repository and done the install. So all I had to do was to start the dashboard.</p>\r\n<pre>$ npm start\r\nYou can now view dashboard-base in the browser.\r\n  Local:            http://localhost:3000/\r\n  On Your Network:  http://10.0.1.158:3000/\r\nNote that the development build is not optimized.\r\nTo create a production build, use npm run build.\r\n</pre>\r\n<p>The dashboard will open in a browser window and prompt for your secret that is specified part of the image.</p>\r\n<figure><img src=\"https://www2.fauna.com/l/517431/2018-08-30/6f5xyz/517431/120901/Docker_Blog_Connect_to_FaunaDB.png\" data-image=\"q119xvjxcdv6\"></figure>\r\n<p>Once the you click on the “Use Secret” button you can access the dashboard to&nbsp;create databases, classes etc.<br></p>\r\n<p>As you can see, in matter of minutes we could setup a cluster, access the dashboard and start creating data in FaunaDB. If you want even faster access you can try the always ON FaunaDB Serverless Cloud. It is super simple to get<a href=\"https://app.fauna.com/account\"> started</a> and you don’t have to worry about running or managing a database. We will also provide a docker compose file soon that will make your local setup even easier. Keep an eye on our <a href=\"https://github.com/fauna/faunadb-docker\">git repo</a> for docker.</p>\r\n<p>In the coming blog posts, I will talk about how to get started with FaunaDB using Python and also setup a multi-DC cluster with Docker.</p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-09-11T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1369,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "8b405a83-2ba1-4e7b-9346-231bcad343b8",
        "siteSettingsId": 1369,
        "fieldLayoutId": 4,
        "contentId": 1033,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Using FaunaDB with Begin.com",
        "slug": "using-faunadb-with-begin-com",
        "uri": "blog/using-faunadb-with-begin-com",
        "dateCreated": "2018-09-20T09:50:12-07:00",
        "dateUpdated": "2020-02-24T08:37:36-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/using-faunadb-with-begin-com",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/using-faunadb-with-begin-com",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://begin.com/\" data-redactor-span=\"true\">Begin.com</a>&nbsp;is a serverless application host with a focus on ease of development. Apps built with Begin follow a standard package format allowing them to be managed by Begin.com or deployed to the developer’s own AWS environment. In this post, I’ll show the process I went through to build a new Begin app, and use it to query FaunaDB Serverless Cloud. The takeaway is&nbsp;<strong>how easily FaunaDB connects with serverless applications</strong>.</p>\n<h3>Step 1: Deploy the Begin starter app</h3>\n<p>When you first create an app with Begin, you’re dropped into a combination deployment log and help wizard. I appreciate this approach as it gives new developers the hand holding they might need and experienced developers a quick way to grasp the advanced functionality.</p>\n<figure><img src=\"{asset:6926:url}\" data-image=\"6926\"></figure>\n<h3>Step 2: Generate a JSON function with Begin</h3>\n<p>By following the tutorial, you’ll end up with a few boilerplate functions in your app, and a local clone of its git repo. At this point, I created a new JSON function called&nbsp;<strong>fauna</strong>&nbsp;with the wizard, and Begin committed the boilerplate code to my repo. I pulled those changes to my workstation, ran&nbsp;<code>npm install faunadb</code>&nbsp;in the subdirectory corresponding to the new function, and added a small query to the code. Begin’s code browser shows you the current version of each function; here you can see me importing the FaunaDB driver and creating a client object.</p><figure><img src=\"{asset:6927:url}\" data-image=\"6927\"></figure>\n\n<h3>Step 3: Deploy and test changes using Begin</h3>\n<p>Between each change, I tested locally and pushed to master, which initiated a deploy to staging on Begin. Deploys include the output of verification, linting, testing, and all the other stages, so you can debug if needed.</p><figure><img src=\"{asset:6928:url}\" data-image=\"6928\"></figure>\n\n<h3>Step 4: Configure FaunaDB connection secrets</h3>\n<p>Once I had the dependencies resolved and deploy working, I checked out Begin’s environment management features. It was simple to store my FaunaDB connection secret and reference it via the JavaScript&nbsp;<code>process.env</code>&nbsp;API. Each environment variable can be specified for each of testing, staging, and production. I used the same FaunaDB server secret (now revoked) for testing and staging, but for a real app I’d probably use different databases.</p><figure><img src=\"{asset:6929:url}\" data-image=\"6929\"></figure>\n\n<p>The end result is an easy to manage function deployed to AWS Lambda, with all repetition and ceremony removed. Begin also has features for other data types besides HTML and JSON, but these two are all you need to get started.</p>\n<h3>Step 5: Run your first query</h3>\n<p>Here’s the output of my first query via Begin’s function API.</p>\n\n<figure><img src=\"{asset:6930:url}\" data-image=\"6930\"></figure>\n<p>You can find&nbsp;<a href=\"https://github.com/jchris/begin-functions-app\" data-redactor-span=\"true\">my FaunaDB hello world code here</a>, or follow the above instructions to get started with Begin.</p>",
        "blogCategory": [
            "10",
            "1530"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-09-05T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1368,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "1eb8db3e-d273-44dd-8090-61bfdaf959f2",
        "siteSettingsId": 1368,
        "fieldLayoutId": 4,
        "contentId": 1032,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Webcast Recording: Serverless Best Practices with FaunaDB & Netlify",
        "slug": "webcast-video-serverless-best-practices-with-netlify",
        "uri": "blog/webcast-video-serverless-best-practices-with-netlify",
        "dateCreated": "2018-09-20T09:49:04-07:00",
        "dateUpdated": "2019-08-19T20:56:41-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/webcast-video-serverless-best-practices-with-netlify",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/webcast-video-serverless-best-practices-with-netlify",
        "isCommunityPost": false,
        "blogBodyText": "<p>Last week, David Wells of Netlify and I led a webcast on&nbsp;<strong>Serverless Best Practices</strong>. For people developing serverless applications, it is an introduction to important topics you should consider for application performance and agility.</p>\r\n<p>The best practices we consider include:</p>\r\n<ul><li>Continuous Integration / Continuous Deployment</li><li>Data management and GraphQL</li><li>Multi-cloud operations</li><li>Cold start mitigation via warmers, fast loading, and direct database access</li><li>Infrastructure as code allows for repeatable deployments.</li></ul>\r\n<figure><a href=\"https://www2.fauna.com/l/517431/2018-08-14/6dvyt6\" data-redactor-span=\"true\"><img src=\"https://lh5.googleusercontent.com/s0VI6P8Lv1mpKkcfuJxPqe69U64JhNtqJPM6aay1Lrb7soJa-4xYd2aXUY7Yk8YEq3qq0usJg1L7A5Gjg0pcZw5X6zd3JdXFY0I741W4kV9Z16G7B8D7tP0X-7M-0sJZjN3LrZ0q\" width=\"624\" height=\"407\" data-image=\"g62nos7cf0q5\"></a></figure>\r\n<p><a href=\"https://www2.fauna.com/l/517431/2018-08-14/6dvyt6\" data-redactor-span=\"true\">Click here</a>&nbsp;for the recorded webcast. There is a live demo at about 15 minutes in.</p>",
        "blogCategory": [
            "1530",
            "1866"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-08-27T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 1367,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b1c3d823-cf8b-46ab-a600-a10b3d662caa",
        "siteSettingsId": 1367,
        "fieldLayoutId": 4,
        "contentId": 1031,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with FaunaDB using Go | Part 4",
        "slug": "getting-started-with-faunadb-using-go-part-4",
        "uri": "blog/getting-started-with-faunadb-using-go-part-4",
        "dateCreated": "2018-09-20T09:45:01-07:00",
        "dateUpdated": "2019-02-26T09:52:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-faunadb-using-go-part-4",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-faunadb-using-go-part-4",
        "isCommunityPost": false,
        "blogBodyText": "<figure style=\"margin: 0px 0px 16px; padding: 0px; border: 0px; font-family: &quot;Trebuchet MS&quot;, &quot;Helvetica Neue&quot;, Helvetica, Tahoma, sans-serif; font-size: 16px; font-style: normal; font-variant-caps: normal; font-weight: normal; font-stretch: inherit; line-height: inherit; vertical-align: baseline; display: block; caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51); letter-spacing: normal; orphans: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; text-align: center;\"><a href=\"https://youtu.be/rW0tHZ7ypo4\" data-redactor-span=\"true\" style=\"margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; text-decoration: none; color: rgb(37, 107, 201); cursor: pointer;\"><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" alt=\"Go & Fauna\" title=\"Go & Fauna\" style=\"margin: 0px; padding: 0px; border: 0px; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; vertical-align: middle; cursor: pointer; max-width: 100%; height: auto;\" data-image=\"9zgp04eu1wbo\"></a></figure>\r\n<p></p>\r\n<p>In the first&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go\" data-redactor-span=\"true\">part</a>&nbsp;of this series, we developed the basics necessary to connect to the FaunaDB Cloud. Using that client, we then created our first database. In the second&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go-part-2\" data-redactor-span=\"true\">part</a>, we developed the concept of the database-specific client and used that to create our first class. The third&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go-part-3\" data-redactor-span=\"true\">part</a>&nbsp;of the series saw the addition of instances or data to our class.</p>\r\n<p>Picking up where we left off, this video will:</p>\r\n<ol><li>Create an index based on a key value in our data.&nbsp;</li><li>Use that index to retrieve instances of our class and extract specific values from that instance.</li></ol>\r\n<p>The final code from this video can be found&nbsp;<a href=\"https://gist.github.com/CaryBourgeois/3cc190bb0993b55347d3867f73366fb1\" data-redactor-span=\"true\">here</a>.&nbsp;</p>\r\n<p>As you go through these videos, please feel free to provide feedback and suggest other specific functionalities you would like to have demonstrated using this approach.</p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p><center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Lj2vTuVtAo\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p></center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-08-17T20:37:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 393,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "7058b81c-797e-4c02-b481-4bb64a075a0b",
        "siteSettingsId": 393,
        "fieldLayoutId": 4,
        "contentId": 228,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with FaunaDB using Go | Part 3",
        "slug": "getting-started-with-faunadb-using-go-part-3",
        "uri": "blog/getting-started-with-faunadb-using-go-part-3",
        "dateCreated": "2018-08-24T15:42:14-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-faunadb-using-go-part-3",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-faunadb-using-go-part-3",
        "isCommunityPost": false,
        "blogBodyText": "<figure style=\"text-align: center;\"><a href=\"https://youtu.be/rW0tHZ7ypo4\"><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" alt=\"Go & Fauna\" title=\"Go & Fauna\" data-image=\"zkxzr507fsdi\"></a></figure>\r\n<p>\r\n</p>\r\n<p>In the&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go\">first part</a> of this series, we developed the basics necessary to connect to the FaunaDB Cloud. Using that client, we then created our first database. In the&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go-part-2\">second part</a>, we developed the concept of the database specific client and used that to create our first class. In RDBMS terms, we have now created a schema and a table. In this part, we will demonstrate how to add instances or records to our class.<br></p>\r\n<p>Picking up where we left off, this video will:</p>\r\n<ol><li>Create an instance in a class that contains our schemaless object. </li><li>Return the reference to the instance just created.</li><li>Use that reference to read/extract the data values it contains.</li></ol>\r\n<p>The final code from this video can be found <a href=\"https://gist.github.com/CaryBourgeois/4d5e7eab118dd17703a2b8cc5096e0fa\">here</a>. </p>\r\n<p>As you go through these videos, please feel free to provide feedback and suggest other specific functionalities you would like to have demonstrated using this approach.</p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p><center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rW0tHZ7ypo4\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p></center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<figure style=\"text-align:center;\"><a href=\"https://youtu.be/rW0tHZ7ypo4\"><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" alt=\"Go &amp; Fauna\" title=\"Go &amp; Fauna\" /></a></figure><p>\n</p>\n<p>In the <a href=\"https://blog.fauna.com/getting-started-with-faunadb-using-go\">first part</a> of this series, we developed the basics necessary to connect to the FaunaDB Cloud. Using that client, we then created our first database. In the <a href=\"https://blog.fauna.com/getting-started-with-faunadb-using-go-part-2\">second part</a>, we developed the concept of the database specific client and used that to create our first class. In RDBMS terms, we have now created a schema and a table. In this part, we will demonstrate how to add instances or records to our class.<br /></p>\n<p>Picking up where we left off, this video will:</p>\n<ol><li>Create an instance in a class that contains our schemaless object. </li><li>Return the reference to the instance just created.</li><li>Use that reference to read/extract the data values it contains.</li></ol><p>The final code from this video can be found <a href=\"https://gist.github.com/CaryBourgeois/4d5e7eab118dd17703a2b8cc5096e0fa\">here</a>. </p>\n<p>As you go through these videos, please feel free to provide feedback and suggest other specific functionalities you would like to have demonstrated using this approach.</p>\n<center>\n<figure></figure></center>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-08-16T16:39:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 534,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "dd67fb51-6d80-41f8-966d-c7af5dada942",
        "siteSettingsId": 534,
        "fieldLayoutId": 4,
        "contentId": 367,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "New Serverless Pricing Model for FaunaDB Cloud",
        "slug": "new-serverless-pricing-model-for-faunadb-cloud",
        "uri": "blog/new-serverless-pricing-model-for-faunadb-cloud",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-08-21T14:31:34-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/new-serverless-pricing-model-for-faunadb-cloud",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/new-serverless-pricing-model-for-faunadb-cloud",
        "isCommunityPost": false,
        "blogBodyText": "<p><a href=\"https://app.fauna.com/sign-up\">FaunaDB Cloud</a> was launched last year as the industry’s first (and still only) multi-cloud serverless database service. We’ve seen growing interest in our offering--serving 2M queries daily and counting!<br></p>\r\n<p>We continue to add new capabilities to FaunaDB based on your feedback, and make them available as part of our Cloud offering. A popular request was a simpler pricing model that is more predictable, easier to understand, and easier to budget for. So, we’re excited to present the new serverless pricing that that went into effect Monday, August 13th.</p>\r\n<figure><center><img src=\"https://storage.pardot.com/517431/119913/fauna_pricing_table.png\" data-image=\"z8evqldnkx5d\"></center></figure>\r\n<h2>New Serverless Pricing Details</h2>\r\n<p>The Fauna points system has been replaced with a new pricing model, based on concrete resources (read ops, write ops, storage, data transfer out, etc.). Cost per query function is easily understandable and documented. We’re also introducing a new daily free quota. Small apps can be hosted on FaunaDB cloud for free...forever.</p>\r\n<h3>Free for 3 Months</h3>\r\n<p>All new users will get 3 free months of FaunaDB Cloud. After that, resource-based billing will kick in with the free daily usage tiers applied (as described above).</p>\r\n<h3>Existing Users with Remaining Free Points</h3>\r\n<p>Existing users' free points are being reset to a three month free period starting Monday, August 13th.</p>\r\n<h3>Existing Paying Users</h3>\r\n<p>Existing users, currently paying based on points, are being converted to resource-based pricing. We will also waive your charges for next three billing cycles. We will follow up with you directly as well. &nbsp;</p>\r\n<h3>New Fixed Pricing Tiers</h3>\r\n<p>We work with many small to mid-sized businesses who need competitive yet reliable, fixed price plans. Should you fall in that category, please <a href=\"https://www2.fauna.com/e/517431/request-info/6dn7vq/478423283\">request</a> info for a fixed price plan customized to your business. We hope that these fixed plans will help you adopt FaunaDB in a model that best suits your business.</p>\r\n<p>We’re excited about these changes. If you have questions or comments about the new model, don’t hesitate to contact us at <a href=\"mailto:priority@fauna.com\">priority@fauna.com</a>.</p>",
        "blogCategory": [
            "1530",
            "1531"
        ],
        "mainBlogImage": [],
        "bodyText": "<p><a href=\"https://app.fauna.com/sign-up\">FaunaDB Cloud</a> was launched last year as the industry’s first (and still only) multi-cloud serverless database service. We’ve seen growing interest in our offering--serving 2M queries daily and counting!<br /></p>\n<p>We continue to add new capabilities to FaunaDB based on your feedback, and make them available as part of our Cloud offering. A popular request was a simpler pricing model that is more predictable, easier to understand, and easier to budget for. So, we’re excited to present the new serverless pricing that that went into effect Monday, August 13th.</p>\n<figure><center><img src=\"https://storage.pardot.com/517431/119913/fauna_pricing_table.png\" alt=\"\" /></center></figure><h2>New Serverless Pricing Details</h2>\n<p>The Fauna points system has been replaced with a new pricing model, based on concrete resources (read ops, write ops, storage, data transfer out, etc.). Cost per query function is easily understandable and documented. We’re also introducing a new daily free quota. Small apps can be hosted on FaunaDB cloud for free...forever.</p>\n<h3>Free for 3 Months</h3>\n<p>All new users will get 3 free months of FaunaDB Cloud. After that, resource-based billing will kick in with the free daily usage tiers applied (as described above).</p>\n<h3>Existing Users with Remaining Free Points</h3>\n<p>Existing users' free points are being reset to a three month free period starting Monday, August 13th.</p>\n<h3>Existing Paying Users</h3>\n<p>Existing users, currently paying based on points, are being converted to resource-based pricing. We will also waive your charges for next three billing cycles. We will follow up with you directly as well. </p>\n<h3>New Fixed Pricing Tiers</h3>\n<p>We work with many small to mid-sized businesses who need competitive yet reliable, fixed price plans. Should you fall in that category, please <a href=\"https://www2.fauna.com/e/517431/request-info/6dn7vq/478423283\">request</a> info for a fixed price plan customized to your business. We hope that these fixed plans will help you adopt FaunaDB in a model that best suits your business.</p>\n<p>We’re excited about these changes. If you have questions or comments about the new model, don’t hesitate to contact us at <a href=\"mailto:priority@fauna.com\">priority@fauna.com</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-08-10T20:43:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 533,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "7ed0c4cf-b618-4ac1-a51c-513908426bc4",
        "siteSettingsId": 533,
        "fieldLayoutId": 4,
        "contentId": 366,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with FaunaDB using Go | Part 2",
        "slug": "getting-started-with-faunadb-using-go-part-2",
        "uri": "blog/getting-started-with-faunadb-using-go-part-2",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-faunadb-using-go-part-2",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-faunadb-using-go-part-2",
        "isCommunityPost": false,
        "blogBodyText": "<figure><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" data-image=\"i2tjx5n2w951\"></figure>\r\n<p>In the&nbsp;<a href=\"https://fauna.com/blog/getting-started-with-faunadb-using-go\">first part</a> of this series, we developed the basics necessary to connect to the FaunaDB Cloud. Using that client, we then created our first database.</p>\r\n<p>Picking up where we left off, this video will:<br></p>\r\n<ol><li>Go through the process of creating a secure database specific client </li><li>Create a first class using that client </li><li>Explore the various data objects in the FaunaDB data model, making distinctions between them and their RDBMS equivalents. </li></ol>\r\n<p>The final code from this video can be found <a href=\"https://gist.github.com/CaryBourgeois/8e5712566a2aa547abf1104a2c9e06b5\">here</a>. </p>\r\n<p>As you go through these videos, please feel free to provide feedback and suggest other specific functionalities you would like to have demonstrated using this approach.<br><br></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p><center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YOsZLq-RF90?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p></center><p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>\r\n<p></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "1319"
        ],
        "bodyText": "<figure><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" alt=\"\" /></figure><p>In the <a href=\"https://blog.fauna.com/getting-started-with-faunadb-using-go\">first part</a> of this series, we developed the basics necessary to connect to the FaunaDB Cloud. Using that client, we then created our first database.</p>\n<p>Picking up where we left off, this video will:<br /></p>\n<ol><li>Go through the process of creating a secure database specific client </li><li>Create a first class using that client </li><li>Explore the various data objects in the FaunaDB data model, making distinctions between them and their RDBMS equivalents. </li></ol><p>The final code from this video can be found <a href=\"https://gist.github.com/CaryBourgeois/8e5712566a2aa547abf1104a2c9e06b5\">here</a>. </p>\n<p>As you go through these videos, please feel free to provide feedback and suggest other specific functionalities you would like to have demonstrated using this approach.<br /><br /></p>\n<center>\n<figure></figure></center>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-08-03T19:54:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 536,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "37c693b0-dcb1-46c4-8293-bc747433b091",
        "siteSettingsId": 536,
        "fieldLayoutId": 4,
        "contentId": 369,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with FaunaDB using Go",
        "slug": "getting-started-with-faunadb-using-go",
        "uri": "blog/getting-started-with-faunadb-using-go",
        "dateCreated": "2018-08-27T07:43:26-07:00",
        "dateUpdated": "2020-02-24T09:14:58-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-faunadb-using-go",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-faunadb-using-go",
        "isCommunityPost": false,
        "blogBodyText": "\n<p>This is the first in a series of short videos, each (hopefully) less than 10 minutes. These videos are introductions to FaunaDB, specifically how to interact with it using the Fauna Query Language (FQL) through the Go driver. Fauna supports many drivers including Java, JavaScript, Python, C#, and Go. I chose Go for its easy to read syntax and its strong typing. While not strictly required, understanding the types used when interacting with FaunaDB makes the FQL easier to follow.</p>\n\n<p>In this first video we simply connect to FaunaDB and create a database. Following videos will build upon this to show basic data modelling and CRUD activities. </p>\n<p>This series has three prerequisites:</p>\n<ol><li>An account at <a href=\"https://fauna.com/\">fauna.com</a>. This will give you access to the FaunaDB Cloud. Alternatively, you can use the developer version of Fauna. </li><li>Local install of <a href=\"https://golang.org/doc/install\">Go</a>.</li><li>The FaunaDB Go driver. Installation instructions and more information on the driver are available here: <a href=\"https://github.com/fauna/faunadb-go\">https://github.com/fauna/faunadb-go</a></li></ol>\n<p>As you go through these videos, please feel free to provide feedback and any other specific functionalities you would like to have demonstrated using this approach.</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p><center><p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9A4ceXuu4mk?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p></center><p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [
            "1165"
        ],
        "bodyText": "<figure><img src=\"https://storage.pardot.com/517431/118459/go_fauna.png\" alt=\"\" /></figure><p>This is the first in a series of short videos, each (hopefully) less than 10 minutes. These videos are introductions to FaunaDB, specifically how to interact with it using the Fauna Query Language (FQL) through the Go driver. Fauna supports many drivers including Java, JavaScript, Python, C#, and Go. I chose Go for its easy to read syntax and its strong typing. While not strictly required, understanding the types used when interacting with FaunaDB makes the FQL easier to follow.</p>\n<p>In this first video we simply connect to FaunaDB and create a database. Following videos will build upon this to show basic data modelling and CRUD activities. </p>\n<p>This series has three prerequisites:</p>\n<ol><li>An account at <a href=\"https://fauna.com/\">fauna.com</a>. This will give you access to the FaunaDB Cloud. Alternatively, you can use the developer version of Fauna. </li><li>Local install of <a href=\"https://golang.org/doc/install\">Go</a>.</li><li>The FaunaDB Go driver. Installation instructions and more information on the driver are available here: <a href=\"https://github.com/fauna/faunadb-go\">https://github.com/fauna/faunadb-go</a></li></ol><p>As you go through these videos, please feel free to provide feedback and any other specific functionalities you would like to have demonstrated using this approach.</p>\n<center>\n<figure></figure></center>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "479",
        "postDate": "2018-07-25T11:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 531,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e84b7574-e0bb-4a62-960d-2b3742642db3",
        "siteSettingsId": 531,
        "fieldLayoutId": 4,
        "contentId": 364,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Tutorial: How to Create and Query a Ledger with FaunaDB",
        "slug": "tutorial-how-to-create-and-query-a-ledger-with-faunadb",
        "uri": "blog/tutorial-how-to-create-and-query-a-ledger-with-faunadb",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-04-24T09:49:37-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/tutorial-how-to-create-and-query-a-ledger-with-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/tutorial-how-to-create-and-query-a-ledger-with-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>In the&nbsp;<a href=\"https://blog.fauna.com/why-strong-consistency-with-event-driven\">previous article</a>, we learned how FaunaDB provides the ideal platform for event-driven programming by providing mission critical, ACID transactions at a global scale. This article helps you get started using FaunaDB for such transactions. If you are new to FaunaDB, we’d recommend reading the&nbsp;<a href=\"https://fauna.com/documentation/howto/crud\">FaunaDB CRUD documentation</a>&nbsp;for a background on FaunaDB query language, but you don’t need it to follow along. We’ll only be covering basic FaunaDB queries from the command line. (But stay tuned for the next tutorial in this series on how to query FaunaDB via a full Java application.)</p>\r\n<figure><img src=\"https://storage.pardot.com/517431/117323/Blog_Image_tutorial_event_driven_2.png\" data-image=\"a820wdaj44ow\"><figcaption>Image by Semantic Scholar (https://bit.ly/2JRwreU)</figcaption></figure>\r\n<p>In this tutorial, we will walk through manually adding ledger entries and then querying the ledger. If you just want the raw code, it’s available at this <a href=\"https://gist.github.com/retroryan/7448032411c415330cc5fd81ff549b3b\">Event-Sourcing with FaunaDB gist</a>.</p>\r\n<h2>Prerequisites</h2>\r\n<p>If you haven’t already, <a href=\"https://fauna.com/sign-up\">sign up for a free Fauna account</a>.</p>\r\n<p>Then, install the <a href=\"https://www.npmjs.com/package/fauna-shell\">Fauna Shell</a>:</p>\r\n<pre class=\"language-javascript\">$ npm install -g fauna-shell</pre>\r\n<p>Once installed, tell Fauna that you want to login.<br></p>\r\n<pre class=\"language-javascript\">$ fauna cloud-login\r\n</pre>\r\n<p>Enter your Fauna credentials when prompted.<br></p>\r\n<pre class=\"language-javascript\">Email: myemail@email.com \r\nPassword: **********</pre>\r\n<p>Press enter. Once you are logged in on a machine, you don’t need to log in again. Next, create the database where the ledger will live:<br></p>\r\n<pre class=\"language-javascript\">$ fauna create-database main_ledger</pre>\r\n<p>Now, open the Fauna Shell in the new database:</p>\r\n<pre class=\"language-javascript\">$ fauna shell main_ledger</pre>\r\n<p>We’re ready to go!</p>\r\n<h2>Setup the Schema</h2>\r\n<p>Our database schema consists of a single class called <strong>ledger</strong>. This class holds a unique ledger for each client. The following example shows the schema of a ledger entry:</p>\r\n<pre class=\"language-javascript\">'{\"clientId\":50,\"counter\":10,\"type\":\"DEPOSIT\",\"description\":\r\n\"NEW DEPOSIT\", \"amount\":42.11}'</pre>\r\n<p>First, create the ledger class. All data will be stored in a single ledger class for simplicity:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; CreateClass({ name: \"ledger\" })\r\n{\r\n  \"ref\": Class(\"ledger\"),\r\n  \"ts\": 1532019955672424,\r\n  \"history_days\": 30,\r\n  \"name\": \"ledger\"\r\n}</pre>\r\n<p>Along with a unique client id is a counter, which is the unique id for just that client’s ledger entry. The combination of <code>clientId</code> + <code>counter</code> will ensure that every entry in the ledger is unique.</p>\r\n<p>Next we need to create two separate indexes. Copy and paste the following into the shell:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; CreateIndex(\r\n    {\r\n      name: \"UNIQUE_ENTRY_CONSTRAINT\",\r\n      source: Class(\"ledger\"),\r\n      terms: [{ field: [\"data\", \"clientId\"] }],\r\n      values: [{ field: [\"data\", \"counter\"] }],\r\n      unique: true,\r\n      active: true\r\n    })</pre>\r\n<p>The first index enforces a uniqueness constraint on the ledger with the term, plus values of <code>clientId</code> and <code>counter</code>. We can not add class reference to the list of values of because it would make the uniqueness constraint <code>clientId</code> + <code>counter</code> + class reference. This method would allow duplicates of entries with <code>clientId</code> + <code>counter</code>.</p>\r\n<p>Next, enter the following in the shell:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; CreateIndex(\r\n    {\r\n      name: \"ledger_client_id\",\r\n      source: Class(\"ledger\"),\r\n      terms: [{ field: [\"data\", \"clientId\"] }],\r\n      values: [{ field: [\"data\", \"counter\"], reverse:true }, { field: [\"ref\"] }],\r\n      unique: false,\r\n      serialized: true,\r\n      active: true\r\n    })</pre>\r\n<p>This index provides the lookup and reference of all the entries for a particular client sorted by the <code>counter</code> in reverse order. When sorting the ledger by <code>counter</code> in reverse order, we can easily find the last entry in the ledger.</p>\r\n<p>You can verify the indexes were created properly by running a query to find the index:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Get(Index(\"ledger_client_id\"))\r\n{\r\n  \"ref\": Index(\"ledger_client_id\"),\r\n  \"ts\": 1531245138484000,\r\n  \"active\": true,\r\n  \"partitions\": 1,\r\n  \"name\": \"ledger_client_id\",\r\n  \"source\": Class(\"ledger\"),\r\n  \"terms\": [\r\n    {\r\n      \"field\": [\r\n        \"data\",\r\n        \"clientId\"\r\n      ]\r\n    }\r\n  ],\r\n  \"values\": [\r\n    {\r\n      \"field\": [\r\n        \"data\",\r\n        \"counter\"\r\n      ],\r\n      \"reverse\": true\r\n    },\r\n    {\r\n      \"field\": [\r\n        \"ref\"\r\n      ]\r\n    }\r\n  ],\r\n  \"unique\": false,\r\n  \"serialized\": true\r\n}</pre>\r\n<h2>Adding Entries to the Ledger</h2>\r\n<p>FaunaDB queries are built using one or more nested expressions. Each expression returns an expression so that they can be nested. This example walks through how to build these nested expressions.</p>\r\n<p>Let’s assume the client is issuing a call to insert a ledger entry for <code>clientId</code> 50 and the last entry in the ledger has a <code>counter</code> of 20.</p>\r\n<p>The core expression to insert a ledger event would be the ‘create class’ expression. Data is the value of the class instance in json format:</p>\r\n<pre class=\"language-javascript\">Create(Class(\"ledger\"),\r\n             { data: \r\n{\"clientId\":50,\"counter\":21,\"type\":\"DEPOSIT\",\"description\":\r\n\"NEW DEPOSIT\", \"amount\":28.19} })</pre>\r\n<p>Add several entries to the ledger as a starting point, updating the counter for each one.</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Create(Class(\"ledger\"),\r\n               { data: {\"clientId\":50,\"counter\":0,\"type\":\r\n\"DEPOSIT\",\"description\":\"NEW DEPOSIT\", \"amount\":28.19} })\r\n{\r\n  \"ref\": Ref(Class(\"ledger\"), \"205271124881179148\"),\r\n  \"ts\": 1532020649624717,\r\n  \"data\": {\r\n    \"clientId\": 50,\r\n    \"counter\": 0,\r\n    \"type\": \"DEPOSIT\",\r\n    \"description\": \"NEW DEPOSIT\",\r\n    \"amount\": 28.19\r\n  }\r\n}</pre>\r\n<p>Notice that if we try to enter duplicate entries, the ‘create’ fails:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Create(Class(\"ledger\"),\r\n               { data: {\"clientId\":50,\"counter\":0,\"type\":\r\n\"DEPOSIT\", \"description\":\"NEW DEPOSIT\", \"amount\":28.19} })\r\nError: instance not unique</pre>\r\n<p>Since expressions return a data structure, all FaunaDB queries can be nested to select the values out of the return expressions. In this case, we want the <code>counter</code> that was saved so we can use the select to return only the <code>counter</code> from the results by using a select. Essentially, the select is saying “select the data element of the array and then, from that array, select out the <code>counter</code> value”:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Select([\"data\", \"counter\"], Create(Class(\"ledger\"),{ \r\ndata: \r\n{\"clientId\":50,\"counter\":5,\"type\":\"DEPOSIT\",\"description\":\"NEW \r\nDEPOSIT\", \"amount\":28.19} }))\r\n5</pre>\r\n<h2>Finding the latest ledger entry</h2>\r\n<p>We can build on this core expression to create a query that only inserts the ledger entry if the <code>counter</code> is one plus the last entry. That ensures the client has the latest <code>counter</code> and the events are inserted in order. </p>\r\n<p>Now, let’s create a query that gets the last <code>counter</code> value out of the index <code>ledger_client_id</code>. The first part of this query would be to read the first page of entries out of the index with paginate. This returns a nested array of indexed entries along with the reference to the class instances, like this:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Paginate(Match(Index(\"ledger_client_id\"), 50))\r\n{\r\n  \"data\": [\r\n    [\r\n      5,\r\n      Ref(Class(\"ledger\"), \"205271417149719052\")\r\n    ],\r\n    [\r\n      0,\r\n      Ref(Class(\"ledger\"), \"205271124881179148\")\r\n    ]\r\n  ]\r\n}</pre>\r\n<p>Notice the entries are returned in reverse order to the <code>counter</code> value. This is because the index was created with the flag of reverse set to true. This stores the values sorted in reverse order.</p>\r\n<p>Next, select the <code>counter</code> from the first entry out of this two-dimensional array. You can do this by referencing the entry you want in the nested array value, similar to how it is done with other programming languages. The last parameter of 0 is the default value:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Select([0,0],\r\n            Paginate(Match(Index(\"ledger_client_id\"), 50)), 0\r\n)</pre>\r\n<p>That returns the counter of the last ledger entry or 0, for example:<br></p>\r\n<pre class=\"language-javascript\">5</pre>\r\n<p>Now, add one to that value and save it in a temporary variable <code>latest</code>, which will be used later in the query. This can be done with the <code>Add</code> and <code>Let</code> expressions. <code>Let</code> binds values to variables for reference in later parts of the query:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Let(\r\n    {latest: Add(\r\n        Select([0,0],\r\n            Paginate(Match(Index(\"ledger_client_id\"), 50)),0\r\n         ),1)\r\n    },\r\n    Var(\"latest\")\r\n  )\r\n    6\r\n</pre>\r\n<p>\r\nThe second parameter to <code>Let</code> is the expression that is run after binding the variables. In this case, we simply return the variable binding. Running this query only will set the variable <code>latest</code>, and then return the variable <code>latest</code> which is 5 because 4 is the last <code>counter</code> (4+1).</p>\r\n<h2 class=\"language-javascript\">Conditional Ledger Entry Creation</h2>\r\n<p>When inserting an entry, make sure the <code>counter</code> that is being inserted is correct. This can be done by using an ‘if’ expression. A simplified example would be:</p>\r\n<pre class=\"language-javascript\">faunadb&gt;  If(Equals(20, 20),\r\n         [\"saved\", 7],\r\n         [\"not_saved\",9]\r\n         )\r\n[ 'saved', 7 ]</pre>\r\n<p>This returns a flag indicating whether the entry was saved and the <code>counter</code> that was used; or, in an error condition, the <code>counter</code> that should have been saved.</p>\r\n<pre class=\"language-javascript\">[ 'not_saved', 9 ]</pre>\r\n<p>Any value can be returned from this array and, in this case, we want to return the <code>counter</code> that was saved:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; If(\r\n        Equals(5, 5),\r\n        [\"saved\",\r\n            Select([\"data\", \"counter\"], Create(Class(\"ledger\"),\r\n              { data: {\"clientId\":50,\"counter\":5,\"type\":\r\n\"DEPOSIT\", \"description\":\"NEW DEPOSIT\", \"amount\":28.19} }))\r\n        ],\r\n        [\"not_saved\",6]\r\n      )</pre>\r\n<p>If successful, you will see:</p>\r\n<pre class=\"language-javascript\">[ 'saved', 5 ]</pre>\r\n<p>Or, if it failed:</p>\r\n<pre class=\"language-javascript\">[ 'not_saved', 6 ]</pre>\r\n<h2>Putting it all together</h2>\r\n<p>Finally, let’s pull this all together in a single query. A powerful feature of FaunaDB is that this can all be combined into a single query that is executed as a single atomic operation. What it does is get the last ledger entry <code>counter</code> and check if the <code>counter</code> we are adding is the expected value. Only then do we do the insert:</p>\r\n<pre class=\"language-javascript\">faunadb&gt; Let(\r\n    {latest: Add(\r\n        Select([0,0],\r\n            Paginate(Match(Index(\"ledger_client_id\"), 50)),0\r\n         ),1),\r\n      counter: 7\r\n    },\r\n    If(Equals(Var(\"counter\"), Var(\"latest\")),\r\n        [\"saved\",\r\n            Select([\"data\", \"counter\"], Create(Class(\"ledger\"),\r\n              { data: {\"clientId\":50,\"counter\":Var(\"counter\"),\r\n\"type\": \"DEPOSIT\",\"description\":\"NEW DEPOSIT\", \"amount\":28.19} }))\r\n        ],\r\n        [\"not_saved\",Var(\"latest\")]\r\n        )\r\n)\r\n[ 'saved', 7]</pre>\r\n<p>That is a lot to process, but hang in there and go through it a couple of times! Note that the expression <code>Var(\"latest\")</code> is used to access a variable binding. Breaking it down, here is what the major parts are doing:</p>\r\n<ol><li>The <code>Let</code> creates bindings to the variables <code>latest</code> and <code>counter</code>. The variable <code>Counter</code> in a real application would be the counter value of the ledger that is passed by the client into the query. &nbsp;</li><li>The <code>Let</code> then executes an ‘If’ query to check that <code>counter</code> and <code>latest</code> are the same value.</li><li>If <code>counter</code> and <code>latest</code> are the same value, the if statement will evaluate to true and &nbsp;create the class instance and returns the <code>counter</code>.</li><li>If <code>counter</code> and <code>latest</code> are not the same value, it returns an error.</li></ol>\r\n<p>In this example, the entry was saved, so the query returns the flag <code>saved</code> and the new <code>counter</code> value to indicate that the the ledger entry was successfully saved successfully.</p>\r\n<h2>Summary</h2>\r\n<p>Although these FaunaDB queries seem fairly complicated at first pass, they encapsulate a lot of business logic in a single transaction that would be significantly more complicated in the application tier. Most databases don’t support nested queries that allow combining reading and writing in the same transaction. Thus, performing the same business logic in the application tier would require multiple queries to the database and some type of locking to ensure that the counter is not updated after the value is read.</p>\r\n<p>In this tutorial, we have outlined the core of the event-sourcing model needed to build a complete event-sourcing application with FaunaDB. In the next tutorial, we will take these values and use them in a complete Java application. For a sneak peak <a href=\"https://github.com/retroryan/fauna-event-sourcing\">take a look at this repo</a>.</p>\r\n<p><em>Special thanks to my colleague Ben Edwards for helping out with writing and proofreading of this article.</em></p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>In the <a href=\"https://blog.fauna.com/why-strong-consistency-with-event-driven\">previous article</a>, we learned how FaunaDB provides the ideal platform for event-driven programming by providing mission critical, ACID transactions at a global scale. This article helps you get started using FaunaDB for such transactions. If you are new to FaunaDB, we’d recommend reading the <a href=\"https://fauna.com/documentation/howto/crud\">FaunaDB CRUD documentation</a> for a background on FaunaDB query language, but you don’t need it to follow along. We’ll only be covering basic FaunaDB queries from the command line. (But stay tuned for the next tutorial in this series on how to query FaunaDB via a full Java application.)</p>\n<figure><img src=\"https://storage.pardot.com/517431/117323/Blog_Image_tutorial_event_driven_2.png\" alt=\"\" /><figcaption>Image by Semantic Scholar (https://bit.ly/2JRwreU)</figcaption></figure><p>In this tutorial, we will walk through manually adding ledger entries and then querying the ledger. If you just want the raw code, it’s available at this <a href=\"https://gist.github.com/retroryan/7448032411c415330cc5fd81ff549b3b\">Event-Sourcing with FaunaDB gist</a>.</p>\n<h2>Prerequisites</h2>\n<p>If you haven’t already, <a href=\"https://fauna.com/sign-up\">sign up for a free Fauna account</a>.</p>\n<p>Then, install the <a href=\"https://www.npmjs.com/package/fauna-shell\">Fauna Shell</a>:</p>\n<pre class=\"language-javascript\">$ npm install -g fauna-shell</pre>\n<p>Once installed, tell Fauna that you want to login.<br /></p>\n<pre class=\"language-javascript\">$ fauna cloud-login\n</pre>\n<p>Enter your Fauna credentials when prompted.<br /></p>\n<pre class=\"language-javascript\">Email: myemail@email.com \nPassword: **********</pre>\n<p>Press enter. Once you are logged in on a machine, you don’t need to log in again. Next, create the database where the ledger will live:<br /></p>\n<pre class=\"language-javascript\">$ fauna create-database ‘main_ledger’</pre>\n<p>Now, open the Fauna Shell in the new database:</p>\n<pre class=\"language-javascript\">$ fauna shell main_ledger</pre>\n<p>We’re ready to go!</p>\n<h2>Setup the Schema</h2>\n<p>Our database schema consists of a single class called <strong>ledger</strong>. This class holds a unique ledger for each client. The following example shows the schema of a ledger entry:</p>\n<pre class=\"language-javascript\">'{\"clientId\":50,\"counter\":10,\"type\":\"DEPOSIT\",\"description\":\n\"NEW DEPOSIT\", \"amount\":42.11}'</pre>\n<p>First, create the ledger class. All data will be stored in a single ledger class for simplicity:</p>\n<pre class=\"language-javascript\">faunadb&gt; CreateClass({ name: \"ledger\" })\n{\n \"ref\": Class(\"ledger\"),\n \"ts\": 1532019955672424,\n \"history_days\": 30,\n \"name\": \"ledger\"\n}</pre>\n<p>Along with a unique client id is a counter, which is the unique id for just that client’s ledger entry. The combination of <code>clientId</code> + <code>counter</code> will ensure that every entry in the ledger is unique.</p>\n<p>Next we need to create two separate indexes. Copy and paste the following into the shell:</p>\n<pre class=\"language-javascript\">faunadb&gt; CreateIndex(\n {\n name: \"UNIQUE_ENTRY_CONSTRAINT\",\n source: Class(\"ledger\"),\n terms: [{ field: [\"data\", \"clientId\"] }],\n values: [{ field: [\"data\", \"counter\"] }],\n unique: true\n })</pre>\n<p>The first index enforces a uniqueness constraint on the ledger with the term, plus values of <code>clientId</code> and <code>counter</code>. We can not add class reference to the list of values of because it would make the uniqueness constraint <code>clientId</code> + <code>counter</code> + class reference. This method would allow duplicates of entries with <code>clientId</code> + <code>counter</code>.</p>\n<p>Next, enter the following in the shell:</p>\n<pre class=\"language-javascript\">faunadb&gt; CreateIndex(\n {\n name: \"ledger_client_id\",\n source: Class(\"ledger\"),\n terms: [{ field: [\"data\", \"clientId\"] }],\n values: [{ field: [\"data\", \"counter\"], reverse:true }, { field: [\"ref\"] }],\n unique: false,\n serialized: true\n })</pre>\n<p>This index provides the lookup and reference of all the entries for a particular client sorted by the <code>counter</code> in reverse order. When sorting the ledger by <code>counter</code> in reverse order, we can easily find the last entry in the ledger.</p>\n<p>You can verify the indexes were created properly by running a query to find the index:</p>\n<pre class=\"language-javascript\">faunadb&gt; Get(Index(\"ledger_client_id\"))\n{\n \"ref\": Index(\"ledger_client_id\"),\n \"ts\": 1531245138484000,\n \"active\": true,\n \"partitions\": 1,\n \"name\": \"ledger_client_id\",\n \"source\": Class(\"ledger\"),\n \"terms\": [\n {\n \"field\": [\n \"data\",\n \"clientId\"\n ]\n }\n ],\n \"values\": [\n {\n \"field\": [\n \"data\",\n \"counter\"\n ],\n \"reverse\": true\n },\n {\n \"field\": [\n \"ref\"\n ]\n }\n ],\n \"unique\": false,\n \"serialized\": true\n}</pre>\n<h2>Adding Entries to the Ledger</h2>\n<p>FaunaDB queries are built using one or more nested expressions. Each expression returns an expression so that they can be nested. This example walks through how to build these nested expressions.</p>\n<p>Let’s assume the client is issuing a call to insert a ledger entry for <code>clientId</code> 50 and the last entry in the ledger has a <code>counter</code> of 20.</p>\n<p>The core expression to insert a ledger event would be the ‘create class’ expression. Data is the value of the class instance in json format:</p>\n<pre class=\"language-javascript\">Create(Class(\"ledger\"),\n { data: \n{\"clientId\":50,\"counter\":21,\"type\":\"DEPOSIT\",\"description\":\n\"NEW DEPOSIT\", \"amount\":28.19} })</pre>\n<p>Add several entries to the ledger as a starting point, updating the counter for each one.</p>\n<pre class=\"language-javascript\">faunadb&gt; Create(Class(\"ledger\"),\n { data: {\"clientId\":50,\"counter\":0,\"type\":\n\"DEPOSIT\",\"description\":\"NEW DEPOSIT\", \"amount\":28.19} })\n{\n \"ref\": Ref(Class(\"ledger\"), \"205271124881179148\"),\n \"ts\": 1532020649624717,\n \"data\": {\n \"clientId\": 50,\n \"counter\": 0,\n \"type\": \"DEPOSIT\",\n \"description\": \"NEW DEPOSIT\",\n \"amount\": 28.19\n }\n}</pre>\n<p>Notice that if we try to enter duplicate entries, the ‘create’ fails:</p>\n<pre class=\"language-javascript\">faunadb&gt; Create(Class(\"ledger\"),\n { data: {\"clientId\":50,\"counter\":0,\"type\":\n\"DEPOSIT\", \"description\":\"NEW DEPOSIT\", \"amount\":28.19} })\nError: instance not unique</pre>\n<p>Since expressions return a data structure, all FaunaDB queries can be nested to select the values out of the return expressions. In this case, we want the <code>counter</code> that was saved so we can use the select to return only the <code>counter</code> from the results by using a select. Essentially, the select is saying “select the data element of the array and then, from that array, select out the <code>counter</code> value”:</p>\n<pre class=\"language-javascript\">faunadb&gt; Select([\"data\", \"counter\"], Create(Class(\"ledger\"),{ \ndata: \n{\"clientId\":50,\"counter\":5,\"type\":\"DEPOSIT\",\"description\":\"NEW \nDEPOSIT\", \"amount\":28.19} }))\n5</pre>\n<h2>Finding the latest ledger entry</h2>\n<p>We can build on this core expression to create a query that only inserts the ledger entry if the <code>counter</code> is one plus the last entry. That ensures the client has the latest <code>counter</code> and the events are inserted in order. </p>\n<p>Now, let’s create a query that gets the last <code>counter</code> value out of the index <code>ledger_client_id</code>. The first part of this query would be to read the first page of entries out of the index with paginate. This returns a nested array of indexed entries along with the reference to the class instances, like this:</p>\n<pre class=\"language-javascript\">faunadb&gt; Paginate(Match(Index(\"ledger_client_id\"), 50))\n{\n \"data\": [\n [\n 5,\n Ref(Class(\"ledger\"), \"205271417149719052\")\n ],\n [\n 0,\n Ref(Class(\"ledger\"), \"205271124881179148\")\n ]\n ]\n}</pre>\n<p>Notice the entries are returned in reverse order to the <code>counter</code> value. This is because the index was created with the flag of reverse set to true. This stores the values sorted in reverse order.</p>\n<p>Next, select the <code>counter</code> from the first entry out of this two-dimensional array. You can do this by referencing the entry you want in the nested array value, similar to how it is done with other programming languages. The last parameter of 0 is the default value:</p>\n<pre class=\"language-javascript\">faunadb&gt; Select([0,0],\n Paginate(Match(Index(\"ledger_client_id\"), 50)), 0\n)</pre>\n<p>That returns the counter of the last ledger entry or 0, for example:<br /></p>\n<pre class=\"language-javascript\">5</pre>\n<p>Now, add one to that value and save it in a temporary variable <code>latest</code>, which will be used later in the query. This can be done with the <code>Add</code> and <code>Let</code> expressions. <code>Let</code> binds values to variables for reference in later parts of the query:</p>\n<pre class=\"language-javascript\">faunadb&gt; Let(\n {latest: Add(\n Select([0,0],\n Paginate(Match(Index(\"ledger_client_id\"), 50)),0\n ),1)\n },\n Var(\"latest\")\n )\n 6\n</pre>\n<p>\nThe second parameter to <code>Let</code> is the expression that is run after binding the variables. In this case, we simply return the variable binding. Running this query only will set the variable <code>latest</code>, and then return the variable <code>latest</code> which is 5 because 4 is the last <code>counter</code> (4+1).</p>\n<h2 class=\"language-javascript\">Conditional Ledger Entry Creation</h2>\n<p>When inserting an entry, make sure the <code>counter</code> that is being inserted is correct. This can be done by using an ‘if’ expression. A simplified example would be:</p>\n<pre class=\"language-javascript\">faunadb&gt; If(Equals(20, 20),\n [\"saved\", 7],\n [\"not_saved\",9]\n )\n[ 'saved', 7 ]</pre>\n<p>This returns a flag indicating whether the entry was saved and the <code>counter</code> that was used; or, in an error condition, the <code>counter</code> that should have been saved.</p>\n<pre class=\"language-javascript\">[ 'not_saved', 9 ]</pre>\n<p>Any value can be returned from this array and, in this case, we want to return the <code>counter</code> that was saved:</p>\n<pre class=\"language-javascript\">faunadb&gt; If(\n Equals(5, 5),\n [\"saved\",\n Select([\"data\", \"counter\"], Create(Class(\"ledger\"),\n { data: {\"clientId\":50,\"counter\":5,\"type\":\n\"DEPOSIT\", \"description\":\"NEW DEPOSIT\", \"amount\":28.19} }))\n ],\n [\"not_saved\",6]\n )</pre>\n<p>If successful, you will see:</p>\n<pre class=\"language-javascript\">[ 'saved', 5 ]</pre>\n<p>Or, if it failed:</p>\n<pre class=\"language-javascript\">[ 'not_saved', 6 ]</pre>\n<h2>Putting it all together</h2>\n<p>Finally, let’s pull this all together in a single query. A powerful feature of FaunaDB is that this can all be combined into a single query that is executed as a single atomic operation. What it does is get the last ledger entry <code>counter</code> and check if the <code>counter</code> we are adding is the expected value. Only then do we do the insert:</p>\n<pre class=\"language-javascript\">faunadb&gt; Let(\n {latest: Add(\n Select([0,0],\n Paginate(Match(Index(\"ledger_client_id\"), 50)),0\n ),1),\n counter: 7\n },\n If(Equals(Var(\"counter\"), Var(\"latest\")),\n [\"saved\",\n Select([\"data\", \"counter\"], Create(Class(\"ledger\"),\n { data: {\"clientId\":50,\"counter\":Var(\"counter\"),\n\"type\": \"DEPOSIT\",\"description\":\"NEW DEPOSIT\", \"amount\":28.19} }))\n ],\n [\"not_saved\",Var(\"latest\")]\n )\n)\n[ 'saved', 7]</pre>\n<p>That is a lot to process, but hang in there and go through it a couple of times! Note that the expression <code>Var(\"latest\")</code> is used to access a variable binding. Breaking it down, here is what the major parts are doing:</p>\n<ol><li>The <code>Let</code> creates bindings to the variables <code>latest</code> and <code>counter</code>. The variable <code>Counter</code> in a real application would be the counter value of the ledger that is passed by the client into the query. </li><li>The <code>Let</code> then executes an ‘If’ query to check that <code>counter</code> and <code>latest</code> are the same value.</li><li>If <code>counter</code> and <code>latest</code> are the same value, the if statement will evaluate to true and create the class instance and returns the <code>counter</code>.</li><li>If <code>counter</code> and <code>latest</code> are not the same value, it returns an error.</li></ol><p>In this example, the entry was saved, so the query returns the flag <code>saved</code> and the new <code>counter</code> value to indicate that the the ledger entry was successfully saved successfully.</p>\n<h2>Summary</h2>\n<p>Although these FaunaDB queries seem fairly complicated at first pass, they encapsulate a lot of business logic in a single transaction that would be significantly more complicated in the application tier. Most databases don’t support nested queries that allow combining reading and writing in the same transaction. Thus, performing the same business logic in the application tier would require multiple queries to the database and some type of locking to ensure that the counter is not updated after the value is read.</p>\n<p>In this tutorial, we have outlined the core of the event-sourcing model needed to build a complete event-sourcing application with FaunaDB. In the next tutorial, we will take these values and use them in a complete Java application. For a sneak peak <a href=\"https://github.com/retroryan/fauna-event-sourcing\">take a look at this repo</a>.</p>\n<p><em>Special thanks to my colleague Ben Edwards for helping out with writing and proofreading of this article.</em></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "477",
        "postDate": "2018-07-11T14:58:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 530,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "c6572f61-0a7c-414a-9b50-4f0fb2f261e7",
        "siteSettingsId": 530,
        "fieldLayoutId": 4,
        "contentId": 363,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Verifying Transactional Consistency with Jepsen",
        "slug": "verifying-transactional-consistency-with-jepsen-and-faunadb",
        "uri": "blog/verifying-transactional-consistency-with-jepsen-and-faunadb",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-03-06T14:13:24-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/verifying-transactional-consistency-with-jepsen-and-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/verifying-transactional-consistency-with-jepsen-and-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>Update March 2019, see <a href=\"{entry:1677:url}\">the results of our Jepsen testing</a> here.<br></p><p>I’d like to give an update on our efforts to verify FaunaDB’s transactional correctness guarantees.</p>\r\n<p>FaunaDB is a mission critical NoSQL database. Most database technologies sound like a joke version of the \"good, fast, or cheap\" mantra: usable, scalable, or correct, choose two, but more often just choose one, and sometimes you get zero.&nbsp;Instead, FaunaDB endeavors to be all three at once.&nbsp;Today we will talk about becoming correct.</p>\r\n<figure><a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\"><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qsx/517431/116371/Fauna_Correctness_Report.png?raw=true\" alt=\"FaunaDB Correctness Report\" title=\"FaunaDB Correctness Report\" width=\"50%\" data-image=\"efas790rl6qc\"></a></figure>\r\n<p>We've worked hard on the transaction engine (a lockless, single-phase distributed preprocessor <a href=\"{entry:82:url}\">inspired by Calvin</a>), but we've worked just as hard on the testing infrastructure. As other NoSQL systems like Cassandra and MongoDB found out, it's a lot easier to aspire to operational integrity or scalability&nbsp;than actually have it.&nbsp;</p>\r\n<p>\r\n</p>\r\n<h2>Testing at Fauna</h2>\r\n<p>As part of our engineering process at Fauna, we have built a comprehensive suite of unit and property-based tests and as well as a sophisticated distributed testing framework that is capable of checking the behavior of FaunaDB in the presence of a wide combination of fault injections and operational changes, similar to Netflix's <a href=\"https://en.wikipedia.org/wiki/Chaos_Monkey\">Chaos Monkey</a>.<br></p>\r\n<p>\r\n</p>\r\n<p>Kyle Kingsbury’s&nbsp;<a href=\"http://jepsen.io/\">Jepsen</a>&nbsp;has quickly earned a reputation as the industry standard for distributed systems testing.&nbsp;We have now implemented and verified two tests in Jepsen:</p>\r\n<ol><li>A register test, which checks <strong>linearizability over individual records</strong>.</li><li>A ledger&nbsp;test, which&nbsp;checks <strong>serializability of transactions involving multiple records.&nbsp;</strong></li></ol>\r\n<p>FaunaDB passes these tests.</p>\r\n<h2>A Different Kind of Halting Problem</h2>\r\n<p>Jepsen started with a simple question: <a href=\"https://www.youtube.com/watch?v=fWNaR-rxAic\">what if the guy mowing the lawn never calls?</a>&nbsp;Or more technically speaking: <a href=\"https://www.youtube.com/watch?v=eSaFVX4izsQ\">what happens to production systems when networks fail?</a></p>\r\n<figure><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qvl/517431/116373/Tandem_NonStop.png\" width=\"50%\" data-image=\"3ij9wq2hscy7\"></figure>\r\n<p>Up until this point, correctness in most distributed systems was purely aspirational, potentially excepting Tandem NonStop. It turned out that essentially no systems did what they said on the tin, due to both&nbsp;implementation defects and because they made claims that physics and information science unfortunately do not allow.</p>\r\n<blockquote>Essentially no systems did what they said on the tin, because they made claims that physics unfortunately does not allow.</blockquote>\r\n<p>Jepsen has since become capable of simulating a much larger variety of machine and network failure scenarios, and of checking a system’s more formal isolation characteristics, such as its ability to maintain sequential consistency, snapshot isolation, linearizability, and liveness in the presence of these failures.</p>\r\n<p><a href=\"https://people.mpi-sws.org/~fniksic/popl2018/paper.pdf\">Research</a> <a href=\"https://dl.acm.org/citation.cfm?id=1831736\">has shown</a> that the randomized testing methodology that Jepsen represents is very effective at exposing faults related to partition tolerance. We can have a high degree of confidence in Fauna's distributed&nbsp;correctness if it successfully passes a correctly-implemented set of tests based on randomized failure simulation.&nbsp;</p>\r\n<p>Future ways for us increase our correctness confidence include formal specification and verification like <a href=\"https://lamport.azurewebsites.net/tla/tla.html\">TLA+</a>, dependent typing, etc.</p>\r\n<h2>Read the Results</h2>\r\n<p>Available now is a report of our&nbsp;<a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\">internal Jepsen results</a>&nbsp;written by our own engineering team. So far, we have found no significant flaws in our implementation. FaunaDB is capable of preserving strong ACID guarantees in the face of concurrent operations, cluster reconfigurations, and network partitions. And as a CP system, FaunaDB maintains&nbsp;liveness as long as a majority of the cluster remains available.</p>\r\n<blockquote>FaunaDB is capable of preserving strong ACID guarantees in the face of concurrent operations, cluster reconfigurations, and network partitions.</blockquote>\r\n<p>Stay tuned for an independent analysis and&nbsp;an open-source release of the Fauna Jepsen tests in the future.&nbsp;In the meantime,&nbsp;<a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\">have a look at the report</a>.</p>\r\n<figure data-gramm_id=\"4329c629-0ec2-c67f-79e9-eb11046c2766\" data-gramm=\"true\" spellcheck=\"false\" data-gramm_editor=\"true\"><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qw2/517431/116375/Call_me_Maybe.png\" width=\"50%\" style=\"background-color: initial;\" data-image=\"u9dr5kkai5x8\"></figure>\r\n<p><grammarly-btn></grammarly-btn></p>\r\n<div class=\"_1BN1N Kzi1t _2DJZN\" style=\"z-index: 2; transform: translate(869.805px, 2396.7px);\"><div class=\"_1HjH7\"><div title=\"Protected by Grammarly\" class=\"_3qe6h\">&nbsp;</div></div></div>\r\n<p></p>\r\n<p><em>Thanks to Brandon Mitchell, Nathan Taylor, Jeff Smick, Attila Szegedi, and the whole Fauna engineering team for their hard work on the Jepsen analysis.&nbsp;</em><em>Evan Weaver and&nbsp;Dhruv Gupta contributed to this post.&nbsp;</em></p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "1211"
        ],
        "bodyText": "<p>I’d like to give an update on our efforts to verify FaunaDB’s transactional correctness guarantees.</p>\n<p>FaunaDB is a mission critical NoSQL database. Most database technologies sound like a joke version of the \"good, fast, or cheap\" mantra: usable, scalable, or correct, choose two, but more often just choose one, and sometimes you get zero. Instead, FaunaDB endeavors to be all three at once. Today we will talk about becoming correct.</p>\n<figure><a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\"><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qsx/517431/116371/Fauna_Correctness_Report.png?raw=true\" alt=\"FaunaDB Correctness Report\" title=\"FaunaDB Correctness Report\" /></a></figure><p>We've worked hard on the transaction engine (a lockless, single-phase distributed preprocessor <a href=\"{entry:82:url||%7Bentry%3A82%3Aurl%7D}\">inspired by Calvin</a>), but we've worked just as hard on the testing infrastructure. As other NoSQL systems like Cassandra and MongoDB found out, it's a lot easier to aspire to operational integrity or scalability than actually have it. </p>\n<p>\n</p>\n<h2>Testing at Fauna</h2>\n<p>As part of our engineering process at Fauna, we have built a comprehensive suite of unit and property-based tests and as well as a sophisticated distributed testing framework that is capable of checking the behavior of FaunaDB in the presence of a wide combination of fault injections and operational changes, similar to Netflix's <a href=\"https://en.wikipedia.org/wiki/Chaos_Monkey\">Chaos Monkey</a>.<br /></p>\n<p>\n</p>\n<p>Kyle Kingsbury’s <a href=\"http://jepsen.io/\">Jepsen</a> has quickly earned a reputation as the industry standard for distributed systems testing. We have now implemented and verified two tests in Jepsen:</p>\n<ol><li>A register test, which checks <strong>linearizability over individual records</strong>.</li><li>A ledger test, which checks <strong>serializability of transactions involving\nmultiple records. </strong></li></ol><p>FaunaDB passes these tests.</p>\n<h2>A Different Kind of Halting Problem</h2>\n<p>Jepsen started with a simple question: <a href=\"https://www.youtube.com/watch?v=fWNaR-rxAic\">what if the guy mowing the lawn never calls?</a> Or more technically speaking: <a href=\"https://www.youtube.com/watch?v=eSaFVX4izsQ\">what happens to production systems when networks fail?</a></p>\n<figure><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qvl/517431/116373/Tandem_NonStop.png\" alt=\"\" /></figure><p>Up until this point, correctness in most distributed systems was purely aspirational, potentially excepting Tandem NonStop. It turned out that essentially no systems did what they said on the tin, due to both implementation defects and because they made claims that physics and information science unfortunately do not allow.</p>\n<blockquote>Essentially no systems did what they said on the tin, because they made claims that physics unfortunately does not allow.</blockquote>\n<p>Jepsen has since become capable of simulating a much larger variety of machine and network failure scenarios, and of checking a system’s more formal isolation characteristics, such as its ability to maintain sequential consistency, snapshot isolation, linearizability, and liveness in the presence of these failures.</p>\n<p><a href=\"https://people.mpi-sws.org/~fniksic/popl2018/paper.pdf\">Research</a> <a href=\"https://dl.acm.org/citation.cfm?id=1831736\">has shown</a> that the randomized testing methodology that Jepsen represents is very effective at exposing faults related to partition tolerance. We can have a high degree of confidence in Fauna's distributed correctness if it successfully passes a correctly-implemented set of tests based on randomized failure simulation. </p>\n<p>Future ways for us increase our correctness confidence include formal specification and verification like <a href=\"https://lamport.azurewebsites.net/tla/tla.html\">TLA+</a>, dependent typing, etc.</p>\n<h2>Read the Results</h2>\n<p>Available now is a report of our <a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\">internal Jepsen results</a> written by our own engineering team. So far, we have found no significant flaws in our implementation. FaunaDB is capable of preserving strong ACID guarantees in the face of concurrent operations, cluster reconfigurations, and network partitions. And as a CP system, FaunaDB maintains liveness as long as a majority of the cluster remains available.</p>\n<blockquote>FaunaDB is capable of preserving strong ACID guarantees in the face of concurrent operations, cluster reconfigurations, and network partitions.</blockquote>\n<p>Stay tuned for an independent analysis and an open-source release of the Fauna Jepsen tests in the future. In the meantime, <a href=\"https://www2.fauna.com/l/517431/2018-06-25/6c9gwb/517431/114459/FaunaDB_Correctness_Report_042618.pdf\">have a look at the report</a>.</p>\n<figure><img src=\"https://www2.fauna.com/l/517431/2018-07-13/6d1qw2/517431/116375/Call_me_Maybe.png\" alt=\"\" /></figure><p><em>Thanks to Brandon Mitchell, Nathan Taylor, Jeff Smick, Attila Szegedi, and the whole Fauna engineering team for their hard work on the Jepsen analysis. </em><em>Evan Weaver and Dhruv Gupta contributed to this post. </em></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "473",
        "postDate": "2018-07-03T19:37:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 529,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "5b6c3b6f-c285-47f5-8226-880bc85e442f",
        "siteSettingsId": 529,
        "fieldLayoutId": 4,
        "contentId": 362,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Can MongoDB Really Deliver ACID?",
        "slug": "can-mongodb-really-deliver-acid",
        "uri": "blog/can-mongodb-really-deliver-acid",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/can-mongodb-really-deliver-acid",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/can-mongodb-really-deliver-acid",
        "isCommunityPost": false,
        "blogBodyText": "<p>MongoDB has become a very popular database. To be quite honest, I hadn’t noticed MongoDB’s rise in popularity over the past few years. This is largely due to the fact that I had been on the transactional side of data management and MongoDB was clearly on the analytical/reporting side. MongoDB has historically been a NoSQL document store that favored availability over consistency on the CAP spectrum and unable to do transactions. I had thought their inability to do transactions was a good thing since there have been <a href=\"https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads\">reports</a> of MongoDB losing data and that they were recently the target of a massive <a href=\"https://www.theregister.co.uk/2017/01/09/mongodb/\">ransomware attack</a>. Despite that, they have found a way to attract many users over the last few years and seem to strongly appeal to some segments of the developer community.<br></p>\r\n<p>Furthering my interests in transactional systems, I recently joined Fauna. Fauna offers what might be best described as a “NoSQL v2.0” database. FaunaDB was created by some very smart people out of Twitter (not that I’m biased), who set out to design the type of database they wished they had in order to properly handle the massive amounts of mission critical data at Twitter. They wanted to create an operational, <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">fully transactional database</a> that could scale and perform like NoSQL systems, with the reliability and data correctness of traditional SQL/relational systems. Some of the core design principles for FaunaDB included distributed ACID transactions (with the strictest level of isolation), high security, multi-tenancy, multi-model interface (relational, graph, etc.), horizontal scalability, high availability, temporality, and operational simplicity.</p>\r\n<blockquote>Fauna was designed from the ground up to be able to do distributed transactions, while MongoDB added that capability to their “legacy” database.</blockquote>\r\n<p>As I began reading more about the current offerings of NoSQL solution providers, I was surprised to hear that MongoDB was planning to announce the ability to handle ACID transactions in their newest release. Fauna was designed from the ground up to be able to do distributed transactions while MongoDB added that capability to their “legacy” database. That really piqued my interest.</p>\r\n<p>Conveniently, soon after I started at Fauna, MongoDB hosted their big user event, MongoDB World, in New York City. I was able to attend the keynote and a few sessions that described some of the details of MongoDB’s new transactional capability. During his <a href=\"https://www.mongodb.com/presentations/mongodb-world18--eliot-horowitz-keynote\">keynote session</a>, Eliot Horowitz, MongoDB’s CTO and Cofounder, formally announced that MongoDB v4.0 supports multi-document ACID transactions.</p>\r\n<p>“That’s the first major point about transactions in MongoDB...they are incredibly familiar...they are just like transactions in any traditional relational database you’ve ever used. They’re not some weird variant on transactions, they’re not some cobbled-together thing to make it sound like we have transactions”, Eliot said.</p>\r\n<blockquote>There are different levels of ACID, particularly in <strong>isolation</strong>, so saying you are ACID is actually just the starting point.</blockquote>\r\n<p>So far so good. It’s important to note that the term “transactions” has certain implications for developers and database vendors. When you say “transactions”, you imply conforming to the ACID principles (Atomicity, Consistency, Isolation, and Durability). There are different levels of ACID, particularly in <strong>isolation</strong>, so saying you are ACID is actually just the starting point.<br></p>\r\n<p>During this announcement, MongoDB executives did acknowledge that there are a few restrictions for the first implementation of transactions. In V4.0, MongoDB’s transactions will be limited to single-shards, though they announced that V4.2 will include the ability to do transactions across multi-shard implementations. Later, while exploring MongoDB’s documentation, I found another restriction: while MongoDB has supported multiple storage engines for data in the past, only WiredTiger (one of MongoDB’s storage engines) could be used if transactions were desired.</p>\r\n<blockquote>Transactions, true transactions (not the odd variants), are very difficult to do correctly.</blockquote>\r\n<p>When I think about transactions, I think of it in the context of distributed systems. Transactions, true transactions (not the odd variants), are very difficult to do correctly. Databases are constantly being barraged by reads and writes from many users. When running on a single machine, managing all activities within all the transactions that are running at the same time is difficult; doing it on a distributed database running across a large number of nodes becomes a real tricky proposition. It seems like MongoDB has come to realize that, which is why they are being careful about limiting the initial scope for transactions to single shards only. </p>\r\n<p>MongoDB bought a little headstart in their efforts for implementing transactions. They inherited some of their underlying, enabling technology for performing transactions through their acquisition of WiredTiger and by adding WiredTiger as their most recent storage engine. It makes sense that the only way to support transactions in MongoDB was to force the use of WiredTiger as the storage engine.</p>\r\n<p>“My general belief is that most users, above 50%, are going to use transactions, but for a very small percentage of operations, less than 1% of applications.”</p>\r\n<blockquote> If the CTO believes that MongoDB’s transactional capability will be used so little, why go through all the trouble of adding transactions?</blockquote>\r\n<p>That is very interesting to me. If the CTO believes that MongoDB’s transactional capability will be used so little, why go through all the trouble of adding transactions?<br></p>\r\n<p>During a breakout session I attended, I heard some rather puzzling things having to do with how MongoDB uses clocks and consensus protocols in their new transactionality. I will need to confirm it all through MongoDB’s documentation but what I heard those senior level engineering leaders say confirmed my belief that adding transactions into a reporting/analytics platform is extremely difficult and it may drive you to do some bizarre things within your product’s core.</p>\r\n<p>I’ll get into some of the details of those in my next blog post. In the meantime, for those of you using MongoDB, I would love to hear from you.</p>\r\n<ul><li>What do you like about MongoDB?</li><li>What don’t you like about it?</li><li>Are you thinking of using MongoDB’s new transactional capability? Why or why not?</li><li>Were there any other announcements out of MongoDB World 2018 that were of particular interest to you?</li></ul>\r\n<p>If you are looking for a database with the high performance and scalability of NoSQL but you aren’t ready to (or just can’t) give up data accuracy/consistency and system reliability, make sure you <a href=\"https://app.fauna.com/sign-up\">check out Fauna</a>. Our database was built from the ground up for mission critical transactional applications. We know how difficult transactions are to do correctly, which is why we started with transactions -- we didn’t try to bolt them onto an older system. FaunaDB offers strict ACID compliance and high performance and global scalability -- all on a highly reliable platform with powerful multi-model query capabilities, easy programming and simple operations. And FaunaDB Cloud is perfect for use with modern serverless applications.<br></p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>MongoDB has become a very popular database. To be quite honest, I hadn’t noticed MongoDB’s rise in popularity over the past few years. This is largely due to the fact that I had been on the transactional side of data management and MongoDB was clearly on the analytical/reporting side. MongoDB has historically been a NoSQL document store that favored availability over consistency on the CAP spectrum and unable to do transactions. I had thought their inability to do transactions was a good thing since there have been <a href=\"https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads\">reports</a> of MongoDB losing data and that they were recently the target of a massive <a href=\"https://www.theregister.co.uk/2017/01/09/mongodb/\">ransomware attack</a>. Despite that, they have found a way to attract many users over the last few years and seem to strongly appeal to some segments of the developer community.<br /></p>\n<p>Furthering my interests in transactional systems, I recently joined Fauna. Fauna offers what might be best described as a “NoSQL v2.0” database. FaunaDB was created by some very smart people out of Twitter (not that I’m biased), who set out to design the type of database they wished they had in order to properly handle the massive amounts of mission critical data at Twitter. They wanted to create an operational, <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">fully transactional database</a> that could scale and perform like NoSQL systems, with the reliability and data correctness of traditional SQL/relational systems. Some of the core design principles for FaunaDB included distributed ACID transactions (with the strictest level of isolation), high security, multi-tenancy, multi-model interface (relational, graph, etc.), horizontal scalability, high availability, temporality, and operational simplicity.</p>\n<blockquote>Fauna was designed from the ground up to be able to do distributed transactions, while MongoDB added that capability to their “legacy” database.</blockquote>\n<p>As I began reading more about the current offerings of NoSQL solution providers, I was surprised to hear that MongoDB was planning to announce the ability to handle ACID transactions in their newest release. Fauna was designed from the ground up to be able to do distributed transactions while MongoDB added that capability to their “legacy” database. That really piqued my interest.</p>\n<p>Conveniently, soon after I started at Fauna, MongoDB hosted their big user event, MongoDB World, in New York City. I was able to attend the keynote and a few sessions that described some of the details of MongoDB’s new transactional capability. During his <a href=\"https://www.mongodb.com/presentations/mongodb-world18--eliot-horowitz-keynote\">keynote session</a>, Eliot Horowitz, MongoDB’s CTO and Cofounder, formally announced that MongoDB v4.0 supports multi-document ACID transactions.</p>\n<p>“That’s the first major point about transactions in MongoDB...they are incredibly familiar...they are just like transactions in any traditional relational database you’ve ever used. They’re not some weird variant on transactions, they’re not some cobbled-together thing to make it sound like we have transactions”, Eliot said.</p>\n<blockquote>There are different levels of ACID, particularly in <strong>isolation</strong>, so saying you are ACID is actually just the starting point.</blockquote>\n<p>So far so good. It’s important to note that the term “transactions” has certain implications for developers and database vendors. When you say “transactions”, you imply conforming to the ACID principles (Atomicity, Consistency, Isolation, and Durability). There are different levels of ACID, particularly in <strong>isolation</strong>, so saying you are ACID is actually just the starting point.<br /></p>\n<p>During this announcement, MongoDB executives did acknowledge that there are a few restrictions for the first implementation of transactions. In V4.0, MongoDB’s transactions will be limited to single-shards, though they announced that V4.2 will include the ability to do transactions across multi-shard implementations. Later, while exploring MongoDB’s documentation, I found another restriction: while MongoDB has supported multiple storage engines for data in the past, only WiredTiger (one of MongoDB’s storage engines) could be used if transactions were desired.</p>\n<blockquote>Transactions, true transactions (not the odd variants), are very difficult to do correctly.</blockquote>\n<p>When I think about transactions, I think of it in the context of distributed systems. Transactions, true transactions (not the odd variants), are very difficult to do correctly. Databases are constantly being barraged by reads and writes from many users. When running on a single machine, managing all activities within all the transactions that are running at the same time is difficult; doing it on a distributed database running across a large number of nodes becomes a real tricky proposition. It seems like MongoDB has come to realize that, which is why they are being careful about limiting the initial scope for transactions to single shards only. </p>\n<p>MongoDB bought a little headstart in their efforts for implementing transactions. They inherited some of their underlying, enabling technology for performing transactions through their acquisition of WiredTiger and by adding WiredTiger as their most recent storage engine. It makes sense that the only way to support transactions in MongoDB was to force the use of WiredTiger as the storage engine.</p>\n<p>“My general belief is that most users, above 50%, are going to use transactions, but for a very small percentage of operations, less than 1% of applications.”</p>\n<blockquote> If the CTO believes that MongoDB’s transactional capability will be used so little, why go through all the trouble of adding transactions?</blockquote>\n<p>That is very interesting to me. If the CTO believes that MongoDB’s transactional capability will be used so little, why go through all the trouble of adding transactions?<br /></p>\n<p>During a breakout session I attended, I heard some rather puzzling things having to do with how MongoDB uses clocks and consensus protocols in their new transactionality. I will need to confirm it all through MongoDB’s documentation but what I heard those senior level engineering leaders say confirmed my belief that adding transactions into a reporting/analytics platform is extremely difficult and it may drive you to do some bizarre things within your product’s core.</p>\n<p>I’ll get into some of the details of those in my next blog post. In the meantime, for those of you using MongoDB, I would love to hear from you.</p>\n<ul><li>What do you like about MongoDB?</li><li>What don’t you like about it?</li><li>Are you thinking of using MongoDB’s new transactional capability? Why or why not?</li><li>Were there any other announcements out of MongoDB World 2018 that were of particular interest to you?</li></ul><p>If you are looking for a database with the high performance and scalability of NoSQL but you aren’t ready to (or just can’t) give up data accuracy/consistency and system reliability, make sure you <a href=\"https://app.fauna.com/sign-up\">check out Fauna</a>. Our database was built from the ground up for mission critical transactional applications. We know how difficult transactions are to do correctly, which is why we started with transactions -- we didn’t try to bolt them onto an older system. FaunaDB offers strict ACID compliance and high performance and global scalability -- all on a highly reliable platform with powerful multi-model query capabilities, easy programming and simple operations. And FaunaDB Cloud is perfect for use with modern serverless applications.<br /></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "479",
        "postDate": "2018-06-27T22:47:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 528,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "bd414791-ba52-48eb-820d-d4ae22b9bacc",
        "siteSettingsId": 528,
        "fieldLayoutId": 4,
        "contentId": 361,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Why Strong Consistency Matters with Event-driven Architectures",
        "slug": "why-strong-consistency-with-event-driven",
        "uri": "blog/why-strong-consistency-with-event-driven",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/why-strong-consistency-with-event-driven",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/why-strong-consistency-with-event-driven",
        "isCommunityPost": false,
        "blogBodyText": "<p>Event-driven architectures are widely adopted patterns that model an application as a series of software components or services. Those services react to commands that represent business and/or user actions and result in events. Event-driven architectures help modern applications solve the challenges of scalability, auditability / traceability and immutability. These architectures become especially powerful if you can partition your data into smaller subsets. This allows event processing to scale horizontally in a large distributed architecture, like microservices.</p>\r\n<p>However, event-driven architectures are not a complete panacea. A critical aspect of an event-driven architecture is the causal relationship between datasets; processing the current event often depends on states derived from previous events. This requires an exact ordering of events. In a distributed system, ordering of events becomes a severe challenge because of what has become known as the “<a href=\"https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\">fallacies of distributed computing</a>” where data is obliged to flow over an unreliable network that introduces some level of latency. This leads to the pattern commonly known as eventual consistency, which pushes the complexity of consistency to the application tier. &nbsp;</p>\r\n<p>This becomes especially difficult with the current generation of NoSQL databases that provide global scale by throwing out the features that made prior database systems so attractive, such as flexibility in how the data was modeled and most importantly the ability to enforce the integrity of the application using database transactions. Maintaining these constraints is termed <em>consistency</em> - one of the foundations of ACID transactions featured in most traditional RDBMS. The current generation of NoSQL databases either introduces windows of inconsistency at best (eventual consistency), which must be reconciled (not always feasible), or introduces complex cluster orchestration to partition the stream of events, which must be maintained across all services that process the stream. With consistency often missing in modern NoSQL databases, achieving it can be a major challenge of modern scale-out, event-driven architectures. It’s also important to note that modern NoSQL databases do not keep a history of data revisions and have no way to report on these changes, making it difficult to offer auditability. &nbsp;</p>\r\n<h2>Different Patterns of Event-Driven Architecture</h2>\r\n<p>Martin Fowler clarifies some of the common event-driven patterns in his article “<a href=\"https://martinfowler.com/articles/201701-event-driven.html\">What do you mean by “Event-Driven”?</a>” The two primary patterns we will discuss in this article are event sourcing and Command Query Responsibility Segregation (CQRS).</p>\r\n<p>Event sourcing captures changes to the state of a system as a series of events. The event store then becomes the primary source of truth, and the current state of the system is derived from this event store. More details on how the pattern is defined can be found in the domain-driven design literature (Implementing Domain-Driven Design), or online (<a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing\">Microsoft</a>, <a href=\"https://martinfowler.com/eaaDev/EventSourcing.html\">Martin Fowler</a>, <a href=\"https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson\">InfoQ</a>).</p>\r\n<p>The other common pattern is CQRS, which is the notion of having separate data structures for reading and writing information, allowing for different views or projections of event streams.</p>\r\n<h2>[Example] Modeling a Hotel Points Program as Events</h2>\r\n<p>A hotel points program can be designed similar to the ledger model used in accounting. In this model, every credit/debit transaction is written as an immutable ledger of events. If a mistake is made, a correcting transaction would need to be made, similar to how accountants never change existing entries.</p>\r\n<p>Using the patterns of event sourcing, we can architect a points program to process the commands (such as a hotel stay) and record them as immutable events in the ledger. For example, when a user earns points, the event is stored as a ledger entry of the event type and the number of points earned. In this example the entry would be (“hotel stay”, 32 points). This provides a full audit log of the events that happened, and a custom view of the ledger is created to provide a current account balance.</p>\r\n<p>Such a simple model might seem like it should easily scale on even the most simplistic key-value storage. Unfortunately, this is not the case. For instance, to ensure that a customer never claims entitlements worth more than their points balance, our model requires snapshot isolation or a primitive compare-and-swap-style operation. If we want to allow customers to transfer balances between accounts, we again need a better isolation level. To do this in a scale-out fashion, without baking our concurrency model into the application, our choices for database backends shrink very rapidly. Finally, the choice of databases narrows even further when we need to provide different views of the data. This requires secondary indices that allow querying the data with access patterns that differ from how the data is stored. </p>\r\n<h2>Globally Distributed ACID Transactions in FaunaDB</h2>\r\n<p>The most unique capability of FaunaDB is that it provides ACID-compliant, consistent transactions in a partitioned, globally distributed environment.&nbsp;</p>\r\n<blockquote>When it comes to database technology, traditional thinking has held that it’s impossible to provide both global scale and strong consistency. This ideology has led to the pervasive misconception that the only way to achieve global scale is to sacrifice strong consistency and transactions.&nbsp;</blockquote>\r\n<p>FaunaDB upends this thinking by bringing an entirely new approach to database transaction resolution by providing a globally distributed ACID transaction engine. It has been developed over the past several years by technical leaders with experience at <a href=\"https://blog.fauna.com/video-evan-and-matts-vision-for-a-mission-critical-operational-database\">Twitter</a> and <a href=\"https://blog.fauna.com/author/chrisanderson/\">Couchbase</a>.</p>\r\n<p>The use of globally distributed ACID transactions in FaunaDB allows enforcement of complex business constraints (by enforcing consistency) and linearization of event processing. Offloading this concern to the database frees the developer to focus on application concerns and reduces the complexity of the application code.</p>\r\n<h3>Order of Events in FaunaDB</h3>\r\n<p>Enforcing the complex business constraints discussed earlier often requires the events be captured in an exact order. For example, the bids on an auction, or the trades on the stock exchange, need to be captured in an exact linear history. This linear history relies on a database that offers a stronger guarantee than eventual consistency.</p>\r\n<blockquote>FaunaDB, with ACID transactions, allows events to be captured while preserving the linear order of their history.</blockquote>\r\n<h3>Temporality in FaunaDB</h3>\r\n<p>FaunaDB keeps all instances of a dataset. Instead of overwriting, it creates new ones when a write is performed. This is especially useful when auditing event data and verifying its evolution over time. In an event-driven architecture, this would allow you to view the state of the world and all related events at any point of time. </p>\r\n<h3>Indexes in FaunaDB</h3>\r\n<p>FaunaDB stores data in the form of flexible objects, where each object is an instance of a class. To allow for different access patterns, FaunaDB also allows multiples indexes of these classes so that they can be searched and retrieved by different attributes. Also, an index can contain references to multiple classes. The other unique aspect of FaunaDB is that updates to the index are atomic. This ensures that the index itself also maintains strong consistency alongside the core data and can enforce constraints like uniqueness. &nbsp;</p>\r\n<p>For event-driven architectures, this enables different views or projections of the core data, as well as the CQRS (Command Query Responsibility Segregation) pattern. Also, similar to a relational database, the index allows unique constraints to be put on the events. This enables the ability to provide constraints such as ensuring only one copy of an event is stored in the database.</p>\r\n<h2>Capturing the Current State of the World</h2>\r\n<p>Event sourcing stores the entire history of a system as a stream of events. This could include banking transactions, auction history, etc. For many businesses, this ledger of events is extremely important in understanding what happened. In addition to this ledger of events, businesses also need to capture the current state of the world. Some examples include an account balance, the owner of a legal contract, the winning bid, etc. This “mutable state” can be calculated in multiple ways depending on the business requirements for how real-time the value needs to be. </p>\r\n<p>If this “mutable state” can have a slight delay, then event sourcing only needs to ensure consistency on the write level. Then, snapshots and/or roll-ups using an eventual consistency model can be used for calculating projections out of the streams of events. Different consumers of events can be used to create read models, syndicate data, and prepare batch jobs that are eventually consistent. Calculating these different projections could take several seconds and could be handled later, as the event write itself only cares about making sure the new events were calculated based on the latest available state.</p>\r\n<p>However, if this “mutable state” needs to be real-time, it can be very difficult to calculate in an eventual consistency model since it’s often calculated after the fact with snapshots and roll-ups. Instead, an exact value can be created using Fauna ACID transactions to capture the exact state of the world at all times. This would allow for real-time data and business logic, such as determining if a customer has enough funds available for a purchase.</p>\r\n<h2>Comparison to Other Systems</h2>\r\n<p>FaunaDB is not the only database out there that offers global transactional consistency. Two other databases including Spanner (Google’s database-as-a-service) and CockroachDB (the open source stepchild of Spanner) provide similar functionality, but in a radically different way. Both are based on a protocol known as “Spanner’. In contrast, FaunaDB is based on a very different, patent-pending protocol. The distinction among these three databases boils down to a comparison between the different transactional protocols. <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">Read more about the distinction between our protocol and Spanner’s here.</a></p>\r\n<p>Beyond providing a fundamentally different transactional model, FaunaDB also delivers operational advantages. Baked into the architecture of FaunaDB is a multi-tenant, quality-of-service mechanism. Each workload that runs against FaunaDB has an associated priority. While other databases provide static multi-tenancy with fixed resources for each tenant, FaunaDB’s model is completely dynamic. An entire FaunaDB data infrastructure, with multiple global data centers, can function as a single set of resources such as I/O, compute, and storage. While you won’t notice this when you are coding your first application against FaunaDB, it is extremely beneficial when you need your database to run as the core platform for multiple applications and datasets. The best part is that it completely decouples application development from operational activities. Operations just need to ensure that there is sufficient capacity for all the various applications and data, and can shift the infrastructure topology however needed. </p>\r\n<h2>How Fauna Solves these Challenges</h2>\r\n<p>FaunaDB solves the major challenges of building an event-driven architecture by providing mission critical ACID transactions at a global scale. By using FaunaDB as the foundation of the application architecture, the complexity of maintaining strong consistency and ordering of events is solved by the database. This greatly simplifies the overall application architecture. </p>\r\n<p>In the next article, we will dive into an example of building an event-driven application, starting with a look at the core Fauna queries and features that form the building blocks. </p>\r\n<p>Thanks to Ben Edwards for helping out with the writing and proofreading of the article!</p>\r\n<p>Interested in diving in? <a href=\"https://fauna.com/sign-up\">Create a free account</a> or learn more by visiting our <a href=\"https://fauna.com/documentation/howto/crud\">documentation page</a>!</p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Event-driven architectures are widely adopted patterns that model an application as a series of software components or services. Those services react to commands that represent business and/or user actions and result in events. Event-driven architectures help modern applications solve the challenges of scalability, auditability / traceability and immutability. These architectures become especially powerful if you can partition your data into smaller subsets. This allows event processing to scale horizontally in a large distributed architecture, like microservices.</p>\n<p>However, event-driven architectures are not a complete panacea. A critical aspect of an event-driven architecture is the causal relationship between datasets; processing the current event often depends on states derived from previous events. This requires an exact ordering of events. In a distributed system, ordering of events becomes a severe challenge because of what has become known as the “<a href=\"https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\">fallacies of distributed computing</a>” where data is obliged to flow over an unreliable network that introduces some level of latency. This leads to the pattern commonly known as eventual consistency, which pushes the complexity of consistency to the application tier. </p>\n<p>This becomes especially difficult with the current generation of NoSQL databases that provide global scale by throwing out the features that made prior database systems so attractive, such as flexibility in how the data was modeled and most importantly the ability to enforce the integrity of the application using database transactions. Maintaining these constraints is termed <em>consistency</em> - one of the foundations of ACID transactions featured in most traditional RDBMS. The current generation of NoSQL databases either introduces windows of inconsistency at best (eventual consistency), which must be reconciled (not always feasible), or introduces complex cluster orchestration to partition the stream of events, which must be maintained across all services that process the stream. With consistency often missing in modern NoSQL databases, achieving it can be a major challenge of modern scale-out, event-driven architectures. It’s also important to note that modern NoSQL databases do not keep a history of data revisions and have no way to report on these changes, making it difficult to offer auditability. </p>\n<h2>Different Patterns of Event-Driven Architecture</h2>\n<p>Martin Fowler clarifies some of the common event-driven patterns in his article “<a href=\"https://martinfowler.com/articles/201701-event-driven.html\">What do you mean by “Event-Driven”?</a>” The two primary patterns we will discuss in this article are event sourcing and Command Query Responsibility Segregation (CQRS).</p>\n<p>Event sourcing captures changes to the state of a system as a series of events. The event store then becomes the primary source of truth, and the current state of the system is derived from this event store. More details on how the pattern is defined can be found in the domain-driven design literature (Implementing Domain-Driven Design), or online (<a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing\">Microsoft</a>, <a href=\"https://martinfowler.com/eaaDev/EventSourcing.html\">Martin Fowler</a>, <a href=\"https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson\">InfoQ</a>).</p>\n<p>The other common pattern is CQRS, which is the notion of having separate data structures for reading and writing information, allowing for different views or projections of event streams.</p>\n<h2>[Example] Modeling a Hotel Points Program as Events</h2>\n<p>A hotel points program can be designed similar to the ledger model used in accounting. In this model, every credit/debit transaction is written as an immutable ledger of events. If a mistake is made, a correcting transaction would need to be made, similar to how accountants never change existing entries.</p>\n<p>Using the patterns of event sourcing, we can architect a points program to process the commands (such as a hotel stay) and record them as immutable events in the ledger. For example, when a user earns points, the event is stored as a ledger entry of the event type and the number of points earned. In this example the entry would be (“hotel stay”, 32 points). This provides a full audit log of the events that happened, and a custom view of the ledger is created to provide a current account balance.</p>\n<p>Such a simple model might seem like it should easily scale on even the most simplistic key-value storage. Unfortunately, this is not the case. For instance, to ensure that a customer never claims entitlements worth more than their points balance, our model requires snapshot isolation or a primitive compare-and-swap-style operation. If we want to allow customers to transfer balances between accounts, we again need a better isolation level. To do this in a scale-out fashion, without baking our concurrency model into the application, our choices for database backends shrink very rapidly. Finally, the choice of databases narrows even further when we need to provide different views of the data. This requires secondary indices that allow querying the data with access patterns that differ from how the data is stored. </p>\n<h2>Globally Distributed ACID Transactions in FaunaDB</h2>\n<p>The most unique capability of FaunaDB is that it provides ACID-compliant, consistent transactions in a partitioned, globally distributed environment. </p>\n<blockquote>When it comes to database technology, traditional thinking has held that it’s impossible to provide both global scale and strong consistency. This ideology has led to the pervasive misconception that the only way to achieve global scale is to sacrifice strong consistency and transactions. </blockquote>\n<p>FaunaDB upends this thinking by bringing an entirely new approach to database transaction resolution by providing a globally distributed ACID transaction engine. It has been developed over the past several years by technical leaders with experience at <a href=\"https://blog.fauna.com/video-evan-and-matts-vision-for-a-mission-critical-operational-database\">Twitter</a> and <a href=\"https://blog.fauna.com/author/chrisanderson/\">Couchbase</a>.</p>\n<p>The use of globally distributed ACID transactions in FaunaDB allows enforcement of complex business constraints (by enforcing consistency) and linearization of event processing. Offloading this concern to the database frees the developer to focus on application concerns and reduces the complexity of the application code.</p>\n<h3>Order of Events in FaunaDB</h3>\n<p>Enforcing the complex business constraints discussed earlier often requires the events be captured in an exact order. For example, the bids on an auction, or the trades on the stock exchange, need to be captured in an exact linear history. This linear history relies on a database that offers a stronger guarantee than eventual consistency.</p>\n<blockquote>FaunaDB, with ACID transactions, allows events to be captured while preserving the linear order of their history.</blockquote>\n<h3>Temporality in FaunaDB</h3>\n<p>FaunaDB keeps all instances of a dataset. Instead of overwriting, it creates new ones when a write is performed. This is especially useful when auditing event data and verifying its evolution over time. In an event-driven architecture, this would allow you to view the state of the world and all related events at any point of time. </p>\n<h3>Indexes in FaunaDB</h3>\n<p>FaunaDB stores data in the form of flexible objects, where each object is an instance of a class. To allow for different access patterns, FaunaDB also allows multiples indexes of these classes so that they can be searched and retrieved by different attributes. Also, an index can contain references to multiple classes. The other unique aspect of FaunaDB is that updates to the index are atomic. This ensures that the index itself also maintains strong consistency alongside the core data and can enforce constraints like uniqueness. </p>\n<p>For event-driven architectures, this enables different views or projections of the core data, as well as the CQRS (Command Query Responsibility Segregation) pattern. Also, similar to a relational database, the index allows unique constraints to be put on the events. This enables the ability to provide constraints such as ensuring only one copy of an event is stored in the database.</p>\n<h2>Capturing the Current State of the World</h2>\n<p>Event sourcing stores the entire history of a system as a stream of events. This could include banking transactions, auction history, etc. For many businesses, this ledger of events is extremely important in understanding what happened. In addition to this ledger of events, businesses also need to capture the current state of the world. Some examples include an account balance, the owner of a legal contract, the winning bid, etc. This “mutable state” can be calculated in multiple ways depending on the business requirements for how real-time the value needs to be. </p>\n<p>If this “mutable state” can have a slight delay, then event sourcing only needs to ensure consistency on the write level. Then, snapshots and/or roll-ups using an eventual consistency model can be used for calculating projections out of the streams of events. Different consumers of events can be used to create read models, syndicate data, and prepare batch jobs that are eventually consistent. Calculating these different projections could take several seconds and could be handled later, as the event write itself only cares about making sure the new events were calculated based on the latest available state.</p>\n<p>However, if this “mutable state” needs to be real-time, it can be very difficult to calculate in an eventual consistency model since it’s often calculated after the fact with snapshots and roll-ups. Instead, an exact value can be created using Fauna ACID transactions to capture the exact state of the world at all times. This would allow for real-time data and business logic, such as determining if a customer has enough funds available for a purchase.</p>\n<h2>Comparison to Other Systems</h2>\n<p>FaunaDB is not the only database out there that offers global transactional consistency. Two other databases including Spanner (Google’s database-as-a-service) and CockroachDB (the open source stepchild of Spanner) provide similar functionality, but in a radically different way. Both are based on a protocol known as “Spanner’. In contrast, FaunaDB is based on a very different, patent-pending protocol. The distinction among these three databases boils down to a comparison between the different transactional protocols. <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">Read more about the distinction between our protocol and Spanner’s here.</a></p>\n<p>Beyond providing a fundamentally different transactional model, FaunaDB also delivers operational advantages. Baked into the architecture of FaunaDB is a multi-tenant, quality-of-service mechanism. Each workload that runs against FaunaDB has an associated priority. While other databases provide static multi-tenancy with fixed resources for each tenant, FaunaDB’s model is completely dynamic. An entire FaunaDB data infrastructure, with multiple global data centers, can function as a single set of resources such as I/O, compute, and storage. While you won’t notice this when you are coding your first application against FaunaDB, it is extremely beneficial when you need your database to run as the core platform for multiple applications and datasets. The best part is that it completely decouples application development from operational activities. Operations just need to ensure that there is sufficient capacity for all the various applications and data, and can shift the infrastructure topology however needed. </p>\n<h2>How Fauna Solves these Challenges</h2>\n<p>FaunaDB solves the major challenges of building an event-driven architecture by providing mission critical ACID transactions at a global scale. By using FaunaDB as the foundation of the application architecture, the complexity of maintaining strong consistency and ordering of events is solved by the database. This greatly simplifies the overall application architecture. </p>\n<p>In the next article, we will dive into an example of building an event-driven application, starting with a look at the core Fauna queries and features that form the building blocks. </p>\n<p>Thanks to Ben Edwards for helping out with the writing and proofreading of the article!</p>\n<p>Interested in diving in? <a href=\"https://fauna.com/sign-up\">Create a free account</a> or learn more by visiting our <a href=\"https://fauna.com/documentation/howto/crud\">documentation page</a>!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-06-25T21:05:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 527,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "549b31d8-ba5e-4ee1-8a43-83d8d97e4fe9",
        "siteSettingsId": 527,
        "fieldLayoutId": 4,
        "contentId": 360,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Survive Cloud Vendor Crashes with Netlify and FaunaDB",
        "slug": "survive-cloud-vendor-crashes-with-netlify-and-faunadb",
        "uri": "blog/survive-cloud-vendor-crashes-with-netlify-and-faunadb",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2020-02-24T09:20:04-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/survive-cloud-vendor-crashes-with-netlify-and-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/survive-cloud-vendor-crashes-with-netlify-and-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>I recently posted about FaunaDB’s unified data model, and how we take advantage of our <a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">global ACID transactions</a> as a foundation for users to interact with the data, using the correct query model for their applications. This week, Werner Vogels, the CTO of Amazon.com, wrote a post about <a href=\"https://www.allthingsdistributed.com/2018/06/purpose-built-databases-in-aws.html\">Amazon’s choice to maintain a fleet of databases</a>, each purpose-built for a particular workload. From the vendor’s standpoint, this may make sense as a way to segment your customers. However, for users, it introduces an abundance of unnecessary data movement between different databases - your application becomes a series of tubes. <em>Set aside the question of whether you are better off with a handful of narrowly tailored databases (or a general purpose operational database). This post is about the choice between investing in a technology that binds you to a single cloud vendor versus running your critical services across multiple clouds.</em></p>\n<blockquote>If you want to be above the fray when major changes happen, you need to architect your business for vendor independence.</blockquote>\n<p>One of the obvious problems with keeping enterprise data in cloud services is vendor lock-in. Once you are hooked on a cloud provider, you’re at the mercy of their business decisions. Whether or not a particular decision impacts you is less important than the fact that things are out of your control. It should be expected that businesses will make choices we can’t predict. The weakness here is part of the cloud architecture. If you want to be above the fray when major changes happen, you need to architect your business for vendor independence.</p>\n<p>Let’s look at a few scenarios for cloud vendor independence:</p>\n<ul><li><strong>Fully independent:</strong> Every service is deployed across multiple cloud vendors with robust failure handling. Even in the case of a complete vendor outage, your application goes on operating.</li><li><strong>Tightly coupled: </strong>Your app is tightly coupled to multiple clouds, with a service mix that uses best-of-breed specialty services from many vendors.</li><li><strong>Mission critical multi-cloud: </strong>Critical services are run across multiple cloud vendors. In this case, some application features might suffer downtime during a vendor outage, but critical services continue running.</li><li><strong>One cloud: </strong>Your app is tightly coupled to a single cloud vendor, using vendor specific services for critical operations.</li></ul>\n<h3>Fully-Independent Services</h3>\n<p>Viewed purely through the lens of vendor risk management, <strong>being fully independent is the safest course of action.</strong> However, this option has costs derived from having to work with virtual machines instead of higher-level services. For instance, instead of using a queuing service, you are likely to end up managing a cluster of virtual machines on each cloud, running a queue. This operational complexity will be multiplied by every service your application utilizes.</p>\n<p>All of this does not even scratch the surface of the complexity introduced by building intelligent load balancing and failover into your architecture. To become fully independent of cloud vendors, your system must not only run on multiple vendors; it must run across them all. In the end, you want a single unified system that can route around vendor failures, not multiple siloed copies of the system. Orchestrating a correct and robust multi-cloud service is an engineering challenge. Doing it for every service in your mix is a serious undertaking. The number of enterprises that complete this heavy lifting is small because, for most businesses, the risk/reward tradeoff doesn’t make sense. However, architects should think about this scenario so they can understand the delta between where they are now, and where they could be for maximum independence.</p>\n<h3>Tightly Coupled Services</h3>\n<p>Tightly coupled services represents another extreme. Using best-of-breed products across available providers might offer the fastest time-to-market, but also the greatest risk. Rather than depending on specialized services offered within one cloud, you are reaching across vendors in a way that multiplies rather than reduces your risk. If any one of your cloud providers suffers downtime, your business could go down. <br></p>\n<p>This wild-cloud architecture might be the right choice for startups and small experiments where the biggest risk is not getting to market at all. However, once your product has traction and starts to scale, you’ll end up undoing these architectural choices as your attitude towards risk shifts. Businesses that scale up without addressing vendor risk will find that when they start to care about costs, their negotiating leverage is limited. For the best deal, you want more than one supplier.</p>\n<h3>Mission Critical Multi-Cloud</h3>\n<blockquote>Once you have identified your critical services, you can focus on making them robustly multi-cloud.&nbsp;</blockquote>\n<p>Mission critical multi-cloud is a reachable goal with many of the benefits of the most risk-averse option, but also allows the flexibility to utilize cutting edge cloud services. The key is differentiating between mission critical services and everything else. For instance, if your ad-targeting engine fails your app can continue to function with un-targeted ads. If user uploaded video is not being encoded due to a service outage, that is not as bad as the user being unable to see their profile or existing videos.</p>\n<figure><img src=\"{asset:6932:url}\" data-image=\"6932\"></figure><figcaption>Map of Datacenters</figcaption>\n<p></p>\n<p>Once you have identified your critical services, you can focus on making them robustly multi-cloud. Recognizing and isolating the risk also gives you more room to utilize cutting edge specialty cloud services outside of your critical path. If your database and web-servers run on a seamless multi-cloud mix, the risk of utilizing a vendor specific photo classifier is lower, while the ease of integration goes up. If your critical services transparently abstract a mix of vendors, you can plug-in specialty services like photo or voice recognition without incurring additional transit fees. The risk profile of a vendor outage is lower in this environment because it won’t impact mission critical services. </p>\n<blockquote>By architecting your application to isolate vendor risk, in the end you can be more confident exploring cutting-edge vendor capabilities because you know any downside impact will be contained to non-critical services.</blockquote>\n<p>By architecting your application to isolate vendor risk, in the end you can be more confident exploring cutting-edge vendor capabilities because you know any downside impact will be contained to non-critical services. If your application’s image recognition feature has downtime, that’s entirely different than if your identity and authorization capability is broken. Don’t let shiny new cloud features lull you into storing your data with a single vendor.</p>\n<h3>Stuck in One Cloud</h3>\n<p>The last option is to stick to a particular vendor’s cloud and cross your fingers that none of the surprises are bad. For new teams, this can be seductive, but hopefully mere awareness of the option to run your critical services like databases and web front ends in a robust multi-cloud architecture gives you a new sense of the hidden costs of playing in one cloud.</p>\n<h1>Robust multi-cloud mission critical services</h1>\n<p>The two most common mission critical services are databases and web servers. Luckily, there are emerging alternatives that take the elbow grease out of operating multi-cloud architectures.</p>\n<blockquote>A multi-cloud architecture allows even more options about where to place servers for maximum end-user performance.</blockquote>\n<h3>Databases</h3>\n<p>Perhaps the most mission-critical component of any technology business is its database. To avoid cloud-specific databases like AWS DynamoDB or Google Firebase, you’ll either need to run something on bare virtual machine instances, or look for a database with a strong multi-cloud offering. Fauna Cloud runs on <a href=\"https://blog.fauna.com/global-multi-cloud-replication-in-faunadb-serverless-cloud\">AWS and GCP, and with Azure nodes on the roadmap</a>, and FaunaDB runs anywhere you can run a JVM. Keep in mind that simply deploying to the datacenters is only a start. You also need correctness and operational simplicity, or your move to multi-cloud has set your application back in other ways. </p>\n<p>FaunaDB’s <a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">multi-region ACID transactions</a> mean queries hitting different clouds will always see consistent results, so your data is safe and correct even in the case of a cloud vendor outage. Correctness at global scale makes FaunaDB a good fit for <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">distributed ledger applications</a> which must maintain transactionality over multiple sites. Multi-region ACID transactions also make it easy to control the locality of your data, so users can query the closest datacenter leading to snappier, more responsive applications.</p>\n<figure><img src=\"{asset:6933:url}\" data-image=\"6933\"></figure>\n<figcaption>FaunaDB Cloud Usage</figcaption>\n<p></p>\n<p>Operational simplicity means <a href=\"https://blog.fauna.com/fauna-topology-operations\">one node type (running anywhere you can run a JVM</a>) that stays fully online throughout topology changes. Single-step administrator commands include safety checks to prevent operations that could cause data loss. For example, if you try to remove a node that would create data loss, the admin tools will return an error instead of erasing data.</p>\n<h3>Web Servers</h3>\n<p>Your uptime depends on your web servers. If you’re running multi-cloud, then one provider’s downtime won’t impact your users. A multi-cloud architecture allows even more options about where to place web servers for maximum end-user performance. This way, best practices such as running a stateless web-tier deployed to multiple clouds with a content delivery network (CDN) are not out of reach.</p>\n<p><a href=\"https://www.netlify.com/\">Netlify</a> is a web hosting provider that runs across multiple clouds with a high-performance CDN, and simplifies all aspects of deploying your sites. In addition to web files, Netlify supports continuous deployment from git, serverless functions-as-a-service, dynamic forms, and identity authentication. They are rapidly becoming a standard for simple sites, and are adding features to support complex web application hosting as well.</p>\n<figure><img src=\"{asset:6934:url}\" data-image=\"6934\"></figure>\n<figcaption>Netlify Application Manager</figcaption>\n<p></p>\n<p><a href=\"https://www.netlify.com/blog/2018/05/14/how-netlify-migrated-to-a-fully-multi-cloud-infrastructure/\">Netlify shares their story</a> of moving to a multi-cloud architecture, and the benefits they’ve seen in this blog post:</p>\n<blockquote>As a result of the migration, we can now swap between different cloud providers without any user impact. This includes the databases, web servers, API servers, and object replication. We can easily move the entire brains of our service between Google, Amazon, and Rackspace in around 10 minutes with no service interruptions.</blockquote>\n<p>As a Netlify customer, if any one of the cloud providers goes down, Netlify’s origin servers and content delivery network will continue to serve traffic and users will be happy.</p>\n<h1>Conclusion</h1>\n<p>Using the latest cloud innovations is valuable, but do you want to put all your eggs in one basket? While relying on a single vendor’s cloud exposes your business to unnecessary risk, running every service in triplicate isn’t practical either. A good compromise is to identify your most critical data and services and migrate them to a multi-cloud approach first. The good news is that there are a handful of products like FaunaDB and Netlify designed for multi-cloud, and combining them with best of breed cloud vendor services is a good solution. This architecture combines the assurance that comes from multi-cloud critical services, with the freedom to explore specialized vendor offerings you get by having your data already stored in multi vendor clouds.</p>\n<p>With a transactionally multi-cloud database like FaunaDB, your data is available with no impact on uptime even if a replica crashes. See a <a href=\"https://blog.fauna.com/demonstrating-transactional-correctness-in-failure-situations\">this blog post for a demo of FaunaDB running smoothly despite replica failures</a>. More interested in theory? Read about <a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">how we achieve distributed ACID transactions</a> with our consensus algorithm.</p>",
        "blogCategory": [
            "8",
            "1530"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>I recently posted about FaunaDB’s unified data model, and how we take advantage of our <a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">global ACID transactions</a> as a foundation for users to interact with the data, using the correct query model for their applications. This week, Werner Vogels, the CTO of Amazon.com, wrote a post about <a href=\"https://www.allthingsdistributed.com/2018/06/purpose-built-databases-in-aws.html\">Amazon’s choice to maintain a fleet of databases</a>, each purpose-built for a particular workload. From the vendor’s standpoint, this may make sense as a way to segment your customers. However, for users, it introduces an abundance of unnecessary data movement between different databases - your application becomes a series of tubes. <em>Set aside the question of whether you are better off with a handful of narrowly tailored databases (or a general purpose operational database). This post is about the choice between investing in a technology that binds you to a single cloud vendor versus running your critical services across multiple clouds.</em></p>\n<blockquote>If you want to be above the fray when major changes happen, you need to architect your business for vendor independence.</blockquote>\n<p>One of the obvious problems with keeping enterprise data in cloud services is vendor lock-in. Once you are hooked on a cloud provider, you’re at the mercy of their business decisions. Whether or not a particular decision impacts you is less important than the fact that things are out of your control. It should be expected that businesses will make choices we can’t predict. The weakness here is part of the cloud architecture. If you want to be above the fray when major changes happen, you need to architect your business for vendor independence.</p>\n<p>Let’s look at a few scenarios for cloud vendor independence:</p>\n<ul><li><strong>Fully independent:</strong> Every service is deployed across multiple cloud vendors with robust failure handling. Even in the case of a complete vendor outage, your application goes on operating.</li><li><strong>Tightly coupled: </strong>Your app is tightly coupled to multiple clouds, with a service mix that uses best-of-breed specialty services from many vendors.</li><li><strong>Mission critical multi-cloud: </strong>Critical services are run across multiple cloud vendors. In this case, some application features might suffer downtime during a vendor outage, but critical services continue running.</li><li><strong>One cloud: </strong>Your app is tightly coupled to a single cloud vendor, using vendor specific services for critical operations.</li></ul><h3>Fully-Independent Services</h3>\n<p>Viewed purely from a lense of vendor risk management, <strong>being fully independent is the safest course of action.</strong> However, this option has costs derived from having to work with virtual machines instead of higher-level services. For instance, instead of using a queuing service, you are likely to end up managing a cluster of virtual machines on each cloud, running a queue. This operational complexity will be multiplied by every service your application utilizes.</p>\n<p>All of this does not even scratch the surface of the complexity introduced by building intelligent load balancing and failover into your architecture. To become fully independent of cloud vendors, your system must not only run on multiple vendors; it must run across them all. In the end, you want a single unified system that can route around vendor failures, not multiple siloed copies of the system. Orchestrating a correct and robust multi-cloud service is an engineering challenge. Doing it for every service in your mix is a serious undertaking. The number of enterprises that complete this heavy lifting is small because, for most businesses, the risk/reward tradeoff doesn’t make sense. However, architects should think about this scenario so they can understand the delta between where they are now, and where they could be for maximum independence.</p>\n<h3>Tightly Coupled Services</h3>\n<p>Tightly coupled services represents another extreme. Using best-of-breed products across available providers might offer the fastest time-to-market, but also the greatest risk. Rather than depending on specialized services offered within one cloud, you are reaching across vendors in a way that multiplies rather than reduces your risk. If any one of your cloud providers suffers downtime, your business could go down. <br /></p>\n<p>This wild-cloud architecture might be the right choice for startups and small experiments where the biggest risk is not getting to market at all. However, once your product has traction and starts to scale, you’ll end up undoing these architectural choices as your attitude towards risk shifts. Businesses that scale up without addressing vendor risk will find that when they start to care about costs, their negotiating leverage is limited. For the best deal, you want more than one supplier.</p>\n<h3>Mission Critical Multi-Cloud</h3>\n<blockquote>Once you have identified your critical services, you can focus on making them robustly multi-cloud. </blockquote>\n<p>Mission critical multi-cloud is a reachable goal with many of the benefits of the most risk-averse option, but also allows the flexibility to utilize cutting edge cloud services. The key is differentiating between mission critical services and everything else. For instance, if your ad-targeting engine fails your app can continue to function with un-targeted ads. If user uploaded video is not being encoded due to a service outage, that is not as bad as the user being unable to see their profile or existing videos.</p>\n<figure><img src=\"https://lh6.googleusercontent.com/t6CXJK_v1TbdgVtR0nzlu-Q3l1t4qkulv77BjR-5WoUqkzdSf9p6KhGafwitacZfkl0wxZzH5Qz7QjHDsfAhGHjplHRy94yK_qKuYVioVzPneL4H4IxFO-FCXCBuYoAa8DMYD9tK\" width=\"624\" height=\"380\" alt=\"Map of Datacenters\" title=\"Map of Datacenters\" /></figure><figcaption>Map of Datacenters</figcaption><p>Once you have identified your critical services, you can focus on making them robustly multi-cloud. Recognizing and isolating the risk also gives you more room to utilize cutting edge specialty cloud services outside of your critical path. If your database and web-servers run on a seamless multi-cloud mix, the risk of utilizing a vendor specific photo classifier is lower, while the ease of integration goes up. If your critical services transparently abstract a mix of vendors, you can plug-in specialty services like photo or voice recognition without incurring additional transit fees. The risk profile of a vendor outage is lower in this environment because it won’t impact mission critical services. </p>\n<blockquote>By architecting your application to isolate vendor risk, in the end you can be more confident exploring cutting-edge vendor capabilities because you know any downside impact will be contained to non-critical services.</blockquote>\n<p>By architecting your application to isolate vendor risk, in the end you can be more confident exploring cutting-edge vendor capabilities because you know any downside impact will be contained to non-critical services. If your application’s image recognition feature has downtime, that’s entirely different than if your identity and authorization capability is broken. Don’t let shiny new cloud features lull you into storing your data with a single vendor.</p>\n<h3>Stuck in One Cloud</h3>\n<p>The last option is to stick to a particular vendor’s cloud and cross your fingers that none of the surprises are bad. For new teams, this can be seductive, but hopefully mere awareness of the option to run your critical services like databases and web front ends in a robust multi-cloud architecture gives you a new sense of the hidden costs of playing in one cloud.</p>\n<h1>Robust multi-cloud mission critical services</h1>\n<p>The two most common mission critical services are databases and web servers. Luckily, there are emerging alternatives that take the elbow grease out of operating multi-cloud architectures.</p>\n<blockquote>A multi-cloud architecture allows even more options about where to place servers for maximum end-user performance.</blockquote>\n<h3>Databases</h3>\n<p>Perhaps the most mission-critical component of any technology business is its database. To avoid cloud-specific databases like AWS DynamoDB or Google Firebase, you’ll either need to run something on bare virtual machine instances, or look for a database with a strong multi-cloud offering. Fauna Cloud runs on <a href=\"https://blog.fauna.com/global-multi-cloud-replication-in-faunadb-serverless-cloud\">AWS and GCP, and with Azure nodes on the roadmap</a>, and FaunaDB runs anywhere you can run a JVM. Keep in mind that simply deploying to the datacenters is only a start. You also need correctness and operational simplicity, or your move to multi-cloud has set your application back in other ways. </p>\n<p>FaunaDB’s <a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">multi-region ACID transactions</a> mean queries hitting different clouds will always see consistent results, so your data is safe and correct even in the case of a cloud vendor outage. Correctness at global scale makes FaunaDB a good fit for <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">distributed ledger applications</a> which must maintain transactionality over multiple sites. Multi-region ACID transactions also make it easy to control the locality of your data, so users can query the closest datacenter leading to snappier, more responsive applications.</p>\n<figure><img src=\"https://lh3.googleusercontent.com/Jl9BzNsCEZESWAbMokuIJEz1HZW5TPVWluGX8icGJ63CKN7qsKYqM_CwWYj8M00doxycoTvb4lesYNqqAXrIT897XOfneCrnxdeqa4FCRPTBuEzeY6X501fnHP5pDCnsWkp-m5Fb\" width=\"624\" height=\"445\" alt=\"FaunaDB Cloud Usage\" title=\"FaunaDB Cloud Usage\" /></figure><figcaption>FaunaDB Cloud Usage</figcaption><p>Operational simplicity means <a href=\"https://blog.fauna.com/fauna-topology-operations\">one node type (running anywhere you can run a JVM</a>) that stays fully online throughout topology changes. Single-step administrator commands include safety checks to prevent operations that could cause data loss. For example, if you try to remove a node that would create data loss, the admin tools will return an error instead of erasing data.</p>\n<h3>Web Servers</h3>\n<p>Your uptime depends on your web servers. If you’re running multi-cloud, then one provider’s downtime won’t impact your users. A multi-cloud architecture allows even more options about where to place web servers for maximum end-user performance. This way, best practices such as running a stateless web-tier deployed to multiple clouds with a content delivery network (CDN) are not out of reach.</p>\n<p><a href=\"https://www.netlify.com/\">Netlify</a> is a web hosting provider that runs across multiple clouds with a high-performance CDN, and simplifies all aspects of deploying your sites. In addition to web files, Netlify supports continuous deployment from git, serverless functions-as-a-service, dynamic forms, and identity authentication. They are rapidly becoming a standard for simple sites, and are adding features to support complex web application hosting as well.</p>\n<figure><img src=\"https://lh4.googleusercontent.com/7L1l_gAXYtH97VvsWivHzS5GUEAZ_ewN7D9BJu_GhmtAU6qibkxng1FSrzjeYyMXUfTdcmiZgogijKeuta0CUP843BxDgI4rPcHlWyvwLw9_IAkNB461FkJcXV5Z4z6b434NrOrq\" width=\"624\" height=\"439\" alt=\"Netlify Application Manager\" title=\"Netlify Application Manager\" /></figure><figcaption>Netlify Application Manager</figcaption><p><a href=\"https://www.netlify.com/blog/2018/05/14/how-netlify-migrated-to-a-fully-multi-cloud-infrastructure/\">Netlify shares their story</a> of moving to a multi-cloud architecture, and the benefits they’ve seen in this blog post:</p>\n<blockquote>As a result of the migration, we can now swap between different cloud providers without any user impact. This includes the databases, web servers, API servers, and object replication. We can easily move the entire brains of our service between Google, Amazon, and Rackspace in around 10 minutes with no service interruptions.</blockquote>\n<p>As a Netlify customer, if any one of the cloud providers goes down, Netlify’s origin servers and content delivery network will continue to serve traffic and users will be happy.</p>\n<h1>Conclusion</h1>\n<p>Using the latest cloud innovations is valuable, but do you want to put all your eggs in one basket? While relying on a single vendor’s cloud exposes your business to unnecessary risk, running every service in triplicate isn’t practical either. A good compromise is to identify your most critical data and services and migrate them to a multi-cloud approach first. The good news is that there are a handful of products like FaunaDB and Netlify designed for multi-cloud, and combining them with best of breed cloud vendor services is a good solution. This architecture combines the assurance that comes from multi-cloud critical services, with the freedom to explore specialized vendor offerings you get by having your data already stored in multi vendor clouds.</p>\n<p>With a transactionally multi-cloud database like FaunaDB, your data is available with no impact on uptime even if a replica crashes. See a <a href=\"https://blog.fauna.com/demonstrating-transactional-correctness-in-failure-situations\">this blog post for a demo of FaunaDB running smoothly despite replica failures</a>. More interested in theory? Read about <a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">how we achieve distributed ACID transactions</a> with our consensus algorithm.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-06-20T18:55:33-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 526,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9e586960-d827-4719-8328-da5a1efaf734",
        "siteSettingsId": 526,
        "fieldLayoutId": 4,
        "contentId": 359,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Demonstrating Transactional Correctness in Failure Situations",
        "slug": "demonstrating-transactional-correctness-in-failure-situations",
        "uri": "blog/demonstrating-transactional-correctness-in-failure-situations",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/demonstrating-transactional-correctness-in-failure-situations",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/demonstrating-transactional-correctness-in-failure-situations",
        "isCommunityPost": false,
        "blogBodyText": "<p>Guaranteeing correct ACID transactions in a distributed database is a primary goal of Fauna. This is especially true in cases of network partition, node, and/or data center loss. <strong>This short video demo will show how Fauna maintains its guarantee of strictly serializable transactions in the case of a lost data-center.</strong> This is a worst-case scenario and demonstrates several of the major classes of failure likely to occur.</p>\r\n<blockquote><strong><strong>This short video demo will show how Fauna maintains its guarantee of strictly serializable transactions in the case of a lost data-center.</strong></strong></blockquote>\r\n<p>The demo simulates a distributed Fauna cluster running in 3 data-centers. The workload is that of a simple ledger. Each transaction will read the balance of both the customer, who is the source, and the customer who will receive the transfer. A validation calculation will be made to ensure the source customer has sufficient funds. Finally, the transaction will be written and the balances of the source and destination customer will be updated.<strong> Each transaction thus comprises 5 operations: two reads, a write/create, and two updates. These have to execute as a single atomic transaction.</strong><br></p>\r\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_Xujgy-agFM?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure>\r\n<blockquote><strong>Each transaction thus comprises 5 operations: two reads, a write/create, and two updates. These have to execute as a single atomic transaction.</strong></blockquote>\r\n<p>The simulation randomly writes transactions to one of the available data centers. After a constant number of transactions, a random node is chosen and an aggregate of all the customer balances is calculated. Since all reads are local in Fauna, this is a good way to demonstrate how all nodes are in a consistent state.</p>\r\n<p>Finally, one of the datacenters is taken down for a period of time and then recovered. <strong>As you will see in the demo, the loss of the data center is gracefully handled by the cluster.</strong> More importantly, you will see that at no time does the state of the system mishandle a transaction or in any way cause the customer balances to enter into an inconsistent state.</p>\r\n<blockquote><strong>As you will see in the demo, the loss of the data center is gracefully handled by the cluster.</strong></blockquote>\r\n<p>If you have a workload that would benefit from Fauna’s distributed ACID transactions, you should <a href=\"https://fauna.com/sign-up\">take a test drive</a> or <a href=\"https://fauna.com/request-info\">contact us</a> to get a better understanding of how Fauna delivers this game-changing functionality.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Guaranteeing correct ACID transactions in a distributed database is a primary goal of Fauna. This is especially true in cases of network partition, node, and/or data center loss. <strong>This short video demo will show how Fauna maintains its guarantee of strictly serializable transactions in the case of a lost data-center.</strong> This is a worst-case scenario and demonstrates several of the major classes of failure likely to occur.</p>\n<blockquote><strong><strong>This short video demo will show how Fauna maintains its guarantee of strictly serializable transactions in the case of a lost data-center.</strong></strong></blockquote>\n<p>The demo simulates a distributed Fauna cluster running in 3 data-centers. The workload is that of a simple ledger. Each transaction will read the balance of both the customer, who is the source, and the customer who will receive the transfer. A validation calculation will be made to ensure the source customer has sufficient funds. Finally, the transaction will be written and the balances of the source and destination customer will be updated.<strong> Each transaction thus comprises 5 operations: two reads, a write/create, and two updates. These have to execute as a single atomic transaction.</strong><br /></p>\n<figure></figure><blockquote><strong>Each transaction thus comprises 5 operations: two reads, a write/create, and two updates. These have to execute as a single atomic transaction.</strong></blockquote>\n<p>The simulation randomly writes transactions to one of the available data centers. After a constant number of transactions, a random node is chosen and an aggregate of all the customer balances is calculated. Since all reads are local in Fauna, this is a good way to demonstrate how all nodes are in a consistent state.</p>\n<p>Finally, one of the datacenters is taken down for a period of time and then recovered. <strong>As you will see in the demo, the loss of the data center is gracefully handled by the cluster.</strong> More importantly, you will see that at no time does the state of the system mishandle a transaction or in any way cause the customer balances to enter into an inconsistent state.</p>\n<blockquote><strong>As you will see in the demo, the loss of the data center is gracefully handled by the cluster.</strong></blockquote>\n<p>If you have a workload that would benefit from Fauna’s distributed ACID transactions, you should <a href=\"https://fauna.com/sign-up\">take a test drive</a> or <a href=\"https://fauna.com/request-info\">contact us</a> to get a better understanding of how Fauna delivers this game-changing functionality.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "475",
        "postDate": "2018-06-12T22:01:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 525,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "ea7e4fe5-8cdc-43b9-8963-437cb36c757d",
        "siteSettingsId": 525,
        "fieldLayoutId": 4,
        "contentId": 358,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The Database that Stays Alive Even When You Issue a Command to Remove the Last Replica",
        "slug": "fauna-topology-operations",
        "uri": "blog/fauna-topology-operations",
        "dateCreated": "2018-08-27T07:12:44-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/fauna-topology-operations",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/fauna-topology-operations",
        "isCommunityPost": false,
        "blogBodyText": "<p>Operational simplicity is often the bane of a database administrator’s (DBA) life. Deploying a simple configuration is one thing, but building and maintaining a cluster across multiple data centers can be spine-chilling.<br></p>\r\n<blockquote><strong><em>FaunaDB is easy to deploy, easy to cluster, and easy to maintain.</em></strong></blockquote>\r\n<p>At Fauna, reducing operational complexity is one of the core tenets of product design. In everything we deliver, our goal is to make our users’ lives simpler, more productive, and more efficient. <strong><em>As a result, FaunaDB is easy to deploy, easy to cluster, and easy to maintain. </em></strong></p>\r\n<h1>The Basics of Fauna Operations</h1>\r\n<p>FaunaDB is deployed and scaled as a collection of nodes, each of which operate within a cluster in an autonomous fashion. There are no additional pieces of management software, such as a dedicated cluster manager to deploy. Each node participates in the cluster. As nodes are added or removed, these nodes communicate with each other to arrive at the necessary state. Figure 1 illustrates a typical FaunaDB cluster topology. </p>\r\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186076-caaad73c-6b44-11e8-8aad-06048f04e016.png\" width=\"506\" height=\"309\" style=\"background-color: initial;\" data-image=\"mh5mbfa58cch\"><br></figure>\r\n<p>Figure 1</p>\r\n<p>As discussed, a FaunaDB cluster consists of individual nodes operating together to self-manage the system. &nbsp;</p>\r\n<p>Every node is a computer with a unique IP address. Nodes are grouped into replicas, with every node belonging to exactly one replica. The main significance for grouping nodes into replicas is that a replica contains a full copy of the data. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy within a replica. Having multiple replicas, each containing the full set of data, is what provides redundancy in a FaunaDB cluster.</p>\r\n<p><strong>Reducing Operational Complexity with FaunaDB</strong></p>\r\n<p>FaunaDB provides a database administrator with first class commands to manipulate the cluster. These commands are designed to establish the FOS methodology - <strong>F</strong>ully online, <strong>O</strong>perationally simple, and <strong>S</strong>afe. </p>\r\n<p><strong>Fully online</strong></p>\r\n<p>This refers to a database’s ability to be fully operational while the command is executing. Most databases employ the rolling window operational scheme, which makes portions of the system unavailable for a duration while the command is taking place. This limits the database capacity and availability of the command, and requires scheduling with the business operations to avoid peak usage times. </p>\r\n<blockquote><strong><em>FaunaDB ensures that your database is online and available before, during, and after the execution of a command</em></strong>.</blockquote>\r\n<p>In contrast, <strong><em>FaunaDB ensures that your database is online and available before, during, and after the execution of a command</em></strong>. As an example, FaunaDB’s Remove node command (as seen in figure 2) achieves 100% global availability when invoked by an administrator. Rest assured, your applications continue to function at full throttle while you manage your cluster!</p>\r\n<p> &nbsp;</p>\r\n<p>\r\n</p>\r\n<figure><img width=\"302\" alt=\"screen shot 2018-06-08 at 8 19 17 am\" src=\"https://user-images.githubusercontent.com/253/41319578-20887920-6e51-11e8-802c-5730d64eecd0.png\" data-image=\"mu51dwngqvta\"></figure>\r\n<p>&nbsp;</p>\r\n<p></p>\r\n<p>Figure 2</p>\r\n<p><strong>Operationally simple </strong></p>\r\n<p>To make an administrator’s daily life simple, a database command should be a single command, not a series of steps an administrator must orchestrate. </p>\r\n<p>Introducing a series of commands means database administrators would need to remember or, even worse, look up in the manual when to take such steps and under what conditions. Remembering a few steps is not too complex, but understanding how to react to each possible error along the way requires specific database experience. Lacking this experience leads to incorrect usage, unwanted errors, and complexity. In addition, the command needs to accomplish the steps automatically. </p>\r\n<p>FaunaDB challenges this traditional approach by using transactional commands.<strong><em> Each operational command is complete in itself, and designed to fit into your devops process.</em></strong></p>\r\n<blockquote><strong><em>Each operational command is complete in itself, and designed to fit into your devops process.</em></strong></blockquote>\r\n<p>For example, the remove command (as seen in Figure 2) accomplishes all the work required to remove the node from the cluster and replica for the database administrator. There is no pre or post work required for the database administrator by the command. &nbsp;</p>\r\n<p>Another complex situation that arises in a production environment deals with scale. While removing a single node that is fully online is admirable, in many cases database administrators need to change the layout of <em>many</em> nodes at once. Waiting for a single node to complete in order to start the next operations can be expensive and take an excessive amount of wasted resources. Let’s look into this situation in great detail. If you have 50 nodes in a cluster and need to delete 10 nodes and add 2 new nodes one at a time (i.e. serially), then the first deletion of a node will spread the data to the other 49 node.&nbsp;But, as a DBA, you know that we have to delete 9 of the nodes to which data was just sent. Doing it one at a time would be mind-numbing, slow, and a waste of resources.</p>\r\n<p>The FaunaDB operational commands are parallel, stackable (i.e. not serial), and optimized immediately for the desired end state of the cluster. You can provide the Fauna database server with 10 remove node commands and 2 add node commands, and the end state will not only be immediately recognized to be 42 nodes but data will also be automatically optimized to achieve a 42 node cluster. No intermediate data state is required. In the middle of moving the data to the 42 node state, the DBA might realize that 11 nodes need to be removed instead of the 10 nodes that were discussed. Just issue the remove node command and immediately the new cluster is optimized for a 41 node state. FaunaDB’s cluster management behavior reflects the team’s years of experience with scaling high stakes deployments in fast moving environments <a href=\"https://www.infoq.com/news/2017/03/faunadb\">like Twitter</a>.</p>\r\n<p><strong>Safe</strong></p>\r\n<p>Oftentimes, systems are complex, having many parts that can lead to mistakes being made in a variety of ways. The smallests of mistakes can have a catastrophic impact on your operational data and render your applications in an undesirable state. Safety implies the database server protects the administrator from making mistakes that will harm the operational state of the cluster. </p>\r\n<blockquote><strong><em>In FaunaDB, we designed the remove node command (and other topology commands) such that they do not allow a DBA to modify the cluster configuration into an unwanted or unrecoverable state</em></strong>.</blockquote>\r\n<p>For example, let’s consider the removal of a cluster node again. A simple typo in a script could lead to the removal of a node that contains the last copy of live data. <strong><em>In FaunaDB, we designed the remove node command (and other topology commands) such that they do not allow a DBA to modify the cluster configuration into an unwanted or unrecoverable state</em></strong>. Losing the last copy of data would meet this criteria and FaunaDB would prevent such a situation. Another example would be attempting to remove a node with an active transaction log, such that the transactional integrity would be compromised. In these ways, and many others, FaunaDB ensures even the most complex multi-region clusters are safe for the business.</p>\r\n<p><strong>Conclusion</strong></p>\r\n<p>FaunaDB modernizes database operations by making them simpler, efficient, and low-cost. Simplicity addresses not just a technical pain point, but also a critical business issue for all enterprises — large or small. Database operations are often the single largest hurdle in business agility. At Fauna, we built our operational interface with significant input from experienced database administrators managing complex multi-datacenter environments. The Fauna team went the extra mile to ensure cluster operations are fully online, operationally simple, and safe. Rest assured, we are your champions, dear DBA. </p>\r\n<p>To learn more about FaunaDB’s architecture and capabilities, take a look at <a href=\"https://fauna.com/whitepaper\">our whitepaper</a>.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Operational simplicity is often the bane of a database administrator’s (DBA) life. Deploying a simple configuration is one thing, but building and maintaining a cluster across multiple data centers can be spine-chilling.<br /></p>\n<blockquote><strong><em>FaunaDB is easy to deploy, easy to cluster, and easy to maintain.</em></strong></blockquote>\n<p>At Fauna, reducing operational complexity is one of the core tenets of product design. In everything we deliver, our goal is to make our users’ lives simpler, more productive, and more efficient. <strong><em>As a result, FaunaDB is easy to deploy, easy to cluster, and easy to maintain. </em></strong></p>\n<h1>The Basics of Fauna Operations</h1>\n<p>FaunaDB is deployed and scaled as a collection of nodes, each of which operate within a cluster in an autonomous fashion. There are no additional pieces of management software, such as a dedicated cluster manager to deploy. Each node participates in the cluster. As nodes are added or removed, these nodes communicate with each other to arrive at the necessary state. Figure 1 illustrates a typical FaunaDB cluster topology. </p>\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186076-caaad73c-6b44-11e8-8aad-06048f04e016.png\" width=\"506\" height=\"309\" alt=\"\" /><br /></figure><p>Figure 1</p>\n<p>As discussed, a FaunaDB cluster consists of individual nodes operating together to self-manage the system. </p>\n<p>Every node is a computer with a unique IP address. Nodes are grouped into replicas, with every node belonging to exactly one replica. The main significance for grouping nodes into replicas is that a replica contains a full copy of the data. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy within a replica. Having multiple replicas, each containing the full set of data, is what provides redundancy in a FaunaDB cluster.</p>\n<p><strong>Reducing Operational Complexity with FaunaDB</strong></p>\n<p>FaunaDB provides a database administrator with first class commands to manipulate the cluster. These commands are designed to establish the FOS methodology - <strong>F</strong>ully online, <strong>O</strong>perationally simple, and <strong>S</strong>afe. </p>\n<p><strong>Fully online</strong></p>\n<p>This refers to a database’s ability to be fully operational while the command is executing. Most databases employ the rolling window operational scheme, which makes portions of the system unavailable for a duration while the command is taking place. This limits the database capacity and availability of the command, and requires scheduling with the business operations to avoid peak usage times. </p>\n<blockquote><strong><em>FaunaDB ensures that your database is online and available before, during, and after the execution of a command</em></strong>.</blockquote>\n<p>In contrast, <strong><em>FaunaDB ensures that your database is online and available before, during, and after the execution of a command</em></strong>. As an example, FaunaDB’s Remove node command (as seen in figure 2) achieves 100% global availability when invoked by an administrator. Rest assured, your applications continue to function at full throttle while you manage your cluster!</p>\n<p> </p>\n<p>\n</p>\n<figure><img width=\"302\" alt=\"screen shot 2018-06-08 at 8 19 17 am\" src=\"https://user-images.githubusercontent.com/253/41319578-20887920-6e51-11e8-802c-5730d64eecd0.png\" /></figure><p> </p>\n<p>Figure 2</p>\n<p><strong>Operationally simple </strong></p>\n<p>To make an administrator’s daily life simple, a database command should be a single command, not a series of steps an administrator must orchestrate. </p>\n<p>Introducing a series of commands means database administrators would need to remember or, even worse, look up in the manual when to take such steps and under what conditions. Remembering a few steps is not too complex, but understanding how to react to each possible error along the way requires specific database experience. Lacking this experience leads to incorrect usage, unwanted errors, and complexity. In addition, the command needs to accomplish the steps automatically. </p>\n<p>FaunaDB challenges this traditional approach by using transactional commands.<strong><em> Each operational command is complete in itself, and designed to fit into your devops process.</em></strong></p>\n<blockquote><strong><em>Each operational command is complete in itself, and designed to fit into your devops process.</em></strong></blockquote>\n<p>For example, the remove command (as seen in Figure 2) accomplishes all the work required to remove the node from the cluster and replica for the database administrator. There is no pre or post work required for the database administrator by the command. </p>\n<p>Another complex situation that arises in a production environment deals with scale. While removing a single node that is fully online is admirable, in many cases database administrators need to change the layout of <em>many</em> nodes at once. Waiting for a single node to complete in order to start the next operations can be expensive and take an excessive amount of wasted resources. Let’s look into this situation in great detail. If you have 50 nodes in a cluster and need to delete 10 nodes and add 2 new nodes one at a time (i.e. serially), then the first deletion of a node will spread the data to the other 49 node. But, as a DBA, you know that we have to delete 9 of the nodes to which data was just sent. Doing it one at a time would be mind-numbing, slow, and a waste of resources.</p>\n<p>The FaunaDB operational commands are parallel, stackable (i.e. not serial), and optimized immediately for the desired end state of the cluster. You can provide the Fauna database server with 10 remove node commands and 2 add node commands, and the end state will not only be immediately recognized to be 42 nodes but data will also be automatically optimized to achieve a 42 node cluster. No intermediate data state is required. In the middle of moving the data to the 42 node state, the DBA might realize that 11 nodes need to be removed instead of the 10 nodes that were discussed. Just issue the remove node command and immediately the new cluster is optimized for a 41 node state. FaunaDB’s cluster management behavior reflects the team’s years of experience with scaling high stakes deployments in fast moving environments <a href=\"https://www.infoq.com/news/2017/03/faunadb\">like Twitter</a>.</p>\n<p><strong>Safe</strong></p>\n<p>Oftentimes, systems are complex, having many parts that can lead to mistakes being made in a variety of ways. The smallests of mistakes can have a catastrophic impact on your operational data and render your applications in an undesirable state. Safety implies the database server protects the administrator from making mistakes that will harm the operational state of the cluster. </p>\n<blockquote><strong><em>In FaunaDB, we designed the remove node command (and other topology commands) such that they do not allow a DBA to modify the cluster configuration into an unwanted or unrecoverable state</em></strong>.</blockquote>\n<p>For example, let’s consider the removal of a cluster node again. A simple typo in a script could lead to the removal of a node that contains the last copy of live data. <strong><em>In FaunaDB, we designed the remove node command (and other topology commands) such that they do not allow a DBA to modify the cluster configuration into an unwanted or unrecoverable state</em></strong>. Losing the last copy of data would meet this criteria and FaunaDB would prevent such a situation. Another example would be attempting to remove a node with an active transaction log, such that the transactional integrity would be compromised. In these ways, and many others, FaunaDB ensures even the most complex multi-region clusters are safe for the business.</p>\n<p><strong>Conclusion</strong></p>\n<p>FaunaDB modernizes database operations by making them simpler, efficient, and low-cost. Simplicity addresses not just a technical pain point, but also a critical business issue for all enterprises — large or small. Database operations are often the single largest hurdle in business agility. At Fauna, we built our operational interface with significant input from experienced database administrators managing complex multi-datacenter environments. The Fauna team went the extra mile to ensure cluster operations are fully online, operationally simple, and safe. Rest assured, we are your champions, dear DBA. </p>\n<p>To learn more about FaunaDB’s architecture and capabilities, take a look at <a href=\"https://fauna.com/whitepaper\">our whitepaper</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "475",
        "postDate": "2018-06-09T01:23:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 524,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b869b51c-0d15-47cf-bd95-07cc62db1ecb",
        "siteSettingsId": 524,
        "fieldLayoutId": 4,
        "contentId": 357,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introduction to FaunaDB Clusters",
        "slug": "introduction-to-faunadb-clusters",
        "uri": "blog/introduction-to-faunadb-clusters",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introduction-to-faunadb-clusters",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introduction-to-faunadb-clusters",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is a mission critical, NoSQL database architected specifically for operationally distributed environments. The database focuses on six key principles: ACID transactions, global scale, reliability, operational simplicity, security and developer friendliness.&nbsp;&nbsp;</p>\r\n<p>FaunaDB was built to scale horizontally within a datacenter for maximizing throughput while spanning globally distributed sites easily - ensuring reliability and local performance. </p>\r\n<blockquote>FaunaDB was built to be scalable and reliable without compromising operational simplicity.</blockquote>\r\n<p>This blog post will focus on understanding the operational topology and organization of FaunaDB.</p>\r\n<h1>Evolution of the Database</h1>\r\n<p>The modern database had it commercial roots in the 1960s with the release of the IMS database to customers with deep pockets, such as Caterpillar and the Apollo space program. The next big step in database technology came in the 1970s when Edgar Codd described the theory of relational databases, followed by the release of DB2 on a mainframe. While these innovations made databases more affordable for governments and fortune 500 type businesses, the price prevented a majority of businesses from utilizing databases. Business again forced technology to progress. In the 1980s Unix and the transactional client-server database came together with affordable computer hardware. This technology evolution opened the door to many new business models, such as order inventory systems, HR systems, and point of sale. In the beginning, many of these business models where operational during local business hours, and then extended business hours but, with the popularity of the internet and the availability of global information for business at the turn of the century, the need for always available databases and intense scaling was paramount. This led the way for a radical split in the database landscape between the businesses which spent money scaling transactional databases and those which utilized a new class of databases technology revolving around the CAP theorem (see Figure 1) or an eventually consistent database model. </p>\r\n<p></p>\r\n<figure><img width=\"647\" alt=\"CAP theorem\" src=\"https://user-images.githubusercontent.com/253/41186069-bf10d2f0-6b44-11e8-9dae-5b31d51076f6.png\" data-image=\"hizmpzezmdxf\"></figure>\r\n<p><br>Figure 1</p>\r\n<p></p>\r\n<p>This recent class of database creates a networked cluster of commodity based hardware and sacrifices data correctness and the principles of ACID transactions to achieve scale. The belief is that not all business models require 100% correctness, but nearly correct would be good enough. The responsibility was moved to the application program to understand the business model and guarantee sufficient correctness for their business. &nbsp;&nbsp;&nbsp;</p>\r\n<p>It’s been noted that the CAP theorem leaves some things unstated. <a href=\"https://en.wikipedia.org/wiki/PACELC_theorem\">Daniel Abadi’s PACELC</a> asks what happens during a partition vs when there are no partitions.&nbsp;There are other discussions that point out how different operations can fall into different categories. You can read <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">Daniel Abadi’s take on FaunaDB’s consensus algorithm here.</a></p>\r\n<p>Database clustering has been historically a rocket scientist's task. This was because most databases were built for a client-server deployment style. When the internet was born, the need for data distribution came along. Vendors invented complex data replication schemes that required additional hardware and software. They still do so. These techniques are a drain on your finances and impede reliability.</p>\r\n<p>NoSQL databases promised to address scale (albeit at expense of enterprise capabilities like ACID compliance). They never lived up to the promise. Have you ever tried installing a Cassandra or a Mongo Cluster? It still requires rocket science.</p>\r\n<p>FaunaDB was built to be scalable and reliable without compromising operational simplicity. We built clustering into the core of the system to make this work. It wasn't an afterthought. We wanted to build a database that would work just as advertised - deploy a node and go, no more managers of managers or special hardware and software. </p>\r\n<p>Fauna is leading the way to the next evolution of database technology with its mesh cluster. Fauna was built with cloud, containers, and serverless in mind and utilizes new research and technology to provide the next step in database evolution. Fauna takes advantage of commodity and cloud based hardware to achieve scaling while providing 100% adherence to the principles of ACID. This removes the burden of data correctness from the application developer and places that responsibility on the database. &nbsp;&nbsp;Application programs can focus on the usability of the application rather than the correctness logic, making Fauna a friendly database for developers.  </p>\r\n<h1>FaunaDB Clustering Basics</h1>\r\n<p>FaunaDB uses a mesh-oriented approach to clustering: Deploy a node, point it to its peer, and go. The system takes care of everything else. &nbsp;&nbsp;</p>\r\n<p>The key components of a FaunaDB cluster are:</p>\r\n<ol><li>Node</li><li>Replica</li><li>Shard/partition</li></ol>\r\n<p>The example cluster referenced in Figure 2 is what will be exploring in great detail, starting with the inside and working our way out. Along the way we will explore the interaction of each layer and how both scale-out (horizontal scaling) and scale-up (or vertical scaling) is achieved by FaunaDB.</p>\r\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186076-caaad73c-6b44-11e8-8aad-06048f04e016.png\" data-image=\"w6mc7w9ml7im\"></figure>\r\n<p></p>\r\n<p>Figure 2</p>\r\n<p>Fauna physical data layout can be described entirely by three simple concepts, node, replica and cluster. </p>\r\n<h2>Node</h2>\r\n<p>A node is a single computer with its own IP address in which FaunaDB is installed and running. The Fauna nodes can run on all public cloud services, including Amazon, Azure, Bluemix, and Google, in addition to running on a private cloud, an on-premise solution, a virtual machine, a docker image, and several other platforms. </p>\r\n<p>When considering sizing a Fauna system, it is important to look at the individual node sizing. While Fauna scales very well vertically, the price tag for an extremely large complex system requiring a single large server can be cost prohibitive for many companies. This is why Fauna concentrated a significant amount of effort on scaling horizontally on commodity hardware. This class of hardware generally provides a better cost per transaction throughput. </p>\r\n<blockquote>Fauna concentrated a significant amount of effort on scaling horizontally on commodity hardware.</blockquote>\r\n<p>A FaunaDB cluster does not have a single master node. Instead there are some nodes which take on a multiple responsibility. All nodes will store, retrieve, and route requests for data. In addition, one or more nodes may also handle the transaction log. If the transaction throughput is high, the ability to split the transaction workload amongst several nodes is essential. This is just one of the many ways the Fauna server demonstrates its horizontal scaling capabilities for large systems.</p>\r\n<h2>Replica</h2>\r\n<p>A replica is comprised of a group of one or more node(s). A node can belong only to a single replica. A fundamental property of a replica is that it contains a complete copy of the Fauna data, including end user data, system data, and the transaction log. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy provided by Fauna within a replica. Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster. Another common term for a replica would be a datacenter.</p>\r\n<blockquote>Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster.</blockquote>\r\n<p>A great advantage of Fauna when sizing the system is that each replica is capable of being a different type and size. A company which has a large presence in the South with its own IT infrastructure can host Fauna on-premise with more hardware and capacity. In this scenario, the same company is expanding on both the East and West coasts and the remote offices are small with limited IT infrastructure. These two replicas can be located in the cloud with a smaller hardware footprint. This allows a business to maximum cost effectiveness while enjoying “custom-fit” geographical proximity.</p>\r\n<h2>Cluster</h2>\r\n<p>A FaunaDB cluster is the highest entity. Under the cluster is both a physical layout and logical layout. To review, the physical layout it is comprised of individual nodes. Every node is a computer with a unique IP address. Nodes are grouped into replicas, with every node belonging to exactly one replica. The main significance for this grouping is that a replica contains a full copy of the data and can vary in size. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy within a replica.</p>\r\n<blockquote>Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster.</blockquote>\r\n<p>The FaunaDB Cluster has a logical data model comprised of databases, collections, and documents. This data model <a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">implements</a> a semi-structured, schema free object-relational data model, a strict superset of the relational, document, object-oriented, and graph paradigms. &nbsp;The model starts with the database which can be laid out in traditional flat fashion, in a hierarchical or a combination of flat and hierarchical as seen in Figure 3.</p>\r\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186071-c1621618-6b44-11e8-9af3-a6ff14be3a85.png\" data-image=\"mto1iwblz4ji\"></figure>\r\n<p></p>\r\n<p>Figure 3</p>\r\n<p>While many vendors can have multiple databases in their system, few SQL or NoSQL vendors have the concept of a hierarchical database. This hierarchical organization of data provides many benefits in the multi-tenant scenario: &nbsp;</p>\r\n<ul><li>The ability for a single company to share resources between two, three or a yet to be determine number of projects by simply creating a database under or alongside another database. &nbsp;&nbsp;</li><li>The simplification of access control, resource allocation and sharing, and data layout, is kept uniform by simply inheriting the properties and resources. &nbsp;</li><li>Eliminates the need to manage properties and resources for each database individually. </li></ul>\r\n<p>Below the database exists collections/classes which are similar to a table in a relational database. However, like a document database, full or partially shared schema within a class is optional, not mandatory. Collections/classes contain zero or more records, or semi-structured documents, which can include recursively nested objects and arrays as well as scalar types.</p>\r\n<h1>Conclusion</h1>\r\n<p>This introduction to both the physical and logical data models of FaunaDB clustering demonstrates the simple elegance behind its design. While many transactional databases increase their complexity when scaling and require specialized hardware, FaunaDB goes the extra mile to keep things simple while scaling both vertically and horizontally on all styles of hardware. It simply addresses not only a technical pain point, but also a critical business issue for all enterprises - large or small.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB is a mission critical, NoSQL database architected specifically for operationally distributed environments. The database focuses on six key principles: ACID transactions, global scale, reliability, operational simplicity, security and developer friendliness. </p>\n<p>FaunaDB was built to scale horizontally within a datacenter for maximizing throughput while spanning globally distributed sites easily - ensuring reliability and local performance. </p>\n<blockquote>FaunaDB was built to be scalable and reliable without compromising operational simplicity.</blockquote>\n<p>This blog post will focus on understanding the operational topology and organization of FaunaDB.</p>\n<h1>Evolution of the Database</h1>\n<p>The modern database had it commercial roots in the 1960s with the release of the IMS database to customers with deep pockets, such as Caterpillar and the Apollo space program. The next big step in database technology came in the 1970s when Edgar Codd described the theory of relational databases, followed by the release of DB2 on a mainframe. While these innovations made databases more affordable for governments and fortune 500 type businesses, the price prevented a majority of businesses from utilizing databases. Business again forced technology to progress. In the 1980s Unix and the transactional client-server database came together with affordable computer hardware. This technology evolution opened the door to many new business models, such as order inventory systems, HR systems, and point of sale. In the beginning, many of these business models where operational during local business hours, and then extended business hours but, with the popularity of the internet and the availability of global information for business at the turn of the century, the need for always available databases and intense scaling was paramount. This led the way for a radical split in the database landscape between the businesses which spent money scaling transactional databases and those which utilized a new class of databases technology revolving around the CAP theorem (see Figure 1) or an eventually consistent database model. </p>\n\n<figure><img width=\"647\" alt=\"CAP theorem\" src=\"https://user-images.githubusercontent.com/253/41186069-bf10d2f0-6b44-11e8-9dae-5b31d51076f6.png\" /></figure><p><br />Figure 1</p>\n\n<p>This recent class of database creates a networked cluster of commodity based hardware and sacrifices data correctness and the principles of ACID transactions to achieve scale. The belief is that not all business models require 100% correctness, but nearly correct would be good enough. The responsibility was moved to the application program to understand the business model and guarantee sufficient correctness for their business. </p>\n<p>It’s been noted that the CAP theorem leaves some things unstated. <a href=\"https://en.wikipedia.org/wiki/PACELC_theorem\">Daniel Abadi’s PACELC</a> asks what happens during a partition vs when there are no partitions. There are other discussions that point out how different operations can fall into different categories. You can read <a href=\"https://blog.fauna.com/distributed-consistency-at-scale-spanner-vs-calvin\">Daniel Abadi’s take on FaunaDB’s consensus algorithm here.</a></p>\n<p>Database clustering has been historically a rocket scientist's task. This was because most databases were built for a client-server deployment style. When the internet was born, the need for data distribution came along. Vendors invented complex data replication schemes that required additional hardware and software. They still do so. These techniques are a drain on your finances and impede reliability.</p>\n<p>NoSQL databases promised to address scale (albeit at expense of enterprise capabilities like ACID compliance). They never lived up to the promise. Have you ever tried installing a Cassandra or a Mongo Cluster? It still requires rocket science.</p>\n<p>FaunaDB was built to be scalable and reliable without compromising operational simplicity. We built clustering into the core of the system to make this work. It wasn't an afterthought. We wanted to build a database that would work just as advertised - deploy a node and go, no more managers of managers or special hardware and software. </p>\n<p>Fauna is leading the way to the next evolution of database technology with its mesh cluster. Fauna was built with cloud, containers, and serverless in mind and utilizes new research and technology to provide the next step in database evolution. Fauna takes advantage of commodity and cloud based hardware to achieve scaling while providing 100% adherence to the principles of ACID. This removes the burden of data correctness from the application developer and places that responsibility on the database. Application programs can focus on the usability of the application rather than the correctness logic, making Fauna a friendly database for developers. </p>\n<h1>FaunaDB Clustering Basics</h1>\n<p>FaunaDB uses a mesh-oriented approach to clustering: Deploy a node, point it to its peer, and go. The system takes care of everything else. </p>\n<p>The key components of a FaunaDB cluster are:</p>\n<ol><li>Node</li><li>Replica</li><li>Shard/partition</li></ol><p>The example cluster referenced in Figure 2 is what will be exploring in great detail, starting with the inside and working our way out. Along the way we will explore the interaction of each layer and how both scale-out (horizontal scaling) and scale-up (or vertical scaling) is achieved by FaunaDB.</p>\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186076-caaad73c-6b44-11e8-8aad-06048f04e016.png\" alt=\"\" /></figure><p>Figure 2</p>\n<p>Fauna physical data layout can be described entirely by three simple concepts, node, replica and cluster. </p>\n<h2>Node</h2>\n<p>A node is a single computer with its own IP address in which FaunaDB is installed and running. The Fauna nodes can run on all public cloud services, including Amazon, Azure, Bluemix, and Google, in addition to running on a private cloud, an on-premise solution, a virtual machine, a docker image, and several other platforms. </p>\n<p>When considering sizing a Fauna system, it is important to look at the individual node sizing. While Fauna scales very well vertically, the price tag for an extremely large complex system requiring a single large server can be cost prohibitive for many companies. This is why Fauna concentrated a significant amount of effort on scaling horizontally on commodity hardware. This class of hardware generally provides a better cost per transaction throughput. </p>\n<blockquote>Fauna concentrated a significant amount of effort on scaling horizontally on commodity hardware.</blockquote>\n<p>A FaunaDB cluster does not have a single master node. Instead there are some nodes which take on a multiple responsibility. All nodes will store, retrieve, and route requests for data. In addition, one or more nodes may also handle the transaction log. If the transaction throughput is high, the ability to split the transaction workload amongst several nodes is essential. This is just one of the many ways the Fauna server demonstrates its horizontal scaling capabilities for large systems.</p>\n<h2>Replica</h2>\n<p>A replica is comprised of a group of one or more node(s). A node can belong only to a single replica. A fundamental property of a replica is that it contains a complete copy of the Fauna data, including end user data, system data, and the transaction log. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy provided by Fauna within a replica. Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster. Another common term for a replica would be a datacenter.</p>\n<blockquote>Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster.</blockquote>\n<p>A great advantage of Fauna when sizing the system is that each replica is capable of being a different type and size. A company which has a large presence in the South with its own IT infrastructure can host Fauna on-premise with more hardware and capacity. In this scenario, the same company is expanding on both the East and West coasts and the remote offices are small with limited IT infrastructure. These two replicas can be located in the cloud with a smaller hardware footprint. This allows a business to maximum cost effectiveness while enjoying “custom-fit” geographical proximity.</p>\n<h2>Cluster</h2>\n<p>A FaunaDB cluster is the highest entity. Under the cluster is both a physical layout and logical layout. To review, the physical layout it is comprised of individual nodes. Every node is a computer with a unique IP address. Nodes are grouped into replicas, with every node belonging to exactly one replica. The main significance for this grouping is that a replica contains a full copy of the data and can vary in size. Within a replica, a particular piece of data is normally found on exactly one node; there is no data redundancy within a replica.</p>\n<blockquote>Having multiple replicas, each containing the full set of data, is what provides redundancy in a Fauna database cluster.</blockquote>\n<p>The FaunaDB Cluster has a logical data model comprised of databases, collections, and documents. The data model implements a s<a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">emi-structured, schema free object-relational data model, a strict superset of the relational, document, object-oriented, and graph paradigms.</a> The model starts with the database which can be laid out in traditional flat fashion, in a hierarchical or a combination of flat and hierarchical as seen in Figure 3.</p>\n<figure><img src=\"https://user-images.githubusercontent.com/253/41186071-c1621618-6b44-11e8-9af3-a6ff14be3a85.png\" alt=\"\" /></figure><p>Figure 3</p>\n<p>While many vendors can have multiple databases in their system, few SQL or NoSQL vendors have the concept of a hierarchical database. This hierarchical organization of data provides many benefits in the multi-tenant scenario: </p>\n<ul><li>The ability for a single company to share resources between two, three or a yet to be determine number of projects by simply creating a database under or alongside another database. </li><li>The simplification of access control, resource allocation and sharing, and data layout, is kept uniform by simply inheriting the properties and resources. </li><li>Eliminates the need to manage properties and resources for each database individually. </li></ul><p>Below the database exists collections/classes which are similar to a table in a relational database. However, like a document database, full or partially shared schema within a class is optional, not mandatory. Collections/classes contain zero or more records, or semi-structured documents, which can include recursively nested objects and arrays as well as scalar types.</p>\n<h1>Conclusion</h1>\n<p>This introduction to both the physical and logical data models of FaunaDB clustering demonstrates the simple elegance behind its design. While many transactional databases increase their complexity when scaling and require specialized hardware, FaunaDB goes the extra mile to keep things simple while scaling both vertically and horizontally on all styles of hardware. It simply addresses not only a technical pain point, but also a critical business issue for all enterprises - large or small.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-06-06T00:42:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 523,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9800e61f-078a-4253-a523-14643e163054",
        "siteSettingsId": 523,
        "fieldLayoutId": 4,
        "contentId": 356,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Using ACID Transactions to Combine Queries and Ensure Integrity",
        "slug": "using-acid-transactions-to-combine-queries-and-ensure-integrity",
        "uri": "blog/using-acid-transactions-to-combine-queries-and-ensure-integrity",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-03-14T12:26:26-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/using-acid-transactions-to-combine-queries-and-ensure-integrity",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/using-acid-transactions-to-combine-queries-and-ensure-integrity",
        "isCommunityPost": false,
        "blogBodyText": "<p>ACID transactions are a key element of the success and applicability of RDBMS long term, as they provide the system with the flexibility to model aggregate data structures regardless of the data access pattern. Historically, ACID databases have been able to add new features that work with existing data, in part because of the integrity that can be maintained via transactions. &nbsp;In this blog post we’ll look at query examples that reflect real-world development challenges.</p>\r\n<p>Updating multiple documents in an ACID transaction is a near-universal pattern when querying FaunaDB. There are several ways to do it, from a bare list of operations, to an explicit Do statement, to using Map and Paginate to iterate over result sets. As you’ll notice, this article is organized by use case where you’ll see how to express each one in FaunaDB’s query language.</p>\r\n<h2>Create a New Workspace</h2>\r\n<p>Many applications set up new workspaces at various points in the application lifecycle. Whether it is creating a new gameworld for players to populate, a new rich document for users to collaborate on, or a new online store, some default schema and data items need to be provisioned. In a fast moving user-onboard flow, you don’t want users encountering an incoherent state. To prevent this from happening, create your default objects in a single transaction. </p>\r\n<blockquote>In a fast moving user-onboard flow, you don’t want users encountering an incoherent state. To prevent this from happening, create your default objects in a single transaction.</blockquote>\r\n<p>Here is a quick query that creates a few classes with their indexes in a single transaction. It’s from an example game, where it sets up the schema for a new gameworld.</p>\r\n<pre class=\"language-javascript\">q.Let({\r\n  players: q.Select(\"ref\",q.CreateClass({name: \"players\"})),\r\n  items : q.Select(\"ref\", q.CreateClass({name: \"items\"})),\r\n  purchases : q.Select(\"ref\", q.CreateClass({name: \"purchases\"}))},\r\nq.Do(\r\n  q.CreateIndex( {\r\n    name: \"players\",\r\n    source: q.Var(\"players\")\r\n  }),\r\n  q.CreateIndex( {\r\n    name: \"items_for_sale\",\r\n    source: q.Var(\"items\"),\r\n    terms: [{\r\n      field: [\"data\", \"for_sale\"]\r\n    }]\r\n  }),\r\n  q.CreateIndex( {\r\n    name: \"purchases\",\r\n    source: q.Var(\"purchases\")\r\n  }),\r\n  q.CreateIndex( {\r\n    name: \"items_by_owner\",\r\n    source: q.Var(\"items\"),\r\n    terms: [{\r\n      field: [\"data\", \"owner\"]\r\n    }]\r\n  })))</pre>\r\n<p>Here is another query in <a href=\"https://github.com/fauna/animal-exchange/blob/master/session-service/handler.js#L105\">the Animal Exchange example source code</a>, where all the animals are created and assigned owners.</p>\r\n<pre class=\"language-javascript\">const animals = [\"&#x1f404;\",\"&#x1f406;\",\"&#x1f43f;\",\"&#x1f407;\",\"&#x1f408;\",\"&#x1f40b;\",\"&#x1f40d;\",\"&#x1f40e;\",\"&#x1f412;\",\"&#x1f418;\",\r\n  \"&#x1f419;\",\"&#x1f41b;\",\"&#x1f41d;\",\"&#x1f41e;\",\"&#x1f423;\",\"&#x1f42c;\",\"&#x1f42f;\",\"&#x1f438;\",\"&#x1f439;\",\"&#x1f429;\"].map((emoji) =&gt; {\r\n    return {\r\n      label : emoji,\r\n      for_sale : Math.random() &lt; 0.2,\r\n      owner : players[Math.floor(Math.random()*players.length)].ref,\r\n      price : Math.ceil(Math.random()*40)\r\n    }\r\n});\r\nreturn client.query(\r\n  q.Foreach(animals, (animal) =&gt;\r\n    q.Create(q.Class(\"items\"), {data : animal}))\r\n);</pre>\r\n<p>You’ll also see queries like this when you are saving a complex UI corresponding to many user fields. Each control or field in the UI can be persisted with its own logic, and all updates composed into a single transaction.</p>\r\n<h2>Sell Items Between Accounts</h2>\r\n<p>Typically, when implementing an ecommerce transaction, the preconditions include as much logic as the transfer itself. Before an item can be purchased, the system must ensure it’s for sale, in stock, and that the purchaser can afford it. Only after the preconditions have passed does it make sense to move funds from buyer to seller, change item ownership, and/or reduce stock levels.</p>\r\n<blockquote>Before an item can be purchased, the system must ensure it’s for sale, in stock, and that the purchaser can afford it.&nbsp;</blockquote>\r\n<p>In FaunaDB, complex transactions are described and sent to the server all at once. So, a transaction transfering an items from a buyer to a seller will look something like this:</p>\r\n<pre class=\"language-javascript\">q.Let({\r\n  buyer : q.Get(player.ref),\r\n  item : q.Get(item.ref)\r\n}, q.Let({\r\n  isForSale : q.Select([\"data\", \"for_sale\"], q.Var(\"item\")),\r\n  itemPrice : q.Select([\"data\", \"price\"], q.Var(\"item\")),\r\n  buyerBalance : q.Select([\"data\", \"credits\"], q.Var(\"buyer\")),\r\n  seller : q.Get(q.Select([\"data\", \"owner\"], q.Var(\"item\")))\r\n}, q.If(q.Not(q.Var(\"isForSale\")),\r\n    \"purchase failed: item not for sale\",\r\n    q.If(q.Equals(q.Select(\"ref\", q.Var(\"buyer\")), q.Select(\"ref\", q.Var(\"seller\"))),\r\n      q.Do(\r\n        q.Update(q.Select(\"ref\", q.Var(\"item\")), {\r\n          data : {\r\n            for_sale : false\r\n          }\r\n        }),\r\n        \"item removed from sale\"\r\n      ),\r\n      // check balance\r\n      q.If(q.LT(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\")),\r\n        \"purchase failed: insufficient funds\",\r\n        // all clear! record the purchase, update the buyer, seller and item.\r\n        q.Do(\r\n          q.Create(q.Class(\"purchases\"), {\r\n            data : {\r\n              item : q.Select(\"ref\", q.Var(\"item\")),\r\n              price : q.Var(\"itemPrice\"),\r\n              buyer : q.Select(\"ref\", q.Var(\"buyer\")),\r\n              seller : q.Select(\"ref\", q.Var(\"seller\"))\r\n            }\r\n          }),\r\n          q.Update(q.Select(\"ref\", q.Var(\"buyer\")), {\r\n            data : {\r\n              credits : q.Subtract(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\"))\r\n            }\r\n          }),\r\n          q.Update(q.Select(\"ref\", q.Var(\"seller\")), {\r\n            data : {\r\n              credits : q.Add(q.Select([\"data\", \"credits\"], q.Var(\"seller\")), q.Var(\"itemPrice\"))\r\n            }\r\n          }),\r\n          q.Update(q.Select(\"ref\", q.Var(\"item\")), {\r\n            data : {\r\n              owner : q.Select(\"ref\", q.Var(\"buyer\")),\r\n              for_sale : false\r\n            }\r\n          }),\r\n          \"purchase success\"\r\n        )\r\n      )\r\n    )\r\n   )))\r\n);</pre>\r\n<p>To see a full breakdown of a similar query, see my <a href=\"https://blog.fauna.com/talk-video-build-a-serverless-distributed-ledger-without-the-blockchain\">talk at GOTO Berlin about distributed ledgers</a>, or read <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">this article introducing an example distributed ledger application</a>. The principles and concepts behind ledger queries are broadly applicable to other applications, making the code worth studying further.</p>\r\n<h2>Update Matching Records</h2>\r\n<p>If you need to update multiple matching records in a single query, you can do it by looping over the result set and running update logic. This can be useful any time you want to correct a batch of data, or make special case changes that impact the records which match a query. In the following query, we find customers with a pro plan in the finance industry and update their marketing-qualified-lead level to increase by 50 percent.</p>\r\n<pre class=\"language-javascript\">q.Map(q.Paginate(q.Match(q.Index(\"customers-by-plan\"), \"pro\")),\r\nfunction(row) { \r\n    return q.Let({customer: q.Get(q.Select(0, row))}, )\r\n        q.If(q.Equals(\"finance\", q.Select([\"data\",\"industry\"], q.Var(\"customer\")),\r\n            q.Update(q.Var(\"customer\")), {data : {\r\n                mqlScore: q.Multiply(1.5,q.Select([\"data\",\"mqlScore\"], q.Var(\"customer\")))\r\n        }})\r\n})</pre>\r\n<p>In many cases this logic does not be performed transactionally. However, in cases where a discount is required to be available to a subset of customers simultaneously, ACID transactions are necessary.</p>\r\n<h2>Maintain Aggregates</h2>\r\n<p>An important use for ACID transactions is in maintaining custom aggregates alongside your data changes. If you need to maintain a counter or other <a href=\"https://ai.google/research/pubs/pub61\">aggregate data structure</a> as you write, you can apply changes to that document at the same time as you’re update the other documents. In the example we’ll add a record and update a counter in a single transaction.</p>\r\n<blockquote>An important use for ACID transactions is in maintaining custom aggregates alongside your data changes.</blockquote>\r\n<p>First, we’ll need a class to store our aggregates in. Let’s imagine we want to maintain more than one type of aggregate in our application, so we’ll add a tag field.</p>\r\n<pre class=\"language-javascript\">q.CreateClass({name:\"aggregates\"})</pre>\r\n<p>We can use this class to store all the aggregates in our system, so we can index the aggregates by tag for reading. </p>\r\n<pre class=\"language-javascript\">q.CreateIndex(\r\n    {\r\n      name: \"aggregates-by-tag\",\r\n      source: q.Class(\"aggregates\"),\r\n      terms: [{ field: [\"data\", \"tag\"] }],\r\n      values: [{ field: [\"data\", \"value\", \"count\"] },\r\n        { field: [\"data\", \"value\", \"sum\"] },\r\n        { field: [\"ref\"] }]\r\n    })</pre>\r\n<p>Now, say we want to create an aggregate to keep stats like counts and averages across our purchases. We can create a document with the tag we plan to use and seed it with an empty counter.</p>\r\n<pre class=\"language-javascript\">q.Create(q.Class(\"aggregates\"), {data : \r\n    {tag : \"purchase-stats\", value : {count : 0, sum: 0}})</pre>\r\n<p>Now that we have prepared our aggregate, we need to keep it up to date as we make transactions. To do this, we can cache the Ref of the aggregate document in our application. This query should run once per process start, and the Ref can be kept around.</p>\r\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"aggregates-by-tag\"), \"purchase-stats\"), \r\n    function(row) { return q.Select(2, row)})</pre>\r\n<p>This will return all of the Refs for aggregates for the \"purchase-stats\" tag. In a moment we’ll discuss why there might be more than one. For now we can just hang onto one.</p>\r\n<pre class=\"language-javascript\">var myPurchaseStatsRef = aggregateRefs[Math.floor(Math.random()*aggregateRefs.length)];</pre>\r\n<p>On insert, we update a the aggregate document with statistics each time we write new data. In practice it might look like this:</p>\r\n<pre class=\"language-javascript\">var newPurchase = {amount : 40, items : [\"...\"]}\r\nclient.query(\r\n  q.Do(\r\n    q.Let({\"stats\": q.Get(myPurchaseStatsRef)}, \r\n      q.Update(myPurchaseStatsRef, {data : { value : { \r\n        count : q.Add(1, q.Select([\"data\",\"value\",\"count\"], q.Var(\"stats\"))),\r\n        sum : q.Add(newPurchase.amount, q.Select([\"data\",\"value\",\"sum\"], q.Var(\"stats\")))\r\n    }})), \r\n    q.Create(q.Class(\"purchases\"), {data : newPurchase})\r\n  )\r\n)</pre>\r\n<p>It’s important to note that we use q.Do to join multiple statements into a single transaction. This serializes access to the aggregate document, so that it always reflects correct values. However, under heavy concurrent load, this means all requests are waiting for their turn to update the aggregate. &nbsp;Best practice would be to spread that load across multiple documents. Luckily, just by issuing another create query for a purchase-status document with a count of zero, the code presented above will bind randomly to one of the stats documents. The optimal number of stats documents is probably somewhere between the number of machines in your FaunaDB replica and the number of concurrent application server process you’re running.</p>\r\n<blockquote>On insert, we update a the aggregate document with statistics each time we write new data.</blockquote>\r\n<p>To query your replica, you can fetch the values from the index and reduce them in your application.</p>\r\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"aggregates-by-tag\"), \"purchase-stats\"))</pre>\r\n<p>This will return an array of count / sum pairs, and you can add them up and divide to find the total average. If scale is not an issue for your application, you can skip the sharding, and just create a single key, but the solution complexity with a sharded aggregate is low enough that I’d recommend using it as the first option.</p>\r\n<h2>Conclusion</h2>\r\n<p>FaunaDB transactions can easily accommodate complex logic. Query language features like iteration, the Do statement, and precondition logic like If can be combined to express your business operations in queries that are easy to read and develop as your native programming language.</p>\r\n<p>If you want to learn more about <a href=\"https://fauna.com/documentation/reference/queryapi\">FaunaDB’s query API, read the documentation here</a>. Or read here to <a href=\"https://blog.fauna.com/the-life-of-a-faunadb-query\">learn more about how queries are processed</a>.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>ACID transactions are a key element of the success and applicability of RDBMS long term, as they provide the system with the flexibility to model aggregate data structures regardless of the data access pattern. Historically, ACID databases have been able to add new features that work with existing data, in part because of the integrity that can be maintained via transactions. In this blog post we’ll look at query examples that reflect real-world development challenges.</p>\n<p>Updating multiple documents in an ACID transaction is a near-universal pattern when querying FaunaDB. There are several ways to do it, from a bare list of operations, to an explicit Do statement, to using Map and Paginate to iterate over result sets. As you’ll notice, this article is organized by use case where you’ll see how to express each one in FaunaDB’s query language.</p>\n<h2>Create a New Workspace</h2>\n<p>Many applications set up new workspaces at various points in the application lifecycle. Whether it is creating a new gameworld for players to populate, a new rich document for users to collaborate on, or a new online store, some default schema and data items need to be provisioned. In a fast moving user-onboard flow, you don’t want users encountering an incoherent state. To prevent this from happening, create your default objects in a single transaction. </p>\n<blockquote>In a fast moving user-onboard flow, you don’t want users encountering an incoherent state. To prevent this from happening, create your default objects in a single transaction.</blockquote>\n<p>Here is a quick query that creates a few classes with their indexes in a single transaction. It’s from an example game, where it sets up the schema for a new gameworld.</p>\n<pre class=\"language-javascript\">q.Let({\n players: q.CreateClass({name: \"players\"}),\n items : q.CreateClass({name: \"items\"}),\n purchases : q.CreateClass({name: \"purchases\"})},\nq.Do(\n q.CreateIndex( {\n name: \"players\",\n source: q.Var(\"players\")\n }),\n q.CreateIndex( {\n name: \"items_for_sale\",\n source: q.Var(\"items\"),\n terms: [{\n field: [\"data\", \"for_sale\"]\n }]\n }),\n q.CreateIndex( {\n name: \"purchases\",\n source: q.Var(\"purchases\")\n }),\n q.CreateIndex( {\n name: \"items_by_owner\",\n source: q.Var(\"items\"),\n terms: [{\n field: [\"data\", \"owner\"]\n }]\n })))</pre>\n<p>Here is another query in <a href=\"https://github.com/fauna/animal-exchange/blob/master/session-service/handler.js#L105\">the Animal Exchange example source code</a>, where all the animals are created and assigned owners.</p>\n<pre class=\"language-javascript\">const animals = [\"&#x1f404;\",\"&#x1f406;\",\"&#x1f43f;\",\"&#x1f407;\",\"&#x1f408;\",\"&#x1f40b;\",\"&#x1f40d;\",\"&#x1f40e;\",\"&#x1f412;\",\"&#x1f418;\",\n \"&#x1f419;\",\"&#x1f41b;\",\"&#x1f41d;\",\"&#x1f41e;\",\"&#x1f423;\",\"&#x1f42c;\",\"&#x1f42f;\",\"&#x1f438;\",\"&#x1f439;\",\"&#x1f429;\"].map((emoji) =&gt; {\n return {\n label : emoji,\n for_sale : Math.random() &lt; 0.2,\n owner : players[Math.floor(Math.random()*players.length)].ref,\n price : Math.ceil(Math.random()*40)\n }\n});\nreturn client.query(\n q.Foreach(animals, (animal) =&gt;\n q.Create(q.Class(\"items\"), {data : animal}))\n);</pre>\n<p>You’ll also see queries like this when you are saving a complex UI corresponding to many user fields. Each control or field in the UI can be persisted with its own logic, and all updates composed into a single transaction.</p>\n<h2>Sell Items Between Accounts</h2>\n<p>Typically, when implementing an ecommerce transaction, the preconditions include as much logic as the transfer itself. Before an item can be purchased, the system must ensure it’s for sale, in stock, and that the purchaser can afford it. Only after the preconditions have passed does it make sense to move funds from buyer to seller, change item ownership, and/or reduce stock levels.</p>\n<blockquote>Before an item can be purchased, the system must ensure it’s for sale, in stock, and that the purchaser can afford it. </blockquote>\n<p>In FaunaDB, complex transactions are described and sent to the server all at once. So, a transaction transfering an items from a buyer to a seller will look something like this:</p>\n<pre class=\"language-javascript\">q.Let({\n buyer : q.Get(player.ref),\n item : q.Get(item.ref)\n}, q.Let({\n isForSale : q.Select([\"data\", \"for_sale\"], q.Var(\"item\")),\n itemPrice : q.Select([\"data\", \"price\"], q.Var(\"item\")),\n buyerBalance : q.Select([\"data\", \"credits\"], q.Var(\"buyer\")),\n seller : q.Get(q.Select([\"data\", \"owner\"], q.Var(\"item\")))\n}, q.If(q.Not(q.Var(\"isForSale\")),\n \"purchase failed: item not for sale\",\n q.If(q.Equals(q.Select(\"ref\", q.Var(\"buyer\")), q.Select(\"ref\", q.Var(\"seller\"))),\n q.Do(\n q.Update(q.Select(\"ref\", q.Var(\"item\")), {\n data : {\n for_sale : false\n }\n }),\n \"item removed from sale\"\n ),\n // check balance\n q.If(q.LT(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\")),\n \"purchase failed: insufficient funds\",\n // all clear! record the purchase, update the buyer, seller and item.\n q.Do(\n q.Create(q.Class(\"purchases\"), {\n data : {\n item : q.Select(\"ref\", q.Var(\"item\")),\n price : q.Var(\"itemPrice\"),\n buyer : q.Select(\"ref\", q.Var(\"buyer\")),\n seller : q.Select(\"ref\", q.Var(\"seller\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"buyer\")), {\n data : {\n credits : q.Subtract(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"seller\")), {\n data : {\n credits : q.Add(q.Select([\"data\", \"credits\"], q.Var(\"seller\")), q.Var(\"itemPrice\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"item\")), {\n data : {\n owner : q.Select(\"ref\", q.Var(\"buyer\")),\n for_sale : false\n }\n }),\n \"purchase success\"\n )\n )\n )\n )))\n);</pre>\n<p>To see a full breakdown of a similar query, see my <a href=\"https://blog.fauna.com/talk-video-build-a-serverless-distributed-ledger-without-the-blockchain\">talk at GOTO Berlin about distributed ledgers</a>, or read <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">this article introducing an example distributed ledger application</a>. The principles and concepts behind ledger queries are broadly applicable to other applications, making the code worth studying further.</p>\n<h2>Update Matching Records</h2>\n<p>If you need to update multiple matching records in a single query, you can do it by looping over the result set and running update logic. This can be useful any time you want to correct a batch of data, or make special case changes that impact the records which match a query. In the following query, we find customers with a pro plan in the finance industry and update their marketing-qualified-lead level to increase by 50 percent.</p>\n<pre class=\"language-javascript\">q.Map(q.Paginate(q.Match(q.Index(\"customers-by-plan\"), \"pro\")),\nfunction(row) { \n return q.Let({customer: q.Get(q.Select(0, row))}, )\n q.If(q.Equals(\"finance\", q.Select([\"data\",\"industry\"], q.Var(\"customer\")),\n q.Update(q.Var(\"customer\")), {data : {\n mqlScore: q.Multiply(1.5,q.Select([\"data\",\"mqlScore\"], q.Var(\"customer\")))\n }})\n})</pre>\n<p>In many cases this logic does not be performed transactionally. However, in cases where a discount is required to be available to a subset of customers simultaneously, ACID transactions are necessary.</p>\n<h2>Maintain Aggregates</h2>\n<p>An important use for ACID transactions is in maintaining custom aggregates alongside your data changes. If you need to maintain a counter or other <a href=\"https://ai.google/research/pubs/pub61\">aggregate data structure</a> as you write, you can apply changes to that document at the same time as you’re update the other documents. In the example we’ll add a record and update a counter in a single transaction.</p>\n<blockquote>An important use for ACID transactions is in maintaining custom aggregates alongside your data changes.</blockquote>\n<p>First, we’ll need a class to store our aggregates in. Let’s imagine we want to maintain more than one type of aggregate in our application, so we’ll add a tag field.</p>\n<pre class=\"language-javascript\">q.CreateClass({name:\"aggregates\"})</pre>\n<p>We can use this class to store all the aggregates in our system, so we can index the aggregates by tag for reading. </p>\n<pre class=\"language-javascript\">q.CreateIndex(\n {\n name: \"aggregates-by-tag\",\n source: q.Class(\"aggregates\"),\n terms: [{ field: [\"data\", \"tag\"] }],\n values: [{ field: [\"data\", \"value\"] },{ field: [\"ref\"] }]\n })</pre>\n<p>Now, say we want to create an aggregate to keep stats like counts and averages across our purchases. We can create a document with the tag we plan to use and seed it with an empty counter.</p>\n<pre class=\"language-javascript\">q.Create(q.Class(\"aggregates\"), {data : \n {tag : \"purchase-stats\", value : {count : 0, sum: 0}})</pre>\n<p>Now that we have prepared our aggregate, we need to keep it up to date as we make transactions. To do this, we can cache the Ref of the aggregate document in our application. This query should run once per process start, and the Ref can be kept around.</p>\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"aggregates-by-tag\"), \"purchase-stats\"), \n function(row) { return q.Select(1, row)})</pre>\n<p>This will return all of the Refs for aggregates for the \"purchase-stats\" tag. In a moment we’ll discuss why there might be more than one. For now we can just hang onto one.</p>\n<pre class=\"language-javascript\">var myPurchaseStatsRef = aggregateRefs[Math.floor(Math.random()*aggregateRefs.length)];</pre>\n<p>On insert, we update a the aggregate document with statistics each time we write new data. In practice it might look like this:</p>\n<pre class=\"language-javascript\">var newPurchase = {amount : 40, items : [\"...\"]}\nclient.query(\n q.Do(\n q.Let({\"stats\": q.Get(myPurchaseStatsRef)}, \n q.Update(myPurchaseStatsRef, {data : { value : { \n count : q.Add(1, q.Select([\"data\",\"value\",\"count\"], q.Var(\"stats\"))),\n sum : q.Add(newPurchase.amount, q.Select([\"data\",\"value\",\"sum\"], q.Var(\"stats\")))\n }})), \n q.Create(q.Class(\"purchases\"), {data : newPurchase})\n )\n)</pre>\n<p>It’s important to note that we use q.Do to join multiple statements into a single transaction. This serializes access to the aggregate document, so that it always reflects correct values. However, under heavy concurrent load, this means all requests are waiting for their turn to update the aggregate. Best practice would be to spread that load across multiple documents. Luckily, just by issuing another create query for a purchase-status document with a count of zero, the code presented above will bind randomly to one of the stats documents. The optimal number of stats documents is probably somewhere between the number of machines in your FaunaDB replica and the number of concurrent application server process you’re running.</p>\n<blockquote>On insert, we update a the aggregate document with statistics each time we write new data.</blockquote>\n<p>To query your replica, you can fetch the values from the index and reduce them in your application.</p>\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"aggregates-by-tag\"), \"purchase-stats\"), \n function(row) { return row[0] })</pre>\n<p>This will return an array of count / sum pairs, and you can add them up and divide to find the total average. If scale is not an issue for your application, you can skip the sharding, and just create a single key, but the solution complexity with a sharded aggregate is low enough that I’d recommend using it as the first option.</p>\n<h2>Conclusion</h2>\n<p>FaunaDB transactions can easily accommodate complex logic. Query language features like iteration, the Do statement, and precondition logic like If can be combined to express your business operations in queries that are easy to read and develop as your native programming language.</p>\n<p>If you want to learn more about <a href=\"https://fauna.com/documentation/reference/queryapi\">FaunaDB’s query API, read the documentation here</a>. Or read here to <a href=\"https://blog.fauna.com/the-life-of-a-faunadb-query\">learn more about how queries are processed</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-06-01T15:52:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 519,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "f80c699f-4ce7-4c1b-8909-0f1f70da394e",
        "siteSettingsId": 519,
        "fieldLayoutId": 4,
        "contentId": 352,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Video: Evan and Matt Share Their Vision for a Mission Critical Operational Database",
        "slug": "video-evan-and-matts-vision-for-a-mission-critical-operational-database",
        "uri": "blog/video-evan-and-matts-vision-for-a-mission-critical-operational-database",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/video-evan-and-matts-vision-for-a-mission-critical-operational-database",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/video-evan-and-matts-vision-for-a-mission-critical-operational-database",
        "isCommunityPost": false,
        "blogBodyText": "<p>Learn how Evan and Matt’s experience scaling Twitter inspired them to fill the gap left by existing database technologies. While all of the legacy SQL and NoSQL options require businesses to make tradeoffs, FaunaDB is based on real-world business needs, and designed to be a general purpose solution.&nbsp;</p>\r\n<p>\r\n</p>\r\n\r\n<figure><iframe width=\"500\" height=\"281\" src=\"//www.youtube.com/embed/1dIIk69RJ8o\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure>\r\n<p>Based on their experience building Twitter’s scalable storage, they saw that there was nothing in the marketplace that could meet common requirements for enterprise and consumer databases:</p>\r\n<ul>\r\n<li>Global scale</li>\r\n<li>Strong consistency with ACID transactions</li>\r\n<li>Flexible data modeling including joins, foreign keys, constraints, and views</li>\r\n<li>Operational safety and simplicity</li>\r\n</ul>\r\n<p>FaunaDB aims to be the broadly capable data platform our founders wished they could have used at Twitter. Read more about <a href=\"{entry:53:url}\">the story and the vision that inspired FaunaDB</a>, or <a href=\"https://fauna.com/whitepaper\">learn more about the database itself</a>.&nbsp;</p>",
        "blogCategory": [
            "8",
            "1461"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Learn how Evan and Matt’s experience scaling Twitter inspired them to fill the gap left by existing database technologies. While all of the legacy SQL and NoSQL options require businesses to make tradeoffs, FaunaDB is based on real-world business needs, and designed to be a general purpose solution. </p>\n<p>\n</p>\n<figure></figure><p>Based on their experience building Twitter’s scalable storage, they saw that there was nothing in the marketplace that could meet common requirements for enterprise and consumer databases:</p>\n<ul><li>Global scale</li>\n<li>Strong consistency with ACID transactions</li>\n<li>Flexible data modeling including joins, foreign keys, constraints, and views</li>\n<li>Operational safety and simplicity</li>\n</ul><p>FaunaDB aims to be the broadly capable data platform our founders wished they could have used at Twitter. Read more about <a href=\"{entry:53:url||https://fauna.com/team/evan}\">the story and the vision that inspired FaunaDB</a>, or <a href=\"https://fauna.com/whitepaper\">learn more about the database itself</a>. </p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-05-30T21:28:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 518,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "3af1e1fa-cbae-4893-8287-2adc143507ca",
        "siteSettingsId": 518,
        "fieldLayoutId": 4,
        "contentId": 351,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Data Security in the Age of Serverless Apps",
        "slug": "data-security-in-the-age-of-cloud-native-apps",
        "uri": "blog/data-security-in-the-age-of-cloud-native-apps",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-08-21T14:33:39-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/data-security-in-the-age-of-cloud-native-apps",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/data-security-in-the-age-of-cloud-native-apps",
        "isCommunityPost": false,
        "blogBodyText": "<p>Cloud native deployments without cloud-native technology can increase the chances of unexpected access to services and data. In 2017, <a href=\"https://www.zdnet.com/article/mongodb-ransacking-starts-again-hackers-ransom-26000-unsecured-instances/\">26,000 unsecured MongoDB instances were deleted</a> in addition to <a href=\"https://krebsonsecurity.com/tag/mongodb/\">other high-profile breaches</a>. A cloud native database can’t encourage insecure practices in development and then expect production deployments to follow best practices. That’s why every connection to FaunaDB uses a key secret corresponding to a particular level of access, and why FaunaDB makes it easy to model application security in the database. This article will show you why modeling security in the database is good for more than just defense-in-depth, when your database understands your data you get additional security and flexibility.</p>\r\n<blockquote>FaunaDB’s security model is applied to every query in the API.</blockquote>\r\n<figure style=\"float: left; margin: 0px 10px 10px 0px;\"><img src=\"{asset:416:url}\" data-image=\"92hwcfjx7wqr\"></figure>\r\n<p>With GDPR coming into effect, now is a good time to think about data security. FaunaDB offers several security APIs that can support your privacy initiatives. What you can learn from this article:</p>\r\n<ul><li>How FaunaDB features like access keys, tokens, delegation, and data expiry can be used to ensure privacy.</li><li>How common application scenarios like multi-tenant SaaS hosting, social content publishing, and collaborative workgroups can model security in the database.</li><li>How database security features can support GDPR principles.</li></ul>\r\n<h1>FaunaDB Security API</h1>\r\n<p>FaunaDB’s security model is applied to every query in the API. Each client is instantiated with a connection secret, which corresponds to an access key. Access keys can either have database level access, or object level access. Administrative keys are not used for data operations, a default choice designed to limit proliferation of dangerous keys.</p>\r\n<h3>Database Access Keys</h3>\r\n<p>Database keys can be one of a few levels: admin, server, server-readonly, and client. These apply to a database and for admin keys, to the ability to manage sub-databases in the multi-tenant tree.</p>\r\n<p><strong>Admin keys</strong> are used to create databases, and set priority levels. An admin key in a particular database can create sub-databases in that database, provision access keys for sub-databases, and update the priority and other settings for each sub-database. Priority is inherited from parent databases. More about <a href=\"https://blog.fauna.com/secure-hierarchical-multi-tenancy-patterns\">administering the database hierarchical multi-tenancy tree in this article.</a></p>\r\n<p>When you use the dashboard or another tool to create a database in FaunaDB, the connection uses an admin key. Simple apps will only require one database, but most FaunaDB deployments will use hierarchy to define resource and access sharing. Admin keys lay out production, staging, and development environments, and can be used at runtime for applications which self-provision databases (for new customer signup, for instance). </p>\r\n<p><strong>Server keys</strong> are used when you want to work inside a database. Use an admin key (or the dashboard) to create a server key for the database you want to store data in. Server keys are the ones you’ll end up pasting into your code following best practices to deliver to your application runtime. Server keys can write to any class and bypass access control rules, so in applications where you are using object-level access control (discussed below) you’ll primarily use server keys for schema manipulations and administrative queries. In applications where the data is all part of the same security context, your application might do all its work with server keys. This is probably the majority of applications, because let’s face it: no matter how easy the API is, many applications that could benefit from object level security at the database layer never get around to modeling it. Hopefully, this post shows you enough of the benefits of deeper security modeling, especially around GDPR and similar constraints, that you consider applying these lessons in your app.</p>\r\n<blockquote>Many applications that could benefit from object level security at the database layer never get around to modeling it.</blockquote>\r\n<p><strong>Readonly keys</strong> are a kind of server key which cannot modify data. This is nice because it’s less dangerous to run queries which skip the access control policy if you know they are read only. These are the keys I’d give to a more-privileged process that doesn’t need to write, but needs to do arbitrary reads. I’d still suggest writing through a token-based connection to take advantage of ACL features, as some constraints are imposed for correctness reasons, and being forced to write through a <a href=\"https://fauna.com/documentation/reference/glossary#user-defined-functions\">user-defined function</a> or other defined interface can be less risky to the organization.</p>\r\n<p><strong>Client keys</strong> can only access objects which have declared “public” in their ACL. So for instance to allow for anonymous self-service provisioning of new users through a FaunaDB client connection, you can create a user class and mark it’s creation permission as “public.” Then provision a FaunaDB client key and paste it in your code. If you use an index to find which object to login as, you’ll need to mark that index as “public” too. Visitors can issue queries to create new users, and to look themselves up by login token and test credentials. More about that in the next section.</p>\r\n<h3>Object Access Control</h3>\r\n<p>When you decide to model security at the application layer, a lot of your work takes place at the object level. Constructions like single user data repositories, social activity feeds, and collaborative workspaces are all built on simple FaunaDB primitives like access control lists, authentication tokens, access delegation, and document expiry.</p>\r\n<p>This is done by tagging documents with <strong>access control lists</strong> (ACLs) that correspond to who can do what kind of operations on the document. FaunaDB’s API is flexible enough to express role-based access control (RBAC) and other access control patterns in terms of object ACLs and the objects they list.</p>\r\n<p>A database connection can be established on behalf of any document that has credentials, by provisioning a <strong>token</strong> for those credentials and <a href=\"https://fauna.com/documentation/intro/security#token-access\">using the token as a connection string</a>. The connection established using this token will have access to the any documents which list the credentialed document in the ACL.</p>\r\n<p>Listing every user and process that has access to a document as an individual entry in the ACL can get cumbersome, and granting access to every member of a class limits flexibility. Access <strong>delegation</strong> allows objects to grant their access capabilities to other objects. So for instance, all real estate listings could list the agent-team document as a writer in their ACL. The agent-team document lists all the members of the team as delegates, so they can modify listings without being directly included in the listing ACL. That way, if team membership changes, you only have to change the agent-team document, not every single document with an ACL. It also means users can have more than one reason they can apply a certain operation, so security roles are composable.</p>\r\n<blockquote>Designing for privacy at all levels prevents data from accidentally being used for purposes other than that for which consent was given.</blockquote>\r\n<p>FaunaDB’s <strong>time-to-live document expiry</strong> (TTL) feature can automatically remove old data. An app built around a 24-hour lifetime for social content could easily and automatically tag all content to expire after a day. GDPR’s principle of data minimization suggests TTLs should be applied to most intermediate data created by data processors, so that data artifacts created as part of generating recommendations or other processing are automatically deleted once their purpose has been served. Designing for privacy at all levels prevents data from accidentally being used for purposes other than that for which consent was given. By deleting data automatically, the chances of unwittingly violating GDPR are reduced.</p>\r\n<h1>Modeling Application Security</h1>\r\n<p>FaunaDB’s security focus encompasses a wide variety of modern application paradigms. In the following sections, we’ll talk about three patterns that encompass a wide swath of the application types that exist today. These patterns are multi-tenant SaaS hosting, social content publishing, and collaborative workgroups. When applications model security constraints in the database using FaunaDB, GDPR principles like data minimisation, the right to access and the right to erasure can be supported with features like automatic data expiry, authentication, authorization, and access delegation. </p>\r\n<blockquote>When applications model security constraints in the database using FaunaDB, GDPR principles like data minimisation, the right to access and the right to erasure can be supported with features like automatic data expiry, authentication, authorization, and access delegation.</blockquote>\r\n<h2>SaaS Data Repository</h2>\r\n<p>If your service is offered in the cloud as a stand-alone customer experience, for instance in applications like vertical-specific enterprise SaaS, individual user storage and sync, or self-contained game worlds, the SaaS data repository layout is for you. FaunaDB multi-tenancy means a single cluster can support an arbitrarily complex tree of databases, inheriting access control and QoS prioritization.</p>\r\n<p>In database management systems without support for multi-tenancy, the complexity of maintaining a database per tenant can overwhelm the benefits. In my experience discovering customer requirements at Couchbase, we saw a hunger for simpler multi-tenant management and resource control, especially when data sharing requirements are more complex for instance in telecom or banking use cases that wanted to create a database per account. FaunaDB’s hierarchical approach makes it easier to manage quality-of-service (QoS) for a large fleet of databases, and support a mixture of popular and less used databases. Read about the <a href=\"https://blog.fauna.com/secure-hierarchical-multi-tenancy-patterns\">patterns that can be achieved with FaunaDB’s hierarchical multi-tenancy</a>.</p>\r\n<p>Keeping user and customer data in isolated databases is aligned with GDPR principles, because it is easy to manage a single customer’s database as a unit, and harder to run queries aggregating user behavior from users who don’t opt-in. Databases can be deleted or copied, granting right of erasure and access. A database-per-customer design is also congruent with the principles of consent and data minimisation, as access for processors can be granted and revoked for the entire data set, and old customer accounts, inactive game worlds, or unused individual storage areas can be deleted. In addition to deletion and access control, Fauna is working on a locality control feature to allow replication topology to be configured per-database, so you can manage a single cluster, but keep each customer’s data inside its jurisdiction.</p>\r\n<p>This multi-tenancy pattern is concerned with databases, FaunaDB’s container for data. If your customer application is multi-user, like retail, hospitality, or compliance, the object level patterns in the rest of the article will also apply. They are more fine grained than the data container, and can model most shared and collaborative data flows, within an individual database.</p>\r\n<h2>Social Publishing</h2>\r\n<p>In a social publishing app, users can post content and follow other users. The main screen of the app is the recent content from everyone the user follows. In this model, objects can be created and read by anyone in the user class, but only updated by their original author. Additionally, users can grant and revoke permissions for the application to run certain kinds of processing on their data, depending on which application feature-sets the user engages in. Querying a model like this is discussed in <a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">our post about how FaunaDB unifies document, relational, graph, and temporal data models</a>.</p>\r\n<figure><img src=\"{asset:462:url}\" data-image=\"jx7m8cuudomy\"></figure>\r\n<p></p>\r\n<p><a href=\"https://gdpr-info.eu/chapter-2/\">GDPR’s principle of data minimisation</a> requires applications to collect only the data they need, and use it only for the purpose for which consent was given. In our application, various kinds of processing can be run on user content, but only if the user opts in. So for instance, without consent for processing, users would not receive content recommendations. Additionally, the push notification process can only access the data of users who opt in, and similarly users who have not consented to ad targeting will only see un-targeted ads.</p>\r\n<blockquote>Users manage processing permissions by delegating access to their content via FaunaDB APIs.<br></blockquote>\r\n<p>This is implemented in FaunaDB with the following access control rules:</p>\r\n<ul><li>Connections to the database can be authenticated as a user instance or as a processing agent.</li><li>User instances can read and author social content, and update content for which they are the author. User instances can create and delete following relationships where they are the follower.</li><li>Processing agents can only read social content or following relationships for users that have opted-in to the particular feature the process supports. Users manage processing permissions by delegating access to their content via FaunaDB APIs.</li><li>Data stored by the processing agent in conjunction with a particular feature can only be read using the key corresponding to the user’s consent. If consent is revoked, the key is removed and access is no longer possible, even before data deletion takes place.</li></ul>\r\n<p>This pattern is also aligned with the GDPR’s right of erasure and right of access. By accessing the database via a user and their delegates, you can get an accurate snapshot of all the data stored for a particular user, for download or deletion.</p>\r\n<blockquote>By accessing the database via a user and their delegates, you can get an accurate snapshot of all the data stored for a particular user, for download or deletion.</blockquote>\r\n<p>In a production app, you might extend this model by decomposing the user’s access into capabilities, with a document per capability. So the user would have a document for messaging, one for photos, etc. As well as keys corresponding to access which are granted to particular processes belonging to the features which the user opted into. If the user later opts of of ad-targeting, push notifications, or content recommendation, the key is simply thrown away, and the data accessed and stored by those processes is no longer accessible.</p>\r\n<h2>Collaborative Workgroups</h2>\r\n<p>Collaborative workgroup patterns show up in all sorts of applications. Everything from shared document editing, to electronic medical records, to inventory tracking and retail point-of-sale shares a basic model where groups are defined and can share access to collaborative data. The important aspect of this model is defining a group, and then deciding which users are members of it, and which objects belong to the group.</p>\r\n<p>In the real estate listing example discussed at the beginning of the article, the group is the agent-team, the users are the agents, and the app objects are listings data, documents, onsite comments, photos, etc.</p>\r\n<figure><img src=\"{asset:463:url}\" data-image=\"rno4r6odz0x8\"></figure>\r\n<p>In FaunaDB the model above would be represented by tagging the app objects on the way in, with ACL entries granting the correct group(s) access to read, write and modify the objects themselves. By writing authorship and ownership information into your objects you can better track how data flows in your system.<br></p>\r\n<blockquote>FaunaDB’s temporal retention allows changes in data to be tracked over time.<br></blockquote>\r\n<p>One of the biggest challenges with the GDPR for collaborative workgroup software is the potential for collaborators to introduce personal data without proper safeguards. Knowing which collaborators had access to which application objects at which time can be crucial for containing breaches and mitigating leaks. FaunaDB’s temporal retention allows changes in data to be tracked over time. In this model you can query group membership and session status, for tracking and audit purposes.</p>\r\n<h1>Conclusion</h1>\r\n<p>Security and privacy go beyond the features offered by a database, and extend into how inviting it is to build programs that are aligned with principles like the GDPR. This article shows how common application scenarios, when built with FaunaDB’s security primitives, can be extended in alignment with GDPR principles. I hope that by showing this additional benefit to deep modeling of application security in the database, you are further convinced that it’s worth doing.</p>\r\n<p>If you are interested in another benefit of modeling application rules in the database, <a href=\"https://www.youtube.com/watch?v=iCqKeOHrb2k&index=17&list=PLnwBrRU5CSTmruZzR8Z06j3pGglBZcdDr\">take a look at this talk from ServerlessConf Austin about how I built the object security model for a multi-user TodoMVC clone.</a> The benefit here is that AWS Lambda functions can be supplied a FaunaDB access token by AWS API Gateway proxied requests, so your code connects to the database as the user who is using the app. Jump to 11 minutes in if you want to skip the FaunaDB pitch.</p>",
        "blogCategory": [
            "8",
            "10",
            "1530"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Cloud native deployments without cloud-native technology can increase the chances of unexpected access to services and data. In 2017, <a href=\"https://www.zdnet.com/article/mongodb-ransacking-starts-again-hackers-ransom-26000-unsecured-instances/\">26,000 unsecured MongoDB instances were deleted</a> in addition to <a href=\"https://krebsonsecurity.com/tag/mongodb/\">other high-profile breaches</a>. A cloud native database can’t encourage insecure practices in development and then expect production deployments to follow best practices. That’s why every connection to FaunaDB uses a key secret corresponding to a particular level of access, and why FaunaDB makes it easy to model application security in the database. This article will show you why modeling security in the database is good for more than just defense-in-depth, when your database understands your data you get additional security and flexibility.</p>\n<blockquote>FaunaDB’s security model is applied to every query in the API.</blockquote>\n<figure style=\"float:left;margin:0px 10px 10px 0px;\"><img src=\"{asset:416:url||https://fauna.com/assets/site/blog-legacy/db3.png}\" alt=\"\" /></figure><p>With GDPR coming into effect, now is a good time to think about data security. FaunaDB offers several security APIs that can support your privacy initiatives. What you can learn from this article:</p>\n<ul><li>How FaunaDB features like access keys, tokens, delegation, and data expiry can be used to ensure privacy.</li><li>How common application scenarios like multi-tenant SaaS hosting, social content publishing, and collaborative workgroups can model security in the database.</li><li>How database security features can support GDPR principles.</li></ul><h1>FaunaDB Security API</h1>\n<p>FaunaDB’s security model is applied to every query in the API. Each client is instantiated with a connection secret, which corresponds to an access key. Access keys can either have database level access, or object level access. Administrative keys are not used for data operations, a default choice designed to limit proliferation of dangerous keys.</p>\n<h3>Database Access Keys</h3>\n<p>Database keys can be one of a few levels: admin, server, server-readonly, and client. These apply to a database and for admin keys, to the ability to manage sub-databases in the multi-tenant tree.</p>\n<p><strong>Admin keys</strong> are used to create databases, and set priority levels. An admin key in a particular database can create sub-databases in that database, provision access keys for sub-databases, and update the priority and other settings for each sub-database. Priority is inherited from parent databases. More about <a href=\"https://blog.fauna.com/secure-hierarchical-multi-tenancy-patterns\">administering the database hierarchical multi-tenancy tree in this article.</a></p>\n<p>When you use the dashboard or another tool to create a database in FaunaDB, the connection uses an admin key. Simple apps will only require one database, but most FaunaDB deployments will use hierarchy to define resource and access sharing. Admin keys lay out production, staging, and development environments, and can be used at runtime for applications which self-provision databases (for new customer signup, for instance). </p>\n<p><strong>Server keys</strong> are used when you want to work inside a database. Use an admin key (or the dashboard) to create a server key for the database you want to store data in. Server keys are the ones you’ll end up pasting into your code following best practices to deliver to your application runtime. Server keys can write to any class and bypass access control rules, so in applications where you are using object-level access control (discussed below) you’ll primarily use server keys for schema manipulations and administrative queries. In applications where the data is all part of the same security context, your application might do all its work with server keys. This is probably the majority of applications, because let’s face it: no matter how easy the API is, many applications that could benefit from object level security at the database layer never get around to modeling it. Hopefully, this post shows you enough of the benefits of deeper security modeling, especially around GDPR and similar constraints, that you consider applying these lessons in your app.</p>\n<blockquote>Many applications that could benefit from object level security at the database layer never get around to modeling it.</blockquote>\n<p><strong>Readonly keys</strong> are a kind of server key which cannot modify data. This is nice because it’s less dangerous to run queries which skip the access control policy if you know they are read only. These are the keys I’d give to a more-privileged process that doesn’t need to write, but needs to do arbitrary reads. I’d still suggest writing through a token-based connection to take advantage of ACL features, as some constraints are imposed for correctness reasons, and being forced to write through a <a href=\"https://fauna.com/documentation/reference/glossary#user-defined-functions\">user-defined function</a> or other defined interface can be less risky to the organization.</p>\n<p><strong>Client keys</strong> can only access objects which have declared “public” in their ACL. So for instance to allow for anonymous self-service provisioning of new users through a FaunaDB client connection, you can create a user class and mark it’s creation permission as “public.” Then provision a FaunaDB client key and paste it in your code. If you use an index to find which object to login as, you’ll need to mark that index as “public” too. Visitors can issue queries to create new users, and to look themselves up by login token and test credentials. More about that in the next section.</p>\n<h3>Object Access Control</h3>\n<p>When you decide to model security at the application layer, a lot of your work takes place at the object level. Constructions like single user data repositories, social activity feeds, and collaborative workspaces are all built on simple FaunaDB primitives like access control lists, authentication tokens, access delegation, and document expiry.</p>\n<p>This is done by tagging documents with <strong>access control lists</strong> (ACLs) that correspond to who can do what kind of operations on the document. FaunaDB’s API is flexible enough to express role-based access control (RBAC) and other access control patterns in terms of object ACLs and the objects they list.</p>\n<p>A database connection can be established on behalf of any document that has credentials, by provisioning a <strong>token</strong> for those credentials and <a href=\"https://fauna.com/documentation/intro/security#token-access\">using the token as a connection string</a>. The connection established using this token will have access to the any documents which list the credentialed document in the ACL.</p>\n<p>Listing every user and process that has access to a document as an individual entry in the ACL can get cumbersome, and granting access to every member of a class limits flexibility. Access <strong>delegation</strong> allows objects to grant their access capabilities to other objects. So for instance, all real estate listings could list the agent-team document as a writer in their ACL. The agent-team document lists all the members of the team as delegates, so they can modify listings without being directly included in the listing ACL. That way, if team membership changes, you only have to change the agent-team document, not every single document with an ACL. It also means users can have more than one reason they can apply a certain operation, so security roles are composable.</p>\n<blockquote>Designing for privacy at all levels prevents data from accidentally being used for purposes other than that for which consent was given.</blockquote>\n<p>FaunaDB’s <strong>time-to-live document expiry</strong> (TTL) feature can automatically remove old data. An app built around a 24-hour lifetime for social content could easily and automatically tag all content to expire after a day. GDPR’s principle of data minimization suggests TTLs should be applied to most intermediate data created by data processors, so that data artifacts created as part of generating recommendations or other processing are automatically deleted once their purpose has been served. Designing for privacy at all levels prevents data from accidentally being used for purposes other than that for which consent was given. By deleting data automatically, the chances of unwittingly violating GDPR are reduced.</p>\n<h1>Modeling Application Security</h1>\n<p>FaunaDB’s security focus encompasses a wide variety of modern application paradigms. In the following sections, we’ll talk about three patterns that encompass a wide swath of the application types that exist today. These patterns are multi-tenant SaaS hosting, social content publishing, and collaborative workgroups. When applications model security constraints in the database using FaunaDB, GDPR principles like data minimisation, the right to access and the right to erasure can be supported with features like automatic data expiry, authentication, authorization, and access delegation. </p>\n<blockquote>When applications model security constraints in the database using FaunaDB, GDPR principles like data minimisation, the right to access and the right to erasure can be supported with features like automatic data expiry, authentication, authorization, and access delegation.</blockquote>\n<h2>SaaS Data Repository</h2>\n<p>If your service is offered in the cloud as a stand-alone customer experience, for instance in applications like vertical-specific enterprise SaaS, individual user storage and sync, or self-contained game worlds, the SaaS data repository layout is for you. FaunaDB multi-tenancy means a single cluster can support an arbitrarily complex tree of databases, inheriting access control and QoS prioritization.</p>\n<p>In database management systems without support for multi-tenancy, the complexity of maintaining a database per tenant can overwhelm the benefits. In my experience discovering customer requirements at Couchbase, we saw a hunger for simpler multi-tenant management and resource control, especially when data sharing requirements are more complex for instance in telecom or banking use cases that wanted to create a database per account. FaunaDB’s hierarchical approach makes it easier to manage quality-of-service (QoS) for a large fleet of databases, and support a mixture of popular and less used databases. Read about the <a href=\"https://blog.fauna.com/secure-hierarchical-multi-tenancy-patterns\">patterns that can be achieved with FaunaDB’s hierarchical multi-tenancy</a>.</p>\n<p>Keeping user and customer data in isolated databases is aligned with GDPR principles, because it is easy to manage a single customer’s database as a unit, and harder to run queries aggregating user behavior from users who don’t opt-in. Databases can be deleted or copied, granting right of erasure and access. A database-per-customer design is also congruent with the principles of consent and data minimisation, as access for processors can be granted and revoked for the entire data set, and old customer accounts, inactive game worlds, or unused individual storage areas can be deleted. In addition to deletion and access control, Fauna is working on a locality control feature to allow replication topology to be configured per-database, so you can manage a single cluster, but keep each customer’s data inside its jurisdiction.</p>\n<p>This multi-tenancy pattern is concerned with databases, FaunaDB’s container for data. If your customer application is multi-user, like retail, hospitality, or compliance, the object level patterns in the rest of the article will also apply. They are more fine grained than the data container, and can model most shared and collaborative data flows, within an individual database.</p>\n<h2>Social Publishing</h2>\n<p>In a social publishing app, users can post content and follow other users. The main screen of the app is the recent content from everyone the user follows. In this model, objects can be created and read by anyone in the user class, but only updated by their original author. Additionally, users can grant and revoke permissions for the application to run certain kinds of processing on their data, depending on which application feature-sets the user engages in. Querying a model like this is discussed in <a href=\"https://blog.fauna.com/unifying-relational-document-graph-and-temporal-data-models\">our post about how FaunaDB unifies document, relational, graph, and temporal data models</a>.</p>\n<figure><img src=\"{asset:462:url||https://fauna.com/assets/site/blog-legacy/Screen-Shot-2018-05-30-at-2.26.09-PM.png}\" alt=\"\" /></figure><p><a href=\"https://gdpr-info.eu/chapter-2/\">GDPR’s principle of data minimisation</a> requires applications to collect only the data they need, and use it only for the purpose for which consent was given. In our application, various kinds of processing can be run on user content, but only if the user opts in. So for instance, without consent for processing, users would not receive content recommendations. Additionally, the push notification process can only access the data of users who opt in, and similarly users who have not consented to ad targeting will only see un-targeted ads.</p>\n<blockquote>Users manage processing permissions by delegating access to their content via FaunaDB APIs.<br /></blockquote>\n<p>This is implemented in FaunaDB with the following access control rules:</p>\n<ul><li>Connections to the database can be authenticated as a user instance or as a processing agent.</li><li>User instances can read and author social content, and update content for which they are the author. User instances can create and delete following relationships where they are the follower.</li><li>Processing agents can only read social content or following relationships for users that have opted-in to the particular feature the process supports. Users manage processing permissions by delegating access to their content via FaunaDB APIs.</li><li>Data stored by the processing agent in conjunction with a particular feature can only be read using the key corresponding to the user’s consent. If consent is revoked, the key is removed and access is no longer possible, even before data deletion takes place.</li></ul><p>This pattern is also aligned with the GDPR’s right of erasure and right of access. By accessing the database via a user and their delegates, you can get an accurate snapshot of all the data stored for a particular user, for download or deletion.</p>\n<blockquote>By accessing the database via a user and their delegates, you can get an accurate snapshot of all the data stored for a particular user, for download or deletion.</blockquote>\n<p>In a production app, you might extend this model by decomposing the user’s access into capabilities, with a document per capability. So the user would have a document for messaging, one for photos, etc. As well as keys corresponding to access which are granted to particular processes belonging to the features which the user opted into. If the user later opts of of ad-targeting, push notifications, or content recommendation, the key is simply thrown away, and the data accessed and stored by those processes is no longer accessible.</p>\n<h2>Collaborative Workgroups</h2>\n<p>Collaborative workgroup patterns show up in all sorts of applications. Everything from shared document editing, to electronic medical records, to inventory tracking and retail point-of-sale shares a basic model where groups are defined and can share access to collaborative data. The important aspect of this model is defining a group, and then deciding which users are members of it, and which objects belong to the group.</p>\n<p>In the real estate listing example discussed at the beginning of the article, the group is the agent-team, the users are the agents, and the app objects are listings data, documents, onsite comments, photos, etc.</p>\n<figure><img src=\"{asset:463:url||https://fauna.com/assets/site/blog-legacy/Screen-Shot-2018-05-30-at-2.27.38-PM.png}\" alt=\"\" /></figure><p>In FaunaDB the model above would be represented by tagging the app objects on the way in, with ACL entries granting the correct group(s) access to read, write and modify the objects themselves. By writing authorship and ownership information into your objects you can better track how data flows in your system.<br /></p>\n<blockquote>FaunaDB’s temporal retention allows changes in data to be tracked over time.<br /></blockquote>\n<p>One of the biggest challenges with the GDPR for collaborative workgroup software is the potential for collaborators to introduce personal data without proper safeguards. Knowing which collaborators had access to which application objects at which time can be crucial for containing breaches and mitigating leaks. FaunaDB’s temporal retention allows changes in data to be tracked over time. In this model you can query group membership and session status, for tracking and audit purposes.</p>\n<h1>Conclusion</h1>\n<p>Security and privacy go beyond the features offered by a database, and extend into how inviting it is to build programs that are aligned with principles like the GDPR. This article shows how common application scenarios, when built with FaunaDB’s security primitives, can be extended in alignment with GDPR principles. I hope that by showing this additional benefit to deep modeling of application security in the database, you are further convinced that it’s worth doing.</p>\n<p>If you are interested in another benefit of modeling application rules in the database, <a href=\"https://www.youtube.com/watch?v=iCqKeOHrb2k&amp;index=17&amp;list=PLnwBrRU5CSTmruZzR8Z06j3pGglBZcdDr\">take a look at this talk from ServerlessConf Austin about how I built the object security model for a multi-user TodoMVC clone.</a> The benefit here is that AWS Lambda functions can be supplied a FaunaDB access token by AWS API Gateway proxied requests, so your code connects to the database as the user who is using the app. Jump to 11 minutes in if you want to skip the FaunaDB pitch.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-05-22T16:03:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 517,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "7bb29a67-7b45-4053-9874-5c783035a017",
        "siteSettingsId": 517,
        "fieldLayoutId": 4,
        "contentId": 350,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Unifying Relational, Document, Graph, and Temporal Data Models",
        "slug": "unifying-relational-document-graph-and-temporal-data-models",
        "uri": "blog/unifying-relational-document-graph-and-temporal-data-models",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/unifying-relational-document-graph-and-temporal-data-models",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/unifying-relational-document-graph-and-temporal-data-models",
        "isCommunityPost": false,
        "blogBodyText": "<p>Different business requirements drive the need for different data models. Consequently, databases evolved and specialized&nbsp;to keep pace. Today, a software system might use a relational database for transactional data, a graph database for social identity management, and a time series database for analytics, all within the same application. From an operational standpoint, this is a scenario every enterprise would love to avoid.</p>\r\n<p>To address this issue, some databases attempt to offer a multi-model approach. These databases offer multiple modeling techniques (e.g. relational, document, graph, etc.). within the same database. However, these systems introduce model-specific interfaces that are often distinct, and cannot be used in combination. They prevent data from being accessed using the right approach at the right time. </p>\r\n<blockquote>FaunaDB abstracts data in a way that plays well with various models, so mixing and matching graph, relational, temporal and document access in a single query feels natural and doesn’t require context switches.</blockquote>\r\n<p>FaunaDB takes a different approach to this problem. The Fauna uses a multi-model approach that unifies the ability to read and write documents, with the ability to use relational, graph and other styles of data interactions within the same query language, giving developers flexibility to choose the right approach in context. The query language runs with ACID transactions and temporal retention, on top of cloud-native distributed storage, so your workloads run with enterprise class support, no matter which data model you’re using.</p>\r\n<p>Database models have evolved over time. Let’s examine some of the current approaches available in the market. </p>\r\n<h3>The Relational Model</h3>\r\n<p>The relational model was first described in 1969 and is optimized for the concerns of an era when a megabyte of storage cost hundreds of dollars. Data stored in the relational model can be queried and viewed in formats that support application use-cases, even while the underlying data fields are stored in a normalized schema designed to prevent data duplication. By specifying the individual data items, their types, and their relations to each other, this model supports efforts to maintain data integrity. It also means that as data access patterns change over time, new indexes and queries can work with the existing data. The joins and constraints offered by the relational model facilitate a single system of record serving many different use cases. Databases built on the relational model are able to evolve with their application over time, at the cost of modeling the shape of the data in a schema.</p>\r\n<blockquote>Databases built on the relational model are able to evolve with their application over time, at the cost of modeling the shape of the data in a schema.</blockquote>\r\n<p>This strength can also be a weakness, in that changes to the data model must be thoughtfully considered and carefully applied. Simple schema changes must be coordinated across development and production environments, and when the production environment involves many servers the application tier may require coordinated upgrades. This can be especially burdensome when multiple teams are building features that use different parts of the database, as even though the teams aren’t collaborating on code, they still have to agree about the database schema. </p>\r\n<p>Because relational databases encode data into tables where all records must have the same shape, applications that consume variable data from messy real world sources may see little benefit to encoding a schema at all. Instead the schema emerges downstream as the data is queried and aggregated. The document model has historically been a better fit for this.</p>\r\n<p>Imagine you are building a social media app where users can post content and follow other users. The main screen of the app is the recent content from everyone the user follows. </p>\r\n<figure></figure>\r\n<figure><img src=\"{asset:427:url}\" data-image=\"6mkvtrmmqkog\"></figure>\r\n<figure><br></figure>\r\n<p>The data model starts simple, with a table to track users, a table to track content, and a table for the following relationship. But when as soon as there are multiple types of content the photo team and the audio team will have to coordinate schema changes to the Content table. This friction compounds each time some part of the database needs to be changed by more than one team. In a complex fast moving app, this friction can motivate migrating to another data model, like the document model which is designed to accommodate varying data types. The alternative is to deepen the relational model of your application, introducing a class to capture the variation in content types. </p>\r\n<figure><img src=\"{asset:428:url}\" style=\"font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 16px; font-family: &quot;Trebuchet MS&quot;, &quot;Helvetica Neue&quot;, Helvetica, Tahoma, sans-serif; vertical-align: middle; cursor: pointer; max-width: 100%; height: auto; color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);\" data-image=\"n8n6jmzq6htu\"></figure>\r\n<p>You’ll either have to join through the class also, to find publication dates, or remember to update the publication object every time it’s associated content object is changed. Did I mention the upcoming requirement to keep old versions of the content around to facilitate undo? Making each of these tables temporally aware adds additional complexity that must be addressed anytime the database is used.</p>\r\n<h3>The Document Model</h3>\r\n<p>In the document model, data can be stored with any structure. Typically, it is stored as it is received or presented, for ease of development. Each document is independent, with no joins, constraints, or relations between documents. There are as many query styles as there are document databases with options ranging from map reduce to full text search, and no standard API. The biggest advantages of a document database are the flexibility to store data as it comes without configuring a schema first, and the performance benefits those simplified data access patterns bring. The lack of schema means developers can freely iterate on application features without coordinating with other teams about schema changes, a productivity boost that can especially benefit time-to-market. The simple data access patterns give developers clarity about what the high performance path will be, so as long as apps are built with performance in mind, the document model can lead to a responsive user experience.</p>\r\n<blockquote>The biggest advantage of a document database is the flexibility to store data as it comes without configuring a schema first.</blockquote>\r\n<p>However, over time the ease of no schema can lead to maintenance headaches. Without relational integrity application errors compound as database errors, manifesting as invoices without purchasers, groups without owners, and other hard to repair issues. Without the integrity that comes from relational constraints, the ease of use of a document database is about the shape of the data that is stored, but challenges arise with queries and updates that are incongruent with the originally planned data access paths. At some point document database contents must be migrated to a fresh format, so that conditional application logic to deal with old document structures can be removed. Without periodic data cleanup, done by rewriting the contents of the database in the structures preferred by the latest version of the application code, the code will accrue conditional logic at the cost of maintainability. This downside of the document model strikes later in the development process, so by the time you get there it’s too late to turn back.</p>\r\n<p>In our social media example, a document database would cope just fine with having different types of content to publish. Where it gets challenging is creating the individualized activity feeds. Without support for joins or complex queries, application developers resort to stitching together the results of fan-out queries in code, adding delay and complexity to each page view. An alternative is precaching the timeline view by writing new content to it as it is published. Doing the work on write instead of on read is not usually a first choice, because speculatively building data artifacts for inactive users can be relatively expensive. That making N extra copies of each data item is considered a solution shows where this approach leads. Data format changes and cleanup become even more challenging when data is copied through a pipeline. Code all along the app might have to deal with an intermix of old and new data formats, adding unwelcome maintenance burden.</p>\r\n<h3>The Graph Model</h3>\r\n<p>The graph model is useful for finding patterns in relationships. If you’re optimizing container shipping it helps to be able to compare potential container routes on attributes like cost and speed, for instance in preparing for regulatory changes or in case of disruption to major shipping routes. Queries like this can sometimes be most efficiently executed by visiting the relevant data objects. Graph databases optimize for querying across deep relationships. They are useful to draw insights from data that might otherwise be hard to find, because they focus on how data items are connected.</p>\r\n<blockquote>Graph APIs in the same context as operational and transactional data are sufficient&nbsp;for many use cases, even without specialized graph storage and processing.</blockquote>\r\n<p>The graph model is typically implemented to optimize fast searches, not scalability. Specialized graph databases rely on scale up infrastructure utilizing a single server with large amounts of memory to make processing large data sets feasible. They are not suited to running on clusters of commodity hardware, so while they are useful for niche cases they have not seen broad adoption. The recursive features of SQL have seen more adoption, showing that graph APIs in the same context as operational and transactional data suffice for many use cases, even without specialized graph storage and processing. </p>\r\n<h3>The Temporal Model</h3>\r\n<p>Enterprise applications need to track changes, and social applications need to present the latest updates from friends. All sorts of applications can benefit from auto-expiring records, especially with requirements like the GDPR becoming common. Event and snapshot feeds can also be useful for <a href=\"https://blog.fauna.com/connecting-external-indexers-and-data-pipelines\">updating external indexers and other integration tasks</a>.</p>\r\n<p>Implementing temporality in relational databases requires adding additional dimensions to your schema to <a href=\"https://en.wikipedia.org/wiki/Temporal_database\">track valid time and transaction time</a> for each record. Recovering from mistakes, auditing change history, and querying old versions of the data are supported natively by FaunaDB’s temporal snapshot storage.</p>\r\n<figure><img src=\"{asset:429:url}\" style=\"vertical-align: middle; cursor: pointer; max-width: 100%; height: auto;\" data-image=\"5wvjaoelhyys\"></figure>\r\n<p>Tracking changes to all classes in the relational model requires adding complexity to all queries. FaunaDB’s unified support for temporal event tracking makes supporting use cases like event sourcing and audit logging as easy as querying the event APIs.</p>\r\n<h3>FaunaDB’s Unified Multi-Model Interface</h3>\r\n<p>FaunaDB was built from the ground up to provide a unified model that encompasses document-relational, graph, and temporal paradigms, while leaving room for future additions.</p>\r\n<p>\r\n</p>\r\n<p><strong>Relational:</strong> FaunaDB brings the best aspects of the relational model into the cloud era, by allowing flexible documents by default, but offering relational features like constraints, indexes, and joins when you want them. This combination means you can start by simply storing your data as is, and then define indexes and begin using joins as needed. Schema changes aren’t necessary when your application starts storing a new field, so unnecessary coordination among developers is minimized. Relational features give your schema the flexibility to serve additional use cases as your application matures, so you can store data in a format that makes sense today and be assured that unanticipated queries won’t require data migrations.</p>\r\n<blockquote>FaunaDB’s model combines relational correctness and integrity with the ease of use of documents, so your data stays useful as your application evolves.</blockquote>\r\n<p>\r\n</p>\r\n<p><strong>Document:</strong> FaunaDB’s approach gives developers and architects the choice about what data structures to use, and at what granularity to store and index them. Unlike the primitive document model, the combination of FaunaDB’s relational features with its flexible document storage supports applications that evolve and develop far beyond their initial use case. Adding fields requires no coordination with other developers, while at the same time the schema can enforce uniqueness and provide indexes to fit the application needs. Just as RDBMS systems have proven capable of keeping up with application changes over decades, the flexibility of FaunaDB’s unified model means your database can keep up with your application for the long term, not just through its initial development cycle. With a single system of record, efforts can focus on integrity at the source so that all queries benefit. FaunaDB’s model combines relational correctness and integrity with the ease of use of documents, so your data stays useful as your application evolves.</p>\r\n<p><strong>Graph: </strong>For graph workloads, FaunaDB object references can be followed for scalable and easy to use graph queries that integrate with OLTP data. By fitting seamlessly into the query language, graph predicates can interoperate with other features like access control and temporality. Running in the same database as the system of record means graph queries take advantage of ACID snapshot isolation, global scalability, and FaunaDB’s other operational capabilities. Graph traversal is a standard part of the query API.</p>\r\n<p><strong>Temporal: </strong>Some datasets are inherently time based, like social media feeds and event logs. FaunaDB makes traversing changes in snapshot order easy; for these applications, the time dimension can be handled by FaunaDB’s event view. The tutorial that accompanies the example query below explains how to use event queries to build an activity feed from a complex join across follower relationships and authors. Native support for this type of query can simplify your social media applications so users can keep up to date with the latest changes, without a lot of code on your part to track what they’ve seen.</p>\r\n<blockquote>FaunaDB’s approach gives developers and architects the choice about what data structures to use, and at what granularity to store and index them.&nbsp;</blockquote>\r\n<p>Unlike single model or multi-model databases, wherein adding new data models to these databases clutters the API and creates problematic trade-offs, FaunaDB abstracts data in a way that plays well with various models, so mixing and matching graph, relational, temporal and document access in a single query feels natural and doesn’t require context switches. FaunaDB accomplishes this by building on top of a robust cluster operations system designed to keep queries and workloads from impacting each other. The ACID transaction pipeline and indexer mechanism provide a flexible foundation for supporting multiple data models, all with intuitive strong consistency guarantees. For more information about how queries are processed by FaunaDB, this blog post about <a href=\"https://blog.fauna.com/the-life-of-a-faunadb-query\">the life of a query</a> goes into details.</p>\r\n<h3>Example Query</h3>\r\n<p>This example shows a combination of document-relational, graph, and temporal data models, by querying a social graph for accounts the reader is following, joining the posts from those accounts, and using temporal events to present the latest updates to the reader.</p>\r\n<pre class=\"language-javascript\">q.Map(\r\n    q.Paginate(\r\n      q.Join(\r\n        q.Match(\r\n          q.Index(\"followees_by_follower\"),\r\n          q.Select(\r\n            \"ref\",\r\n            q.Get(q.Match(q.Index(\"people_by_name\"), \"carol\")))),\r\n        q.Index(\"posts_by_author\")),\r\n      { after: 1520225699165542, events: true }),\r\n    function(event) {\r\n      return q.Get(q.Select(\"resource\", event));\r\n    })</pre>\r\n<p>The document for the user named \"carol\" is looked up in the \"people_by_name\" index, and the users that Carol follows are loaded via a graph-style \"followees_by_follower\" relationship index. Those authors are joined to their posts via the \"posts_by_author\" index, and the set of posts is paginated using event mode. The timestamp argument to <strong>after </strong>(1520225699165542) corresponds to the snapshot as of which the user had last viewed the feed, so event pagination begins from that point, leaving out earlier articles.</p>\r\n<h3>Conclusion</h3>\r\n<p>Unifying multiple data models in a single query language requires a product that’s been designed with the right type of abstractions. Early NoSQL databases are characterized by thin abstractions exposing developers to the underlying implementation, with API features that map to physical data layouts such as B-tree indexes, column scans, and quorum CRUD operations. FaunaDB takes a different approach, abstracting the underlying cluster to provide robust quality-of-service management, simple operations, and ACID transactions for all query types. This abstraction allows the database engine to safely interleave workloads with different I/O characteristics. New query capabilities can be added without impacting the performance of existing applications. The result is a simpler stack with one unified database cluster to support all your applications.</p>\r\n<blockquote>The freedom to adapt your query model to changing requirements gives you a solid foundation for the future of your organization's data.\r\n</blockquote>\r\n<p>Choosing a database with a unified query model like FaunaDB allows you start with the relaxed constraints of the document model, but add relational constraints and graph queries as necessary. The freedom to adapt your query model to changing requirements gives you a solid foundation for the future of your organization's data.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Different business requirements drive the need for different data models. Consequently, databases evolved and specialized to keep pace. Today, a software system might use a relational database for transactional data, a graph database for social identity management, and a time series database for analytics, all within the same application. From an operational standpoint, this is a scenario every enterprise would love to avoid.</p>\n<p>To address this issue, some databases attempt to offer a multi-model approach. These databases offer multiple modeling techniques (e.g. relational, document, graph, etc.). within the same database. However, these systems introduce model-specific interfaces that are often distinct, and cannot be used in combination. They prevent data from being accessed using the right approach at the right time. </p>\n<blockquote>FaunaDB abstracts data in a way that plays well with various models, so mixing and matching graph, relational, temporal and document access in a single query feels natural and doesn’t require context switches.</blockquote>\n<p>FaunaDB takes a different approach to this problem. The Fauna uses a multi-model approach that unifies the ability to read and write documents, with the ability to use relational, graph and other styles of data interactions within the same query language, giving developers flexibility to choose the right approach in context. The query language runs with ACID transactions and temporal retention, on top of cloud-native distributed storage, so your workloads run with enterprise class support, no matter which data model you’re using.</p>\n<p>Database models have evolved over time. Let’s examine some of the current approaches available in the market. </p>\n<h3>The Relational Model</h3>\n<p>The relational model was first described in 1969 and is optimized for the concerns of an era when a megabyte of storage cost hundreds of dollars. Data stored in the relational model can be queried and viewed in formats that support application use-cases, even while the underlying data fields are stored in a normalized schema designed to prevent data duplication. By specifying the individual data items, their types, and their relations to each other, this model supports efforts to maintain data integrity. It also means that as data access patterns change over time, new indexes and queries can work with the existing data. The joins and constraints offered by the relational model facilitate a single system of record serving many different use cases. Databases built on the relational model are able to evolve with their application over time, at the cost of modeling the shape of the data in a schema.</p>\n<blockquote>Databases built on the relational model are able to evolve with their application over time, at the cost of modeling the shape of the data in a schema.</blockquote>\n<p>This strength can also be a weakness, in that changes to the data model must be thoughtfully considered and carefully applied. Simple schema changes must be coordinated across development and production environments, and when the production environment involves many servers the application tier may require coordinated upgrades. This can be especially burdensome when multiple teams are building features that use different parts of the database, as even though the teams aren’t collaborating on code, they still have to agree about the database schema. </p>\n<p>Because relational databases encode data into tables where all records must have the same shape, applications that consume variable data from messy real world sources may see little benefit to encoding a schema at all. Instead the schema emerges downstream as the data is queried and aggregated. The document model has historically been a better fit for this.</p>\n<p>Imagine you are building a social media app where users can post content and follow other users. The main screen of the app is the recent content from everyone the user follows. </p>\n<figure></figure><figure><img src=\"{asset:427:url||https://fauna.com/assets/site/blog-legacy/fdm1b.png}\" alt=\"\" /></figure><figure><br /></figure><p>The data model starts simple, with a table to track users, a table to track content, and a table for the following relationship. But when as soon as there are multiple types of content the photo team and the audio team will have to coordinate schema changes to the Content table. This friction compounds each time some part of the database needs to be changed by more than one team. In a complex fast moving app, this friction can motivate migrating to another data model, like the document model which is designed to accommodate varying data types. The alternative is to deepen the relational model of your application, introducing a class to capture the variation in content types. </p>\n<figure><img src=\"{asset:428:url||https://fauna.com/assets/site/blog-legacy/fdm2.png}\" alt=\"\" /></figure><p>You’ll either have to join through the class also, to find publication dates, or remember to update the publication object every time it’s associated content object is changed. Did I mention the upcoming requirement to keep old versions of the content around to facilitate undo? Making each of these tables temporally aware adds additional complexity that must be addressed anytime the database is used.</p>\n<h3>The Document Model</h3>\n<p>In the document model, data can be stored with any structure. Typically, it is stored as it is received or presented, for ease of development. Each document is independent, with no joins, constraints, or relations between documents. There are as many query styles as there are document databases with options ranging from map reduce to full text search, and no standard API. The biggest advantages of a document database are the flexibility to store data as it comes without configuring a schema first, and the performance benefits those simplified data access patterns bring. The lack of schema means developers can freely iterate on application features without coordinating with other teams about schema changes, a productivity boost that can especially benefit time-to-market. The simple data access patterns give developers clarity about what the high performance path will be, so as long as apps are built with performance in mind, the document model can lead to a responsive user experience.</p>\n<blockquote>The biggest advantage of a document database is the flexibility to store data as it comes without configuring a schema first.</blockquote>\n<p>However, over time the ease of no schema can lead to maintenance headaches. Without relational integrity application errors compound as database errors, manifesting as invoices without purchasers, groups without owners, and other hard to repair issues. Without the integrity that comes from relational constraints, the ease of use of a document database is about the shape of the data that is stored, but challenges arise with queries and updates that are incongruent with the originally planned data access paths. At some point document database contents must be migrated to a fresh format, so that conditional application logic to deal with old document structures can be removed. Without periodic data cleanup, done by rewriting the contents of the database in the structures preferred by the latest version of the application code, the code will accrue conditional logic at the cost of maintainability. This downside of the document model strikes later in the development process, so by the time you get there it’s too late to turn back.</p>\n<p>In our social media example, a document database would cope just fine with having different types of content to publish. Where it gets challenging is creating the individualized activity feeds. Without support for joins or complex queries, application developers resort to stitching together the results of fan-out queries in code, adding delay and complexity to each page view. An alternative is precaching the timeline view by writing new content to it as it is published. Doing the work on write instead of on read is not usually a first choice, because speculatively building data artifacts for inactive users can be relatively expensive. That making N extra copies of each data item is considered a solution shows where this approach leads. Data format changes and cleanup become even more challenging when data is copied through a pipeline. Code all along the app might have to deal with an intermix of old and new data formats, adding unwelcome maintenance burden.</p>\n<h3>The Graph Model</h3>\n<p>The graph model is useful for finding patterns in relationships. If you’re optimizing container shipping it helps to be able to compare potential container routes on attributes like cost and speed, for instance in preparing for regulatory changes or in case of disruption to major shipping routes. Queries like this can sometimes be most efficiently executed by visiting the relevant data objects. Graph databases optimize for querying across deep relationships. They are useful to draw insights from data that might otherwise be hard to find, because they focus on how data items are connected.</p>\n<blockquote>Graph APIs in the same context as operational and transactional data are sufficient for many use cases, even without specialized graph storage and processing.</blockquote>\n<p>The graph model is typically implemented to optimize fast searches, not scalability. Specialized graph databases rely on scale up infrastructure utilizing a single server with large amounts of memory to make processing large data sets feasible. They are not suited to running on clusters of commodity hardware, so while they are useful for niche cases they have not seen broad adoption. The recursive features of SQL have seen more adoption, showing that graph APIs in the same context as operational and transactional data suffice for many use cases, even without specialized graph storage and processing. </p>\n<h3>The Temporal Model</h3>\n<p>Enterprise applications need to track changes, and social applications need to present the latest updates from friends. All sorts of applications can benefit from auto-expiring records, especially with requirements like the GDPR becoming common. Event and snapshot feeds can also be useful for <a href=\"https://blog.fauna.com/connecting-external-indexers-and-data-pipelines\">updating external indexers and other integration tasks</a>.</p>\n<p>Implementing temporality in relational databases requires adding additional dimensions to your schema to <a href=\"https://en.wikipedia.org/wiki/Temporal_database\">track valid time and transaction time</a> for each record. Recovering from mistakes, auditing change history, and querying old versions of the data are supported natively by FaunaDB’s temporal snapshot storage.</p>\n<figure><img src=\"{asset:429:url||https://fauna.com/assets/site/blog-legacy/fdm3.png}\" alt=\"\" /></figure><p>Tracking changes to all classes in the relational model requires adding complexity to all queries. FaunaDB’s unified support for temporal event tracking makes supporting use cases like event sourcing and audit logging as easy as querying the event APIs.</p>\n<h3>FaunaDB’s Unified Multi-Model Interface</h3>\n<p>FaunaDB was built from the ground up to provide a unified model that encompasses document-relational, graph, and temporal paradigms, while leaving room for future additions.</p>\n<p>\n</p>\n<p><strong>Relational:</strong> FaunaDB brings the best aspects of the relational model into the cloud era, by allowing flexible documents by default, but offering relational features like constraints, indexes, and joins when you want them. This combination means you can start by simply storing your data as is, and then define indexes and begin using joins as needed. Schema changes aren’t necessary when your application starts storing a new field, so unnecessary coordination among developers is minimized. Relational features give your schema the flexibility to serve additional use cases as your application matures, so you can store data in a format that makes sense today and be assured that unanticipated queries won’t require data migrations.</p>\n<blockquote>FaunaDB’s model combines relational correctness and integrity with the ease of use of documents, so your data stays useful as your application evolves.</blockquote>\n<p>\n</p>\n<p><strong>Document:</strong> FaunaDB’s approach gives developers and architects the choice about what data structures to use, and at what granularity to store and index them. Unlike the primitive document model, the combination of FaunaDB’s relational features with its flexible document storage supports applications that evolve and develop far beyond their initial use case. Adding fields requires no coordination with other developers, while at the same time the schema can enforce uniqueness and provide indexes to fit the application needs. Just as RDBMS systems have proven capable of keeping up with application changes over decades, the flexibility of FaunaDB’s unified model means your database can keep up with your application for the long term, not just through its initial development cycle. With a single system of record, efforts can focus on integrity at the source so that all queries benefit. FaunaDB’s model combines relational correctness and integrity with the ease of use of documents, so your data stays useful as your application evolves.</p>\n<p><strong>Graph: </strong>For graph workloads, FaunaDB object references can be followed for scalable and easy to use graph queries that integrate with OLTP data. By fitting seamlessly into the query language, graph predicates can interoperate with other features like access control and temporality. Running in the same database as the system of record means graph queries take advantage of ACID snapshot isolation, global scalability, and FaunaDB’s other operational capabilities. Graph traversal is a standard part of the query API.</p>\n<p><strong>Temporal: </strong>Some datasets are inherently time based, like social media feeds and event logs. FaunaDB makes traversing changes in snapshot order easy; for these applications, the time dimension can be handled by FaunaDB’s event view. The tutorial that accompanies the example query below explains how to use event queries to build an activity feed from a complex join across follower relationships and authors. Native support for this type of query can simplify your social media applications so users can keep up to date with the latest changes, without a lot of code on your part to track what they’ve seen.</p>\n<blockquote>FaunaDB’s approach gives developers and architects the choice about what data structures to use, and at what granularity to store and index them. </blockquote>\n<p>Unlike single model or multi-model databases, wherein adding new data models to these databases clutters the API and creates problematic trade-offs, FaunaDB abstracts data in a way that plays well with various models, so mixing and matching graph, relational, temporal and document access in a single query feels natural and doesn’t require context switches. FaunaDB accomplishes this by building on top of a robust cluster operations system designed to keep queries and workloads from impacting each other. The ACID transaction pipeline and indexer mechanism provide a flexible foundation for supporting multiple data models, all with intuitive strong consistency guarantees. For more information about how queries are processed by FaunaDB, this blog post about <a href=\"https://blog.fauna.com/the-life-of-a-faunadb-query\">the life of a query</a> goes into details.</p>\n<h3>Example Query</h3>\n<p>This example shows a combination of document-relational, graph, and temporal data models, by querying a social graph for accounts the reader is following, joining the posts from those accounts, and using temporal events to present the latest updates to the reader. You can read the details of this query in our <a href=\"https://fauna.com/tutorials/activity_feed\">activity feed tutorial, where you learn to build a social news feed using FaunaDB.</a></p>\n<pre class=\"language-javascript\">q.Map(\n q.Paginate(\n q.Join(\n q.Match(\n q.Index(\"followees_by_follower\"),\n q.Select(\n \"ref\",\n q.Get(q.Match(q.Index(\"people_by_name\"), \"carol\")))),\n q.Index(\"posts_by_author\")),\n { after: 1520225699165542, events: true }),\n function(event) {\n return q.Get(q.Select(\"resource\", event));\n })</pre>\n<p>The document for the user named \"carol\" is looked up in the \"people_by_name\" index, and the users that Carol follows are loaded via a graph-style \"followees_by_follower\" relationship index. Those authors are joined to their posts via the \"posts_by_author\" index, and the set of posts is paginated using event mode. The timestamp argument to <strong>after </strong>(1520225699165542) corresponds to the snapshot as of which the user had last viewed the feed, so event pagination begins from that point, leaving out earlier articles.</p>\n<h3>Conclusion</h3>\n<p>Unifying multiple data models in a single query language requires a product that’s been designed with the right type of abstractions. Early NoSQL databases are characterized by thin abstractions exposing developers to the underlying implementation, with API features that map to physical data layouts such as B-tree indexes, column scans, and quorum CRUD operations. FaunaDB takes a different approach, abstracting the underlying cluster to provide robust quality-of-service management, simple operations, and ACID transactions for all query types. This abstraction allows the database engine to safely interleave workloads with different I/O characteristics. New query capabilities can be added without impacting the performance of existing applications. The result is a simpler stack with one unified database cluster to support all your applications.</p>\n<blockquote>The freedom to adapt your query model to changing requirements gives you a solid foundation for the future of your organization's data.\n</blockquote>\n<p>Choosing a database with a unified query model like FaunaDB allows you start with the relaxed constraints of the document model, but add relational constraints and graph queries as necessary. The freedom to adapt your query model to changing requirements gives you a solid foundation for the future of your organization's data.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "470",
        "postDate": "2018-05-18T21:32:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 516,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "f3ce684b-1a41-41b8-bee0-d1a50f942a2b",
        "siteSettingsId": 516,
        "fieldLayoutId": 4,
        "contentId": 349,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Prioritize Workloads with FaunaDB's Quality-of-service API",
        "slug": "prioritize-workloads-with-quality-of-service-api",
        "uri": "blog/prioritize-workloads-with-quality-of-service-api",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/prioritize-workloads-with-quality-of-service-api",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/prioritize-workloads-with-quality-of-service-api",
        "isCommunityPost": false,
        "blogBodyText": "<p>All organizations are challenged to achieve the highest possible utilization of the resources they deploy and the processes they use to manage those resources. Database operations teams feel this pressure acutely. &nbsp;This is especially true in the face of an evolving environment that is pushing them toward a shared services (or DBaaS) model. In these cases any single workload should not jeopardize the operation of the entire cluster or even a few of its adjacent workloads.</p>\r\n<p></p>\r\n<figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ybyHrACgagw\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\" style=\"background-color: initial;\"></iframe></figure>\r\n<p></p>\r\n<p></p>\r\n<p>Support for multi-tenancy was a design goal for FaunaDB from the very beginning. As a distributed transactional ACID compliant database, Fauna addresses the scalability pain typical of RDBMS systems and the transactional deficiencies of NoSQL solutions. To achieve this, Fauna includes strong security features including a row-level access control system. Fauna also includes a policy-based workload resource manager. The demo in this blog post shows this workload management functionality and what it means to database operations.</p>\r\n<p>FaunaDB implements a policy-based workload resource manager that dynamically allocates resources and enforces quality of service (QoS) at the database or client level. Fauna’s approach to transactions and more specifically the deep level of knowledge Fauna has around the work associated with those transactions allows monitoring and scheduling of both IO and compute resources. This means that a priority context can be attached to workloads in the system. For example, a production database can be given a priority of 100 and we could give an analytics database a priority of 10. With these priorities the production database will receive 10x more resources than the developer database when the overall system became constrained. This scheduling or relative throttling of the developer database only comes into play in cases where the system resources are limited.</p>\r\n<p>So what does this look like in practice? The short demo above is an illustration of QoS on a three datacenter FaunaDB system. In the demo we will show a primary workload running at high priority and a secondary workload at a lower priority. You can see how Fauna dynamically allocates resource between the two workloads and what happens when the workloads change. Take a look and let us know what you think!</p>\r\n<p></p>",
        "blogCategory": [
            "3",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>All organizations are challenged to achieve the highest possible utilization of the resources they deploy and the processes they use to manage those resources. Database operations teams feel this pressure acutely. This is especially true in the face of an evolving environment that is pushing them toward a shared services (or DBaaS) model. In these cases any single workload should not jeopardize the operation of the entire cluster or even a few of its adjacent workloads.</p>\n<figure></figure><p>Support for multi-tenancy was a design goal for FaunaDB from the very beginning. As a distributed transactional ACID compliant database, Fauna addresses the scalability pain typical of RDBMS systems and the transactional deficiencies of NoSQL solutions. To achieve this, Fauna includes strong security features including a row-level access control system. Fauna also includes a policy-based workload resource manager. The demo in this blog post shows this workload management functionality and what it means to database operations.</p>\n<p>FaunaDB implements a policy-based workload resource manager that dynamically allocates resources and enforces quality of service (QoS) at the database or client level. Fauna’s approach to transactions and more specifically the deep level of knowledge Fauna has around the work associated with those transactions allows monitoring and scheduling of both IO and compute resources. This means that a priority context can be attached to workloads in the system. For example, a production database can be given a priority of 100 and we could give an analytics database a priority of 10. With these priorities the production database will receive 10x more resources than the developer database when the overall system became constrained. This scheduling or relative throttling of the developer database only comes into play in cases where the system resources are limited.</p>\n<p>So what does this look like in practice? The short demo above is an illustration of QoS on a three datacenter FaunaDB system. In the demo we will show a primary workload running at high priority and a secondary workload at a lower priority. You can see how Fauna dynamically allocates resource between the two workloads and what happens when the workloads change. Take a look and let us know what you think!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-05-08T15:54:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 515,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "471c1c62-58a7-435e-9b29-0233e5cb3124",
        "siteSettingsId": 515,
        "fieldLayoutId": 4,
        "contentId": 348,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Secure Hierarchical Multi-tenancy Patterns",
        "slug": "secure-hierarchical-multi-tenancy-patterns",
        "uri": "blog/secure-hierarchical-multi-tenancy-patterns",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2020-05-26T09:06:58-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/secure-hierarchical-multi-tenancy-patterns",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/secure-hierarchical-multi-tenancy-patterns",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB’s multi-tenant architecture is designed for operational simplicity, allowing a single FaunaDB cluster to support multiple regions and as many applications and databases as desired. Robust quality-of-service (QoS) controls mean your performance-sensitive production queries will never slow down due to analytic workloads, developer experiments, or operational movement. This capability supports use cases like SaaS backends, branch-office applications, app and game studios with a stable of content, data isolation for customer confidentiality (GDPR), and other shared services including identity management. FaunaDB’s multi-tenancy is a robust solution for managing access to database resources for teams and applications. Also, it can be paired with FaunaDB’s object level access control for fine-grained authentication and authorization, allowing for safe, direct mobile and web client access to databases in FaunaDB.</p>\n<p>FaunaDB allows databases to be arranged in a hierarchy, so each database can contain other databases. Certain security and QoS rules are inherited down the tree. This makes managing groups of applications, or resource usage&nbsp;by teams across your organization, or customers with different payment plans, as easy as a few FaunaDB queries.</p>\n<p>In this blog post, we’ll review the APIs for creating a database hierarchy, and how to manage quality-of-service at the connection and database level. We’ll also talk about practical uses for multi-tenancy and QoS.</p>\n<h3>Setting Database Priority</h3>\n<p>Database priority allows you to specify which databases are first in line should the cluster ever become overloaded. This way you can confidently run a large query with low priority, and know it won't impact the smaller queries your production users depend on.</p>\n<p>Managing database priority is as easy as setting a numerical field on the database schema object. Here is an example that creates two databases in the top level of an account, one called “production” with high priority, and the other called “development” which will see reduced performance in overload scenarios, while production apps are not impacted.</p>\n<p><em>Note: these code samples are written in Python. Our </em><a href=\"https://fauna.com/documentation/reference/queryapi\"><em>query language reference can help you&nbsp;translate these into any of our supported languages</em></a><em>.</em></p>\n<pre class=\"language-python\">adminClient = FaunaClient(secret='YOUR_FAUNADB_ADMIN_SECRET');\nadminClient.query(q.create_database({ \n    \"name\": \"production\", \"priority\": 88 }));\nadminClient.query(q.create_database({\n    \"name\": \"development\", \"priority\": 5 }));</pre>\n<p>Both of these databases are created in a container determined by the client secret used. Each connection connects to a particular database, and all operations run in a database root. The next section illustrates with examples.</p>\n<h3>Nesting Databases</h3>\n<p>Inside of both the high and low priority databases we could create as many nested databases as we like. Databases inherit priority from their containing database, so to extend the above example, you could create fast and slow apps, by&nbsp;setting relative priority among databases in the same container. If you need to throttle or boost one of your production apps, that's a query away.</p>\n<p>Before we can create nested databases in the production database&nbsp;we need to connect in the right context. A connection to the “production” database will allow us to create nested databases and other schema objects inside it. To get a connection with those capabilities, we need a key secret. We’ll use our admin client to create an admin key inside the “production” database and use the key’s secret to connect and create some nested databases. </p>\n<pre class=\"language-python\">productionKey = adminClient.query(q.create_key({\n    \"database\": q.database(\"production\"), \n    \"role\": \"admin\" \n}));\nproductionClient = FaunaClient({ \n    \"secret\": productionKey.secret });\nproductionClient.query(q.create_database({ \n    \"name\": \"ShoppingCart\" }));\nproductionClient.query(q.create_database({ \n    \"name\": \"IdentityService\" }));</pre>\n<p>The productionKey we created can modify databases inside the production context, but it cannot access or modify databases in the development context. If we want to work on those, we need to do the same for the “development” database, creating an admin key that can create nested databases. Here we create some database inside the development database:</p>\n<pre class=\"language-python\">developmentKey = adminClient.query(q.create_key({\n    \"database\": q.database(\"development\"), \n    \"role\": \"admin\" \n}));\ndevelopmentClient = FaunaClient({ \n    \"secret\": developmentKey.secret });\ndevelopmentClient.query(q.create_database({ \n    \"name\": \"HackDay\" }));\ndevelopmentClient.query(q.create_database({ \n    \"name\": \"TeamNotes\" }));</pre>\n<p>Say we wanted to create a record in the “TeamNotes” database we just created, first we’d&nbsp;create a server key for accessing that database. Server keys can make schema and data changes but they can’t modify the database tenancy tree. Note we use the existing development client to create the server key. Once we have it we can use it to work in the TeamNotes database.</p>\n<pre class=\"language-python\">teamNotesKey = developmentClient.query(q.create_key({\n    \"database\": q.database(\"TeamNotes\"), \n    \"role\": \"server\" \n}));\nteamNotesClient = FaunaClient({ \n    \"secret\": teamNotesKey.secret });\nteamNotesClient.query(q.create_class({ \n    \"name\": \"todos\" }));\nteamNotesClient.query(q.create(q.class_expr(\"todos\"), {\n    \"data\": {  \"title\": \"Organize team BBQ.\" }\n}));</pre>\n<p>These examples should be enough to get you started. If you want to go further and restrict access within a particular database, so that certain keys can only apply certain operations, you can achieve that level of control with FaunaDB’s object-level security. Learn more about <a href=\"https://fauna.com/documentation/intro/security\">access control in the documentation</a>, or if you prefer a hands-on approach, see this <a href=\"https://github.com/fauna/todomvc-fauna-spa/blob/master/src/TodoModel.js#L157\">code which makes use of object level access control</a> to give users access to todo lists. Object level access control is beyond the scope of this post, but it’s an important complement to the security capabilities multi-tenancy offers.</p>\n<h3>Organizing Databases</h3>\n<p>Here are a few quick examples to get you thinking about how you could use hierarchical multi-tenancy in your enterprise. There are not meant to be gospel, just suggestions for jumping off points to address your own requirements.</p>\n<h4>Development Studio</h4>\n<p>Development houses, whether they specialize in line-of-business apps, consumer experiences, retail, or gaming, tend to end up owning a lot of product maintenance and uptime as their portfolio grows. Running a database service for each product quickly becomes unmanageable. Here’s how a shop like that might organize their FaunaDB databases.</p>\n<ul><li>Production</li><ul><li>Clients</li><ul><li>GlobalBrand</li><ul><li>CouponApp</li><li>LoyaltyNotifier</li><li>CustomerServiceApp</li></ul><li>BigLender</li><ul><li>MortgageApplication</li><li>InsuranceOnsite</li><li>RateEstimator</li></ul><li>RetailShop</li><ul><li>Catalog</li><li>Inventory</li><li>PointOfSale</li><li>Payroll</li><li>Compliance</li><li>PromotionalApps</li><ul><li>SpringSale</li><li>BlackFriday</li></ul></ul></ul><li>InHouse</li><ul><li>TimeAndTicketTracker</li><li>Vendors</li><li>Shipping</li></ul></ul><li>Development</li><ul><li>Clients</li><ul><li>GlobalBrand</li><li>BigLender</li><li>RetailShop</li><li>RideFinder</li></ul><li>Teams</li><ul><li>FrontEnd</li><li>Support</li><li>Admin</li><li>Exec</li></ul></ul></ul>\n<p>By organizing across development and production, they can give each team and client app a place to innovate, while also running the production traffic on the same cluster. By setting priority higher on the production root, they ensure performance for live traffic comes before experimental workloads.&nbsp;This solution works equally well in FaunaDB Cloud as it does on-premise with FaunaDB Enterprise.</p>\n<h4>SaaS Backend</h4>\n<p>Software-as-a-Service is characterized by self-service signup and payment. If users can onboard themselves, the business can cast a wider net. In this case we see a few root databases, and then a customer database with databases for each customer. It’s possible to set priority for individual customers, for instance if they are using a premium plan.</p>\n<ul><li>Admin</li><li>Billing</li><li>Customers</li><ul><li>ab1d237c8ea</li><li>7c8e5b1d23a</li><li>1d687cbc8ea</li><li>c8e5ab1d237</li></ul></ul>\n<p>This looks a lot like the backend of FaunaDB Cloud &mdash; it’s possible for each customer to have their own arbitrary schema and multi-tenant tree inside their top-level database. So your SaaS product can even allow your customers to automatically provision databases for their individual sites or users.</p>\n<p>Resource usage can be aggregated on a per-database level, so it’s possible to bill your customers individually in proportion to their database usage.</p>\n<h3>Conclusion</h3>\n<p>Anytime you might want shared access to database resources, FaunaDB is your friend. From an enterprise-wide cluster that can be used for workloads across the organization, to high performance shared identity services, to flexible SaaS backends that can include both your large and small customers, FaunaDB multi-tenancy is designed to support you. For further reading <a href=\"https://fauna.com/whitepaper\">our technical whitepaper</a> will help you see how our multi-tenancy capabilities fit into the bigger picture and <a href=\"https://fauna.com/tutorials/multitenant\">our multi-tenancy tutorial introduces the API</a>.</p>",
        "blogCategory": [
            "3",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB’s multi-tenant architecture is designed for operational simplicity, allowing a single FaunaDB cluster to support multiple regions and as many applications and databases as desired. Robust quality-of-service (QoS) controls mean your performance-sensitive production queries will never slow down due to analytic workloads, developer experiments, or operational movement. This capability supports use cases like SaaS backends, branch-office applications, app and game studios with a stable of content, data isolation for customer confidentiality (GDPR), and other shared services including identity management. FaunaDB’s multi-tenancy is a robust solution for managing access to database resources for teams and applications. Also, it can be paired with FaunaDB’s object level access control for fine-grained authentication and authorization, allowing for safe, direct mobile and web client access to databases in FaunaDB.</p>\n<p>FaunaDB allows databases to be arranged in a hierarchy, so each database can contain other databases. Certain security and QoS rules are inherited down the tree. This makes managing groups of applications, or resource usage by teams across your organization, or customers with different payment plans, as easy as a few FaunaDB queries.</p>\n<p>In this blog post, we’ll review the APIs for creating a database hierarchy, and how to manage quality-of-service at the connection and database level. We’ll also talk about practical uses for multi-tenancy and QoS.</p>\n<h3>Setting Database Priority</h3>\n<p>Database priority allows you to specify which databases are first in line should the cluster ever become overloaded. This way you can confidently run a large query with low priority, and know it won't impact the smaller queries your production users depend on.</p>\n<p>Managing database priority is as easy as setting a numerical field on the database schema object. Here is an example that creates two databases in the top level of an account, one called “production” with high priority, and the other called “development” which will see reduced performance in overload scenarios, while production apps are not impacted.</p>\n<p><em>Note: these code samples are written in Python. Our </em><a href=\"https://fauna.com/documentation/reference/queryapi\"><em>query language reference can help you translate these into any of our supported languages</em></a><em>.</em></p>\n<pre class=\"language-python\">adminClient = FaunaClient(secret='YOUR_FAUNADB_ADMIN_SECRET');\nadminClient.query(q.create_database({ \n \"name\": \"production\", \"priority\": 88 }));\nadminClient.query(q.create_database({\n \"name\": \"development\", \"priority\": 5 }));</pre>\n<p>Both of these databases are created in a container determined by the client secret used. Each connection connects to a particular database, and all operations run in a database root. The next section illustrates with examples.</p>\n<h3>Nesting Databases</h3>\n<p>Inside of both the high and low priority databases we could create as many nested databases as we like. Databases inherit priority from their containing database, so to extend the above example, you could create fast and slow apps, by setting relative priority among databases in the same container. If you need to throttle or boost one of your production apps, that's a query away.</p>\n<p>Before we can create nested databases in the production database we need to connect in the right context. A connection to the “production” database will allow us to create nested databases and other schema objects inside it. To get a connection with those capabilities, we need a key secret. We’ll use our admin client to create an admin key inside the “production” database and use the key’s secret to connect and create some nested databases. </p>\n<pre class=\"language-python\">productionKey = adminClient.query(q.create_key({\n \"database\": q.database(\"production\"), \n \"role\": \"admin\" \n}));\nproductionClient = FaunaClient({ \n \"secret\": productionKey.secret });\nproductionClient.query(q.create_database({ \n \"name\": \"ShoppingCart\" }));\nproductionClient.query(q.create_database({ \n \"name\": \"IdentityService\" }));</pre>\n<p>The productionKey we created can modify databases inside the production context, but it cannot access or modify databases in the development context. If we want to work on those, we need to do the same for the “development” database, creating an admin key that can create nested databases. Here we create some database inside the development database:</p>\n<pre class=\"language-python\">developmentKey = adminClient.query(q.create_key({\n \"database\": q.database(\"development\"), \n \"role\": \"admin\" \n}));\ndevelopmentClient = FaunaClient({ \n \"secret\": developmentKey.secret });\ndevelopmentClient.query(q.create_database({ \n \"name\": \"HackDay\" }));\ndevelopmentClient.query(q.create_database({ \n \"name\": \"TeamNotes\" }));</pre>\n<p>Say we wanted to create a record in the “TeamNotes” database we just created, first we’d create a server key for accessing that database. Server keys can make schema and data changes but they can’t modify the database tenancy tree. Note we use the existing development client to create the server key. Once we have it we can use it to work in the TeamNotes database.</p>\n<pre class=\"language-python\">teamNotesKey = developmentClient.query(q.create_key({\n \"database\": q.database(\"TeamNotes\"), \n \"role\": \"server\" \n}));\nteamNotesClient = FaunaClient({ \n \"secret\": teamNotesKey.secret });\nteamNotesClient.query(q.create_class({ \n \"name\": \"todos\" }));\nteamNotesClient.query(q.create(q.class_expr(\"todos\"), {\n \"data\": { \"title\": \"Organize team BBQ.\" }\n}));</pre>\n<p>These examples should be enough to get you started. If you want to go further and restrict access within a particular database, so that certain keys can only apply certain operations, you can achieve that level of control with FaunaDB’s object-level security. Learn more about <a href=\"https://fauna.com/documentation/intro/security\">access control in the documentation</a>, or if you prefer a hands-on approach, see this <a href=\"https://github.com/fauna/todomvc-fauna-spa/blob/master/src/TodoModel.js#L157\">code which makes use of object level access control</a> to give users access to todo lists. Object level access control is beyond the scope of this post, but it’s an important complement to the security capabilities multi-tenancy offers.</p>\n<h3>Organizing Databases</h3>\n<p>Here are a few quick examples to get you thinking about how you could use hierarchical multi-tenancy in your enterprise. There are not meant to be gospel, just suggestions for jumping off points to address your own requirements.</p>\n<h4>Development Studio</h4>\n<p>Development houses, whether they specialize in line-of-business apps, consumer experiences, retail, or gaming, tend to end up owning a lot of product maintenance and uptime as their portfolio grows. Running a database service for each product quickly becomes unmanageable. Here’s how a shop like that might organize their FaunaDB databases.</p>\n<ul><li>Production<ul><li>Clients<ul><li>GlobalBrand<ul><li>CouponApp</li><li>LoyaltyNotifier</li><li>CustomerServiceApp</li></ul></li><li>BigLender<ul><li>MortgageApplication</li><li>InsuranceOnsite</li><li>RateEstimator</li></ul></li><li>RetailShop<ul><li>Catalog</li><li>Inventory</li><li>PointOfSale</li><li>Payroll</li><li>Compliance</li><li>PromotionalApps<ul><li>SpringSale</li><li>BlackFriday</li></ul></li></ul></li></ul></li><li>InHouse<ul><li>TimeAndTicketTracker</li><li>Vendors</li><li>Shipping</li></ul></li></ul></li><li>Development<ul><li>Clients<ul><li>GlobalBrand</li><li>BigLender</li><li>RetailShop</li><li>RideFinder</li></ul></li><li>Teams<ul><li>FrontEnd</li><li>Support</li><li>Admin</li><li>Exec</li></ul></li></ul></li></ul><p>By organizing across development and production, they can give each team and client app a place to innovate, while also running the production traffic on the same cluster. By setting priority higher on the production root, they ensure performance for live traffic comes before experimental workloads. This solution works equally well in FaunaDB Cloud as it does on-premise with FaunaDB Enterprise.</p>\n<h4>SaaS Backend</h4>\n<p>Software-as-a-Service is characterized by self-service signup and payment. If users can onboard themselves, the business can cast a wider net. In this case we see a few root databases, and then a customer database with databases for each customer. It’s possible to set priority for individual customers, for instance if they are using a premium plan.</p>\n<ul><li>Admin</li><li>Billing</li><li>Customers<ul><li>ab1d237c8ea</li><li>7c8e5b1d23a</li><li>1d687cbc8ea</li><li>c8e5ab1d237</li></ul></li></ul><p>This looks a lot like the backend of FaunaDB Cloud — it’s possible for each customer to have their own arbitrary schema and multi-tenant tree inside their top-level database. So your SaaS product can even allow your customers to automatically provision databases for their individual sites or users.</p>\n<p>Resource usage can be aggregated on a per-database level, so it’s possible to bill your customers individually in proportion to their database usage.</p>\n<h3>Conclusion</h3>\n<p>Anytime you might want shared access to database resources, FaunaDB is your friend. From an enterprise-wide cluster that can be used for workloads across the organization, to high performance shared identity services, to flexible SaaS backends that can include both your large and small customers, FaunaDB multi-tenancy is designed to support you. For further reading <a href=\"https://fauna.com/whitepaper\">our technical whitepaper</a> will help you see how our multi-tenancy capabilities fit into the bigger picture and <a href=\"https://fauna.com/tutorials/multitenant\">our multi-tenancy tutorial introduces the API</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "478",
        "postDate": "2018-05-02T19:43:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 514,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "6fb8bc36-f43b-4e80-bda3-2067356860f6",
        "siteSettingsId": 514,
        "fieldLayoutId": 4,
        "contentId": 347,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The Life of a FaunaDB Query",
        "slug": "the-life-of-a-faunadb-query",
        "uri": "blog/the-life-of-a-faunadb-query",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/the-life-of-a-faunadb-query",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/the-life-of-a-faunadb-query",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is a transactional NoSQL database for mission critical data. The engineering team at Fauna, tasked with building the next generation of high-performance scalable databases, identified three criteria in FaunaDB’s design phase:</p>\r\n<ul><li>An Internet-first communication protocol;&nbsp;</li><li>Low-latency and consistent read operations;&nbsp;</li><li>High-throughput, minimally latent and consistent write operations.</li></ul>\r\n<p>Each of these three key features can be observed in action by watching a query propagate through a FaunaDB cluster.</p>\r\n<p>The recommended way of interfacing with a FaunaDB cluster is by writing transaction queries with one of our <a href=\"https://app.fauna.com/drivers\">host language query drivers</a>. These drivers sit in between application code and the network, marshalling data traveling over the wire and providing an idiomatic interface tailored for each language. However, we will be demonstrating queries directly in our wire format, which we chose to be JSON.  An example of JSON-flavored input and output is shown in Figure 1.</p>\r\n<figure><img src=\"{asset:440:url}\" data-image=\"e1ei7yxygz44\"><figcaption>Figure 1</figcaption></figure>\r\n<p>The reasons for choosing JSON are numerous. In addition to being human-readable and flexible, JSON is the lingua franca of the modern Internet. Speaking JSON over HTTP means clients and apps can interface with a FaunaDB cluster, such as our <a href=\"https://fauna.com/serverless\">Managed Cloud Service</a>, directly over the open Internet. This means developers need not build layers of middleware that translate between web requests to a cordoned-off database, or be surprised by ancient middleboxes being confused by traffic on a non-standard port. Unsurprisingly, then, the entry point to any FaunaDB node is its HTTP endpoint, which consumes JSON blobs to evaluate.<br></p>\r\n<h2>Enter the JSON</h2>\r\n<figure><img src=\"{asset:439:url}\" data-image=\"9rmm03vb4lfn\"><figcaption>Transforming a JSON Query Into a FaunaDB Transaction&nbsp;</figcaption></figure>\r\n<p>\r\n</p>\r\n<p>The steps required to take a JSON query and transform it into a FaunaDB transaction can be observed by ways in which the system can fail in this process. First, we validate the well-formedness of the HTTP body and confirm that it is indeed valid JSON (figure 2).<br></p>\r\n<figure><img src=\"{asset:446:url}\" style=\"background-color: initial; display: block; margin: auto;\" data-image=\"ixtv2z8qb586\"><figcaption>Figure 2<br></figcaption></figure>\r\n<p>When successful, the JSON is parsed into an abstract syntax tree (AST) for our query evaluator to execute. It is here that the wire protocol's flexibility becomes a liability, however. JSON's objects are unordered key-value pairs and values cannot be constrained by type. Therefore, the components of the AST need to be validated against our language specification to ensure parameters' types match&nbsp;<a href=\"https://fauna.com/documentation\">our documentation</a>&nbsp;(figure 4), and that the overall shape of the expression matches a valid form in our query language (figure 3).<br></p>\r\n<figure><img src=\"{asset:442:url}\" data-image=\"srjv7g2tus7a\"><figcaption>Figure 3</figcaption></figure>\r\n<figure><img src=\"{asset:437:url}\" style=\"background-color: initial; display: block; margin: auto;\" data-image=\"1mrykcmqb5g8\"><figcaption>Figure 4</figcaption></figure>\r\n<p>Our AST parser also infers the effects of the transaction to be evaluated. Certain transactions may read data (recall figure 1), write new data (figure 5), perform read and write operations (figure 6, which mutates the instance we read in figure 1); or, a query may even contain pure expressions that require no IO whatsoever (figure 7)! In this last case, the database can immediately evaluate the query and return whatever the expression evaluates to. We are not so fortunate in the other two cases, however.<br></p>\r\n<figure><img src=\"{asset:445:url}\" style=\"background-color: initial;\" alt=\"Figure 5\" title=\"Figure 5\" data-image=\"cvk9azwmkfez\"><figcaption>Figure 5</figcaption></figure>\r\n<figure><img src=\"{asset:441:url}\" alt=\"Figure 6\" title=\"Figure 6\" data-image=\"m39ykbeymkkh\"><figcaption>Figure 6</figcaption></figure>\r\n<figure><img src=\"{asset:444:url}\" alt=\"Figure 7\" title=\"Figure 7\" data-image=\"urhgvzxc1g90\"><figcaption>Figure 7</figcaption></figure>\r\n<h2>From Local Evaluation to Global Fanout</h2>\r\n<p>Because of FaunaDB’s ACID guarantees, any replica in a cluster will have had previous write transactions already propagated to them. And, FaunaDB's strong consistency guarantees the most recently-committed version of an instance are readable. Therefore, read-only transactions don’t need to be synchronized across the entire cluster in the same way that a write transaction would be. In this case, our read interface will pull data either from local storage or nearby nodes in the same replica, yielding a document containing the result of the read, which is then rendered and sent back to the client.</p>\r\n<figure><img src=\"{asset:447:url}\" data-image=\"172eb3jy1li2\"><figcaption>FaunaDB Read & Write Interfaces</figcaption></figure>\r\n<p>FaunaDB has a similar write interface for handling transactions that require updating state. Queries pass through the write interface but, unlike with the read interface, a JSON result to be sent back to the client is not produced just yet. Instead, whenever the interpreter encounters a write operation to evaluate, the write interface creates a descriptor of each of the values to be updated, in the form of a before-and-after diff of each to-be-modified document. (Later, we will see why the write effect’s context comprises more than just the value to be written to disk.)<br></p>\r\n<p>After the query has been evaluated, these so-called write effects are collated and passed to our <a href=\"https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database\">transaction engine</a>, to be applied across the cluster.</p>\r\n<p>At this point, we should pause and briefly consider a larger view of the system. Up to this point, our discussion has just been with respect to a single database node. If you are running our <a href=\"https://app.fauna.com/releases/latest\">Developer Edition</a> of FaunaDB, that single node is the entirety of the database, offering no data sharding, failover, or redundancy. For local laptop development, that might be enough, but for a production-ready system we need more. [Editor's note, <a href=\"https://fauna.com/blog/faunadb-single-node-free-for-commercial-use\">FaunaDB Enterprise is now available for free download.</a>]</p>\r\n<figure style=\"float: left; margin: 0px 10px 10px 0px;\"><img src=\"{asset:438:url}\" alt=\"FaunaDB Nodes\" title=\"FaunaDB Nodes\" data-image=\"esdfetvupcrb\"><figcaption>FaunaDB Cluster</figcaption></figure>\r\n<p>FaunaDB nodes running in production environments, on the other hand, form a database cluster out of a constellation of nodes; the cluster is grouped into a flexible number of geography-spanning replicas, that may in turn contain a flexible number of individual FaunaDB nodes, responsible for a subset of the total dataset. When a transaction necessitates a write effect, some substrate connecting all nodes is required in order to have that write propagated from the originating node to all nodes replicating that data.  This substrate must do so in a manner that ensures consistency in the presence of crashing nodes, failing networks and other concurrently-processing writes. This is the mandate of the transaction engine.</p>\r\n<figure style=\"float: left; margin: 0px 10px 10px 0px;\"><img src=\"{asset:443:url}\" data-image=\"4r10wibylrbh\"><figcaption>FaunaDB Transaction Engine</figcaption></figure>\r\n<p>Having been passed to the transaction engine, the write effects are added to a log of transactions concurrently executing on this particular node. The log can be thought of as the official ordering of transactions on a node; if one write effect occurs in the log before another, it means FaunaDB considers it to have happened before the other. However, every node in the cluster must be of one mind with respect to the contents of this log.  For this, we use an implementation of the Raft consensus protocol, optimized for efficient operation over wide-area networks, to obtain this.</p>\r\n<p>Using our Raft implementation, on a set epoch interval, nodes’ logs are disseminated to all others, and are coordinated via <a href=\"https://fauna.com/blog/distributed-consistency-at-scale-spanner-vs-calvin\">our deterministic concurrency control system</a> to agree on the ordering of all the log’s entries’ write effects. Using a modern and optimized consensus protocol rather than the classical distributed transaction technique of two-phase commit with row locking drastically improves performance without sacrificing correctness; Raft requires fewer cross-cluster round trips, which is critical for a geographically-diverse deployment. Further, batching these operations on an epoch boundary rather than coordinating on every write amortizes that cost of global coordination across the system.&nbsp;The end result of this is that all hitherto pending transactions across the whole cluster have been replicated and been given a total ordering, and are ready to be consumed by the final piece of the puzzle, which manages where the transaction’s data physically resides.</p>\r\n<h2>At Long Last, Data Hits Disks</h2>\r\n<p>The data subsystem is where IO finally occurs: all to-be-processed write effects are pulled off the log, ordered by their logical timestamp. Our concurrency control protocol is optimistic in that it is only at this final stage that we detect whether a write effect’s read dependencies, including the write location itself, has been processed with a later timestamp. If no such writes exist, then the write effect’s diff is applied and synced to durable storage.  If any do exist, however, then the transaction needs to be aborted and retried, since its diff now depends on the effects of another transaction.&nbsp;</p>\r\n<p>Once all writes belonging to a transaction have been applied on a quorum of nodes across the cluster, the client will be informed of its success.</p>\r\n<h2>Conclusion</h2>\r\n<p>In this article we started with a JSON query and followed it through each of FaunaDB’s subsystems, showing you how the moving pieces contribute to a high-performance distributed database.</p>\r\n<p>FaunaDB is a complex system internally, but we have worked hard to design a friendly user interface for both database operators and application developers. Scalable, secure, transactional, global, multi-cloud, multi-tenant, temporal, and highly available, FaunaDB is designed to simplify development of distributed applications while making database operations dramatically easier.</p>\r\n<p>Looking for more hands-on experience with FaunaDB? Check out the recent <a href=\"https://fauna.com/blog/getting-started-with-faunadb-fauna-query-language-fql\">Fauna Query Language tutorial</a> in the Getting Started w/ FaunaDB blog series for developers. Sign up for a free <a href=\"https://fauna.com/sign-up\">FaunaDB Cloud account</a> to begin experimenting with strongly-consistent transactions of your own. Also, if you found this post interesting, and building such a system sounds enticing, be sure to check our <a href=\"https://fauna.com/careers\">jobs page</a> to see if anything strikes your fancy!</p>",
        "blogCategory": [
            "1462"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB is a transactional NoSQL database for mission critical data. The engineering team at Fauna, tasked with building the next generation of high-performance scalable databases, identified three criteria in FaunaDB’s design phase:</p>\n<ul><li>An Internet-first communication protocol; </li><li>Low-latency and consistent read operations; </li><li>High-throughput, minimally latent and consistent write operations.</li></ul><p>Each of these three key features can be observed in action by watching a query propagate through a FaunaDB cluster.</p>\n<p>The recommended way of interfacing with a FaunaDB cluster is by writing transaction queries with one of our <a href=\"https://fauna.com/drivers\">host language query drivers</a>. These drivers sit in between application code and the network, marshalling data traveling over the wire and providing an idiomatic interface tailored for each language. However, we will be demonstrating queries directly in our wire format, which we chose to be JSON. An example of JSON-flavored input and output is shown in Figure 1.</p>\n<figure><img src=\"{asset:440:url||https://fauna.com/assets/site/blog-legacy/image4.png}\" alt=\"\" /><figcaption>Figure 1</figcaption></figure><p>The reasons for choosing JSON are numerous. In addition to being human-readable and flexible, JSON is the lingua franca of the modern Internet. Speaking JSON over HTTP means clients and apps can interface with a FaunaDB cluster, such as our <a href=\"https://fauna.com/serverless\">Managed Cloud Service</a>, directly over the open Internet. This means developers need not build layers of middleware that translate between web requests to a cordoned-off database, or be surprised by ancient middleboxes being confused by traffic on a non-standard port. Unsurprisingly, then, the entry point to any FaunaDB node is its HTTP endpoint, which consumes JSON blobs to evaluate.<br /></p>\n<h2>Enter the JSON</h2>\n<figure><img src=\"{asset:439:url||https://fauna.com/assets/site/blog-legacy/image3.png}\" alt=\"\" /><figcaption>Transforming a JSON Query Into a FaunaDB Transaction </figcaption></figure><p>\n</p>\n<p>The steps required to take a JSON query and transform it into a FaunaDB transaction can be observed by ways in which the system can fail in this process. First, we validate the well-formedness of the HTTP body and confirm that it is indeed valid JSON (figure 2).<br /></p>\n<figure><img src=\"{asset:446:url||https://fauna.com/assets/site/blog-legacy/image10.png}\" alt=\"\" /><figcaption>Figure 2<br /></figcaption></figure><p>When successful, the JSON is parsed into an abstract syntax tree (AST) for our query evaluator to execute. It is here that the wire protocol's flexibility becomes a liability, however. JSON's objects are unordered key-value pairs and values cannot be constrained by type. Therefore, the components of the AST need to be validated against our language specification to ensure parameters' types match <a href=\"https://fauna.com/documentation\">our documentation</a> (figure 4), and that the overall shape of the expression matches a valid form in our query language (figure 3).<br /></p>\n<figure><img src=\"{asset:442:url||https://fauna.com/assets/site/blog-legacy/image6.png}\" alt=\"\" /><figcaption>Figure 3</figcaption></figure><figure><img src=\"{asset:437:url||https://fauna.com/assets/site/blog-legacy/image1.png}\" alt=\"\" /><figcaption>Figure 4</figcaption></figure><p>Our AST parser also infers the effects of the transaction to be evaluated. Certain transactions may read data (recall figure 1), write new data (figure 5), perform read and write operations (figure 6, which mutates the instance we read in figure 1); or, a query may even contain pure expressions that require no IO whatsoever (figure 7)! In this last case, the database can immediately evaluate the query and return whatever the expression evaluates to. We are not so fortunate in the other two cases, however.<br /></p>\n<figure><img src=\"{asset:445:url||https://fauna.com/assets/site/blog-legacy/image9.png}\" alt=\"Figure 5\" title=\"Figure 5\" /><figcaption>Figure 5</figcaption></figure><figure><img src=\"{asset:441:url||https://fauna.com/assets/site/blog-legacy/image5.png}\" alt=\"Figure 6\" title=\"Figure 6\" /><figcaption>Figure 6</figcaption></figure><figure><img src=\"{asset:444:url||https://fauna.com/assets/site/blog-legacy/image8.png}\" alt=\"Figure 7\" title=\"Figure 7\" /><figcaption>Figure 7</figcaption></figure><h2>From Local Evaluation to Global Fanout</h2>\n<p>Because of FaunaDB’s ACID guarantees, any replica in a cluster will have had previous write transactions already propagated to them. And, FaunaDB's strong consistency guarantees the most recently-committed version of an instance are readable. Therefore, read-only transactions don’t need to be synchronized across the entire cluster in the same way that a write transaction would be. In this case, our read interface will pull data either from local storage or nearby nodes in the same replica, yielding a document containing the result of the read, which is then rendered and sent back to the client.</p>\n<figure><img src=\"{asset:447:url||https://fauna.com/assets/site/blog-legacy/image11.png}\" alt=\"\" /><figcaption>FaunaDB Read &amp; Write Interfaces</figcaption></figure><p>FaunaDB has a similar write interface for handling transactions that require updating state. Queries pass through the write interface but, unlike with the read interface, a JSON result to be sent back to the client is not produced just yet. Instead, whenever the interpreter encounters a write operation to evaluate, the write interface creates a descriptor of each of the values to be updated, in the form of a before-and-after diff of each to-be-modified document. (Later, we will see why the write effect’s context comprises more than just the value to be written to disk.)<br /></p>\n<p>After the query has been evaluated, these so-called write effects are collated and passed to our <a href=\"{entry:26:url||https://fauna.com/use-cases/distributed-ledger}\">transaction engine</a>, to be applied across the cluster.</p>\n<p>At this point, we should pause and briefly consider a larger view of the system. Up to this point, our discussion has just been with respect to a single database node. If you are running our <a href=\"{entry:48:url||https://fauna.com/company-deprecated}\">Developer Edition</a> of FaunaDB, that single node is the entirety of the database, offering no data sharding, failover, or redundancy. For local laptop development, that might be enough, but for a production-ready system we need more.</p>\n<figure style=\"float:left;margin:0px 10px 10px 0px;\"><img src=\"{asset:438:url||https://fauna.com/assets/site/blog-legacy/image2.png}\" alt=\"FaunaDB Nodes\" title=\"FaunaDB Nodes\" /><figcaption>FaunaDB Cluster</figcaption></figure><p>FaunaDB nodes running in production environments, on the other hand, form a database cluster out of a constellation of nodes; the cluster is grouped into a flexible number of geography-spanning replicas, that may in turn contain a flexible number of individual FaunaDB nodes, responsible for a subset of the total dataset. When a transaction necessitates a write effect, some substrate connecting all nodes is required in order to have that write propagated from the originating node to all nodes replicating that data. This substrate must do so in a manner that ensures consistency in the presence of crashing nodes, failing networks and other concurrently-processing writes. This is the mandate of the transaction engine.</p>\n<figure style=\"float:left;margin:0px 10px 10px 0px;\"><img src=\"{asset:443:url||https://fauna.com/assets/site/blog-legacy/image7.png}\" alt=\"\" /><figcaption>FaunaDB Transaction Engine</figcaption></figure><p>Having been passed to the transaction engine, the write effects are added to a log of transactions concurrently executing on this particular node. The log can be thought of as the official ordering of transactions on a node; if one write effect occurs in the log before another, it means FaunaDB considers it to have happened before the other. However, every node in the cluster must be of one mind with respect to the contents of this log. For this, we use an implementation of the Raft consensus protocol, optimized for efficient operation over wide-area networks, to obtain this.</p>\n<p>Using our Raft implementation, on a set epoch interval, nodes’ logs are disseminated to all others, and are coordinated via <a href=\"{entry:80:url||%7Bentry%3A80%3Aurl%7D}\">our deterministic concurrency control system</a> to agree on the ordering of all the log’s entries’ write effects. Using a modern and optimized consensus protocol rather than the classical distributed transaction technique of two-phase commit with row locking drastically improves performance without sacrificing correctness; Raft requires fewer cross-cluster round trips, which is critical for a geographically-diverse deployment. Further, batching these operations on an epoch boundary rather than coordinating on every write amortizes that cost of global coordination across the system. The end result of this is that all hitherto pending transactions across the whole cluster have been replicated and been given a total ordering, and are ready to be consumed by the final piece of the puzzle, which manages where the transaction’s data physically resides.</p>\n<h2>At Long Last, Data Hits Disks</h2>\n<p>The data subsystem is where IO finally occurs: all to-be-processed write effects are pulled off the log, ordered by their logical timestamp. Our concurrency control protocol is optimistic in that it is only at this final stage that we detect whether a write effect’s read dependencies, including the write location itself, has been processed with a later timestamp. If no such writes exist, then the write effect’s diff is applied and synced to durable storage. If any do exist, however, then the transaction needs to be aborted and retried, since its diff now depends on the effects of another transaction. </p>\n<p>Once all writes belonging to a transaction have been applied on a quorum of nodes across the cluster, the client will be informed of its success.</p>\n<h2>Conclusion</h2>\n<p>In this article we started with a JSON query and followed it through each of FaunaDB’s subsystems, showing you how the moving pieces contribute to a high-performance distributed database.</p>\n<p>FaunaDB is a complex system internally, but we have worked hard to design a friendly user interface for both database operators and application developers. Scalable, secure, transactional, global, multi-cloud, multi-tenant, temporal, and highly available, FaunaDB is designed to simplify development of distributed applications while making database operations dramatically easier.</p>\n<p>Looking for more hands-on experience with FaunaDB? Check out the recent <a href=\"{entry:302:url||https://fauna.com/logo-groups/investors-2}\">Fauna Query Language tutorial</a> in the Getting Started w/ FaunaDB blog series for developers. Sign up for a free <a href=\"https://fauna.com/sign-up\">FaunaDB Cloud account</a> to begin experimenting with strongly-consistent transactions of your own. Also, if you found this post interesting, and building such a system sounds enticing, be sure to check our <a href=\"https://fauna.com/careers\">jobs page</a> to see if anything strikes your fancy!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-04-24T17:08:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 513,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "692bd4a1-8fd7-459e-a759-dfee16e9e0a8",
        "siteSettingsId": 513,
        "fieldLayoutId": 4,
        "contentId": 346,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Connecting External Indexers and Data Pipelines",
        "slug": "connecting-external-indexers-and-data-pipelines",
        "uri": "blog/connecting-external-indexers-and-data-pipelines",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-10-08T12:27:22-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/connecting-external-indexers-and-data-pipelines",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/connecting-external-indexers-and-data-pipelines",
        "isCommunityPost": false,
        "blogBodyText": "<p>For this blog post, we’ll focus on the pattern to feed changes from FaunaDB's transactional data&nbsp;into a secondary&nbsp;data processing pipeline. The foundation of FaunaDB’s flexibility is our ability to integrate with the data ecosystem. The pattern illustrated here can be used to keep any system up to date with FaunaDB, from full text indexers to machine learning classifiers.<br></p>\n<p>When building a search index (or other eventually consistent derived data) from a database, you typically want to build it from the database’s current state — not by replaying the database history. And when keeping the index up to date with new database changes, typically you don’t care about all the intermediate states, you just want to update your index to reflect the latest version of each document. Deduplicating the feed can be a big optimization if your objects have multiple updates in quick succession, leading to faster, more responsive indexes.</p>\n<p>However, if you’re building something like&nbsp;a real-time fraud detection system, you might want your system to ingest the full feed of intermediate changes. This article is about how to build a deduplicated feed for bringing external processors up to date with the latest changes. If you’re interested in&nbsp;the full feed including intermediate versions,&nbsp; FaunaDB’s event queries are the answer, and&nbsp;<a href=\"https://docs.fauna.com/fauna/current/tutorials/temporality.html\">our temporal timeline tutorial is a great place to start.</a> The pattern in the article you're reading now&nbsp;is designed for full-text search, aggregation, geographic indexes&nbsp;or any other derived data set that represents an eventually consistent view of the current state of your data.</p>\n<h3>Fetching New Updates Consistently</h3>\n<p>Each update in FaunaDB has a timestamp that is available on instance objects under their <code>ts</code> field. This is a synthetic timestamp, which corresponds to an agreed-upon value derived by the FaunaDB cluster, and which corresponds to the snapshot containing the object’s update. This logical clock is always increasing (unlike a clock tracking real time) and doesn’t depend on physical time resolution, making it safe to use in this context. As you’ll see, our logic for correctness depends on the fact that FaunaDB’s timestamps are monotonically increasing.</p>\n<p>Indexing on this timestamp value makes it easy to sort instances by their snapshot order. If we ingest the instances in this order, we can track a high-water-mark, and use it to safely resume our data feed after a process restart or server failure. What this looks like in practice is first a query to load all objects from the beginning of time, then polling logic to request changes since the high-water mark. Each time we poll we record the new high-water-mark, so that if we’re restarted we know where to pick up from.</p>\n<p>One nuance is that without proper use of snapshots, it’s possible that frequently updated items can twinkle just beyond the horizon of our queries. By running our pagination queries within the same snapshot through each complete set we ensure that instances will be ingested with <a href=\"https://en.wikipedia.org/wiki/Unbounded_nondeterminism\">fairness</a>. Once we’ve paginated to the end of the set in a given snapshot, we can start again with the database’s new current snapshot, from our new high-water-mark. No matter how frequently updated&nbsp;an instance is, it will be found by snapshot pagination.</p>\n<p>Assuming we have a class called posts, we can create an index on the <code>ts</code> and <code>ref</code> fields.</p>\n<pre class=\"language-javascript\">q.CreateIndex({\n    name: \"posts-changes\",\n    source : q.Class(\"posts\"), \n    values : [{field: [\"ts\"]}, {field: [\"ref\"]}]})</pre>\n<p>Once this index is active, we paginate over the database’s current contents. If the database is only a few records you might be able to retrieve it all in one query. But most databases are large enough to need multiple pages of results to fully traverse all data.</p>\n<p>Before we begin pagination we need to pick a snapshot timestamp to paginate from that we can reuse as we fetch each page of the set. The best way to do this is to ask the database for it’s representation of now:</p>\n<pre class=\"language-javascript\">q.Time(\"now\")\n// =&gt; q.Time(\"2018-04-16T23:38:56.568191Z\")</pre>\n<p>We can store this value in a local variable in our program and use it for each query in the set. The first query would look like:</p>\n<pre class=\"language-javascript\">q.At(q.Time(\"2018-04-16T23:38:56.568191Z\"), \n    q.Map(q.Paginate(q.Match(q.Index(\"posts-changes\"))), \n      function(_ts, ref){return q.Get(ref)}))</pre>\n<p>This will return the first page of results in that snapshot, and a cursor for the next page of results if there is one, in the <code>after</code> field. Refer to the <a href=\"https://fauna.com/documentation/reference/queryapi#paginate_set_params\">Paginate function documentation</a> for details. As long as there is a page linked in the <code>after</code> field, we keep fetching pages and processing them.</p>\n<p>Processing the results will look like looping over the documents (returned by <code>q.Get(ref)</code> in the query) and feeding them to your search index, machine learning classifier, or other downstream system. Your life will be a easier if this ingest processing is <a href=\"https://en.wikipedia.org/wiki/Idempotence\">idempotent</a> because that means&nbsp;accidentally processing some documents twice is OK. Most indexers are idempotent. At the end of each page, update your high-water-mark by persisting it to the local filesystem or in FaunaDB.</p>\n<p>Once we have processed all the pages in the index the result set won’t include an <code>after</code> field. The highest <code>ts</code> we’ve seen can become our new high-water-mark. Now we’ve completed a full snapshotted index pass and we’re ready to start again at the new&nbsp;latest snapshot so we can repeat the process. We’ll query again for <code>q.Time(\"now\")</code> to find the new&nbsp;snapshot and start fetching the snapshotted&nbsp;index starting at our high-water-mark. (To do this&nbsp;we make a synthetic <code>after</code> cursor from our high-water-mark and start the process again to fetch the latest updates.)</p>\n<p>In JavaScript, making the query with the new snapshot time and the high-water-mark would look something like this. (This illustration leaves out looping over the paginated pages for simplicity, and concerns itself with only the first call. Astute readers will recognize that a full implementation might look recursive as it tracks the high-water-mark.)</p>\n<pre class=\"language-javascript\">client.query(q.Time(\"now\")).then(function(snapshotTime) {\n// =&gt; q.Time(\"2018-04-16T23:38:58.658231Z\")\n    const syntheticCursor = {after : highWaterMark};\n    return client.query(q.At(snapshotTime,\n        q.Map(q.Paginate(q.Match(q.Index(\"posts-changes\")), \n            syntheticCursor), function(_ts, ref){return q.Get(ref)})))\n})</pre>\n<h3>Operating Your Connector</h3>\n<p>The less frequently you poll for new data, the more deduplication that can happen. So your secondary system might not have to work as hard depending on your document update patterns. But if you poll too infrequently, users can get frustrated waiting for their changes to become searchable. A happy medium is to define the fastest polling interval you are comfortable with (maybe 250ms), and then let the unit of work that is your indexer ingest over a snapshot throttle you to appropriately sized chunks of work. However, if updates come in faster than your ingester can handle them you’ll end up with a lengthening queue of work. </p>\n<p>An easy way to tell if you need to allocate more resources to your external indexer, is if the ingester high-water-mark is falling behind the cluster’s current snapshot. Falling behind a little during traffic spikes is probably OK, but you don’t want to be trending further and further behind over the long term.</p>\n<p>If you redefine your external data pipeline, for instance by indexing a new field, you might want to rebuild from scratch. This is as easy as erasing your target system, setting the high-water-mark to zero, and running your scripts again.</p>\n<p>Stay tuned for an open-source example that implements the full approach.</p>",
        "blogCategory": [
            "3",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>For this blog post, we’ll focus on the pattern to feed changes from FaunaDB's transactional data into a secondary data processing pipeline. The foundation of FaunaDB’s flexibility is our ability to integrate with the data ecosystem. The pattern illustrated here can be used to keep any system up to date with FaunaDB, from full text indexers to machine learning classifiers.<br /></p>\n<p>When building a search index (or other eventually consistent derived data) from a database, you typically want to build it from the database’s current state — not by replaying the database history. And when keeping the index up to date with new database changes, typically you don’t care about all the intermediate states, you just want to update your index to reflect the latest version of each document. Deduplicating the feed can be a big optimization if your objects have multiple updates in quick succession, leading to faster, more responsive indexes.</p>\n<p>However, if you’re building something like a real-time fraud detection system, you might want your system to ingest the full feed of intermediate changes. This article is about how to build a deduplicated feed for bringing external processors up to date with the latest changes. If you’re interested in the full feed including intermediate versions, FaunaDB’s event queries are the answer, and <a href=\"https://fauna.com/tutorials/timeline\">our temporal timeline tutorial is a great place to start.</a> The pattern in the article you're reading now is designed for full-text search, aggregation, geographic indexes or any other derived data set that represents an eventually consistent view of the current state of your data.</p>\n<h3>Fetching New Updates Consistently</h3>\n<p>Each update in FaunaDB has a timestamp that is available on instance objects under their <code>ts</code> field. This is a synthetic timestamp, which corresponds to an agreed-upon value derived by the FaunaDB cluster, and which corresponds to the snapshot containing the object’s update. This logical clock is always increasing (unlike a clock tracking real time) and doesn’t depend on physical time resolution, making it safe to use in this context. As you’ll see, our logic for correctness depends on the fact that FaunaDB’s timestamps are monotonically increasing.</p>\n<p>Indexing on this timestamp value makes it easy to sort instances by their snapshot order. If we ingest the instances in this order, we can track a high-water-mark, and use it to safely resume our data feed after a process restart or server failure. What this looks like in practice is first a query to load all objects from the beginning of time, then polling logic to request changes since the high-water mark. Each time we poll we record the new high-water-mark, so that if we’re restarted we know where to pick up from.</p>\n<p>One nuance is that without proper use of snapshots, it’s possible that frequently updated items can twinkle just beyond the horizon of our queries. By running our pagination queries within the same snapshot through each complete set we ensure that instances will be ingested with <a href=\"https://en.wikipedia.org/wiki/Unbounded_nondeterminism\">fairness</a>. Once we’ve paginated to the end of the set in a given snapshot, we can start again with the database’s new current snapshot, from our new high-water-mark. No matter how frequently updated an instance is, it will be found by snapshot pagination.</p>\n<p>Assuming we have a class called posts, we can create an index on the <code>ts</code> and <code>ref</code> fields.</p>\n<pre class=\"language-javascript\">q.CreateIndex({\n name: \"posts-changes\",\n source : q.Class(\"posts\"), \n values : [{field: [\"ts\"]}, {field: [\"ref\"]}]})</pre>\n<p>Once this index is active, we paginate over the database’s current contents. If the database is only a few records you might be able to retrieve it all in one query. But most databases are large enough to need multiple pages of results to fully traverse all data.</p>\n<p>Before we begin pagination we need to pick a snapshot timestamp to paginate from that we can reuse as we fetch each page of the set. The best way to do this is to ask the database for it’s representation of now:</p>\n<pre class=\"language-javascript\">q.Time(\"now\")\n// =&gt; q.Time(\"2018-04-16T23:38:56.568191Z\")</pre>\n<p>We can store this value in a local variable in our program and use it for each query in the set. The first query would look like:</p>\n<pre class=\"language-javascript\">q.At(q.Time(\"2018-04-16T23:38:56.568191Z\"), \n q.Map(q.Paginate(q.Match(q.Index(\"posts-changes\"))), \n function(_ts, ref){return q.Get(ref)}))</pre>\n<p>This will return the first page of results in that snapshot, and a cursor for the next page of results if there is one, in the <code>after</code> field. Refer to the <a href=\"https://fauna.com/documentation/reference/queryapi#paginate_set_params\">Paginate function documentation</a> for details. As long as there is a page linked in the <code>after</code> field, we keep fetching pages and processing them.</p>\n<p>Processing the results will look like looping over the documents (returned by <code>q.Get(ref)</code> in the query) and feeding them to your search index, machine learning classifier, or other downstream system. Your life will be a easier if this ingest processing is <a href=\"https://en.wikipedia.org/wiki/Idempotence\">idempotent</a> because that means accidentally processing some documents twice is OK. Most indexers are idempotent. At the end of each page, update your high-water-mark by persisting it to the local filesystem or in FaunaDB.</p>\n<p>Once we have processed all the pages in the index the result set won’t include an <code>after</code> field. The highest <code>ts</code> we’ve seen can become our new high-water-mark. Now we’ve completed a full snapshotted index pass and we’re ready to start again at the new latest snapshot so we can repeat the process. We’ll query again for <code>q.Time(\"now\")</code> to find the new snapshot and start fetching the snapshotted index starting at our high-water-mark. (To do this we make a synthetic <code>after</code> cursor from our high-water-mark and start the process again to fetch the latest updates.)</p>\n<p>In JavaScript, making the query with the new snapshot time and the high-water-mark would look something like this. (This illustration leaves out looping over the paginated pages for simplicity, and concerns itself with only the first call. Astute readers will recognize that a full implementation might look recursive as it tracks the high-water-mark.)</p>\n<pre class=\"language-javascript\">client.query(q.Time(\"now\")).then(function(snapshotTime) {\n// =&gt; q.Time(\"2018-04-16T23:38:58.658231Z\")\n const syntheticCursor = {after : highWaterMark};\n return client.query(q.At(snapshotTime,\n q.Map(q.Paginate(q.Match(q.Index(\"posts-changes\")), \n syntheticCursor), function(_ts, ref){return q.Get(ref)})))\n})</pre>\n<h3>Operating Your Connector</h3>\n<p>The less frequently you poll for new data, the more deduplication that can happen. So your secondary system might not have to work as hard depending on your document update patterns. But if you poll too infrequently, users can get frustrated waiting for their changes to become searchable. A happy medium is to define the fastest polling interval you are comfortable with (maybe 250ms), and then let the unit of work that is your indexer ingest over a snapshot throttle you to appropriately sized chunks of work. However, if updates come in faster than your ingester can handle them you’ll end up with a lengthening queue of work. </p>\n<p>An easy way to tell if you need to allocate more resources to your external indexer, is if the ingester high-water-mark is falling behind the cluster’s current snapshot. Falling behind a little during traffic spikes is probably OK, but you don’t want to be trending further and further behind over the long term.</p>\n<p>If you redefine your external data pipeline, for instance by indexing a new field, you might want to rebuild from scratch. This is as easy as erasing your target system, setting the high-water-mark to zero, and running your scripts again.</p>\n<p>Stay tuned for an open-source example that implements the full approach.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "475",
        "postDate": "2018-04-20T16:44:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 512,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "8bfcfa92-31b9-4caf-a2f9-0640330ead15",
        "siteSettingsId": 512,
        "fieldLayoutId": 4,
        "contentId": 345,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Understanding Nothing (or NULL) in FaunaDB",
        "slug": "understanding-nothing-or-null-in-fauna",
        "uri": "blog/understanding-nothing-or-null-in-fauna",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/understanding-nothing-or-null-in-fauna",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/understanding-nothing-or-null-in-fauna",
        "isCommunityPost": false,
        "blogBodyText": "<p><em>This is the first post in an occasional blog series answering questions submitted by the developer community. Here, Fauna Senior Core Engineer John Miller, III, offers practical examples to help you understand the definition and benefits of NULL in FaunaDB compared to SQL. Want to know more about something in FaunaDB? Tweet <a href=\"https://twitter.com/faunadb\" target=\"_blank\">@fauna</a>&nbsp;to connect with the team and get your questions answered.</em><br></p>\r\n<p>In FaunaDB,<strong> null</strong> (or NULL), is a special marker used to indicate that a data value does not exist in the database. It is a representation of \"missing information\". This should not be confused with a value of zero. A null value indicates a <em>lack of a value</em>. A lack of a value is not the same thing as a value of zero, in the same way that a lack of an answer is not the same thing as an answer of \"no\".</p>\r\n<p>For example, consider the question: \"How many cars does Brandon own?\" The answer may be \"zero\" (we <em>know</em> that he owns <em>none</em>) or \"null\" (we <em>do not know</em> how many he owns). SQL null is analogous to a state rather than a value. This usage is quite different from most programming languages where null typically means missing or not pointing to any object.</p>\r\n<p>\r\n</p>\r\n<p><strong>Fauna use of NULL is similar to the traditional programmers' use of null, so it doesn't carry the baggage of the SQL implementation of null.</strong></p>\r\n<p>Fauna treats null as a value that can be directly compared for application programmer simplicity.  This means that NULL == NULL returns true. In a SQL databases NULL == NULL returns false, as NULL compared to anything is never true, it is unknown. In order to search for the null value in SQL databases one must use a special comparator “IS NULL” or “IS NOT NULL”. This is often the source of many programmer errors in SQL, especially when aggregating or combining rows.  Fauna is friendlier to the developer and allows the comparison directly to null as most modern programming languages allow.</p>\r\n<p>Exploring a few practical examples, a Fauna developer might see improvements when dealing with a&nbsp;Fauna-style null value as compared to SQL-style null. A value in Fauna which does not exist in a document or index has a value of null. While SQL generally initializes all values to null, it does not have the concept of non-existent values.</p>\r\n<p>When sorting data in Fauna, null values handling is simple in that nulls come either first or last depending on an ascending or descending ordering requested by the programmer. In SQL when ordering data, nulls always appear at one end of the sorted results set. Inverting the sorted order (i.e. ascending to descending) does not change where nulls are returned. In order to know which side of the results set null values are returned, a programmer must understand which database vendor they are dealing with or utilize special compensation syntax offered by some database vendors for sorting first or last.</p>\r\n<p>In Fauna, if you create a unique index on a value that does not exist, then the non-existent value will have a value of null. This allows only a single unknown state for the value.  In SQL you may have multiple unknown states for the same value. In our example above, there should only exist a single record for the number of unknown cars for Brandon.</p>\r\n<p>The use of the null value in Fauna was designed to be more intuitive to the general programmer. The lack of requiring special code for dealing with nulls should reduce the number of errors that traditional SQL developers encounter when not anticipating the use of nulls and omitting the special code to handle nulls. The experience for the application programmer when using Fauna should be more intuitive and productive, and free from many special cases.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p><em>This is the first post in an occasional blog series answering questions submitted by the developer community. Here, Fauna Senior Core Engineer John Miller, III, offers practical examples to help you understand the definition and benefits of NULL in FaunaDB compared to SQL. Want to know more about something in FaunaDB? Tweet <a href=\"https://twitter.com/faunadb\" target=\"_blank\" rel=\"noreferrer noopener\">@faunadb</a> to connect with the team and get your questions answered.</em><br /></p>\n<p>In FaunaDB,<strong> null</strong> (or NULL), is a special marker used to indicate that a data value does not exist in the database. It is a representation of \"missing information\". This should not be confused with a value of zero. A null value indicates a <em>lack of a value</em>. A lack of a value is not the same thing as a value of zero, in the same way that a lack of an answer is not the same thing as an answer of \"no\".</p>\n<p>For example, consider the question: \"How many cars does Brandon own?\" The answer may be \"zero\" (we <em>know</em> that he owns <em>none</em>) or \"null\" (we <em>do not know</em> how many he owns). SQL null is analogous to a state rather than a value. This usage is quite different from most programming languages where null typically means missing or not pointing to any object.</p>\n<p>\n</p>\n<p><strong>Fauna use of NULL is similar to the traditional programmers' use of null, so it doesn't carry the baggage of the SQL implementation of null.</strong></p>\n<p>Fauna treats null as a value that can be directly compared for application programmer simplicity. This means that NULL == NULL returns true. In a SQL databases NULL == NULL returns false, as NULL compared to anything is never true, it is unknown. In order to search for the null value in SQL databases one must use a special comparator “IS NULL” or “IS NOT NULL”. This is often the source of many programmer errors in SQL, especially when aggregating or combining rows. Fauna is friendlier to the developer and allows the comparison directly to null as most modern programming languages allow.</p>\n<p>Exploring a few practical examples, a Fauna developer might see improvements when dealing with a Fauna-style null value as compared to SQL-style null. A value in Fauna which does not exist in a document or index has a value of null. While SQL generally initializes all values to null, it does not have the concept of non-existent values.</p>\n<p>When sorting data in Fauna, null values handling is simple in that nulls come either first or last depending on an ascending or descending ordering requested by the programmer. In SQL when ordering data, nulls always appear at one end of the sorted results set. Inverting the sorted order (i.e. ascending to descending) does not change where nulls are returned. In order to know which side of the results set null values are returned, a programmer must understand which database vendor they are dealing with or utilize special compensation syntax offered by some database vendors for sorting first or last.</p>\n<p>In Fauna, if you create a unique index on a value that does not exist, then the non-existent value will have a value of null. This allows only a single unknown state for the value. In SQL you may have multiple unknown states for the same value. In our example above, there should only exist a single record for the number of unknown cars for Brandon.</p>\n<p>The use of the null value in Fauna was designed to be more intuitive to the general programmer. The lack of requiring special code for dealing with nulls should reduce the number of errors that traditional SQL developers encounter when not anticipating the use of nulls and omitting the special code to handle nulls. The experience for the application programmer when using Fauna should be more intuitive and productive, and free from many special cases.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-04-17T18:30:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 511,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "45d8da35-35f7-4061-a0b5-65cb0c3d0b86",
        "siteSettingsId": 511,
        "fieldLayoutId": 4,
        "contentId": 344,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Talk Video: Build A Serverless Distributed Ledger with FaunaDB",
        "slug": "talk-video-build-a-serverless-distributed-ledger-without-the-blockchain",
        "uri": "blog/talk-video-build-a-serverless-distributed-ledger-without-the-blockchain",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/talk-video-build-a-serverless-distributed-ledger-without-the-blockchain",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/talk-video-build-a-serverless-distributed-ledger-without-the-blockchain",
        "isCommunityPost": false,
        "blogBodyText": "<p>Distributed databases offer better performance, more flexibility and richer security primitives than blockchain-based algorithms. Most contracts are signed by counterparties who know each other, and most business is not conducted anonymously. Real-world distributed ledger applications are better off using a transactional NoSQL database instead of writing bespoke blockchain algorithms.</p>\r\n<p>Using a distributed database for your distributed ledger directly addresses concerns like:</p>\r\n<ul><li>Correctness </li><li>Data retention </li><li>Permissioned access </li><li>Performance and scale </li><li>Reporting </li><li>Application code simplicity </li><li>Failure robustness </li></ul>\r\n<p>Blockchain algorithms must be written specifically to fit business rules, so they can’t innovate in an agile way. Database applications have generations of best practices behind their design and operation.</p>\r\n<p>Unless you have specific requirements that force you to use a blockchain, you’re better off building your distributed ledger using a distributed database like FaunaDB. This video from my talk at <a href=\"https://gotober.com/2017/sessions/335\">GOTOBerlin</a> goes into details, complete with serverless code examples, to show how much more productive you can be when you use the right tool for the job.</p>\r\n<div class=\"video-container\"><figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7XX120vhy_M\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></figure></div>",
        "blogCategory": [
            "3",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Distributed databases offer better performance, more flexibility and richer security primitives than blockchain-based algorithms. Most contracts are signed by counterparties who know each other, and most business is not conducted anonymously. Real-world distributed ledger applications are better off using a transactional NoSQL database instead of writing bespoke blockchain algorithms.</p>\n<p>Using a distributed database for your distributed ledger directly addresses concerns like:</p>\n<ul><li>Correctness </li><li>Data retention </li><li>Permissioned access </li><li>Performance and scale </li><li>Reporting </li><li>Application code simplicity </li><li>Failure robustness </li></ul><p>Blockchain algorithms must be written specifically to fit business rules, so they can’t innovate in an agile way. Database applications have generations of best practices behind their design and operation.</p>\n<p>Unless you have specific requirements that force you to use a blockchain, you’re better off building your distributed ledger using a distributed database like FaunaDB. This video from my talk at <a href=\"https://gotober.com/2017/sessions/335\">GOTOBerlin</a> goes into details, complete with serverless code examples, to show how much more productive you can be when you use the right tool for the job.</p>\n<div class=\"video-container\"><figure></figure></div>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-04-12T16:22:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 510,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "614a13cb-14a3-456a-a571-13f28d8f7e10",
        "siteSettingsId": 510,
        "fieldLayoutId": 4,
        "contentId": 343,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Index Queries in FaunaDB",
        "slug": "index-queries-in-faunadb",
        "uri": "blog/index-queries-in-faunadb",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/index-queries-in-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/index-queries-in-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>Most applications need to look up database records by attribute, for instance to find all customers in a given postal code, or to create a game leaderboard by listing players by high score. Without database indexes, these queries would have to examine every record in order to return the correct result. Indexes are storage structures maintained by the database system to facilitate efficient retrieval of subsets of the data.<br></p>\r\n<figure style=\"float: right; margin: 0px 0px 10px 10px;\"><img src=\"{asset:403:url}\" alt=\"Card puncher\" title=\"Card puncher\" data-image=\"el76nd4tfstj\"><figcaption>Entering data into a card index</figcaption></figure>\r\n<p>There are a few types of indexes, in this post we’ll focus two types: term indexes that support lookup by secondary key, and range indexes which support sorted pagination.</p>\r\n<p>Term-based indexes can be used to match a login token to a user record when processing an authentication request, or for other simple queries like listing all customers in a given postal code, or all content with a particular tag. Beyond conceptual simplicity, an advantage of term-based indexes is that they can be supported by lightweight data structures which scale out easily to support big data without performance degradation. A downside is that they are limited to finding items by exact match on the term, and don’t typically support paginating across terms. As such they are a useful part of the toolkit when building high scale applications, as they can segment the data in a way that is fast to work with.</p>\r\n<p>Range indexes support sorted pagination, so you can browse items by price, songs by artist, albums by year released, players by high score, etc. They are typically backed by something like a B-tree storage engine to allow for efficient scanning. This is fast on a single machine, but scalable systems must partition ranges to allow an individual index to grow beyond a single machine. As the number of partitions increases, the probability of a query encountering a slow-responding node goes up. This means developers of large scale systems should prefer secondary key lookup indexes to range queries when working with big data. But that isn’t always a tradeoff one can make.</p>\r\n<p>FaunaDB offers a developer friendly way to combine the strengths of both approaches when indexing your data. Each record in FaunaDB can be indexed by <strong>term</strong> for fast scalable lookups. Within each term, records are sorted into ranges according to covered <strong>value</strong>. This allows your indexing scheme to evolve as your app grows. For small apps where ease of development is the primary concern, sorting all events by timestamp might make sense. But as your app grows, it could make more sense to group events by day, sorting by timestamp within each day. That way each day’s index is of a manageable fixed size, and older index data becomes read-mostly. These kinds of patterns are adopted by large scale apps the hard way, but doing this in FaunaDB would be as simple as using the event’s date’s day as the index term.</p>\r\n<p>Every scalable database takes its own approach to offering developers control over index locality. I like the way FaunaDB balances giving control over data layout with a simple programmatic API that requires no administrative support to use, so even FaunaDB Cloud customers (or application developers connected to your FaunaDB Enterprise cluster) can control index layout in a scalable way.</p>\r\n<h2>Using Term Indexes</h2>\r\n<p>Here is an example of a simple term index definition in FaunaDB, looking up a user by email address. Note that this index includes a uniqueness constraint, so attempting to create multiple users with duplicate email addresses will fail:</p>\r\n<pre class=\"language-javascript\">q.CreateIndex(\r\n    {\r\n      name: \"users_by_email\",\r\n      source: q.Class(\"users\"),\r\n      terms: [{ field: [\"data\", \"email\"] }],\r\n      unique: true\r\n    })</pre>\r\n<p>Here is a query using that index to load the user with address “<a href=\"mailto:foo@example.com\">foo@example.com</a>”:</p>\r\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"users_by_email\"), \"foo@example.com\"))\r\n</pre>\r\n<p>The result of this query is a reference to the user record with the matching email address. Because of the uniqueness constraint, there can only be one result. Other use cases, such as listing customers by postal code, could return multiple results for a term. Queries that return multiple results from a single term are also efficient, as the values within a term are stored together. In the next section we’ll talk about ordering results within a term.</p>\r\n<p>Because terms are distributed evenly across the cluster, but the data for each term is stored together, this pattern avoids hot spots while keeping cluster crosstalk to a minimum. FaunaDB’s index partitioning spreads the load across the cluster, so there’s no limit to the number of results that a term can contain.<br></p>\r\n<h2>Using Range Queries</h2>\r\n<p>Here is an example of a range query using an index to load all blogs post by their publishing date. The index definition does not specify a term, so all records in the class are indexed under the class ref. At FaunaDB we like to call this a “class index” because it allows pagination of all members of the class. In this case, the covered values we’ll paginate over are stored in the “published_on” field. It’s also possible to omit the value specification as well as the term, in which case you can paginate over all members of the class.</p>\r\n<pre class=\"language-javascript\">q.CreateIndex(\r\n    {\r\n      name: \"posts_by_date\",\r\n      source: q.Class(\"posts\"),\r\n      values: [{ field: [\"data\", \"published_on\"] }]\r\n    })</pre>\r\n<p>To query this index for the most recent posts by date, we run this query to retrieve the last page by date. Note the synthetic cursor used as the second argument to Paginate, `{before: null}` refers to the page after the last page, so we’ll get the last page here, with a marker to the page before it.</p>\r\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"posts_by_date\")), {before: null})</pre>\r\n<p>Our results look something like this:</p>\r\n<pre class=\"language-javascript\">{\r\n  \"before\": [\r\n    1502539900905543,\r\n    q.Ref(\"classes/posts/175743538973114881\")\r\n  ],\r\n  \"after\": [\r\n    null\r\n  ],\r\n  \"data\": [\r\n    1502539900905344,\r\n    1502539900905545\r\n  ]\r\n}</pre>\r\n<p>If we use the new “before” cursor (a combination of a timestamp and a FaunaDB Ref) in place of `null` in the query we just issued, we’ll get the previous page of results. Requesting the page of older results looks like this:</p>\r\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"posts_by_date\")), {before: [\r\n    1502539900905543,\r\n    q.Ref(\"classes/posts/175743538973114881\")\r\n  ]})</pre>\r\n<p>The before cursor comes directly from the previous response, and we can continue to chain requests, using each response’s “before” cursor. Recall that in this example we are browsing blog posts, most recent first. If we wanted to traverse the index in the standard order, we could make our first request without a synthetic cursor, and then use the “after” cursor to chain each page to the next. See <a href=\"https://fauna.com/documentation/reference/queryapi#paginate_set_params\">the pagination documentation here</a> for details.</p>\r\n<p>In my development career, I’ve always found the best way to learn a new pagination API is to play with it by hand. To facilitate this, I find it’s easier to work with small pages. To set the page size smaller, try adding a `{size: 3}` option to your Paginate call, and then play around with synthetic cursors like {before: null} and {after: 0}.</p>\r\n<h2>Hybrid Term and Range Queries</h2>\r\n<p>FaunaDB’s power really shines when you combine both approaches, and break up a range index into logical segments. The FaunaDB index system groups items by term, and within a term sorts them by value. For instance, you can index all posts in a topic by date within their topic. That index definition would look like this:</p>\r\n<pre class=\"language-javascript\">q.CreateIndex(\r\n    {\r\n      name: \"posts_by_date\",\r\n      source: q.Class(\"posts\"),\r\n      terms: [{ field: [\"data\", \"topic\"] }],\r\n      values: [{ field: [\"data\", \"published_on\"] }]\r\n})</pre>\r\n<p>This allows efficient retrieval of posts from any particular topic, and an unlimited number of topics. The number of posts in the popular topics won’t impact the performance of queries in the less popular topics, and the whole system scales fine no matter how many topics exist.</p>\r\n<p>It’s also worth noting that both the terms and values of an index can be compound, pulling data from more than one field. Combined with the ability to specify some covered values to be indexed in reverse order, allows for complex real-world indexing schemes.</p>\r\n<p>Other use cases for hybrid index styles include serving online catalogs, gathering features for machine learning, segmenting networks by city or neighborhood, browsing calendar event entries, etc.</p>\r\n<p>If you made it this far, you’re probably ready to check out the <a href=\"https://fauna.com/documentation/reference/queryapi\">reference documentation to the FaunaDB query API.</a></p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Most applications need to look up database records by attribute, for instance to find all customers in a given postal code, or to create a game leaderboard by listing players by high score. Without database indexes, these queries would have to examine every record in order to return the correct result. Indexes are storage structures maintained by the database system to facilitate efficient retrieval of subsets of the data.<br /></p>\n<figure style=\"float:right;margin:0px 0px 10px 10px;\"><img src=\"{asset:403:url||https://fauna.com/assets/site/blog-legacy/1280px-Card_puncher_-_NARA_-_513295.jpg}\" alt=\"Card puncher\" title=\"Card puncher\" /><figcaption>Entering data into a card index</figcaption></figure><p>There are a few types of indexes, in this post we’ll focus two types: term indexes that support lookup by secondary key, and range indexes which support sorted pagination.</p>\n<p>Term-based indexes can be used to match a login token to a user record when processing an authentication request, or for other simple queries like listing all customers in a given postal code, or all content with a particular tag. Beyond conceptual simplicity, an advantage of term-based indexes is that they can be supported by lightweight data structures which scale out easily to support big data without performance degradation. A downside is that they are limited to finding items by exact match on the term, and don’t typically support paginating across terms. As such they are a useful part of the toolkit when building high scale applications, as they can segment the data in a way that is fast to work with.</p>\n<p>Range indexes support sorted pagination, so you can browse items by price, songs by artist, albums by year released, players by high score, etc. They are typically backed by something like a B-tree storage engine to allow for efficient scanning. This is fast on a single machine, but scalable systems must partition ranges to allow an individual index to grow beyond a single machine. As the number of partitions increases, the probability of a query encountering a slow-responding node goes up. This means developers of large scale systems should prefer secondary key lookup indexes to range queries when working with big data. But that isn’t always a tradeoff one can make.</p>\n<p>FaunaDB offers a developer friendly way to combine the strengths of both approaches when indexing your data. Each record in FaunaDB can be indexed by <strong>term</strong> for fast scalable lookups. Within each term, records are sorted into ranges according to covered <strong>value</strong>. This allows your indexing scheme to evolve as your app grows. For small apps where ease of development is the primary concern, sorting all events by timestamp might make sense. But as your app grows, it could make more sense to group events by day, sorting by timestamp within each day. That way each day’s index is of a manageable fixed size, and older index data becomes read-mostly. These kinds of patterns are adopted by large scale apps the hard way, but doing this in FaunaDB would be as simple as using the event’s date’s day as the index term.</p>\n<p>Every scalable database takes its own approach to offering developers control over index locality. I like the way FaunaDB balances giving control over data layout with a simple programmatic API that requires no administrative support to use, so even FaunaDB Cloud customers (or application developers connected to your FaunaDB Enterprise cluster) can control index layout in a scalable way.</p>\n<h2>Using Term Indexes</h2>\n<p>Here is an example of a simple term index definition in FaunaDB, looking up a user by email address. Note that this index includes a uniqueness constraint, so attempting to create multiple users with duplicate email addresses will fail:</p>\n<pre class=\"language-javascript\">q.CreateIndex(\n {\n name: \"users_by_email\",\n source: q.Class(\"users\"),\n terms: [{ field: [\"data\", \"email\"] }],\n unique: true\n })</pre>\n<p>Here is a query using that index to load the user with address “<a href=\"mailto:foo@example.com\">foo@example.com</a>”:</p>\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"users_by_email\"), \"foo@example.com\"))\n</pre>\n<p>The result of this query is a reference to the user record with the matching email address. Because of the uniqueness constraint, there can only be one result. Other use cases, such as listing customers by postal code, could return multiple results for a term. Queries that return multiple results from a single term are also efficient, as the values within a term are stored together. In the next section we’ll talk about ordering results within a term.</p>\n<p>Because terms are distributed evenly across the cluster, but the data for each term is stored together, this pattern avoids hot spots while keeping cluster crosstalk to a minimum. FaunaDB’s index partitioning spreads the load across the cluster, so there’s no limit to the number of results that a term can contain.<br /></p>\n<h2>Using Range Queries</h2>\n<p>Here is an example of a range query using an index to load all blogs post by their publishing date. The index definition does not specify a term, so all records in the class are indexed under the class ref. At FaunaDB we like to call this a “class index” because it allows pagination of all members of the class. In this case, the covered values we’ll paginate over are stored in the “published_on” field. It’s also possible to omit the value specification as well as the term, in which case you can paginate over all members of the class.</p>\n<pre class=\"language-javascript\">q.CreateIndex(\n {\n name: \"posts_by_date\",\n source: q.Class(\"posts\"),\n values: [{ field: [\"data\", \"published_on\"] }]\n })</pre>\n<p>To query this index for the most recent posts by date, we run this query to retrieve the last page by date. Note the synthetic cursor used as the second argument to Paginate, `{before: null}` refers to the page after the last page, so we’ll get the last page here, with a marker to the page before it.</p>\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"posts_by_date\")), {before: null})</pre>\n<p>Our results look something like this:</p>\n<pre class=\"language-javascript\">{\n \"before\": [\n 1502539900905543,\n q.Ref(\"classes/posts/175743538973114881\")\n ],\n \"after\": [\n null\n ],\n \"data\": [\n 1502539900905344,\n 1502539900905545\n ]\n}</pre>\n<p>If we use the new “before” cursor (a combination of a timestamp and a FaunaDB Ref) in place of `null` in the query we just issued, we’ll get the previous page of results. Requesting the page of older results looks like this:</p>\n<pre class=\"language-javascript\">q.Paginate(q.Match(q.Index(\"posts_by_date\")), {before: [\n 1502539900905543,\n q.Ref(\"classes/posts/175743538973114881\")\n ]})</pre>\n<p>The before cursor comes directly from the previous response, and we can continue to chain requests, using each response’s “before” cursor. Recall that in this example we are browsing blog posts, most recent first. If we wanted to traverse the index in the standard order, we could make our first request without a synthetic cursor, and then use the “after” cursor to chain each page to the next. See <a href=\"https://fauna.com/documentation/reference/queryapi#paginate_set_params\">the pagination documentation here</a> for details.</p>\n<p>In my development career, I’ve always found the best way to learn a new pagination API is to play with it by hand. To facilitate this, I find it’s easier to work with small pages. To set the page size smaller, try adding a `{size: 3}` option to your Paginate call, and then play around with synthetic cursors like {before: null} and {after: 0}.</p>\n<h2>Hybrid Term and Range Queries</h2>\n<p>FaunaDB’s power really shines when you combine both approaches, and break up a range index into logical segments. The FaunaDB index system groups items by term, and within a term sorts them by value. For instance, you can index all posts in a topic by date within their topic. That index definition would look like this:</p>\n<pre class=\"language-javascript\">q.CreateIndex(\n {\n name: \"posts_by_date\",\n source: q.Class(\"posts\"),\n terms: [{ field: [\"data\", \"topic\"] }],\n values: [{ field: [\"data\", \"published_on\"] }]\n})</pre>\n<p>This allows efficient retrieval of posts from any particular topic, and an unlimited number of topics. The number of posts in the popular topics won’t impact the performance of queries in the less popular topics, and the whole system scales fine no matter how many topics exist.</p>\n<p>It’s also worth noting that both the terms and values of an index can be compound, pulling data from more than one field. Combined with the ability to specify some covered values to be indexed in reverse order, allows for complex real-world indexing schemes.</p>\n<p>Other use cases for hybrid index styles include serving online catalogs, gathering features for machine learning, segmenting networks by city or neighborhood, browsing calendar event entries, etc.</p>\n<p>If you made it this far, you’re probably ready to check out the <a href=\"https://fauna.com/documentation/reference/queryapi\">reference documentation to the FaunaDB query API.</a></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-04-06T17:28:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 509,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "12facc57-e2f4-4fef-b872-2b8a6bd128c8",
        "siteSettingsId": 509,
        "fieldLayoutId": 4,
        "contentId": 342,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Stack Overflow and Glitch Notifications in Slack with FaunaDB",
        "slug": "stack-overflow-and-glitch-notifications-in-slack-with-faunadb",
        "uri": "blog/stack-overflow-and-glitch-notifications-in-slack-with-faunadb",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-08-21T14:46:53-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/stack-overflow-and-glitch-notifications-in-slack-with-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/stack-overflow-and-glitch-notifications-in-slack-with-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB user <a href=\"https://github.com/picsoung\">Nicolas Grenié</a> has created two open source applications connecting popular services with Slack chat. If you’ve ever wanted Slack notifications when there’s online activity, his code is a good template, so I’ll link to some highlights from this post. Check out how simple FaunaDB makes these operations.</p>\r\n<p>Nicolas Grenié agrees, remarking in chat: “Fauna is my default go-to solution for any project I am starting that involves a database.”</p>\r\n<h2>Stack Overflow Monitor</h2>\r\n<p>Stack Overflow is a popular question and answer site, making it an important place for companies to keep up with what users are asking about their products. <a href=\"https://medium.freecodecamp.org/monitor-stack-overflow-activity-directly-into-slack-dc778913490f\">Stack Overflow Monitor</a> simply posts a message to a Slack channel when new questions match your search terms. </p>\r\n<p>As a serverless application, it requires no operational support from you once it is up and running. No servers to upgrade or restart, just code. Installing it only requires altering one settings file, and using the <a href=\"https://serverless.com\">serverless</a> command to deploy functions to AWS Lambda. Once configured with Stack Overflow, Slack, and FaunaDB API credentials, and your chosen search terms, it will automatically update your Slack channel as long as you keep your function in place.</p>\r\n<p>The entire <a href=\"https://github.com/picsoung/stackOverflowMonitor/blob/master/handler.js\">application logic is implemented in 101 lines of JavaScript</a> and is triggered by AWS Lambda’s cron-like task scheduler. Each time it runs, it searches Stack Overflow for recent matches, and then uses FaunaDB to look up the message id. If a record exists that means it’s already been posted, otherwise the message is posted to Slack and a record is created. To give you an idea how simple that code is:</p>\r\n<pre class=\"language-javascript\">        client.query(\r\n          q.Create(\r\n            q.Class('questions'),\r\n              {data: question }\r\n            ));\r\n        sendToSlack(question);</pre>\r\n<p>Because it is a simple Stack Overflow monitor, it doesn’t take advantage of FaunaDB’s transactional capabilities. Hardening the script for edge cases might involve tracking Slack API call success / failure state in a FaunaDB object as well, so that no messages are dropped.</p>\r\n<p>To <a href=\"https://medium.freecodecamp.org/monitor-stack-overflow-activity-directly-into-slack-dc778913490f\">install Stack Overflow Monitor, follow the instructions</a> in the blog post.</p>\r\n<h2>Glitch Notifier</h2>\r\n<p><a href=\"https://glitch.com/\">Glitch</a> is a community where developers can help each other learn new skills. Answerers may only be interested in answering a narrow set of questions, so it’s nice to be alerted about activity instead of visiting the site to check for updates. <a href=\"https://dev.to/picsoung/get-better-at-programming-by-helping-others-on-glitch-1b4\">Glitch Notifier</a> is a service that alerts you in a Slack channel when new questions are available on Glitch. </p>\r\n<p>Rather than a single-tenant serverless application, where you deploy an instance according to your parameters, Glitch notifier runs as a SaaS endpoint, allowing Glitch users to connect their Slack accounts and specify Glitch tags to follow. You can <a href=\"https://glitch.com/edit/#!/glitch-notifier?path=questions.js:39:0\">view the source code here</a>, and if you sign up for a Glitch account, you can even launch your own fork of it.</p>\r\n<p>Glitch Notifier uses FaunaDB not only to store the message ids of questions it has processed, but also to store each user’s notification preferences. Glitch questions are tagged, and users tell Glitch notifier which tags they’d like to be alerted on. Here is the query where the notifier loads all the users interested in the tags for a question. </p>\r\n<pre class=\"language-javascript\">  var users_in_db = client.query(\r\n              q.Paginate(\r\n              q.Union(\r\n                q.Map(question.details.tags,\r\n                  function(tag) {\r\n                    return q.Match(q.Index(\"user_by_tag\"), tag)\r\n                  })\r\n              )));</pre>\r\n<p>Once the user list is loaded, Glitch Notifier posts the messages to Slack using the endpoint stored with each user record.</p>\r\n<h2>Conclusion</h2>\r\n<p>Nicolas says helping others learn new skills is “a great way to give back to the community that welcomed me when I began.” We hope his code example above help you get started. If you’d like a code-level introduction to FaunaDB, here is <a href=\"https://fauna.com/documentation/howto/crud\">our hello world tutorial</a>.</p>\r\n<p>FaunaDB costs nothing to get started with, and has a pay-as-you-go model that makes it easy to get started. <a href=\"https://fauna.com/sign-up\">Sign up today</a> and you can create your own apps for free. Here are two more serverless starter kits: <a href=\"https://serverless.com/blog/faunadb-serverless-authentication/\">Authentication Boilerplate</a>, and <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">Distributed Ledger</a>.</p>",
        "blogCategory": [
            "3"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB Cloud user <a href=\"https://github.com/picsoung\">Nicolas Grenié</a> has created two open source applications connecting popular services with Slack chat. If you’ve ever wanted Slack notifications when there’s online activity, his code is a good template, so I’ll link to some highlights from this post. Check out how simple FaunaDB makes these operations.</p>\n<p>Nicolas Grenié agrees, remarking in chat: “Fauna is my default go-to solution for any project I am starting that involves a database.”</p>\n<h2>Stack Overflow Monitor</h2>\n<p>Stack Overflow is a popular question and answer site, making it an important place for companies to keep up with what users are asking about their products. <a href=\"https://medium.freecodecamp.org/monitor-stack-overflow-activity-directly-into-slack-dc778913490f\">Stack Overflow Monitor</a> simply posts a message to a Slack channel when new questions match your search terms. </p>\n<p>As a serverless application, it requires no operational support from you once it is up and running. No servers to upgrade or restart, just code. Installing it only requires altering one settings file, and using the <a href=\"https://serverless.com\">serverless</a> command to deploy functions to AWS Lambda. Once configured with Stack Overflow, Slack, and FaunaDB API credentials, and your chosen search terms, it will automatically update your Slack channel as long as you keep your function in place.</p>\n<p>The entire <a href=\"https://github.com/picsoung/stackOverflowMonitor/blob/master/handler.js\">application logic is implemented in 101 lines of JavaScript</a> and is triggered by AWS Lambda’s cron-like task scheduler. Each time it runs, it searches Stack Overflow for recent matches, and then uses FaunaDB to look up the message id. If a record exists that means it’s already been posted, otherwise the message is posted to Slack and a record is created. To give you an idea how simple that code is:</p>\n<pre class=\"language-javascript\"> client.query(\n q.Create(\n q.Class('questions'),\n {data: question }\n ));\n sendToSlack(question);</pre>\n<p>Because it is a simple Stack Overflow monitor, it doesn’t take advantage of FaunaDB’s transactional capabilities. Hardening the script for edge cases might involve tracking Slack API call success / failure state in a FaunaDB object as well, so that no messages are dropped.</p>\n<p>To <a href=\"https://medium.freecodecamp.org/monitor-stack-overflow-activity-directly-into-slack-dc778913490f\">install Stack Overflow Monitor, follow the instructions</a> in the blog post.</p>\n<h2>Glitch Notifier</h2>\n<p><a href=\"https://glitch.com/\">Glitch</a> is a community where developers can help each other learn new skills. Answerers may only be interested in answering a narrow set of questions, so it’s nice to be alerted about activity instead of visiting the site to check for updates. <a href=\"https://dev.to/picsoung/get-better-at-programming-by-helping-others-on-glitch-1b4\">Glitch Notifier</a> is a service that alerts you in a Slack channel when new questions are available on Glitch. </p>\n<p>Rather than a single-tenant serverless application, where you deploy an instance according to your parameters, Glitch notifier runs as a SaaS endpoint, allowing Glitch users to connect their Slack accounts and specify Glitch tags to follow. You can <a href=\"https://glitch.com/edit/#!/glitch-notifier?path=questions.js:39:0\">view the source code here</a>, and if you sign up for a Glitch account, you can even launch your own fork of it.</p>\n<p>Glitch Notifier uses FaunaDB not only to store the message ids of questions it has processed, but also to store each user’s notification preferences. Glitch questions are tagged, and users tell Glitch notifier which tags they’d like to be alerted on. Here is the query where the notifier loads all the users interested in the tags for a question. </p>\n<pre class=\"language-javascript\"> var users_in_db = client.query(\n q.Paginate(\n q.Union(\n q.Map(question.details.tags,\n function(tag) {\n return q.Match(q.Index(\"user_by_tag\"), tag)\n })\n )));</pre>\n<p>Once the user list is loaded, Glitch Notifier posts the messages to Slack using the endpoint stored with each user record.</p>\n<h2>Conclusion</h2>\n<p>Nicolas says helping others learn new skills is “a great way to give back to the community that welcomed me when I began.” We hope his code example above help you get started. If you’d like a code-level introduction to FaunaDB, here is <a href=\"https://fauna.com/documentation/howto/crud\">our hello world tutorial</a>.</p>\n<p>FaunaDB Cloud costs nothing to get started with, and has a pay-as-you-go model that makes it easy to get started. <a href=\"https://fauna.com/sign-up\">Sign up today</a> and you can create your own apps for free. Here are two more serverless starter kits: <a href=\"https://serverless.com/blog/faunadb-serverless-authentication/\">Authentication Boilerplate</a>, and <a href=\"https://blog.fauna.com/distributed-ledger-without-the-blockchain\">Distributed Ledger</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-03-29T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 522,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "138ebc74-b04d-41e6-8663-efa7d94ab263",
        "siteSettingsId": 522,
        "fieldLayoutId": 4,
        "contentId": 355,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started with FaunaDB: Fauna Query Language (FQL)",
        "slug": "getting-started-with-faunadb-fauna-query-language-fql",
        "uri": "blog/getting-started-with-faunadb-fauna-query-language-fql",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-03-01T18:56:22-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-with-faunadb-fauna-query-language-fql",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-with-faunadb-fauna-query-language-fql",
        "isCommunityPost": false,
        "blogBodyText": "<p>Previously in my Quick Start Guide, I demonstrated how to create a database and perform basic write and read operations through the Cloud dashboard. In this post we dive into the use of FQL (Fauna Query Language), which is the main interface for all interactions with FaunaDB. FQL is a powerful yet approachable and flexible query language. As in my previous post, we'll use Fauna Cloud to illustrate our examples, but you can also apply examples to an instance of FaunaDB deployed on-premises.</p>\r\n<h2>Overview</h2>\r\n<p>FQL is an&nbsp;<em>expression-oriented</em>&nbsp;language. It supports common application-level constructs such as variables, loops and conditional logic. It also supports a rich set of functional programming and relational database utilities such as map, filter, union, join, distinct, etc. While not a general purpose programming language, FQL provides much of the functionality expected from one, allowing for complex, precise manipulation and retrieval of data stored within your database.</p>\r\n<p>Since FaunaDB is a fully consistent and transactional system, an entire query commits atomically. Imagine, for example, a query that loads a range from an index, iterates over the rows, then runs conditional logic to load more data and update some of the documents. Since the entire query commits atomically, no changes are committed if something goes wrong along the way. So you’re not left with data in a half-written state.</p>\r\n<h2>FQL Drivers</h2>\r\n<p>The Fauna Query Language is available throughdrivers in several popular programming languages. As of this writing these include: Java, Javascript, Scala, Ruby, C#, Python, Go and Swift. Each driver provides a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Domain-specific_language\" rel=\"nofollow,noindex\" target=\"_blank\">domain-specific language</a>&nbsp;allowing you to encode queries in the language of your choice.</p>\r\n<p>Each driver is available as an import in its language’s standard library import interface. For Javascript, the driver is available as a&nbsp;<a href=\"https://www.npmjs.com/package/faunadb\" rel=\"nofollow,noindex\" target=\"_blank\">package on npm</a>&nbsp;and is imported like this:</p>\r\n<pre>var fdb = require('faunadb');</pre>\r\n<h2>FaunaDB Query Construction</h2>\r\n<p>With a driver imported into your code, you’ll have tools at your disposal for creating transactional queries. The&nbsp;<code>fdb.query</code>&nbsp;object contains all of the functions to create query expressions programmatically. In Javascript we can save this to a variable&nbsp;<code>q</code>&nbsp;for convenience and brevity:</p>\r\n<pre>var q = fdb.query;</pre>\r\n<p>Constructing a query will often involve nesting together a series of query expressions. Detailed documentation on all of the available functions can be found in our&nbsp;<a href=\"https://app.fauna.com/documentation/reference/queryapi\" rel=\"nofollow,noindex\" target=\"_blank\">Query Syntax Reference</a>.</p>\r\n\r\n<h2>FaunaDB Query Client</h2>\r\n<p>A FaunaDB client is used to send queries to your database. This client needs to have access to your database. Access is granted via a key secret during client instantiation. (Refer to&nbsp;<a href=\"/blog/getting-started-w-faunadb-quickstart-guide\" rel=\"nofollow,noindex\" target=\"_blank\">step 3 of my Quick Start Guide</a>&nbsp;for how to generate a database-level service key.)</p>\r\n<pre>var client = new faunadb.Client({  secret: 'YOUR_FAUNADB_SECRET'\r\n});</pre>\r\n<p>The query you build with the&nbsp;<code>fdb.query</code>&nbsp;functions can be executed against your database by passing it through this client:</p>\r\n<pre>client.query(  q.Get(Ref(\"classes/my_items/1\"))\r\n);</pre>\r\n<p>You can group multiple queries into one client call by bundling them into an array, like this:</p>\r\n<pre>client.query(  [    q.Get(Ref(\"classes/my_items/1\")),    q.Get(Ref(\"classes/my_items/2\")),    q.Get(Ref(\"classes/my_items/3\"))  ]\r\n);</pre>\r\n<h2>Hands-on Examples</h2>\r\n<p>Let’s move on from theoretical chatter about FQL and get you actually executing some queries! There’s no environment or driver setup needed to follow along with these examples since the Cloud dashboard includes a query console where you can execute queries programmatically using the Javascript driver.</p>\r\n<p>Visit&nbsp;<a href=\"https://fauna.com/sign-up\" rel=\"nofollow,noindex\" target=\"_blank\">https://fauna.com/sign-up</a>&nbsp;to sign up for your FaunaDB account.&nbsp;Once you have an account set up, go to&nbsp;&nbsp;<a href=\"https://dashboard.fauna.com/\" rel=\"nofollow,noindex\" target=\"_blank\">https://dashboard.fauna.com/</a>&nbsp;and click the Toggle Query Console link in the bottom right corner:</p><figure><img src=\"{asset:1672:url}\" data-image=\"1672\"><figcaption>The query console in the Cloud dashboard is a convenient way to experiment and execute FQL in the context of your FaunaDB Cloud environment.</figcaption></figure>\r\n\r\n<p>Notes about&nbsp;the query console:</p>\r\n<ul><li>There is no need to import a driver since the Javascript driver is built in.</li><li>There is no need to import a driver since the Javascript driver is built in.</li><li>Your code should contain only query expressions since the dashboard will execute your code in a&nbsp;<code>client.query()</code>&nbsp;statement for you.</li></ul>\r\n<p>A variable&nbsp;<code>q</code>&nbsp;exists in the console which is equivalent to&nbsp;<code>fdb.query</code>&nbsp;as described above.</p>\r\n<p>The below example builds on the examples from ourprevious post where you constructed a database, class and index for a to-do list. Enter the following code snippet in your query console then click&nbsp;<strong>Run</strong>&nbsp;to practice writing and reading programmatically in your database:</p>\r\n<pre>// Create an additional to-do item (an instance of the my_items class)\r\nq.Create(q.Class(\"my_items\"), {  data: { title: \"call dad\" }\r\n});\r\n// Get all the instances of the my_items class\r\nq.Paginate(  q.Match(    q.Index(\"all_my_items\")  )\r\n);</pre>\r\n<h2>Conclusion</h2>\r\n<p>If you’re familiar with other NoSQL solutions, you may have noticed many similarities with their query languages. However, unlike other NoSQL solutions, FQL offers the added benefits of supporting transactions, joins, unique constraints, user-defined functions and other relational features that are typically only found in traditional SQL systems. And since FaunaDB is document-relational, your documents look like JSON, but they can have references to other documents and you can follow these references like a graph in your queries.</p>\r\n<p>The examples in this post cover just the basics of getting started programmatically interacting with FaunaDB. Much more is possible with more advanced queries. Ready to keep learning? Explore our documentation (&nbsp;<a href=\"https://fauna.com/documentation\" rel=\"nofollow,noindex\" target=\"_blank\">https://fauna.com/documentation</a>&nbsp;). You can also check&nbsp;out the Query Syntax Reference (&nbsp;<a href=\"https://fauna.com/documentation/reference/queryapi\" rel=\"nofollow,noindex\" target=\"_blank\">https://fauna.com/documentation/reference/queryapi</a>&nbsp;) since it contains details on all FQL functions.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Previously in my Quick Start Guide, I demonstrated how to create a database and perform basic write and read operations through the Cloud dashboard. In this post we dive into the use of FQL (Fauna Query Language), which is the main interface for all interactions with FaunaDB. FQL is a powerful yet approachable and flexible query language. As in my previous post, we'll use Fauna Cloud to illustrate our examples, but you can also apply examples to an instance of FaunaDB deployed on-premises.</p>\n<h2>Overview</h2>\n<p>FQL is an <em>expression-oriented</em> language. It supports common application-level constructs such as variables, loops and conditional logic. It also supports a rich set of functional programming and relational database utilities such as map, filter, union, join, distinct, etc. While not a general purpose programming language, FQL provides much of the functionality expected from one, allowing for complex, precise manipulation and retrieval of data stored within your database.</p>\n<p>Since FaunaDB is a fully consistent and transactional system, an entire query commits atomically. Imagine, for example, a query that loads a range from an index, iterates over the rows, then runs conditional logic to load more data and update some of the documents. Since the entire query commits atomically, no changes are committed if something goes wrong along the way. So you’re not left with data in a half-written state.</p>\n<h2>FQL Drivers</h2>\n<p>The Fauna Query Language is available throughdrivers in several popular programming languages. As of this writing these include: Java, Javascript, Scala, Ruby, C#, Python, Go and Swift. Each driver provides a <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRG9tYWluLXNwZWNpZmljX2xhbmd1YWdl\" target=\"_blank\" rel=\"noreferrer noopener\">domain-specific language</a> allowing you to encode queries in the language of your choice.</p>\n<p>Each driver is available as an import in its language’s standard library import interface. For Javascript, the driver is available as a <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly93d3cubnBtanMuY29tL3BhY2thZ2UvZmF1bmFkYg==\" target=\"_blank\" rel=\"noreferrer noopener\">package on npm</a> and is imported like this:</p>\n<pre>var fdb = require('faunadb');</pre>\n<h2>FaunaDB Query Construction</h2>\n<p>With a driver imported into your code, you’ll have tools at your disposal for creating transactional queries. The <code>fdb.query</code> object contains all of the functions to create query expressions programmatically. In Javascript we can save this to a variable <code>q</code> for convenience and brevity:</p>\n<pre>var q = fdb.query;</pre>\n<p>Constructing a query will often involve nesting together a series of query expressions. Detailed documentation on all of the available functions can be found in our <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9mYXVuYS5jb20vZG9jdW1lbnRhdGlvbi9yZWZlcmVuY2UvcXVlcnlhcGk=\" target=\"_blank\" rel=\"noreferrer noopener\">Query Syntax Reference</a> . Here’s a short summary of some of the most commonly used functions:</p>\n<figure><img class=\"alignCenter\" src=\"https://img1.tuicool.com/2AFFRfy.png\" alt=\"\" /></figure><h2>FaunaDB Query Client</h2>\n<p>A FaunaDB client is used to send queries to your database. This client needs to have access to your database. Access is granted via a key secret during client instantiation. (Refer to <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9ibG9nLmZhdW5hLmNvbS9nZXR0aW5nLXN0YXJ0ZWQtZmF1bmEtcXVpY2stc3RhcnQtZ3VpZGU=\" target=\"_blank\" rel=\"noreferrer noopener\">step 3 of my Quick Start Guide</a> for how to generate a database-level service key.)</p>\n<pre>var client = new faunadb.Client({ secret: 'YOUR_FAUNADB_SECRET'\n});</pre>\n<p>The query you build with the <code>fdb.query</code> functions can be executed against your database by passing it through this client:</p>\n<pre>client.query( q.Get(Ref(\"classes/my_items/1\"))\n);</pre>\n<p>You can group multiple queries into one client call by bundling them into an array, like this:</p>\n<pre>client.query( [ q.Get(Ref(\"classes/my_items/1\")), q.Get(Ref(\"classes/my_items/2\")), q.Get(Ref(\"classes/my_items/3\")) ]\n);</pre>\n<h2>Hands-on Examples</h2>\n<p>Let’s move on from theoretical chatter about FQL and get you actually executing some queries! There’s no environment or driver setup needed to follow along with these examples since the Cloud dashboard includes a query console where you can execute queries programmatically using the Javascript driver.</p>\n<p>Visit <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9mYXVuYS5jb20vc2lnbi11cA==\" target=\"_blank\" rel=\"noreferrer noopener\">https://fauna.com/sign-up</a> to sign up for your FaunaDB account. Once you have an account set up, go to <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9kYXNoYm9hcmQuZmF1bmEuY29tLw==\" target=\"_blank\" rel=\"noreferrer noopener\">https://dashboard.fauna.com/</a> and click the Toggle Query Console link in the bottom right corner:</p>\n<figure><img class=\"alignCenter\" src=\"https://img0.tuicool.com/y6rueiN.png\" alt=\"\" /><figcaption>The query console in the Cloud dashboard is a convenient way to experiment and execute FQL in the context of your FaunaDB Cloud environment.</figcaption></figure><p>Notes about the query console:</p>\n<ul><li>There is no need to import a driver since the Javascript driver is built in.</li><li>There is no need to import a driver since the Javascript driver is built in.</li><li>Your code should contain only query expressions since the dashboard will execute your code in a <code>client.query()</code> statement for you.</li></ul><p>A variable <code>q</code> exists in the console which is equivalent to <code>fdb.query</code> as described above.</p>\n<p>The below example builds on the examples from ourprevious post where you constructed a database, class and index for a to-do list. Enter the following code snippet in your query console then click <strong>Run</strong> to practice writing and reading programmatically in your database:</p>\n<pre>// Create an additional to-do item (an instance of the my_items class)\nq.Create(q.Class(\"my_items\"), { data: { title: \"call dad\" }\n});\n// Get all the instances of the my_items class\nq.Paginate( q.Match( q.Index(\"all_my_items\") )\n);</pre>\n<h2>Conclusion</h2>\n<p>If you’re familiar with other NoSQL solutions, you may have noticed many similarities with their query languages. However, unlike other NoSQL solutions, FQL offers the added benefits of supporting transactions, joins, key constraints, triggers and other relational features that are typically only found in traditional SQL systems. And since FaunaDB is document-relational, your documents look like JSON, but they can have references to other documents and you can follow these references like a graph in your queries.</p>\n<p>The examples in this post cover just the basics of getting started programmatically interacting with FaunaDB. Much more is possible with more advanced queries. Ready to keep learning? Explore our documentation ( <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9mYXVuYS5jb20vZG9jdW1lbnRhdGlvbg==\" target=\"_blank\" rel=\"noreferrer noopener\">https://fauna.com/documentation</a> ). You can also check out the Query Syntax Reference ( <a href=\"https://tw.saowen.com/rd/aHR0cHM6Ly9mYXVuYS5jb20vZG9jdW1lbnRhdGlvbi9yZWZlcmVuY2UvcXVlcnlhcGk=\" target=\"_blank\" rel=\"noreferrer noopener\">https://fauna.com/documentation/reference/queryapi</a> ) since it contains details on all FQL functions.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-03-26T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 521,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "245727a5-898a-4a69-88aa-257542a5fd02",
        "siteSettingsId": 521,
        "fieldLayoutId": 4,
        "contentId": 354,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started w/ FaunaDB: Quickstart Guide",
        "slug": "getting-started-w-faunadb-quickstart-guide",
        "uri": "blog/getting-started-w-faunadb-quickstart-guide",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-w-faunadb-quickstart-guide",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-w-faunadb-quickstart-guide",
        "isCommunityPost": false,
        "blogBodyText": "<p>The steps below provide the most direct and quickest way to start using FaunaDB. In four minutes you’ll be able to set up your account, create a database and write values to it. Links are provided in the sections below for more detailed information.<br></p>\r\n<ol><li>Sign up</li><li>Create a database</li><li>Get an access key</li><li>Write your first value</li></ol>\r\n<p>&gt; This guide is in the context of our Cloud offering so it makes use of the <a href=\"https://dashboard.fauna.com/db/\">FaunaDB dashboard</a>. Many of these steps can be achieved through a <a href=\"https://fauna.com/documentation/reference/queryapi\">programmatic interface</a> instead -- in the context of our <a href=\"https://fauna.com/documentation/reference/enterpriseconfigref\">Enterprise offering</a>, that’s necessary.</p>\r\n<h2>1/ Sign Up</h2>\r\n<p>A Fauna account allows you to manage your databases, generate access keys and access your data. Create your account and log in at <a href=\"https://fauna.com/login\">https://fauna.com/login</a>.</p>\r\n<h2>2/ Create a Database</h2>\r\n<p>Now that you have an account and are logged in, you can access your FaunaDB dashboard (<a href=\"https://dashboard.fauna.com/\">https://dashboard.fauna.com</a>) to create a new, empty database. Give it any name you want, for example my_todo.</p>\r\n<figure><img src=\"https://lh4.googleusercontent.com/gvS9Bvtaq90o1IljqDZ5LHKt0eZpJ67ECIFWfsSbDcHbHQFh0rTxUTaWv4xqFEvRBc5qD2SRThSTsashblXrQEYwQzpAOheSKSab_AUUC0G5EOLNyOcfnkXuiVqSNEfpATDSMmSy\" width=\"602\" height=\"449\" style=\"border-width: initial; border-style: none; transform: rotate(0rad);\" data-image=\"xwusgb8iq8xx\"></figure>\r\n<p>&gt; You may notice that you already have a database with no name (labeled simply: /). This is a special root database that will contain your other databases. You can create further layers of nested databases (dbs within dbs and on and on!). For now, your new database should live as a direct child of the root database.</p>\r\n<h2>3/ Get an Access Key</h2>\r\n<p>Every interaction with your database requires an access key to verify identity and permissions. Keys have <a href=\"https://fauna.com/documentation/intro/security\"><strong>roles</strong> which establish permissions</a> and are associated with a specific database allowing access to its contents. </p>\r\n<p>The FaunaDB dashboard interface that you’ve been using throughout this guide comes with a built-in <strong>dashboard key</strong> which is an admin key on the root database. This allows you to perform any action available in the dashboard without the need to create any custom keys. Custom keys are however necessary for <a href=\"https://fauna.com/documentation/reference/queryapi\">interacting programmatically</a> with your database from outside the dashboard. So, in case you do want to interact programmatically this step will show you how to set up custom keys. If, for the moment, you don’t have any desire to interact with FaunaDB outside of the dashboard then feel free to skip to the next step where you will write your first value.</p>\r\n<p>You can manage your root-level admin keys here: <a href=\"https://fauna.com/account/keys\">https://fauna.com/account/keys</a>. You will see the aforementioned dashboard key already present in this admin keys interface.</p>\r\n<p>You can manage database-level keys from within the dashboard. To create a database-level key that will let you read and write data in your new database, go to the root database* in your dashboard and click <strong>Options &gt; Manage Keys &gt; Create a Key</strong>. Set the <strong>Role</strong> to <strong>server</strong> and select your database from the <strong>Database</strong> dropdown. You are free to choose the name of your key.</p>\r\n<figure><img src=\"https://lh4.googleusercontent.com/44DT7wJ6WNJiTljyOgf0ci9D20nZgYAE7p1XN6MnuZOafrEtBm7EhmqnNwxC0GXuf2eMcMdvs0PiqnvEZyFmLkyqYjuZepOltMgUob3fb5PVtzxT5gzA5nhVG1Yx5sSQnjzYaIaR\" width=\"602\" height=\"449\" style=\"border-width: initial; border-style: none; transform: rotate(0rad);\" data-image=\"kmdqi7org1ls\"></figure>\r\n<p><em>Important: Your key’s secret will appear only once after it’s been created. Please jot it down in a safe place and don’t share it with anyone.</em></p>\r\n<p>*It’s necessary to create this key from the root database because a key belongs to the containing parent database in order to be scoped to the database it’s intended to access. Learn more about keys and access control in our <a href=\"https://fauna.com/documentation/intro/security\">security docs</a>.</p>\r\n<h2>4/ Write Your First Value</h2>\r\n<p>You now have an empty database and, more importantly, you are able to read and write values in it! Data within FaunaDB is stored as individual records of objects. Each of these records is an instance of a class which you predefine within your database. So to proceed with writing data to your database, you need to create a class which will act as the structure for where your data will reside. Think of classes as a grouping of relatable instances of data records. <strong>The concept of </strong><strong><em>classes</em></strong><strong> in FaunaDB is analogous to </strong><strong><em>tables</em></strong><strong> from the traditional relational database model.</strong> To help visualize the hierarchy:</p>\r\n<table><tbody><tr><td><p><strong>Structure Hierarchy</strong></p></td><td><p><strong>Description</strong></p></td></tr><tr><td><ul><li>Database</li></ul></td><td><p>A grouping of classes.</p></td></tr><tr><td><ul><li>Class</li></ul></td><td><p>A grouping of relatable instances. Analogous to <strong>tables</strong> in SQL.</p></td></tr><tr><td><ul><li>Instance</li></ul></td><td><p>A record, this is your actual <em>data</em>. These are structured documents and can include:</p><ul><li>recursively nested objects</li><li>arrays</li><li>scalar types</li></ul></td></tr></tbody></table>\r\n<h3>Managing Classes</h3>\r\n<p>Create a class through the Cloud dashboard by clicking on <strong>Create Class</strong> within the context of your database. Give it any name you want, but it should be descriptive of the type of data it will contain such as my_items for this example.</p>\r\n<figure><img src=\"https://lh5.googleusercontent.com/Ptukr1ebBb9Jpq_MD_EgZdX9_pmA1O3taVPJqGaPXMVaOIgsfKeIyh22xz_EWrxw4Y-39Rz3XIyphz1PYHVH5Qwl2ncgxE1BAQiu2WUQyL0EnOpEvshOQf7S5ulp76n0GVNgY93n\" width=\"602\" height=\"449\" style=\"border-width: initial; border-style: none; transform: rotate(0rad);\" data-image=\"hs5w5m6qjjdg\"></figure>\r\n<p><em>While optional, it’s recommended to select “Create Class Index” since it permits you to browse instances you’ll create within this class.</em></p>\r\n<h3>Managing Instances</h3>\r\n<p>With a class now created, you are able to make your first data entry. Click the <strong>Create Instance</strong> tab within your class and define a <a href=\"https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON\">JSON object</a> with whatever data you desire, such as:</p>\r\n<pre class=\"language-json\">{\r\n  title: \"buy milk\"\r\n}</pre>\r\n<figure><img src=\"https://lh3.googleusercontent.com/H67txq_SE4O5XFuZ72uSBi3i_PrFuJyc3ljVdKVYlTzmptQLod0B-JWE_zilpFzB5GR-hDgQ_iBKUSNX7HZ6b66zjg5rSL2hrrYIur6ct5QPnHw4cX_tKlZIwmJ2Ul77RVQKJaJh\" width=\"602\" height=\"449\" style=\"border-width: initial; border-style: none; transform: rotate(0rad);\" data-image=\"a4ll4skgimjz\"></figure>\r\n<p>To create multiple records in your my_items class, repeat this step and use different values for title (e.g. \"call mom\", etc.).</p>\r\n<p>As mentioned before, data in FaunaDB are stored as objects and you have full flexibility with the contents of each object and how it’s structured. The key of title above could equally have been name instead and you can add other key-value pairs for a richer data entry, such as:</p>\r\n<pre class=\"language-json\">{\r\n  name: \"buy milk\",\r\n  is_complete: false,\r\n  due_date: 1522429200\r\n}</pre>\r\n<p>At this point, I recommend playing a bit with creating entries with different types of data. Click the <strong>Browse Class</strong> tab to see all the instances you’ve created. Clicking into each of these instances will allow you to see metadata associated with each one as well as edit their data.</p>\r\n<figure><img src=\"https://lh4.googleusercontent.com/DHFnMd60WhZDBti5RGrXcCs06NINflQNj8gvKTJdY1Hi3xUM8nQJ71B8QNZhDqr0ZIz5kVzl-RFt6JuJOqGPipadhshHmaQBmEaSBjQjn8MpKkuA8h1iBulUI9t94uL3p9S7dhSx\" width=\"602\" height=\"449\" style=\"border-width: initial; border-style: none; transform: rotate(0rad);\" data-image=\"bv0p1ro4wnah\"></figure>\r\n<p>If you followed all the steps above, you effectively created the following structure within your FaunaDB:</p>\r\n<pre class=\"language-bash\">FaunaDB\r\n└── /\r\n    ├── my_todo\r\n    │   └── my_items\r\n    │       ├── title: \"buy milk\"\r\n    │       └── title: \"call mom\"\r\n    │\r\n~~~~+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    │\r\n    ├────── keys\r\n    │       ├── Dashboard Key†\r\n    │       │    ├ database: /\r\n    │       │    └ role: admin\r\n    │       │\r\n    │       └── my_key\r\n    │            ├ database: my_todo\r\n    │            └ role: server\r\n    │\r\n    └────── indexes\r\n            └── all_my_items‡</pre>\r\n<p>† You will have this custom key only if you followed step number three to generate a server key for your database.</p>\r\n<p>‡ You will have this index only if you selected the option to create a class index during step number four.</p>\r\n<h2>Conclusion</h2>\r\n<p>Data within FaunaDB is conveniently stored as documents and intuitively accessible as JSON objects. The concept of nestable databases with a further hierarchy of classes and instances provides an extensible organizational model. These properties create an approachable framework for managing your data. The Cloud dashboard provides a convenient and visual way to interact with FaunaDB.</p>\r\n<p>Next up in my Getting Started w/ FaunaDB series, we’ll break out of the Cloud dashboard and dive deep into FQL (Fauna Query Language) which is core to interacting with FaunaDB programmatically.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>The steps below provide the most direct and quickest way to start using FaunaDB. In four minutes you’ll be able to set up your account, create a database and write values to it. Links are provided in the sections below for more detailed information.<br /></p>\n<ol><li>Sign up</li><li>Create a database</li><li>Get an access key</li><li>Write your first value</li></ol><p>&gt; This guide is in the context of our Cloud offering so it makes use of the <a href=\"https://dashboard.fauna.com/db/\">FaunaDB dashboard</a>. Many of these steps can be achieved through a <a href=\"https://fauna.com/documentation/reference/queryapi\">programmatic interface</a> instead -- in the context of our <a href=\"https://fauna.com/documentation/reference/enterpriseconfigref\">Enterprise offering</a>, that’s necessary.</p>\n<h2>1/ Sign Up</h2>\n<p>A Fauna account allows you to manage your databases, generate access keys and access your data. Create your account and log in at <a href=\"https://fauna.com/login\">https://fauna.com/login</a>.</p>\n<h2>2/ Create a Database</h2>\n<p>Now that you have an account and are logged in, you can access your FaunaDB dashboard (<a href=\"https://dashboard.fauna.com/\">https://dashboard.fauna.com</a>) to create a new, empty database. Give it any name you want, for example my_todo.</p>\n<figure><img src=\"https://lh4.googleusercontent.com/gvS9Bvtaq90o1IljqDZ5LHKt0eZpJ67ECIFWfsSbDcHbHQFh0rTxUTaWv4xqFEvRBc5qD2SRThSTsashblXrQEYwQzpAOheSKSab_AUUC0G5EOLNyOcfnkXuiVqSNEfpATDSMmSy\" width=\"602\" height=\"449\" alt=\"\" /></figure><p>&gt; You may notice that you already have a database with no name (labeled simply: /). This is a special root database that will contain your other databases. You can create further layers of nested databases (dbs within dbs and on and on!). For now, your new database should live as a direct child of the root database.</p>\n<h2>3/ Get an Access Key</h2>\n<p>Every interaction with your database requires an access key to verify identity and permissions. Keys have <a href=\"https://fauna.com/documentation/intro/security\"><strong>roles</strong> which establish permissions</a> and are associated with a specific database allowing access to its contents. </p>\n<p>The FaunaDB dashboard interface that you’ve been using throughout this guide comes with a built-in <strong>dashboard key</strong> which is an admin key on the root database. This allows you to perform any action available in the dashboard without the need to create any custom keys. Custom keys are however necessary for <a href=\"https://fauna.com/documentation/reference/queryapi\">interacting programmatically</a> with your database from outside the dashboard. So, in case you do want to interact programmatically this step will show you how to set up custom keys. If, for the moment, you don’t have any desire to interact with FaunaDB outside of the dashboard then feel free to skip to the next step where you will write your first value.</p>\n<p>You can manage your root-level admin keys here: <a href=\"https://fauna.com/account/keys\">https://fauna.com/account/keys</a>. You will see the aforementioned dashboard key already present in this admin keys interface.</p>\n<p>You can manage database-level keys from within the dashboard. To create a database-level key that will let you read and write data in your new database, go to the root database* in your dashboard and click <strong>Options &gt; Manage Keys &gt; Create a Key</strong>. Set the <strong>Role</strong> to <strong>server</strong> and select your database from the <strong>Database</strong> dropdown. You are free to choose the name of your key.</p>\n<figure><img src=\"https://lh4.googleusercontent.com/44DT7wJ6WNJiTljyOgf0ci9D20nZgYAE7p1XN6MnuZOafrEtBm7EhmqnNwxC0GXuf2eMcMdvs0PiqnvEZyFmLkyqYjuZepOltMgUob3fb5PVtzxT5gzA5nhVG1Yx5sSQnjzYaIaR\" width=\"602\" height=\"449\" alt=\"\" /></figure><p><em>Important: Your key’s secret will appear only once after it’s been created. Please jot it down in a safe place and don’t share it with anyone.</em></p>\n<p>*It’s necessary to create this key from the root database because a key belongs to the containing parent database in order to be scoped to the database it’s intended to access. Learn more about keys and access control in our <a href=\"https://fauna.com/documentation/intro/security\">security docs</a>.</p>\n<h2>4/ Write Your First Value</h2>\n<p>You now have an empty database and, more importantly, you are able to read and write values in it! Data within FaunaDB is stored as individual records of objects. Each of these records is an instance of a class which you predefine within your database. So to proceed with writing data to your database, you need to create a class which will act as the structure for where your data will reside. Think of classes as a grouping of relatable instances of data records. <strong>The concept of </strong><strong><em>classes</em></strong><strong> in FaunaDB is analogous to </strong><strong><em>tables</em></strong><strong> from the traditional relational database model.</strong> To help visualize the hierarchy:</p>\n<table><tbody><tr><td><p><strong>Structure Hierarchy</strong></p></td><td><p><strong>Description</strong></p></td></tr><tr><td><ul><li>Database</li></ul></td><td><p>A grouping of classes.</p></td></tr><tr><td><ul><li>Class</li></ul></td><td><p>A grouping of relatable instances. Analogous to <strong>tables</strong> in SQL.</p></td></tr><tr><td><ul><li>Instance</li></ul></td><td><p>A record, this is your actual <em>data</em>. These are structured documents and can include:</p><ul><li>recursively nested objects</li><li>arrays</li><li>scalar types</li></ul></td></tr></tbody></table><h3>Managing Classes</h3>\n<p>Create a class through the Cloud dashboard by clicking on <strong>Create Class</strong> within the context of your database. Give it any name you want, but it should be descriptive of the type of data it will contain such as my_items for this example.</p>\n<figure><img src=\"https://lh5.googleusercontent.com/Ptukr1ebBb9Jpq_MD_EgZdX9_pmA1O3taVPJqGaPXMVaOIgsfKeIyh22xz_EWrxw4Y-39Rz3XIyphz1PYHVH5Qwl2ncgxE1BAQiu2WUQyL0EnOpEvshOQf7S5ulp76n0GVNgY93n\" width=\"602\" height=\"449\" alt=\"\" /></figure><p><em>While optional, it’s recommended to select “Create Class Index” since it permits you to browse instances you’ll create within this class.</em></p>\n<h3>Managing Instances</h3>\n<p>With a class now created, you are able to make your first data entry. Click the <strong>Create Instance</strong> tab within your class and define a <a href=\"https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON\">JSON object</a> with whatever data you desire, such as:</p>\n<pre class=\"language-json\">{\n title: \"buy milk\"\n}</pre>\n<figure><img src=\"https://lh3.googleusercontent.com/H67txq_SE4O5XFuZ72uSBi3i_PrFuJyc3ljVdKVYlTzmptQLod0B-JWE_zilpFzB5GR-hDgQ_iBKUSNX7HZ6b66zjg5rSL2hrrYIur6ct5QPnHw4cX_tKlZIwmJ2Ul77RVQKJaJh\" width=\"602\" height=\"449\" alt=\"\" /></figure><p>To create multiple records in your my_items class, repeat this step and use different values for title (e.g. \"call mom\", etc.).</p>\n<p>As mentioned before, data in FaunaDB are stored as objects and you have full flexibility with the contents of each object and how it’s structured. The key of title above could equally have been name instead and you can add other key-value pairs for a richer data entry, such as:</p>\n<pre class=\"language-json\">{\n name: \"buy milk\",\n is_complete: false,\n due_date: 1522429200\n}</pre>\n<p>At this point, I recommend playing a bit with creating entries with different types of data. Click the <strong>Browse Class</strong> tab to see all the instances you’ve created. Clicking into each of these instances will allow you to see metadata associated with each one as well as edit their data.</p>\n<figure><img src=\"https://lh4.googleusercontent.com/DHFnMd60WhZDBti5RGrXcCs06NINflQNj8gvKTJdY1Hi3xUM8nQJ71B8QNZhDqr0ZIz5kVzl-RFt6JuJOqGPipadhshHmaQBmEaSBjQjn8MpKkuA8h1iBulUI9t94uL3p9S7dhSx\" width=\"602\" height=\"449\" alt=\"\" /></figure><p>If you followed all the steps above, you effectively created the following structure within your FaunaDB:</p>\n<pre class=\"language-bash\">FaunaDB\n└── /\n ├── my_todo\n │ └── my_items\n │ ├── title: \"buy milk\"\n │ └── title: \"call mom\"\n │\n~~~~+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n │\n ├────── keys\n │ ├── Dashboard Key†\n │ │ ├ database: /\n │ │ └ role: admin\n │ │\n │ └── my_key\n │ ├ database: my_todo\n │ └ role: server\n │\n └────── indexes\n └── all_my_items‡</pre>\n<p>† You will have this custom key only if you followed step number three to generate a server key for your database.</p>\n<p>‡ You will have this index only if you selected the option to create a class index during step number four.</p>\n<h2>Conclusion</h2>\n<p>Data within FaunaDB is conveniently stored as documents and intuitively accessible as JSON objects. The concept of nestable databases with a further hierarchy of classes and instances provides an extensible organizational model. These properties create an approachable framework for managing your data. The Cloud dashboard provides a convenient and visual way to interact with FaunaDB.</p>\n<p>Next up in my Getting Started w/ FaunaDB series, we’ll break out of the Cloud dashboard and dive deep into FQL (Fauna Query Language) which is core to interacting with FaunaDB programmatically.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2018-03-21T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 520,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "95f94180-2548-493d-b1af-52e15abfba2e",
        "siteSettingsId": 520,
        "fieldLayoutId": 4,
        "contentId": 353,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Getting Started w/ FaunaDB: An Introduction to Background Resources",
        "slug": "getting-started-introduction-to-faunadb-resources",
        "uri": "blog/getting-started-introduction-to-faunadb-resources",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:56-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/getting-started-introduction-to-faunadb-resources",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/getting-started-introduction-to-faunadb-resources",
        "isCommunityPost": false,
        "blogBodyText": "<p>Here at Fauna we’re putting a lot of effort into simplifying the process to get up and running with FaunaDB. Having recently on-boarded to FaunaDB myself, I want to share some notes and resources which I found useful. This is the first in a series of posts to help you get started with FaunaDB -- even if you don’t have that much expertise in databases to begin with.</p>\r\n<p>The goal for this post is two-fold. First, to level-set across a range of audiences and technical backgrounds. Second, to bring you to a place of understanding for why one would even consider a new database solution given&nbsp;<a href=\"https://db-engines.com/en/ranking\">there are so many in existence</a>. This post serves as the foundational building block for subsequent topics we’ll cover in this series. Plus, after reading this you’ll also be able to approach our&nbsp;<a href=\"https://fauna.com/whitepaper\">Technical White Paper</a>’s content and with a deeper level of understanding of its significance.</p>\r\n<p>A little about myself: I’m a software engineer and I don’t consider myself a “database person”. I studied Computer Science, I develop applications and I know plenty about using databases. However, I have&nbsp;<em>not</em>:</p>\r\n<ul><li>Been a DB admin</li><li>Had hands-on experience keeping a production database operational</li><li>Learned the intricacies of data sharding nor what’s involved in the process or maintenance</li><li>Compared performance profiles between low-level database operations</li></ul>\r\n<p>In other words, beyond writing queries in my application logic to read/write data and occasionally running reports, I’ve never had much regard for what happens on the other side. I.e. when I save and retrieve data, I expect it to behave just as if I were running a local database -- any manipulation to the data occurs in transactions and there are no surprises. Admittedly, I take a lot for granted with this mindset even though I know solving the problem of real-world, globally distributed environments is hard. However, I didn’t respect the challenge, sheer hassle and roadblocks that these solutions are up against enough until a deeper exposure to FaunaDB.</p>\r\n<p>Let’s dive into some recommended resources for getting started with FaunaDB. To start us off, I found it helps to take a step back in order to explore how we got to where we are in the first place.</p>\r\n<h2>SQL vs. NoSQL*</h2>\r\n<p>It turns out we can grossly over-generalise all databases into two categories: the traditional “SQL” database and “everything else”*. The former is what I, and potentially you, learned back in our Computer Science classes at university, with popular examples being Oracle DB, MySQL and Postgres. Schemas, Tables, Rows, Primary/Foreign Keys, etc. are the name of the game and the result is a compact and correct relational model to store and access your data.</p>\r\n<p>These were systems built in an age before the Internet; at a time&nbsp;when storage costs were at a premium and distributed systems were a novelty. With the rise of the Internet/mobile phones/social networks/take your pick, traditional databases couldn’t effectively keep up with the amount of data and availability requirements in demand.</p>\r\n<p>Hello NoSQL, aka “everything else”. Actually, rather than a database&nbsp;<em>type</em>, I see NoSQL as more of a&nbsp;<em>movement</em>&nbsp;-- a response to a need from the Internet and mobile Internet revolution. But by choosing to scale at any cost, this movement sacrificed the most attractive properties of traditional SQL databases: type safety, relational integrity, and transactional correctness.</p>\r\n<p>If you’re thinking to yourself that “surely correctness at scale is a solved problem”, be quick to reconsider. While there have been valiant efforts to mitigate the shortcomings of distributed and horizontally scalable databases -- elaborate caching solutions, stringing various types of DB solutions together each with different purposes, abstracting the burden of resolving collisions into application logic and API services, to name a few -- the reality is that most “solutions” are often a combination of non-holistic solutions which add complexity, cost and commit you to a particular system. Only now is there a new breed -- a third category -- of databases emerging and that’s what sets Fauna apart.</p>\r\n<p>When you learn how FaunaDB&nbsp;<em>combines the safety and integrity of transactions with the scalability of NoSQL,</em>&nbsp;the takeaway is that:</p>\r\n<blockquote>FaunaDB offers the best from SQL and NoSQL.</blockquote>\r\n<p>It provides&nbsp;<em>transactional NoSQL on a global scale</em>. FaunaDB is a great fit for use cases that NoSQL couldn’t address due to fundamental architectural limitations since it lacks transactional integrity, such as banking, point rewards systems and other mission-critical applications. Conversely it provides all the scalability, flexibility, security and performance needed in the 21st century.</p>\r\n<p>*Technically, “SQL” is the language one uses to interact with a relational database while NoSQL typically refers to anything that doesn’t have a relational model. So more accurately, the comparison should be “Relational vs. Non-Relational” but alas the industry’s convention is:&nbsp;<strong>SQL</strong>&nbsp;and&nbsp;<strong>NoSQL</strong>.</p>\r\n<h2>Eventual vs. Strong Consistency</h2>\r\n<p>In a distributed database, these are two principle&nbsp;<a href=\"https://en.wikipedia.org/wiki/Consistency_model\">models of data consistency</a>&nbsp;across the distributed system.&nbsp;<a href=\"https://en.wikipedia.org/wiki/Data_consistency\">Data consistency</a>&nbsp;in this context refers to the following: if one were to read a value, will that value be the same regardless of which node it originates from?</p>\r\n<p><em>Strong Consistency</em>&nbsp;is the more intuitive scenario if we think of a non-distributed system. If there is only one database, or one master node responsible for receiving all inbound requests and replicating it across several copies, you’ll get the same value regardless of which replica you read from. If you’re familiar with RAID on hard drives, this would be similar to a&nbsp;<a href=\"https://10gbps.io/blog/advantages-disadvantages-various-raid-levels/\">RAID 1 (Disk Mirroring)</a>&nbsp;model.</p>\r\n<p>To understand&nbsp;<em>Eventual Consistency</em>, I like this&nbsp;<a href=\"https://stackoverflow.com/questions/10078540/eventual-consistency-in-plain-english\">plain English scenario</a>:</p>\r\n<ul><li>I check the weather forecast to learn that it’ll rain tomorrow</li><li>I inform you that it will rain tomorrow</li><li>Your neighbor tells his wife that it’ll be sunny</li><li>You tell your neighbor that it is actually going to rain tomorrow</li></ul>\r\n<p>Eventually everyone knows the truth (that it's going to rain tomorrow), but for a period, the wife comes away thinking it is going to be sunny even though she asked&nbsp;<em>after</em>&nbsp;one or more of the servers (you and me) had a more up-to-date value.</p>\r\n<p>Until now, a system had to choose between these two different models, sacrificing many other attractive or critical properties of the database (or increase operational complexity) in an effort to maintain them. Choose scale, and productivity suffers. Choose productivity, and scale and correctness suffer.</p>\r\n<blockquote>FaunaDB offers a fully consistent solution without any sacrifices.</blockquote>\r\n<h2>ACID & CAP, Calvin & RAFT</h2>\r\n<p>Acronyms! It’s not fair to glance over the significance of each acronym, but in a way I’ll be doing just that at this point. That’s intentional since each merits a longer discussion and therefore it’s easy to quickly find ourselves plunging into a rabbit hole which I’ll spare you from in this post. I invite you to read further into each of these concepts through links provided below.</p>\r\n<h3>ACID & CAP</h3>\r\n<p>When a database is said to be ACID, it speaks to a&nbsp;<em>property of its transactions</em>&nbsp;which guarantees that data integrity is preserved. Officially, it stands for:</p>\r\n<ul><li><strong>Atomicity:</strong>&nbsp;Indivisibility; either a transaction applies in full or not at all.</li><li><strong>Correctness:</strong>&nbsp;Transactions only apply if they are allowed to.</li><li><strong>Isolation:</strong>&nbsp;Any effects of a transaction is visible&nbsp;<em>in full</em>, never partially.</li><li><strong>Durability:</strong>&nbsp;A committed transaction is permanent.</li></ul>\r\n<p>Compliance with these four properties is easy to take for granted. Afterall, all four properties apply in most of our day-to-day experiences. To make an analogy when saving a text file on your local machine:</p>\r\n<ul><li><strong>A:</strong>&nbsp;You don’t expect to only see a couple paragraphs the next time you open the file.</li><li><strong>C:</strong>&nbsp;You get an error message if you’re not permitted to save the file.</li><li><strong>I:</strong>&nbsp;You expect to find the contents of the file if you subsequently search for any terms contained within.</li><li><strong>D:</strong>&nbsp;Next time you restart your computer, the file should still be there.</li></ul>\r\n<p>However, in scaled, distributed systems satisfying all four of these properties poses a very real and challenging problem.</p>\r\n<p><a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">CAP</a>&nbsp;on the other hand, is a&nbsp;<em>property of the database itself</em>&nbsp;rather than of the transactions within. It’s actually a theorem which states that databases have&nbsp;<strong>three</strong>&nbsp;properties of which only at most&nbsp;<strong>two</strong>&nbsp;can be achieved at a time:</p>\r\n<figure><img src=\"https://s3-us-west-2.amazonaws.com/fauna-assets-west-2/blog/images/cap-venn-diagram.png?mtime=20180321084823\" alt=\"CAP Venn Diagram\" title=\"CAP Venn Diagram\" style=\"box-sizing: inherit; max-width: 100%; height: auto; vertical-align: middle; width: 620px;\" data-image=\"r6tkuuu7n4f5\"></figure>\r\n<p></p>\r\n<ul><li>C:&nbsp;<strong>consistency</strong>; a single value for shared data</li><li>A: 100%&nbsp;<strong>availability</strong>; for both reads and updates</li><li>P: tolerance to network&nbsp;<strong>partitions</strong></li></ul>\r\n<p>The premise is that network partitions (failures on the network) is unavoidable in distributed systems in the real world so when they occur, a system must choose between Consistency or Availability.</p>\r\n<p>FaunaDB chooses the former, so strictly speaking it falls into the&nbsp;<strong>CP</strong>&nbsp;group. However, network partitions -- the only real scenario in which CAP applies -- are much more rare compared to other causes of system outages. Therefore, FaunaDB attains the same level of availability that is practically achievable by other database systems with&nbsp;<strong>AP</strong>&nbsp;guarantees.</p>\r\n<p>For further detail, I suggest reading more about these concepts in the following article:&nbsp;<a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">ACID Transactions in a Globally Distributed Database</a>.</p>\r\n<h3>Calvin & RAFT</h3>\r\n<p>Calvin and RAFT refer to implementation details of the architecture and protocols which make most of what’s been discussed above possible. In short,&nbsp;<a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>&nbsp;is a transaction scheduling and data replication layer which Fauna takes inspiration from in order to provide fully ACID transactions in a distributed database system.</p>\r\n<p>FaunaDB makes use of a distributed log for deterministically resolving the order of transactions and RAFT is the algorithm used for reaching a consensus amongst the distributed nodes involved. To further understand what RAFT is and how it works, I recommend this&nbsp;<a href=\"https://raft.github.io/\">short write-up on RAFT</a>and a&nbsp;<a href=\"http://thesecretlivesofdata.com/raft/\">visual understanding of how the RAFT protocol works</a>.</p>\r\n<h2>No Infrastructure Lock-In</h2>\r\n<p>During the early stages of a business or onset of a project, engineering teams are often faced with the decision of what infrastructure to begin building on. Whether this is the database itself or simply the infrastructure to host the application, it can be a tricky choice. After all, no one can predict the future needs of a product. It’s unavoidable that conditions, requirements, capabilities, etc. change over the evolution of your product or business and soon other, more attractive solutions present themselves as much better choices.</p>\r\n<p>However, in practice the overhead and costs to change systems and migrate data are too high to justify so stakeholders often learn to accept the imperfect condition and move on. This is ok -- even wise, I’d argue -- to some degree but the point is that the situation commits a product to a nonoptimal foundation. In the best case, it’s just an annoyance. Often times it adds unnecessary complexities and operational overhead. On the worst case, it can lead to the demise of a product or business altogether.</p>\r\n<p>When we talk about FaunaDB having a&nbsp;<em>no-infra lock-in</em>&nbsp;architecture, we mean that FaunaDB can operate across different hardware and software-hosting solutions. And this doesn’t only apply to a scenario such as Fauna’s&nbsp;<a href=\"https://fauna.com/serverless\">Cloud offering</a>&nbsp;vs. an&nbsp;<a href=\"https://fauna.com/enterprise\">on-premise/Enterprise offering</a>. You can deploy your FaunaDB to your own distributed machines or operate it on a number of other PaaS cloud services†… or even a combination of any simultaneously if needed.</p>\r\n<blockquote>FaunaDB allows business requirements to dictate infrastructure needs instead of infrastructure constraining innovation.</blockquote>\r\n<p>†&nbsp;<a href=\"https://fauna.com/status\">All major Cloud vendors support FaunaDB natively</a>&nbsp;(e.g. Amazon Web Services, Google Cloud Platform, Microsoft Azure) in addition to Fauna’s own Cloud solution.</p>\r\n<h2>Conclusion</h2>\r\n<p>This all sounds pretty exciting, right? Maybe even a little&nbsp;<em>too</em>&nbsp;good to be true? At first I shared these sentiments as well. But here’s the thing. As I keep learning more, it’s becoming clear that I’m only getting started! I invite you to follow along as I continue to share my learnings in this Getting Started w/ FaunaDB series. Next up, we’ll shift gears with a hands-on approach in my FaunaDB Quickstart Guide to get you set up and interacting with your own database in a matter of minutes.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Hi, I’m Paulo, Fauna’s new developer evangelist. Here at Fauna we’re putting a lot of effort into simplifying the process to get up and running with FaunaDB. Having recently on-boarded to FaunaDB myself, I want to share some notes and resources which I found useful. This is the first in a series of posts to help you get started with FaunaDB -- even if you don’t have that much expertise in databases to begin with.</p>\n<p>The goal for this post is two-fold. First, to level-set across a range of audiences and technical backgrounds. Second, to bring you to a place of understanding for why one would even consider a new database solution given <a href=\"https://db-engines.com/en/ranking\">there are so many in existence</a>. This post serves as the foundational building block for subsequent topics we’ll cover in this series. Plus, after reading this you’ll also be able to approach our <a href=\"https://fauna.com/whitepaper\">Technical White Paper</a>’s content and with a deeper level of understanding of its significance.</p>\n<p>A little about myself: I’m a software engineer and I don’t consider myself a “database person”. I studied Computer Science, I develop applications and I know plenty about using databases. However, I have <em>not</em>:</p>\n<ul><li>Been a DB admin</li><li>Had hands-on experience keeping a production database operational</li><li>Learned the intricacies of data sharding nor what’s involved in the process or maintenance</li><li>Compared performance profiles between low-level database operations</li></ul><p>In other words, beyond writing queries in my application logic to read/write data and occasionally running reports, I’ve never had much regard for what happens on the other side. I.e. when I save and retrieve data, I expect it to behave just as if I were running a local database -- any manipulation to the data occurs in transactions and there are no surprises. Admittedly, I take a lot for granted with this mindset even though I know solving the problem of real-world, globally distributed environments is hard. However, I didn’t respect the challenge, sheer hassle and roadblocks that these solutions are up against enough until a deeper exposure to FaunaDB.</p>\n<p>Let’s dive into some recommended resources for getting started with FaunaDB. To start us off, I found it helps to take a step back in order to explore how we got to where we are in the first place.</p>\n<h2>SQL vs. NoSQL*</h2>\n<p>It turns out we can grossly over-generalise all databases into two categories: the traditional “SQL” database and “everything else”*. The former is what I, and potentially you, learned back in our Computer Science classes at university, with popular examples being Oracle DB, MySQL and Postgres. Schemas, Tables, Rows, Primary/Foreign Keys, etc. are the name of the game and the result is a compact and correct relational model to store and access your data.</p>\n<p>These were systems built in an age before the Internet; at a time when storage costs were at a premium and distributed systems were a novelty. With the rise of the Internet/mobile phones/social networks/take your pick, traditional databases couldn’t effectively keep up with the amount of data and availability requirements in demand.</p>\n<p>Hello NoSQL, aka “everything else”. Actually, rather than a database <em>type</em>, I see NoSQL as more of a <em>movement</em> -- a response to a need from the Internet and mobile Internet revolution. But by choosing to scale at any cost, this movement sacrificed the most attractive properties of traditional SQL databases: type safety, relational integrity, and transactional correctness.</p>\n<p>If you’re thinking to yourself that “surely correctness at scale is a solved problem”, be quick to reconsider. While there have been valiant efforts to mitigate the shortcomings of distributed and horizontally scalable databases -- elaborate caching solutions, stringing various types of DB solutions together each with different purposes, abstracting the burden of resolving collisions into application logic and API services, to name a few -- the reality is that most “solutions” are often a combination of non-holistic solutions which add complexity, cost and commit you to a particular system. Only now is there a new breed -- a third category -- of databases emerging and that’s what sets Fauna apart.</p>\n<p>When you learn how FaunaDB <em>combines the safety and integrity of transactions with the scalability of NoSQL,</em> the takeaway is that:</p>\n<blockquote>FaunaDB offers the best from SQL and NoSQL.</blockquote>\n<p>It provides <em>transactional NoSQL on a global scale</em>. FaunaDB is a great fit for use cases that NoSQL couldn’t address due to fundamental architectural limitations since it lacks transactional integrity, such as banking, point rewards systems and other mission-critical applications. Conversely it provides all the scalability, flexibility, security and performance needed in the 21st century.</p>\n<p>*Technically, “SQL” is the language one uses to interact with a relational database while NoSQL typically refers to anything that doesn’t have a relational model. So more accurately, the comparison should be “Relational vs. Non-Relational” but alas the industry’s convention is: <strong>SQL</strong> and <strong>NoSQL</strong>.</p>\n<h2>Eventual vs. Strong Consistency</h2>\n<p>In a distributed database, these are two principle <a href=\"https://en.wikipedia.org/wiki/Consistency_model\">models of data consistency</a> across the distributed system. <a href=\"https://en.wikipedia.org/wiki/Data_consistency\">Data consistency</a> in this context refers to the following: if one were to read a value, will that value be the same regardless of which node it originates from?</p>\n<p><em>Strong Consistency</em> is the more intuitive scenario if we think of a non-distributed system. If there is only one database, or one master node responsible for receiving all inbound requests and replicating it across several copies, you’ll get the same value regardless of which replica you read from. If you’re familiar with RAID on hard drives, this would be similar to a <a href=\"https://10gbps.io/blog/advantages-disadvantages-various-raid-levels/\">RAID 1 (Disk Mirroring)</a> model.</p>\n<p>To understand <em>Eventual Consistency</em>, I like this <a href=\"https://stackoverflow.com/questions/10078540/eventual-consistency-in-plain-english\">plain English scenario</a>:</p>\n<ul><li>I check the weather forecast to learn that it’ll rain tomorrow</li><li>I inform you that it will rain tomorrow</li><li>Your neighbor tells his wife that it’ll be sunny</li><li>You tell your neighbor that it is actually going to rain tomorrow</li></ul><p>Eventually everyone knows the truth (that it's going to rain tomorrow), but for a period, the wife comes away thinking it is going to be sunny even though she asked <em>after</em> one or more of the servers (you and me) had a more up-to-date value.</p>\n<p>Until now, a system had to choose between these two different models, sacrificing many other attractive or critical properties of the database (or increase operational complexity) in an effort to maintain them. Choose scale, and productivity suffers. Choose productivity, and scale and correctness suffer.</p>\n<blockquote>FaunaDB offers a fully consistent solution without any sacrifices.</blockquote>\n<h2>ACID &amp; CAP, Calvin &amp; RAFT</h2>\n<p>Acronyms! It’s not fair to glance over the significance of each acronym, but in a way I’ll be doing just that at this point. That’s intentional since each merits a longer discussion and therefore it’s easy to quickly find ourselves plunging into a rabbit hole which I’ll spare you from in this post. I invite you to read further into each of these concepts through links provided below.</p>\n<h3>ACID &amp; CAP</h3>\n<p>When a database is said to be ACID, it speaks to a <em>property of its transactions</em> which guarantees that data integrity is preserved. Officially, it stands for:</p>\n<ul><li><strong>Atomicity:</strong> Indivisibility; either a transaction applies in full or not at all.</li><li><strong>Correctness:</strong> Transactions only apply if they are allowed to.</li><li><strong>Isolation:</strong> Any effects of a transaction is visible <em>in full</em>, never partially.</li><li><strong>Durability:</strong> A committed transaction is permanent.</li></ul><p>Compliance with these four properties is easy to take for granted. Afterall, all four properties apply in most of our day-to-day experiences. To make an analogy when saving a text file on your local machine:</p>\n<ul><li><strong>A:</strong> You don’t expect to only see a couple paragraphs the next time you open the file.</li><li><strong>C:</strong> You get an error message if you’re not permitted to save the file.</li><li><strong>I:</strong> You expect to find the contents of the file if you subsequently search for any terms contained within.</li><li><strong>D:</strong> Next time you restart your computer, the file should still be there.</li></ul><p>However, in scaled, distributed systems satisfying all four of these properties poses a very real and challenging problem.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">CAP</a> on the other hand, is a <em>property of the database itself</em> rather than of the transactions within. It’s actually a theorem which states that databases have <strong>three</strong> properties of which only at most <strong>two</strong> can be achieved at a time:</p>\n<figure><img src=\"https://s3-us-west-2.amazonaws.com/fauna-assets-west-2/blog/images/cap-venn-diagram.png?mtime=20180321084823\" alt=\"CAP Venn Diagram\" title=\"CAP Venn Diagram\" /></figure><ul><li>C: <strong>consistency</strong>; a single value for shared data</li><li>A: 100% <strong>availability</strong>; for both reads and updates</li><li>P: tolerance to network <strong>partitions</strong></li></ul><p>The premise is that network partitions (failures on the network) is unavoidable in distributed systems in the real world so when they occur, a system must choose between Consistency or Availability.</p>\n<p>FaunaDB chooses the former, so strictly speaking it falls into the <strong>CP</strong> group. However, network partitions -- the only real scenario in which CAP applies -- are much more rare compared to other causes of system outages. Therefore, FaunaDB attains the same level of availability that is practically achievable by other database systems with <strong>AP</strong> guarantees.</p>\n<p>For further detail, I suggest reading more about these concepts in the following article: <a href=\"https://blog.fauna.com/acid-transactions-in-a-globally-distributed-database\">ACID Transactions in a Globally Distributed Database</a>.</p>\n<h3>Calvin &amp; RAFT</h3>\n<p>Calvin and RAFT refer to implementation details of the architecture and protocols which make most of what’s been discussed above possible. In short, <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a> is a transaction scheduling and data replication layer which Fauna takes inspiration from in order to provide fully ACID transactions in a distributed database system.</p>\n<p>FaunaDB makes use of a distributed log for deterministically resolving the order of transactions and RAFT is the algorithm used for reaching a consensus amongst the distributed nodes involved. To further understand what RAFT is and how it works, I recommend this <a href=\"https://raft.github.io/\">short write-up on RAFT</a>and a <a href=\"http://thesecretlivesofdata.com/raft/\">visual understanding of how the RAFT protocol works</a>.</p>\n<h2>No Infrastructure Lock-In</h2>\n<p>During the early stages of a business or onset of a project, engineering teams are often faced with the decision of what infrastructure to begin building on. Whether this is the database itself or simply the infrastructure to host the application, it can be a tricky choice. After all, no one can predict the future needs of a product. It’s unavoidable that conditions, requirements, capabilities, etc. change over the evolution of your product or business and soon other, more attractive solutions present themselves as much better choices.</p>\n<p>However, in practice the overhead and costs to change systems and migrate data are too high to justify so stakeholders often learn to accept the imperfect condition and move on. This is ok -- even wise, I’d argue -- to some degree but the point is that the situation commits a product to a nonoptimal foundation. In the best case, it’s just an annoyance. Often times it adds unnecessary complexities and operational overhead. On the worst case, it can lead to the demise of a product or business altogether.</p>\n<p>When we talk about FaunaDB having a <em>no-infra lock-in</em> architecture, we mean that FaunaDB can operate across different hardware and software-hosting solutions. And this doesn’t only apply to a scenario such as Fauna’s <a href=\"https://fauna.com/serverless\">Cloud offering</a> vs. an <a href=\"https://fauna.com/enterprise\">on-premise/Enterprise offering</a>. You can deploy your FaunaDB to your own distributed machines or operate it on a number of other PaaS cloud services†… or even a combination of any simultaneously if needed.</p>\n<blockquote>FaunaDB allows business requirements to dictate infrastructure needs instead of infrastructure constraining innovation.</blockquote>\n<p>† <a href=\"https://fauna.com/status\">All major Cloud vendors support FaunaDB natively</a> (e.g. Amazon Web Services, Google Cloud Platform, Microsoft Azure) in addition to Fauna’s own Cloud solution.</p>\n<h2>Conclusion</h2>\n<p>This all sounds pretty exciting, right? Maybe even a little <em>too</em> good to be true? At first I shared these sentiments as well. But here’s the thing. As I keep learning more, it’s becoming clear that I’m only getting started! I invite you to follow along as I continue to share my learnings in this Getting Started w/ FaunaDB series. Next up, we’ll shift gears with a hands-on approach in my FaunaDB Quickstart Guide to get you set up and interacting with your own database in a matter of minutes.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "476",
        "postDate": "2018-03-20T18:58:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 507,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "2c7eda97-8bb5-4a93-ad73-1f5c0c925a92",
        "siteSettingsId": 507,
        "fieldLayoutId": 4,
        "contentId": 340,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Launching a New, More Intuitive FaunaDB Docs Structure",
        "slug": "new-faunadb-docs-ia",
        "uri": "blog/new-faunadb-docs-ia",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/new-faunadb-docs-ia",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/new-faunadb-docs-ia",
        "isCommunityPost": false,
        "blogBodyText": "<p>Happy Tuesday! Well, at least it's happy around here. I'm very excited to reveal, at long last, my final project as a technical writer at Fauna (I've since moved on to Product):&nbsp;<a href=\"https://fauna.com/documentation\" target=\"_blank\">a brand new organization of our documentation</a>! I hope you enjoy it as much as I&nbsp;enjoyed creating it, and please check back frequently as our brand-new technical writer will be updating pages and adding content soon. If you're curious about the how and why of this update, I wrote all about it below.&nbsp;<br><br>Coming to Fauna early last year, I was impressed by both the quantity and quality of docs that had been created without a dedicated technical writer. It's really easy to treat documentation as an afterthought, especially while in the thick of development and growth. It said a lot about my coworkers' passion for FaunaDB that they cared enough to write about it. But even with the existing docs, we were still in the very beginnings of documentation efforts, which is a very exciting place to be.<br></p>\r\n<p>It was immediately apparent that I was facing a unique opportunity to really plan a future-oriented <a href=\"https://en.wikipedia.org/wiki/Information_architecture\" target=\"_blank\">information architecture</a>&nbsp;(IA) that we could use to guide our docs through the growth and change that all startups face. I won't say that is <em>every</em>&nbsp;tech writer's dream, but I think many would relish the opportunity to right the observed wrongs of the past. Of course, that's no small amount of responsibility and challenge. So I decided to build on defendable foundational principles:<strong>&nbsp;</strong>content categories and content types.</p>\r\n<p>In the world of software, there are some high-level, general content categories that apply across the board regardless of what your software is or who's using it:</p>\r\n<ul><li><strong>Product</strong>: Any content that is meant to explain or present the workings, architecture, and operations of or integrations with the software. Generally, these docs will be consistently maintained and users will revisit them as needed.</li><li><strong>Marketing</strong>: Any content specifically meant to entice, persuade, or tell a story about the software or company. Generally, these docs will have an expiration date and be removed, updated, or entirely rewritten as new campaigns replace the old ones.</li><li><strong>Training</strong>: Any content that is a full start-to-finish walkthrough of interrelated tasks designed to help a user achieve an outcome, such as \"Creating a trading app with our software\". Generally, these docs are meant to be used once, and often have a path from one to another to build skills.</li></ul>\r\n<p> \t</p>\r\n<p>Within the Product Docs category, there are several types of content:</p>\r\n<ul><li><strong>Task</strong>: Content that answers the question, \"How do I do X?\" and is a means of guiding users through the steps of accomplishing a single task using the software.</li><li><strong>Reference</strong>: Content that presents a complete list of specifications for a given thing (APIs, configurations, etc.).</li><li><strong>Overview</strong>: Content that introduces a feature or aspect of the software at a high level, answering the questions, \"What is this?\" and \"How does it help me?\"</li><li><strong>Theory</strong>: Content that describes how something was architected and the logic behind why X was chosen over Y; this content is meant to assist users in understanding the underlying assumptions and principles guiding the software.</li></ul>\r\n<p>Using these categories and types along with user personas, we are able to ask of each individual doc: <br></p>\r\n<ul><li>What user is the doc targeted at?</li><li>What is this doc doing for the user?</li></ul>\r\n<p>But how does any of this have anything to do with information architecture (IA)? A really good IA operates along two axes: side navigation (navigational IA) and the path a user will take through the docs at various points of experience (path IA). If you just consider navigational IA, you can end up developing a <a href=\"https://en.wikipedia.org/wiki/Dewey_Decimal_Classification\" target=\"_blank\">Dewey Decimal</a>-like IA which requires any given user to know exactly what they want and what they are looking for. If you just consider user paths, you can develop an overly prescriptive IA that is annoying for users who just want the answer to a specific question. You also want to keep an eye on your product roadmap, because you want room in your IA to expand and address new things being built. The IA should balance the needs of the content you currently have and the content that you want to create for both existing and new features.</p>\r\n<p>With all of this in mind, we decided to implement an IA organized by high-level actions, which would let us have broad sections that could grow to accommodate an ever-expanding feature set, while simultaneously allowing for both a prescriptive path and a low-friction navigation for those looking for a specific thing. To hone our approach, we developed three separate options and user-tested them internally, iterating on the winner until we had a plan.</p>\r\n<p>Before we get into what you can currently see on the <a href=\"https://fauna.com/documentation\" target=\"_blank\">docs site</a>, let me take just a moment to go through the plan we chose and are implementing. We have top-level headings that are the first thing you see in the side navigation when you visit our docs site. These are: </p>\r\n<ul><li>Introduction to</li><li>How to</li><li>Tutorials</li><li>Drivers and Tools</li><li>Reference</li><li>Architecture</li></ul>\r\n<p>Each of these have subheadings which, for the moment, will populate above the main side nav when you click on a top-level heading. Each individual page, including the subheading index pages, will have in-page directions to follow up with additional pages, allowing you to choose the best path for your needs. Topics under each heading, for instance “How to,” are organized weighing difficulty and user interest.</p>\r\n<p>You'll notice that we don't currently have every section available on our side navigation, and that is because we have a LOT of docs to write this year. But we hope you'll find the <a href=\"https://fauna.com/documentation\">new docs IA</a> more intuitive and easy to navigate, especially as we continually add more documentation. Let us know what you think!&nbsp;Have feedback or requests? You can reach us&nbsp;at&nbsp;<strong>docs@fauna.com</strong>.</p>",
        "blogCategory": [
            "8",
            "1531"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Happy Tuesday! Well, at least it's happy around here. I'm very excited to reveal, at long last, my final project as a technical writer at Fauna (I've since moved on to Product): <a href=\"https://fauna.com/documentation\" target=\"_blank\" rel=\"noreferrer noopener\">a brand new organization of our documentation</a>! I hope you enjoy it as much as I enjoyed creating it, and please check back frequently as our brand-new technical writer will be updating pages and adding content soon. If you're curious about the how and why of this update, I wrote all about it below. <br /><br />Coming to Fauna early last year, I was impressed by both the quantity and quality of docs that had been created without a dedicated technical writer. It's really easy to treat documentation as an afterthought, especially while in the thick of development and growth. It said a lot about my coworkers' passion for FaunaDB that they cared enough to write about it. But even with the existing docs, we were still in the very beginnings of documentation efforts, which is a very exciting place to be.<br /></p>\n<p>It was immediately apparent that I was facing a unique opportunity to really plan a future-oriented <a href=\"https://en.wikipedia.org/wiki/Information_architecture\" target=\"_blank\" rel=\"noreferrer noopener\">information architecture</a> (IA) that we could use to guide our docs through the growth and change that all startups face. I won't say that is <em>every</em> tech writer's dream, but I think many would relish the opportunity to right the observed wrongs of the past. Of course, that's no small amount of responsibility and challenge. So I decided to build on defendable foundational principles:<strong> </strong>content categories and content types.</p>\n<p>In the world of software, there are some high-level, general content categories that apply across the board regardless of what your software is or who's using it:</p>\n<ul><li><strong>Product</strong>: Any content that is meant to explain or present the workings, architecture, and operations of or integrations with the software. Generally, these docs will be consistently maintained and users will revisit them as needed.</li><li><strong>Marketing</strong>: Any content specifically meant to entice, persuade, or tell a story about the software or company. Generally, these docs will have an expiration date and be removed, updated, or entirely rewritten as new campaigns replace the old ones.</li><li><strong>Training</strong>: Any content that is a full start-to-finish walkthrough of interrelated tasks designed to help a user achieve an outcome, such as \"Creating a trading app with our software\". Generally, these docs are meant to be used once, and often have a path from one to another to build skills.</li></ul><p> \t</p>\n<p>Within the Product Docs category, there are several types of content:</p>\n<ul><li><strong>Task</strong>: Content that answers the question, \"How do I do X?\" and is a means of guiding users through the steps of accomplishing a single task using the software.</li><li><strong>Reference</strong>: Content that presents a complete list of specifications for a given thing (APIs, configurations, etc.).</li><li><strong>Overview</strong>: Content that introduces a feature or aspect of the software at a high level, answering the questions, \"What is this?\" and \"How does it help me?\"</li><li><strong>Theory</strong>: Content that describes how something was architected and the logic behind why X was chosen over Y; this content is meant to assist users in understanding the underlying assumptions and principles guiding the software.</li></ul><p>Using these categories and types along with user personas, we are able to ask of each individual doc: <br /></p>\n<ul><li>What user is the doc targeted at?</li><li>What is this doc doing for the user?</li></ul><p>But how does any of this have anything to do with information architecture (IA)? A really good IA operates along two axes: side navigation (navigational IA) and the path a user will take through the docs at various points of experience (path IA). If you just consider navigational IA, you can end up developing a <a href=\"https://en.wikipedia.org/wiki/Dewey_Decimal_Classification\" target=\"_blank\" rel=\"noreferrer noopener\">Dewey Decimal</a>-like IA which requires any given user to know exactly what they want and what they are looking for. If you just consider user paths, you can develop an overly prescriptive IA that is annoying for users who just want the answer to a specific question. You also want to keep an eye on your product roadmap, because you want room in your IA to expand and address new things being built. The IA should balance the needs of the content you currently have and the content that you want to create for both existing and new features.</p>\n<p>With all of this in mind, we decided to implement an IA organized by high-level actions, which would let us have broad sections that could grow to accommodate an ever-expanding feature set, while simultaneously allowing for both a prescriptive path and a low-friction navigation for those looking for a specific thing. To hone our approach, we developed three separate options and user-tested them internally, iterating on the winner until we had a plan.</p>\n<p>Before we get into what you can currently see on the <a href=\"https://fauna.com/documentation\" target=\"_blank\" rel=\"noreferrer noopener\">docs site</a>, let me take just a moment to go through the plan we chose and are implementing. We have top-level headings that are the first thing you see in the side navigation when you visit our docs site. These are: </p>\n<ul><li>Introduction to</li><li>How to</li><li>Tutorials</li><li>Drivers and Tools</li><li>Reference</li><li>Architecture</li></ul><p>Each of these have subheadings which, for the moment, will populate above the main side nav when you click on a top-level heading. Each individual page, including the subheading index pages, will have in-page directions to follow up with additional pages, allowing you to choose the best path for your needs. Topics under each heading, for instance “How to,” are organized weighing difficulty and user interest.</p>\n<p>You'll notice that we don't currently have every section available on our side navigation, and that is because we have a LOT of docs to write this year. But we hope you'll find the <a href=\"https://fauna.com/documentation\">new docs IA</a> more intuitive and easy to navigate, especially as we continually add more documentation. Let us know what you think! Have feedback or requests? You can reach us at <strong>docs@fauna.com</strong>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "469",
        "postDate": "2018-03-19T08:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 506,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "34d7caee-eb1c-4841-aafa-4f46e3e56fef",
        "siteSettingsId": 506,
        "fieldLayoutId": 4,
        "contentId": 339,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Defining Company Culture and Why It Matters",
        "slug": "defining-company-culture-why-it-matters",
        "uri": "blog/defining-company-culture-why-it-matters",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/defining-company-culture-why-it-matters",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/defining-company-culture-why-it-matters",
        "isCommunityPost": false,
        "blogBodyText": "<p>How can a company promote a healthy culture? What does that look like in practice? Is culture even important when it comes to the bottom line? These are salient questions in the tech industry that require honest reflection on a company’s values in order to find answers. As Fauna’s Head of People Operations I spend a fair amount of time and effort examining, and re-examining, these questions.</p>\r<p>When I joined the company more than 2.5 years ago, our founders knew it was uncommon for a young, bootstrapped startup to bring in a full-time People Operations employee. Evan and Matt realized from the beginning that if Fauna was going to be a place where people felt appreciated, valued and excited to come to work, it would take more than an industry-changing product. It would require building a team willing to think about what a healthy culture looks like, and more importantly, willing to act accordingly. They predicted that the right culture would allow the diverse team they envisioned to effectively and efficiently find the solutions that would continue to evade less diverse teams.</p>\r<h2>Defining Culture</h2>\r<p>Culture is an iterative process that requires continuous attention. At Fauna, we believe that culture is a direct extension of our values, enhancing recruiting and retention beyond monetary compensation. We’ve found that taking a proactive approach to culture can be a meaningful vehicle for transforming talented individuals into highly productive and professional teams.</p>\r<p>Now, let’s discuss what a healthy culture isn’t --&nbsp;an environment free of any and all conflict. We have disagreements, especially given our diverse perspectives. But it's in how we resolve our differences and find strength in the resulting solutions that’s indicative of a positive culture.</p>\r<p>As an organization, we strive to establish a culture that reflects our company values. When we created our operating principles, we ensured they reflect who we are today, while paving a path for our who we wanted to become. At Fauna we value diversity. In practice that means that&nbsp;we value people, and we value the human element of business. We care about our product, but we care at least&nbsp;equally about our people. This ethos has been central to Fauna since its inception.</p>\r<p>We view our team as an ecosystem -- a series of interconnected systems. In an ecosystem each organism serves a particular purpose, yet they all depend on each other for survival. We are determined to see our company be more than just a positive environment, but instead an enriching ecosystem where everyone contributes their unique skill set toward building the world's foremost database company.</p>\r<h2>Building Trust</h2>\r<p>Building a diverse team means different types of people co-exist, and sometimes there will be problems. It’s easy to be happy when things are great, but when things get tough, how do we handle ourselves, and how do we respond to the people around us? One important lesson we’ve learned is to continually&nbsp;work to&nbsp;build trust amongst the team. Fostering trust between leadership and employees is essential, but trust between peers is also important. This requires putting in emotional energy to help us understand one another in a deliberate way. It also requires transparency and accountability at all levels.</p>\r<p>How does Fauna encourage our team to operate like this? First, we expect our executives and managers to lead by example. This means admitting when they’re wrong, giving credit where it’s due and listening without judgement when someone chooses to speak up. Encouraging accountability from the top down goes a long way in building trust across the entire team. Being consistent in these expectations to build and maintain that trust pays dividends during those more tumultuous times. We’ve learned that it’s much more difficult to constructively address conflict when you’re starting with a trust deficit.</p>\r<p>We’ve also incorporated systems for feedback to allow team members the opportunity to speak up on their own terms - whether they prefer public, private or anonymous.</p>\r<h2>Finding Balance</h2>\r<p>An important aspect of our “people-first” mentality at Fauna centers around encouraging our team to adopt a healthy, sustainable work-life balance. We prioritize well-being by covering 100% of of the cost of health benefits for our employees and their dependents. When it comes to investing in employee welfare, we put our money where our mouth is. This directive permeates our business, from how we approach paid time off and maternity and paternity leave, to how we encourage employees to engage with local communities via our Civic Engagement policy.</p>\r<p>Why do all this? Because benefits do not just attract great talent, but benefits keep top talent. Yet, this is more than a recruiting and retention ploy. This is who we are. When work is a place we spend one-third of our daily lives, we want our team to feel like a family.</p>\r<p>In addition to benefits and time off policies, we encourage boundaries around contacting team members while Out of Office, respect time differences for remotes, limit hours for company-wide meetings and other efforts to allow everyone to make the most out of their non-works hours. We’ve found this to be especially crucial to prevent burn out and allow our team to be to fully engaged and refreshed while in the office.</p>\r<h2>Conclusion</h2>\r<p>Our culture is important to us because we know that how we treat our people, our team, is the real reflection of our core values. This is evidenced by our emphasis as a bootstrapped company to make the well-being of our employees a top priority from day one.&nbsp;I look forward to sharing future posts diving deeper into the Fauna team values and the lessons we’ve learned navigating them.</p>",
        "blogCategory": [],
        "mainBlogImage": [],
        "bodyText": "<p>How can a company promote a healthy culture? What does that look like in practice? Is culture even important when it comes to the bottom line? These are salient questions in the tech industry that require honest reflection on a company’s values in order to find answers. As Fauna’s Head of People Operations I spend a fair amount of time and effort examining, and re-examining, these questions.</p>\n<p>When I joined the company more than 2.5 years ago, our founders knew it was uncommon for a young, bootstrapped startup to bring in a full-time People Operations employee. Evan and Matt realized from the beginning that if Fauna was going to be a place where people felt appreciated, valued and excited to come to work, it would take more than an industry-changing product. It would require building a team willing to think about what a healthy culture looks like, and more importantly, willing to act accordingly. They predicted that the right culture would allow the diverse team they envisioned to effectively and efficiently find the solutions that would continue to evade less diverse teams.</p>\n<h2>Defining Culture</h2>\n<p>Culture is an iterative process that requires continuous attention. At Fauna, we believe that culture is a direct extension of our values, enhancing recruiting and retention beyond monetary compensation. We’ve found that taking a proactive approach to culture can be a meaningful vehicle for transforming talented individuals into highly productive and professional teams.</p>\n<p>Now, let’s discuss what a healthy culture isn’t -- an environment free of any and all conflict. We have disagreements, especially given our diverse perspectives. But it's in how we resolve our differences and find strength in the resulting solutions that’s indicative of a positive culture.</p>\n<p>As an organization, we strive to establish a culture that reflects our company values. When we created our operating principles, we ensured they reflect who we are today, while paving a path for our who we wanted to become. At Fauna we value diversity. In practice that means that we value people, and we value the human element of business. We care about our product, but we care at least equally about our people. This ethos has been central to Fauna since its inception.</p>\n<p>We view our team as an ecosystem -- a series of interconnected systems. In an ecosystem each organism serves a particular purpose, yet they all depend on each other for survival. We are determined to see our company be more than just a positive environment, but instead an enriching ecosystem where everyone contributes their unique skill set toward building the world's foremost database company.</p>\n<h2>Building Trust</h2>\n<p>Building a diverse team means different types of people co-exist, and sometimes there will be problems. It’s easy to be happy when things are great, but when things get tough, how do we handle ourselves, and how do we respond to the people around us? One important lesson we’ve learned is to continually work to build trust amongst the team. Fostering trust between leadership and employees is essential, but trust between peers is also important. This requires putting in emotional energy to help us understand one another in a deliberate way. It also requires transparency and accountability at all levels.</p>\n<p>How does Fauna encourage our team to operate like this? First, we expect our executives and managers to lead by example. This means admitting when they’re wrong, giving credit where it’s due and listening without judgement when someone chooses to speak up. Encouraging accountability from the top down goes a long way in building trust across the entire team. Being consistent in these expectations to build and maintain that trust pays dividends during those more tumultuous times. We’ve learned that it’s much more difficult to constructively address conflict when you’re starting with a trust deficit.</p>\n<p>We’ve also incorporated systems for feedback to allow team members the opportunity to speak up on their own terms - whether they prefer public, private or anonymous.</p>\n<h2>Finding Balance</h2>\n<p>An important aspect of our “people-first” mentality at Fauna centers around encouraging our team to adopt a healthy, sustainable work-life balance. We prioritize well-being by covering 100% of of the cost of health benefits for our employees and their dependents. When it comes to investing in employee welfare, we put our money where our mouth is. This directive permeates our business, from how we approach paid time off and maternity and paternity leave, to how we encourage employees to engage with local communities via our Civic Engagement policy.</p>\n<p>Why do all this? Because benefits do not just attract great talent, but benefits keep top talent. Yet, this is more than a recruiting and retention ploy. This is who we are. When work is a place we spend one-third of our daily lives, we want our team to feel like a family.</p>\n<p>In addition to benefits and time off policies, we encourage boundaries around contacting team members while Out of Office, respect time differences for remotes, limit hours for company-wide meetings and other efforts to allow everyone to make the most out of their non-works hours. We’ve found this to be especially crucial to prevent burn out and allow our team to be to fully engaged and refreshed while in the office.</p>\n<h2>Conclusion</h2>\n<p>Our culture is important to us because we know that how we treat our people, our team, is the real reflection of our core values. This is evidenced by our emphasis as a bootstrapped company to make the well-being of our employees a top priority from day one. I look forward to sharing future posts diving deeper into the Fauna team values and the lessons we’ve learned navigating them.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-09-29T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 480,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "6a6ef66b-e1ae-4f1c-9906-c4296d2bd83d",
        "siteSettingsId": 480,
        "fieldLayoutId": 4,
        "contentId": 313,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How to Build A Distributed Ledger With FaunaDB",
        "slug": "distributed-ledger-without-the-blockchain",
        "uri": "blog/distributed-ledger-without-the-blockchain",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/distributed-ledger-without-the-blockchain",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/distributed-ledger-without-the-blockchain",
        "isCommunityPost": false,
        "blogBodyText": "<p>The blockchain is inspiring a new generation of financial services innovation, but blockchain technology is often pressed into service when it’s not the best fit for requirements, for instance, as a distributed ledger. A distributed ledger allows participants at different sites to maintain shared transaction logs. It can function as the backbone for funds transfer clearing houses or for any other dataset that needs to keep account balances consistent across replicas.</p>\r\n<blockquote>When you are buying and selling, you have to be able to&nbsp;trust your database.<br></blockquote>\r\n<p>Blockchains typically aren’t optimized for transaction rate. Instead they focus on enforcing trust through proof of work. Adding participants does not spread the work out, instead it increases the total amount of work each must do. Most distributed ledgers don’t need to run among untrusted parties, so the blockchain is a poor fit.</p>\r\n<figure><img src=\"{asset:407:url}\" data-image=\"79f0hksj6j9c\"></figure>\r\n<p></p>\r\n<p>When participants are trusted, an ACID compliant distributed database offers a simpler solution for a scalable distributed ledger. Fauna’s CTO Matt Freels gives&nbsp;<a href=\"https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database\">an in-depth look at a ledger use case in this article about transactions.</a>&nbsp;This blog post is hands-on with an example pet store application.</p>\r\n<p>I’ve created a demo app to show how simple transaction programming can be.&nbsp;<a href=\"https://animal-exchange.neocities.org/\">Visit the app and drag and drop some animals to buy and sell them.</a>&nbsp;You’ll see each player’s balance update as you run transactions, and that it’s impossible to overdraft. The data is hosted in FaunaDB Serverless Cloud, with replica sites around the world, all updated consistently.</p>\r\n<h2>Hello Ledger</h2>\r\n<p>Keep reading to install your own copy of the app and explore the transaction queries. Here are the steps to hello world.</p>\r\n<h3>Install the demo app</h3>\r\n<pre class=\"language-sh\">git clone https://github.com/fauna/animal-exchange\r\ncd animal-exchange\r\nnpm install</pre>\r\n<h3>Launch the demo app</h3>\r\n<pre class=\"language-sh\">npm start\r\n</pre>\r\n<p><br>Now you are up and running and enjoying the app locally. If you’d like to modify the code or the schema,&nbsp;<a href=\"https://github.com/fauna/animal-exchange/blob/master/README.md\">full instructions for developing on the app are in the readme.</a></p>\r\n<h3>Look at the code.</h3>\r\n<p>Since you are here for the transactions, let’s look at one important query. Here are&nbsp;<a href=\"https://github.com/fauna/animal-exchange/blob/master/src/model.js\">all the queries</a>, and here is the&nbsp;<a href=\"https://github.com/fauna/animal-exchange/blob/master/session-service/handler.js#L32\">code that defines the schema.</a></p>\r\n<p>And here is the core of the transaction to purchase an item (see the code for the entire transaction):</p>\r\n<pre class=\"language-javascript\">// check balance\r\n    q.If(q.LT(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\")),\r\n      \"purchase failed: insufficient funds\",\r\n      // all clear! record the purchase, update the buyer, seller and item.\r\n      q.Do(\r\n        q.Create(q.Class(\"purchases\"), {\r\n          data : {\r\n            item : q.Select(\"ref\", q.Var(\"item\")),\r\n            price : q.Var(\"itemPrice\"),\r\n            buyer : q.Select(\"ref\", q.Var(\"buyer\")),\r\n            seller : q.Select(\"ref\", q.Var(\"seller\"))\r\n          }\r\n        }),\r\n        q.Update(q.Select(\"ref\", q.Var(\"buyer\")), {\r\n          data : {\r\n            credits : q.Subtract(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\"))\r\n          }\r\n        }),\r\n        q.Update(q.Select(\"ref\", q.Var(\"seller\")), {\r\n          data : {\r\n            credits : q.Add(q.Select([\"data\", \"credits\"], q.Var(\"seller\")), q.Var(\"itemPrice\"))\r\n          }\r\n        }),\r\n        q.Update(q.Select(\"ref\", q.Var(\"item\")), {\r\n          data : {\r\n            owner : q.Select(\"ref\", q.Var(\"buyer\")),\r\n            for_sale : false\r\n          }\r\n        }),\r\n        \"purchase success\"\r\n      )</pre>\r\n<p>Selling an item involves reading and updating a handful of database records.&nbsp;Without ACID transactions, if any one of these operations were to fail, it could leave the database in an inconsistent state. When you are buying and selling, you have to be able to&nbsp;trust your database.</p>\r\n<p>The transaction includes these operations:</p>\r\n<ul><li>Check the credit balance of the purchasing player to ensure they have available funds.</li><li>Check that the item is for sale.</li><li>Update the balances of the selling player and the purchasing player.</li><li>Update the item with the new owner.</li><li>Write a purchase record.</li></ul>\r\n<p>FaunaDB’s ACID properties mean that if any one of these operations fails, for instance because the player is making multiple trades at the same time and exceeds their balance, the entire transaction must fail. This also holds true in cases of hardware or network failure. So you know that any data you read or write to FaunaDB is consistent, taking the guesswork out of writing correct applications.</p>\r\n<p>For more detail about our consistency model and what that means for applications,&nbsp;<a href=\"https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database\">check out Matt’s article.</a></p>",
        "blogCategory": [
            "3",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>The blockchain is inspiring a new generation of financial services innovation, but blockchain technology is often pressed into service when it’s not the best fit for requirements, for instance, as a distributed ledger. A distributed ledger allows participants at different sites to maintain shared transaction logs. It can function as the backbone for funds transfer clearing houses or for any other dataset that needs to keep account balances consistent across replicas.</p>\n<blockquote>When you are buying and selling, you have to be able to trust your database.<br /></blockquote>\n<p>Blockchains typically aren’t optimized for transaction rate. Instead they focus on enforcing trust through proof of work. Adding participants does not spread the work out, instead it increases the total amount of work each must do. Most distributed ledgers don’t need to run among untrusted parties, so the blockchain is a poor fit.</p>\n<figure><img src=\"{asset:407:url||https://fauna.com/assets/site/blog-legacy/blockchain.png}\" alt=\"\" /></figure><p>When participants are trusted, an ACID compliant distributed database offers a simpler solution for a scalable distributed ledger. Fauna’s CTO Matt Freels gives <a href=\"https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database\">an in-depth look at a ledger use case in this article about transactions.</a> This blog post is hands-on with an example pet store application.</p>\n<p>I’ve created a demo app to show how simple transaction programming can be. <a href=\"https://animal-exchange.neocities.org/\">Visit the app and drag and drop some animals to buy and sell them.</a> You’ll see each player’s balance update as you run transactions, and that it’s impossible to overdraft. The data is hosted in FaunaDB Serverless Cloud, with replica sites around the world, all updated consistently.</p>\n<h2>Hello Ledger</h2>\n<p>Keep reading to install your own copy of the app and explore the transaction queries. Here are the steps to hello world.</p>\n<h3>Install the demo app</h3>\n<pre class=\"language-sh\">git clone https://github.com/fauna/animal-exchange\ncd animal-exchange\nnpm install</pre>\n<h3>Launch the demo app</h3>\n<pre class=\"language-sh\">npm start\n</pre>\n<p><br />Now you are up and running and enjoying the app locally. If you’d like to modify the code or the schema, <a href=\"https://github.com/fauna/animal-exchange/blob/master/README.md\">full instructions for developing on the app are in the readme.</a></p>\n<h3>Look at the code.</h3>\n<p>Since you are here for the transactions, let’s look at one important query. Here are <a href=\"https://github.com/fauna/animal-exchange/blob/master/src/model.js\">all the queries</a>, and here is the <a href=\"https://github.com/fauna/animal-exchange/blob/master/session-service/handler.js#L32\">code that defines the schema.</a></p>\n<p>And here is the core of the transaction to purchase an item (see the code for the entire transaction):</p>\n<pre class=\"language-javascript\">// check balance\n q.If(q.LT(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\")),\n \"purchase failed: insufficient funds\",\n // all clear! record the purchase, update the buyer, seller and item.\n q.Do(\n q.Create(q.Class(\"purchases\"), {\n data : {\n item : q.Select(\"ref\", q.Var(\"item\")),\n price : q.Var(\"itemPrice\"),\n buyer : q.Select(\"ref\", q.Var(\"buyer\")),\n seller : q.Select(\"ref\", q.Var(\"seller\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"buyer\")), {\n data : {\n credits : q.Subtract(q.Var(\"buyerBalance\"), q.Var(\"itemPrice\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"seller\")), {\n data : {\n credits : q.Add(q.Select([\"data\", \"credits\"], q.Var(\"seller\")), q.Var(\"itemPrice\"))\n }\n }),\n q.Update(q.Select(\"ref\", q.Var(\"item\")), {\n data : {\n owner : q.Select(\"ref\", q.Var(\"buyer\")),\n for_sale : false\n }\n }),\n \"purchase success\"\n )</pre>\n<p>Selling an item involves reading and updating a handful of database records. Without ACID transactions, if any one of these operations were to fail, it could leave the database in an inconsistent state. When you are buying and selling, you have to be able to trust your database.</p>\n<p>The transaction includes these operations:</p>\n<ul><li>Check the credit balance of the purchasing player to ensure they have available funds.</li><li>Check that the item is for sale.</li><li>Update the balances of the selling player and the purchasing player.</li><li>Update the item with the new owner.</li><li>Write a purchase record.</li></ul><p>FaunaDB’s ACID properties mean that if any one of these operations fails, for instance because the player is making multiple trades at the same time and exceeds their balance, the entire transaction must fail. This also holds true in cases of hardware or network failure. So you know that any data you read or write to FaunaDB is consistent, taking the guesswork out of writing correct applications.</p>\n<p>For more detail about our consistency model and what that means for applications, <a href=\"https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database\">check out Matt’s article.</a></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "477",
        "postDate": "2017-09-20T15:34:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 481,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "9277bb4d-7f4c-4cc2-84fb-354f4bb7faab",
        "siteSettingsId": 481,
        "fieldLayoutId": 4,
        "contentId": 314,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Achieving ACID Transactions in a Globally Distributed Database",
        "slug": "acid-transactions-in-a-globally-distributed-database",
        "uri": "blog/acid-transactions-in-a-globally-distributed-database",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-08-19T21:01:38-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/acid-transactions-in-a-globally-distributed-database",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/acid-transactions-in-a-globally-distributed-database",
        "isCommunityPost": false,
        "blogBodyText": "<p>One of the features of FaunaDB that has generated the most excitement is its strongly consistent distributed ACID transaction engine. In this post we’ll explain how FaunaDB’s Calvin-inspired transaction engine processes ACID transactions in a partitioned, globally distributed environment, and how you can take advantage of them in your applications.</p>\r\n<h1>Why ACID Transactions Matter</h1>\r\n<p>ACID transactions are a powerful tool because they guarantee that operations behave the way developers expect:</p>\r\n<blockquote>A system with ACID transactions makes a promise that data integrity will be preserved even under contention and faults such as node failure and network partitions.</blockquote>\r\n<p>Formally, ACID refers to a set of properties a database system maintains during transaction execution. These properties are:</p>\r\n<ul><li><strong>Atomicity:</strong>&nbsp;Transactions are indivisible units, such that all of their effects apply, or none of them do.</li><li><strong>Consistency (or Correctness):</strong>&nbsp;Transactions are only allowed to apply if they do not violate database constraints such as uniqueness or schema constraints. Not to be confused with CAP Consistency, below.</li><li><strong>Isolation:</strong>&nbsp;The degree to which one transaction’s effects are visible to another transaction. Most systems talk about isolation in terms of a number of isolation levels, discussed in more detail below.</li><li><strong>Durability:</strong>&nbsp;A transaction that is committed is permanent and will survive expected faults such as power loss or node failure.</li></ul>\r\n<p>Until recently, however, ACID transactions haven’t been feasible in the context of globally scalable web applications. The assumption has been that the only options beyond a certain scale are to either sacrifice consistency, or to somehow partition application data and limit operations to those that can be handled by a single partition.</p>\r\n<h1>Conditional Updates</h1>\r\n<p>Here’s a concrete example of an operation that is trivial to implement with ACID transactions, but difficult to do otherwise. Consider an online game which allows players to buy and sell in-game items. The application needs to enforce two rules:</p>\r\n<ol><li>A buyer can only purchase an item if they have enough currency</li><li>The item must actually be for sale</li></ol>\r\n<p>In this example, player records have a “credits” field which stores the number of credits available for them to spend. Here is an example player represented by a FaunaDB query DSL snippet:</p>\r\n<pre class=\"language-scala\">Obj(\r\n  \"ref\" -&gt; Ref(Class(\"player\"), 43254525535),\r\n  \"ts\" -&gt; ...,\r\n  \"data\" -&gt; Obj(\"credits\" -&gt; 40)\r\n)</pre>\r\n<p>And along with it, an item:</p>\r\n<pre class=\"language-scala\">Obj(\r\n  \"ref\" -&gt; Ref(Class(\"items\"), 1291723917),\r\n  \"ts\" -&gt; ...\r\n  \"data\" -&gt; Obj(\r\n    \"description\" -&gt; \"Green Polliwog\",\r\n    \"owner\": Ref(Class(\"players\"), 69329839),\r\n    \"for_sale\": true,\r\n    \"price\": 35\r\n  )\r\n)</pre>\r\n<p>When an item is sold, the buyer, seller, and item are updated, and a purchase record is created, only if our rules above hold. (Sometimes this form of conditional update is informally referred to as “compare and swap”.)</p>\r\n<p>The following FaunaDB query processes the sale:</p>\r\n<pre class=\"language-scala\">Let {\r\n  // look up the buyer and item instances.\r\n  val buyer = Get(Ref(Class(\"players\"), 43254525535))\r\n  val item = Get(Ref(Class(\"items\"), 1291723917))\r\n  val seller = Get(Select(\"data\" / \"owner\", item))\r\n  val buyerRef = Select(\"ref\", buyer)\r\n  val sellerRef = Select(\"ref\", seller)\r\n  val isForSale = Select(\"data\" / \"for_sale\", item)\r\n  val itemPrice = Select(\"data\" / \"price\", item)\r\n  val buyerCredits = Select(\"data\" / \"credits\", buyer)\r\n\r\n  // ensure the buyer and seller are not the same player\r\n  If(Equals(buyerRef, sellerRef),\r\n    \"purchase failed: item already owned\",\r\n\r\n    // check to see if the item is for sale\r\n    If(Not(isForSale),\r\n      \"purchase failed: item not for sale\",\r\n   \r\n      // check the buyer credit balance\r\n      If(LT(buyerCredits, itemPrice),\r\n        \"purchase failed: insufficient credits\",\r\n          \r\n        // all clear! record the purchase, update the buyer, seller, and item.\r\n        Do(\r\n          Create(Class(\"purchases\"), Obj(\r\n            \"data\" -&gt; Obj(\r\n              \"item\" -&gt; Select(\"ref\", item),\r\n              \"price\" -&gt; itemPrice,\r\n              \"buyer\" -&gt; buyerRef,\r\n              \"seller\" -&gt; sellerRef\r\n            )\r\n          )),\r\n   \r\n          Update(buyerRef, Obj(\r\n            \"data\" -&gt; Obj(\r\n              \"credits\" -&gt; Subtract(buyerCredits, itemPrice)\r\n            )\r\n          )),\r\n   \r\n          Update(sellerRef, Obj(\r\n            \"data\" -&gt; Obj(\r\n              \"credits\" -&gt; Add(Select(\"data\" / \"credits\", seller), itemPrice)\r\n            )\r\n          )),\r\n   \r\n          Update(Select(\"ref\", item), Obj(\r\n            \"data\" -&gt; Obj(\r\n              \"owner\" -&gt; buyerRef,\r\n              \"for_sale\" -&gt; false\r\n            )\r\n          )),\r\n   \r\n          \"purchase success\"\r\n        )\r\n      )\r\n    )\r\n  )\r\n}</pre>\r\n<p>As you can see, the logic for the purchase is neatly encapsulated in a single query. Moreover, the results of the transaction are made consistently available to clients such that the entire result is seen at once.</p>\r\n<p>Resolving this kind of purchase logic in a distributed environment without transactions is much more involved. It is up to the developer to identify and handle data in various inconsistent states as well as the specific faults made possible by the architecture they have implemented, and&nbsp;<a href=\"http://hackingdistributed.com/2014/04/06/another-one-bites-the-dust-flexcoin\">a lot can go wrong</a>. A set of formal guarantees like ACID is much easier to reason about.</p>\r\n<p>The only other option would be to manually shard the application into multiple silos, each backed by a traditional, non-distributed RDBMS. This significantly limits the capabilities of the application, and is unworkable in a product where users expect to be able interact with each other without restrictions.</p>\r\n<h1>ACID in FaunaDB</h1>\r\n<p>The challenge of implementing ACID transactions in a distributed system comes down to consistently resolving transactions across data partitions in as efficient and scalable a manner as possible. Furthermore, with geographically distributed replicas, minimizing cross-region network communication is critical in order to maintain reasonable response latencies.</p>\r\n<p>FaunaDB’s distributed transaction engine is inspired by&nbsp;<a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>. Each database has a single partitioned&nbsp;<a href=\"https://raft.github.io/raft.pdf\">RAFT-replicated</a>&nbsp;log that is used to derive a total order of all transaction effects, and that handles all cross-region network communication.</p>\r\n<p>Transactions are processed in two phases: an&nbsp;<em>execute phase</em>&nbsp;and a&nbsp;<em>commit phase</em>.</p>\r\n<p>In the execute phase, the node which received the client’s query (called the “coordinator node”) executes the core transaction logic, dispatching reads to the closest available data partitions and accruing write effects. Before doing so however, the coordinator chooses a snapshot time with which to make all read requests. This snapshot time is chosen in coordination with the transaction log in order to prevent stale reads. Data partitions respond to reads as they are able to, and will delay or drop reads until they are caught up to the time chosen.</p>\r\n<p>If transaction execution accrues any write effects, then the process will enter the commit phase. Along with actual values, prior read results include last update timestamps which act as optimistic lock acquisitions. These read timestamps along with the set of write effects are committed to the database’s transaction log. The transaction is then assigned a logical transaction time based on the order in which its effects are added to the log.</p>\r\n<p>While this transaction time ultimately reflects the causal ordering of a transaction with respect to others, it corresponds closely to real-time as well depending on system clock accuracy. This gives FaunaDB the nice characteristic of having reasonably accurate transaction timestamps which still reflect causal relationships between transactions without relying on synchronized, exotic time sources such as atomic or GPS clocks.</p>\r\n<p>At this point, transaction resolution proceeds in parallel across the database’s data partitions. This approach to transaction resolution allows FaunaDB to provide a strong guarantee of strict serializability while still allowing for perfect processing parallelism in the absence of contention among transactions.</p>\r\n<p>A transaction is considered committed and its write effects durably written to disk if no concurrent write to any read values has occurred. The set of optimistic read locks collected previously are checked, and if any concurrent modification is detected, this commit attempt will fail and the coordinator will retry the transaction or abort if it has timed out.</p>\r\n<h1>ACID and CAP</h1>\r\n<p>It is worth taking a moment to discuss the relationship between ACID transaction properties and the CAP theorem: Strictly speaking a system which supports ACID transactions must choose some form of consistency as framed by CAP.</p>\r\n<p>There has been quite a bit of recent discussion surrounding CAP and the tradeoff between consistency and availability. While out of scope for this post, we agree with&nbsp;<a href=\"http://dbmsmusings.blogspot.co.il/2010/04/problems-with-cap-and-yahoos-little.html\">Daniel Abadi</a>&nbsp;and&nbsp;<a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45855.pdf\">Eric Brewer</a>&nbsp;that industry experience has taught us that availability—in the strict sense that it is defined in the CAP theorem—is overrated: In practice, the uptime of systems that favor availability have not proven greater than what can be achieved with consistent systems.</p>\r\n<h1>More on Isolation Levels</h1>\r\n<p>Of the four ACID properties, Isolation has the most significant impact on the specific details of transaction resolution. Unlike the other properties, isolation is not a binary choice. Therefore, most systems—including FaunaDB—allow for some tradeoff between performance and a strict guarantee of isolation.</p>\r\n<p>Isolation is usually discussed in terms of a set of&nbsp;<a href=\"https://begriffs.com/posts/2017-08-01-practical-guide-sql-isolation.html\">isolation levels</a>&nbsp;which each correspond to a set of write phenomena that are allowed to occur. These write phenomena are each a defined way in which concurrent transaction resolution is allowed to violate transaction isolation.</p>\r\n<p>Since read-only transactions in FaunaDB always have a specific snapshot time but are not sequenced via the transaction log, they run at snapshot isolation, which for read-only transactions is equivalent to serializable.</p>\r\n<p>Read-write transactions in FaunaDB where all reads opt in to optimistic locking as described above are strictly serializable. Transactions that are strictly serializable combine the behavior of&nbsp;<a href=\"http://www.bailis.org/blog/linearizability-versus-serializability\">serializability with linearizability</a>: Strictly serializable transactions have a global, non-overlapping order with respect to each other, and this order corresponds to real time.</p>\r\n<p>(As opposed to instance reads, index reads can be opted in or out of the optimistic lock mechanism, which affects the isolation level of involved transactions. This is discussed below)</p>\r\n<p>In other words, for two strictly serializable transactions A and B, if the response for A is received before B is sent, then B will see the effects of A. This is a higher level of isolation than what is usually discussed in database literature, but is relevant in a distributed context that does not have the capability to coordinate via a shared sense of time.</p>\r\n<p>As an example, consider a read-only transaction that happens after a read-write transaction, but with a stale snapshot time. The two transactions are still serializable, but as this would violate linearizability, they are not strictly serializable.</p>\r\n<p>The reason why all transactions are not strictly serializable is for better read performance. In order to have a hard guarantee of strict serializability, all read transactions would need to involve the database transaction log, requiring a global message exchange in the process. This would result in read-only transactions taking 50-200 milliseconds at minimum. By relaxing the transaction isolation level for reads, they can be served by the closest available data replica and avoid a costly round of global communication.</p>\r\n<p>There is usually no observable difference between the two isolation levels in practice. Regardless of whether read-only transactions were strictly serializable or not, subsequent writes are better off protected by a conditional check in order to guard against concurrent updates.</p>\r\n<h1>Indexes and Write Skew</h1>\r\n<p>As mentioned above, index reads within a read-write transaction skip the optimistic lock check. By skipping this check, index terms which receive a high number of writes (such as an index of all instances in a class) avoid the negative performance of a highly contended optimistic lock. However, this means that by default index reads are done at snapshot isolation and so subject to a specific write phenomenon called write skew.</p>\r\n<p>Here is a practical example of what this means and how it can affect an application.</p>\r\n<p>Let’s say that you are using FaunaDB to implement a todo list app. Tasks are created with a sequential index that is used to maintain the list order. Our task instances looks like this:</p>\r\n<pre class=\"language-scala\">Obj(\r\n  \"ref\" -&gt; Ref(Class(\"tasks\"), 581234928),\r\n  \"ts\" -&gt; ...,\r\n  \"data\" -&gt; Obj(\r\n    \"index\" -&gt; 1,\r\n    \"list\" -&gt; \"Groceries\",\r\n    \"description\" -&gt; \"Buy Tomatoes\"\r\n  )\r\n)</pre>\r\n<p>We’ve created an index that represents lists of tasks. Tasks within a list are sorted in descending index order, so that by default new tasks show up at the top of a list:</p>\r\n<pre class=\"language-scala\">CreateIndex(Obj(\r\n  \"name\" -&gt; \"tasks_by_list\",\r\n  \"source\" -&gt; Class(\"tasks\"),\r\n  \"terms\" -&gt; Obj(\"field\" -&gt; Arr(\"data\", \"list\")),\r\n  \"values\" -&gt; Arr(\r\n    Obj(\"field\" -&gt; Arr(\"data\", \"index\"), \"reverse\" -&gt; true),\r\n    Obj(\"field\" -&gt; Arr(\"data\", \"description\")),\r\n    Obj(\"field\" -&gt; \"ref\")\r\n  ),\r\n))</pre>\r\n<p>Our query to add a new task is pretty straight-forward:</p>\r\n<pre class=\"language-scala\">Let {\r\n  // Get the Groceries list.\r\n  val list = Paginate(Match(Index(\"tasks_by_list\"), \"Groceries\")\r\n  \r\n  Let {\r\n    // get the index from the first item in our page,\r\n    // or 0 if it was empty.\r\n    val last_index = Select(0 / 0, Take(1, list), 0)\r\n    // Add our new item. Generate the next index\r\n    // value by incrementing the last one.\r\n    Create(Class(\"tasks\"), Obj(\r\n      \"project\" -&gt; \"Groceries\",\r\n      \"index\" -&gt; Add(last_index, 1),\r\n      \"description\" -&gt; \"Buy Bananas\"\r\n    ))\r\n  }\r\n}</pre>\r\n<p>However, there’s a problem: If two new tasks are added concurrently, they may end up with the same index, which is not what we want. Here’s how the two creation transactions can sequence with each other that shows how it can happen:</p>\r\n<figure><img src=\"https://raw.githubusercontent.com/fauna/blog-assets/master/images/acid-transactions-diagram-1.png\" alt=\"Illustration that shows write-skew sequence\" style=\"height:auto;vertical-align:middle;margin-bottom:30px;\" data-image=\"mqf8fmiabig9\"></figure>\r\n<p>The second transaction reads the same state of the list, but because concurrent modifications to the list are not checked for, it successfully creates a new task “Buy Bananas” with a conflicting index value.</p>\r\n<p>Solving this is just a matter of flipping a switch on the index configuration so that its state is opted into concurrent modification detection:</p>\r\n<pre class=\"language-scala\">Update(Index(\"tasks_by_list\"), Obj(\"serialized\" -&gt; true))</pre>\r\n<p>Now our query above will work as expected. The update to the list is detected and the second transaction is retried in order to preserve strict serialization:</p>\r\n<figure><img src=\"https://raw.githubusercontent.com/fauna/blog-assets/master/images/acid-transactions-diagram-2.png\" alt=\"Illustration that shows corrected sequence\" style=\"height:auto;vertical-align:middle;margin-bottom:30px;\" data-image=\"ik7r2410brfe\"></figure>\r\n<h1>Conclusion</h1>\r\n<p>In this post we described how distributed ACID transactions work in FaunaDB, as well as why they are an essential building block for applications. In the end, we consider them and strong consistency to be table stakes for any modern, distributed database.</p>\r\n<p>FaunaDB’s transaction support is the foundation for many of its other features, such as temporality, global distribution, low-latency consistent reads, and operational resiliency. You can learn more about the architecture of FaunaDB in our&nbsp;<a href=\"http://www2.fauna.com/techwhitepaper\">architectural white paper</a>.</p>\r\n<p>You can also&nbsp;<a href=\"https://fauna.com/sign-up\">sign up for an account and try FaunaDB for yourself</a>.</p>\r\n<p><a href=\"https://news.ycombinator.com/item?id=15305506\">Discuss this post on Hacker News.</a></p>",
        "blogCategory": [
            "8",
            "1462",
            "1465"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>One of the features of FaunaDB that has generated the most excitement is its strongly consistent distributed ACID transaction engine. In this post we’ll explain how FaunaDB’s Calvin-inspired transaction engine processes ACID transactions in a partitioned, globally distributed environment, and how you can take advantage of them in your applications.</p>\n<h1>Why ACID Transactions Matter</h1>\n<p>ACID transactions are a powerful tool because they guarantee that operations behave the way developers expect:</p>\n<blockquote>A system with ACID transactions makes a promise that data integrity will be preserved even under contention and faults such as node failure and network partitions.</blockquote>\n<p>Formally, ACID refers to a set of properties a database system maintains during transaction execution. These properties are:</p>\n<ul><li><strong>Atomicity:</strong> Transactions are indivisible units, such that all of their effects apply, or none of them do.</li><li><strong>Consistency (or Correctness):</strong> Transactions are only allowed to apply if they do not violate database constraints such as uniqueness or schema constraints. Not to be confused with CAP Consistency, below.</li><li><strong>Isolation:</strong> The degree to which one transaction’s effects are visible to another transaction. Most systems talk about isolation in terms of a number of isolation levels, discussed in more detail below.</li><li><strong>Durability:</strong> A transaction that is committed is permanent and will survive expected faults such as power loss or node failure.</li></ul><p>Until recently, however, ACID transactions haven’t been feasible in the context of globally scalable web applications. The assumption has been that the only options beyond a certain scale are to either sacrifice consistency, or to somehow partition application data and limit operations to those that can be handled by a single partition.</p>\n<h1>Conditional Updates</h1>\n<p>Here’s a concrete example of an operation that is trivial to implement with ACID transactions, but difficult to do otherwise. Consider an online game which allows players to buy and sell in-game items. The application needs to enforce two rules:</p>\n<ol><li>A buyer can only purchase an item if they have enough currency</li><li>The item must actually be for sale</li></ol><p>In this example, player records have a “credits” field which stores the number of credits available for them to spend. Here is an example player represented by a FaunaDB query DSL snippet:</p>\n<pre class=\"language-scala\">Obj(\n \"ref\" -&gt; Ref(Class(\"player\"), 43254525535),\n \"ts\" -&gt; ...,\n \"data\" -&gt; Obj(\"credits\" -&gt; 40)\n)</pre>\n<p>And along with it, an item:</p>\n<pre class=\"language-scala\">Obj(\n \"ref\" -&gt; Ref(Class(\"items\"), 1291723917),\n \"ts\" -&gt; ...\n \"data\" -&gt; Obj(\n \"description\" -&gt; \"Green Polliwog\",\n \"owner\": Ref(Class(\"players\"), 69329839),\n \"for_sale\": true,\n \"price\": 35\n )\n)</pre>\n<p>When an item is sold, the buyer, seller, and item are updated, and a purchase record is created, only if our rules above hold. (Sometimes this form of conditional update is informally referred to as “compare and swap”.)</p>\n<p>The following FaunaDB query processes the sale:</p>\n<pre class=\"language-scala\">Let {\n // look up the buyer and item instances.\n val buyer = Get(Ref(Class(\"players\"), 43254525535))\n val item = Get(Ref(Class(\"items\"), 1291723917))\n Let {\n val buyerRef = Select(\"ref\", buyer)\n val sellerRef = Select(\"data\" / \"owner\", item)\n val seller = Get(Select(\"data\" / \"owner\", item))\n val isForSale = Select(\"data\" / \"for_sale\", item)\n val itemPrice = Select(\"data\" / \"price\", item)\n val buyerCredits = Select(\"data\" / \"credits\", buyer)\n // ensure the buyer and seller are not the same player\n If(Equals(buyerRef, sellerRef),\n \"purchase failed: item already owned\",\n // check to see if the item is for sale\n If(Not(isForSale),\n \"purchase failed: item not for sale\",\n \n // check the buyer credit balance\n If(LT(buyerCredits, itemPrice),\n \"purchase failed: insufficient credits\",\n \n // all clear! record the purchase, update the buyer, seller, and item.\n Do(\n Create(Class(\"purchases\"), Obj(\n \"data\" -&gt; Obj(\n \"item\" -&gt; Select(\"ref\", item),\n \"price\" -&gt; itemPrice,\n \"buyer\" -&gt; buyerRef,\n \"seller\" -&gt; sellerRef\n )\n )),\n \n Update(buyerRef, Obj(\n \"data\" -&gt; Obj(\n \"credits\" -&gt; Subtract(buyerCredits, itemPrice)\n )\n )),\n \n Update(sellerRef, Obj(\n \"data\" -&gt; Obj(\n \"credits\" -&gt; Add(Select(\"data\" / \"credits\", seller), itemPrice)\n )\n )),\n \n Update(Select(\"ref\", item), Obj(\n \"data\" -&gt; Obj(\n \"owner\" -&gt; buyerRef,\n \"for_sale\" -&gt; false\n )\n )),\n \n \"purchase success\"\n )\n )\n )\n )\n }\n}</pre>\n<p>As you can see, the logic for the purchase is neatly encapsulated in a single query. Moreover, the results of the transaction are made consistently available to clients such that the entire result is seen at once.</p>\n<p>Resolving this kind of purchase logic in a distributed environment without transactions is much more involved. It is up to the developer to identify and handle data in various inconsistent states as well as the specific faults made possible by the architecture they have implemented, and <a href=\"http://hackingdistributed.com/2014/04/06/another-one-bites-the-dust-flexcoin\">a lot can go wrong</a>. A set of formal guarantees like ACID is much easier to reason about.</p>\n<p>The only other option would be to manually shard the application into multiple silos, each backed by a traditional, non-distributed RDBMS. This significantly limits the capabilities of the application, and is unworkable in a product where users expect to be able interact with each other without restrictions.</p>\n<h1>ACID in FaunaDB</h1>\n<p>The challenge of implementing ACID transactions in a distributed system comes down to consistently resolving transactions across data partitions in as efficient and scalable a manner as possible. Furthermore, with geographically distributed replicas, minimizing cross-region network communication is critical in order to maintain reasonable response latencies.</p>\n<p>FaunaDB’s distributed transaction engine is inspired by <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>. Each database has a single partitioned <a href=\"https://raft.github.io/raft.pdf\">RAFT-replicated</a> log that is used to derive a total order of all transaction effects, and that handles all cross-region network communication.</p>\n<p>Transactions are processed in two phases: an <em>execute phase</em> and a <em>commit phase</em>.</p>\n<p>In the execute phase, the node which received the client’s query (called the “coordinator node”) executes the core transaction logic, dispatching reads to the closest available data partitions and accruing write effects. Before doing so however, the coordinator chooses a snapshot time with which to make all read requests. This snapshot time is chosen in coordination with the transaction log in order to prevent stale reads. Data partitions respond to reads as they are able to, and will delay or drop reads until they are caught up to the time chosen.</p>\n<p>If transaction execution accrues any write effects, then the process will enter the commit phase. Along with actual values, prior read results include last update timestamps which act as optimistic lock acquisitions. These read timestamps along with the set of write effects are committed to the database’s transaction log. The transaction is then assigned a logical transaction time based on the order in which its effects are added to the log.</p>\n<p>While this transaction time ultimately reflects the causal ordering of a transaction with respect to others, it corresponds closely to real-time as well depending on system clock accuracy. This gives FaunaDB the nice characteristic of having reasonably accurate transaction timestamps which still reflect causal relationships between transactions without relying on synchronized, exotic time sources such as atomic or GPS clocks.</p>\n<p>At this point, transaction resolution proceeds in parallel across the database’s data partitions. This approach to transaction resolution allows FaunaDB to provide a strong guarantee of strict serializability while still allowing for perfect processing parallelism in the absence of contention among transactions.</p>\n<p>A transaction is considered committed and its write effects durably written to disk if no concurrent write to any read values has occurred. The set of optimistic read locks collected previously are checked, and if any concurrent modification is detected, this commit attempt will fail and the coordinator will retry the transaction or abort if it has timed out.</p>\n<h1>ACID and CAP</h1>\n<p>It is worth taking a moment to discuss the relationship between ACID transaction properties and the CAP theorem: Strictly speaking a system which supports ACID transactions must choose some form of consistency as framed by CAP.</p>\n<p>There has been quite a bit of recent discussion surrounding CAP and the tradeoff between consistency and availability. While out of scope for this post, we agree with <a href=\"http://dbmsmusings.blogspot.co.il/2010/04/problems-with-cap-and-yahoos-little.html\">Daniel Abadi</a> and <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45855.pdf\">Eric Brewer</a> that industry experience has taught us that availability—in the strict sense that it is defined in the CAP theorem—is overrated: In practice, the uptime of systems that favor availability have not proven greater than what can be achieved with consistent systems.</p>\n<h1>More on Isolation Levels</h1>\n<p>Of the four ACID properties, Isolation has the most significant impact on the specific details of transaction resolution. Unlike the other properties, isolation is not a binary choice. Therefore, most systems—including FaunaDB—allow for some tradeoff between performance and a strict guarantee of isolation.</p>\n<p>Isolation is usually discussed in terms of a set of <a href=\"https://begriffs.com/posts/2017-08-01-practical-guide-sql-isolation.html\">isolation levels</a> which each correspond to a set of write phenomena that are allowed to occur. These write phenomena are each a defined way in which concurrent transaction resolution is allowed to violate transaction isolation.</p>\n<p>Since read-only transactions in FaunaDB always have a specific snapshot time but are not sequenced via the transaction log, they run at snapshot isolation, which for read-only transactions is equivalent to serializable.</p>\n<p>Read-write transactions in FaunaDB where all reads opt in to optimistic locking as described above are strictly serializable. Transactions that are strictly serializable combine the behavior of <a href=\"http://www.bailis.org/blog/linearizability-versus-serializability\">serializability with linearizability</a>: Strictly serializable transactions have a global, non-overlapping order with respect to each other, and this order corresponds to real time.</p>\n<p>(As opposed to instance reads, index reads can be opted in or out of the optimistic lock mechanism, which affects the isolation level of involved transactions. This is discussed below)</p>\n<p>In other words, for two strictly serializable transactions A and B, if the response for A is received before B is sent, then B will see the effects of A. This is a higher level of isolation than what is usually discussed in database literature, but is relevant in a distributed context that does not have the capability to coordinate via a shared sense of time.</p>\n<p>As an example, consider a read-only transaction that happens after a read-write transaction, but with a stale snapshot time. The two transactions are still serializable, but as this would violate linearizability, they are not strictly serializable.</p>\n<p>The reason why all transactions are not strictly serializable is for better read performance. In order to have a hard guarantee of strict serializability, all read transactions would need to involve the database transaction log, requiring a global message exchange in the process. This would result in read-only transactions taking 50-200 milliseconds at minimum. By relaxing the transaction isolation level for reads, they can be served by the closest available data replica and avoid a costly round of global communication.</p>\n<p>There is usually no observable difference between the two isolation levels in practice. Regardless of whether read-only transactions were strictly serializable or not, subsequent writes are better off protected by a conditional check in order to guard against concurrent updates.</p>\n<h1>Indexes and Write Skew</h1>\n<p>As mentioned above, index reads within a read-write transaction skip the optimistic lock check. By skipping this check, index terms which receive a high number of writes (such as an index of all instances in a class) avoid the negative performance of a highly contended optimistic lock. However, this means that by default index reads are done at snapshot isolation and so subject to a specific write phenomenon called write skew.</p>\n<p>Here is a practical example of what this means and how it can affect an application.</p>\n<p>Let’s say that you are using FaunaDB to implement a todo list app. Tasks are created with a sequential index that is used to maintain the list order. Our task instances looks like this:</p>\n<pre class=\"language-scala\">Obj(\n \"ref\" -&gt; Ref(Class(\"tasks\"), 581234928),\n \"ts\" -&gt; ...,\n \"data\" -&gt; Obj(\n \"index\" -&gt; 1,\n \"list\" -&gt; \"Groceries\",\n \"description\" -&gt; \"Buy Tomatoes\"\n )\n)</pre>\n<p>We’ve created an index that represents lists of tasks. Tasks within a list are sorted in descending index order, so that by default new tasks show up at the top of a list:</p>\n<pre class=\"language-scala\">CreateIndex(Obj(\n \"name\" -&gt; \"tasks_by_list\",\n \"source\" -&gt; Class(\"tasks\"),\n \"terms\" -&gt; Obj(\"field\" -&gt; Arr(\"data\", \"list\")),\n \"values\" -&gt; Arr(\n Obj(\"field\" -&gt; Arr(\"data\", \"index\"), \"reverse\" -&gt; true),\n Obj(\"field\" -&gt; Arr(\"data\", \"description\")),\n Obj(\"field\" -&gt; \"ref\")\n ),\n))</pre>\n<p>Our query to add a new task is pretty straight-forward:</p>\n<pre class=\"language-scala\">Let {\n // Get the Groceries list.\n val list = Paginate(Match(Index(\"tasks_by_list\"), \"Groceries\")\n \n Let {\n // get the index from the first item in our page,\n // or 0 if it was empty.\n val last_index = Select(0 / 0, Take(1, list), 0)\n // Add our new item. Generate the next index\n // value by incrementing the last one.\n Create(Class(\"tasks\"), Obj(\n \"project\" -&gt; \"Groceries\",\n \"index\" -&gt; Add(last_index, 1),\n \"description\" -&gt; \"Buy Bananas\"\n ))\n }\n}</pre>\n<p>However, there’s a problem: If two new tasks are added concurrently, they may end up with the same index, which is not what we want. Here’s how the two creation transactions can sequence with each other that shows how it can happen:</p>\n<figure><img src=\"https://raw.githubusercontent.com/fauna/blog-assets/master/images/acid-transactions-diagram-1.png\" alt=\"Illustration that shows write-skew sequence\" /></figure><p>The second transaction reads the same state of the list, but because concurrent modifications to the list are not checked for, it successfully creates a new task “Buy Bananas” with a conflicting index value.</p>\n<p>Solving this is just a matter of flipping a switch on the index configuration so that its state is opted into concurrent modification detection:</p>\n<pre class=\"language-scala\">Update(Index(\"tasks_by_list\"), Obj(\"serialized\" -&gt; true))</pre>\n<p>Now our query above will work as expected. The update to the list is detected and the second transaction is retried in order to preserve strict serialization:</p>\n<figure><img src=\"https://raw.githubusercontent.com/fauna/blog-assets/master/images/acid-transactions-diagram-2.png\" alt=\"Illustration that shows corrected sequence\" /></figure><h1>Conclusion</h1>\n<p>In this post we described how distributed ACID transactions work in FaunaDB, as well as why they are an essential building block for applications. In the end, we consider them and strong consistency to be table stakes for any modern, distributed database.</p>\n<p>FaunaDB’s transaction support is the foundation for many of its other features, such as temporality, global distribution, low-latency consistent reads, and operational resiliency. You can learn more about the architecture of FaunaDB in our <a href=\"https://fauna.com/pdf/FaunaDB-Technical-Whitepaper.pdf\">architectural white paper</a>.</p>\n<p>You can also <a href=\"https://fauna.com/sign-up\">sign up for an account and try FaunaDB for yourself</a>.</p>\n<p><a href=\"https://news.ycombinator.com/item?id=15305506\">Discuss this post on Hacker News.</a></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-09-14T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 482,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "6c4b9555-e6e9-405b-81d3-3e75b759fdac",
        "siteSettingsId": 482,
        "fieldLayoutId": 4,
        "contentId": 315,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Our $25M Series A: Building a Database and a Company to Last",
        "slug": "fauna-series-a-investment",
        "uri": "blog/fauna-series-a-investment",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/fauna-series-a-investment",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/fauna-series-a-investment",
        "isCommunityPost": false,
        "blogBodyText": "<figure><img src=\"{asset:409:url}\" data-image=\"fo443s4nrk2w\"></figure>\r\n<p></p>\r\n<p>A year ago, almost to the day, we announced our round of seed funding. That investment provided enough fuel for us to expand the team, move into a great new office in San Francisco in Jackson Square, continue working with our early customers, and launch FaunaDB to the public.</p>\r\n<p>Today, I’m excited to announce another milestone. Fauna has raised&nbsp;<a href=\"http://www.prnewswire.com/news-releases/fauna-raises-25m-in-series-a-funding-led-by-point72-ventures-300519364.html\">$25M in series A financing</a>, the largest series A ever for an operational database company. I’d like to explain the justification for this historic investment.</p>\r\n<p>Since launch, FaunaDB Serverless Cloud has seen tremendous developer adoption. Less visibly, FaunaDB has also been embraced by large companies–those with the toughest requirements for distributed databases.</p>\r\n<p>It turns out that enterprises need modern cloud-native architectures that are as safe as traditional pre-cloud databases. They need everything that the cloud promises: modern interfaces, developer-first delivery models, and lower TCO. But they also need traditional database features like ACID transactions, high availability, failover, predictable performance, and guaranteed durability. The balancing act is incredibly difficult.</p>\r\n<p>After our launch, we found that companies in financial services and ecommerce often had the greatest need for a new database. But the need is not restricted to those industries. We are also seeing use cases in SaaS, IoT, gaming, and social media. Our new funding supports that.</p>\r\n<p>This round was led by Point72 Ventures, which is the early-stage venture branch of Point72 Asset Management, funded by Steven Cohen and others. GV (formerly Google Ventures), Costanoa Ventures, and Afore Capital also participated, along with angel investors Beth Axelrod, Elizabeth Weil, and Jason Goldman. Existing investors also participated in the round, including CRV, Data Collective, Quest Venture Partners, the Webb Investment Network, and Ulu Ventures.</p>\r\n<h1>Scaling up</h1>\r\n<p>We raised a large round so that we can scale the company to match the size of the opportunity. We need great engineers, including distributed systems, full-stack, and front end. We’re looking for developer evangelists who can connect to our community with code. With highly qualified leads pouring in every day, we need an experienced sales director, enterprise salespeople, and solutions architects to help them succeed as customers.</p>\r\n<p>If you need convincing, Chris Anderson gives some compelling reasons for joining Fauna in his blog post,&nbsp;<a href=\"https://fauna.com/blog/why-you-should-join-the-next-big-database-startup\">Why I joined the next big database startup and you should too</a>.</p>\r\n<p>If you’re curious about the culture that we’re building at Fauna, our HR director, Amna Pervez, summed it up in her blog post,&nbsp;<a href=\"https://fauna.com/blog/not-your-regular-office-space\">Not your regular office space</a>.</p>\r\n<p>We are intentionally building a culture that values diversity. Diverse teams are strong teams, and we welcome those with alternative identities, backgrounds, and experiences. Our team includes women, men, mothers, fathers, the self-taught, the college-educated, and people of a variety of races, nationalities, ages, and socio-economic backgrounds.</p>\r\n<p>If all this sounds good to you, check out our&nbsp;<a href=\"https://fauna.com/jobs\">job descriptions</a>&nbsp;and see if there’s a fit.</p>\r\n<figure><img src=\"{asset:452:url}\" data-image=\"vfjhf188qkay\"></figure>\r\n<p></p>\r\n<h1>Thanks</h1>\r\n<p>I’m excited to welcome our new investors to the Fauna community. I’m grateful to Dan Gwak of Point72 Ventures for leading the round. I’m also grateful to the enthusiastic support of our seed investors who also joined this round of funding.</p>\r\n<p>I’d like to thank everyone on our team, including my cofounder Matt, our staff, our advisors, our friends, our families, and especially my wife, for the support, sacrifice, and dedication required to get us to this point. And I would like to thank all our early customers for sharing our vision for modern data infrastructure.</p>",
        "blogCategory": [
            "1461",
            "1531"
        ],
        "mainBlogImage": [],
        "bodyText": "<figure><img src=\"{asset:409:url||https://fauna.com/assets/site/blog-legacy/butterflies.png}\" alt=\"\" /></figure><p>A year ago, almost to the day, we announced our round of seed funding. That investment provided enough fuel for us to expand the team, move into a great new office in San Francisco in Jackson Square, continue working with our early customers, and launch FaunaDB to the public.</p>\n<p>Today, I’m excited to announce another milestone. Fauna has raised <a href=\"http://www.prnewswire.com/news-releases/fauna-raises-25m-in-series-a-funding-led-by-point72-ventures-300519364.html\">$25M in series A financing</a>, the largest series A ever for an operational database company. I’d like to explain the justification for this historic investment.</p>\n<p>Since launch, FaunaDB Serverless Cloud has seen tremendous developer adoption. Less visibly, FaunaDB has also been embraced by large companies–those with the toughest requirements for distributed databases.</p>\n<p>It turns out that enterprises need modern cloud-native architectures that are as safe as traditional pre-cloud databases. They need everything that the cloud promises: modern interfaces, developer-first delivery models, and lower TCO. But they also need traditional database features like ACID transactions, high availability, failover, predictable performance, and guaranteed durability. The balancing act is incredibly difficult.</p>\n<p>After our launch, we found that companies in financial services and ecommerce often had the greatest need for a new database. But the need is not restricted to those industries. We are also seeing use cases in SaaS, IoT, gaming, and social media. Our new funding supports that.</p>\n<p>This round was led by Point72 Ventures, which is the early-stage venture branch of Point72 Asset Management, funded by Steven Cohen and others. GV (formerly Google Ventures), Costanoa Ventures, and Afore Capital also participated, along with angel investors Beth Axelrod, Elizabeth Weil, and Jason Goldman. Existing investors also participated in the round, including CRV, Data Collective, Quest Venture Partners, the Webb Investment Network, and Ulu Ventures.</p>\n<h1>Scaling up</h1>\n<p>We raised a large round so that we can scale the company to match the size of the opportunity. We need great engineers, including distributed systems, full-stack, and front end. We’re looking for developer evangelists who can connect to our community with code. With highly qualified leads pouring in every day, we need an experienced sales director, enterprise salespeople, and solutions architects to help them succeed as customers.</p>\n<p>If you need convincing, Chris Anderson gives some compelling reasons for joining Fauna in his blog post, <a href=\"https://fauna.com/blog/why-you-should-join-the-next-big-database-startup\">Why I joined the next big database startup and you should too</a>.</p>\n<p>If you’re curious about the culture that we’re building at Fauna, our HR director, Amna Pervez, summed it up in her blog post, <a href=\"https://fauna.com/blog/not-your-regular-office-space\">Not your regular office space</a>.</p>\n<p>We are intentionally building a culture that values diversity. Diverse teams are strong teams, and we welcome those with alternative identities, backgrounds, and experiences. Our team includes women, men, mothers, fathers, the self-taught, the college-educated, and people of a variety of races, nationalities, ages, and socio-economic backgrounds.</p>\n<p>If all this sounds good to you, check out our <a href=\"https://fauna.com/jobs\">job descriptions</a> and see if there’s a fit.</p>\n<figure><img src=\"{asset:452:url||https://fauna.com/assets/site/blog-legacy/office.png}\" alt=\"\" /></figure><h1>Thanks</h1>\n<p>I’m excited to welcome our new investors to the Fauna community. I’m grateful to Dan Gwak of Point72 Ventures for leading the round. I’m also grateful to the enthusiastic support of our seed investors who also joined this round of funding.</p>\n<p>I’d like to thank everyone on our team, including my cofounder Matt, our staff, our advisors, our friends, our families, and especially my wife, for the support, sacrifice, and dedication required to get us to this point. And I would like to thank all our early customers for sharing our vision for modern data infrastructure.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-09-06T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 483,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a871546e-2748-4a0c-92e2-37cf42fbceb9",
        "siteSettingsId": 483,
        "fieldLayoutId": 4,
        "contentId": 316,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Hello World: Azure Functions with Serverless Framework, Node.js and FaunaDB",
        "slug": "azure-functions-with-serverless-node-js-and-faunadb",
        "uri": "blog/azure-functions-with-serverless-node-js-and-faunadb",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/azure-functions-with-serverless-node-js-and-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/azure-functions-with-serverless-node-js-and-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<figure style=\"float: right; margin: 0px 0px 10px 10px;\"><img src=\"{asset:408:url}\" data-image=\"olya1zz274cn\"></figure>\r\n<p><a href=\"https://azure.microsoft.com/en-us/services/functions/\">Azure Functions</a>&nbsp;offer serverless on-demand execution of your code based on external events like HTTP requests, chat messages, etc. FaunaDB Cloud is purpose-built to be&nbsp;<strong>a database for serverless runtimes</strong>&nbsp;like Azure Functions, AWS Lambda, and Google Cloud Functions. The post shows how to connect to FaunaDB Cloud from JavaScript running in Azure Functions.&nbsp;</p>\r\n<p>Here are three ways FaunaDB Cloud is tailored for serverless:</p>\r\n<ul><li>First, FaunaDB Cloud requires&nbsp;<strong>no provisioning</strong>, so you can start developing right away, and your database usage can scale up and down without any attention required from you.</li><li><p>Second, FaunaDB Cloud provides&nbsp;<strong>strong consistency and ACID transactions</strong>, even across global, multi-region datasets. Your application never has to check for quorums, or consider read repair, CRDTs, or other artifacts of eventual consistency.</p></li><li><p>Finally, the&nbsp;<strong>pay-as-you-go utility pricing model</strong>&nbsp;means you start for free and your costs scale with usage. There’s no need to provision capacity in the hopes of future traffic. In most cases, FaunaDB Serverless Cloud is less expensive than other options, and you never pay for more than you consume.</p></li></ul>\r\n<p>In this post, we’ll show you how to run JavaScript functions in Azure, with FaunaDB Cloud as a database-as-a-service. FaunaDB Cloud runs worldwide in multiple AWS and GCE regions, and is coming soon to Azure. When we spin up Azure support, any code you deploy today will transparently begin hitting our AWS, GCP, and Azure endpoints. With a trivial config change, you’ll see higher performance due to lower latency from FaunaDB Cloud running on the same network as your Azure Functions, and cost savings from not triggering Azure data egress costs.</p>\r\n<p>As FaunaDB Cloud becomes available in more cloud providers and regions, you’ll be able to choose to replicate your data specifically to the providers and regions you need. Your app’s behavior and correctness won’t change, even as you expand your deployment to multiple regions and across cloud providers. Services you run in different clouds will all have a connection to an in-region FaunaDB Cloud endpoint, accessing the same globally consistent and correct data.</p>\r\n<p>Stick around to look at the code, or&nbsp;<a href=\"https://github.com/fauna/azure-faunadb-serverless#serverless-azure-functions-nodejs-template-with-faunadb\">jump to the full instructions to deploy your own Azure Functions with FaunaDB.</a></p>\r\n<p>The JavaScript function below creates a FaunaDB client (line 3) and responds to HTTP requests by issuing a hello-world query. You can learn from&nbsp;<a href=\"https://fauna.com/tutorials/crud\">realistic queries in the FaunaDB tutorials</a>.</p>\r\n<pre class=\"language-javascript\">const faunadb = require('faunadb');\r\nconst q = faunadb.query;\r\nconst client = new faunadb.Client({\r\n  secret: process.env.FAUNADB_SECRET\r\n});\r\nmodule.exports.hello = function (context, req) {\r\n  // Add any necessary telemetry to support diagnosing your function\r\n  context.log('HTTP trigger occurred!');\r\n  // Read properties from the incoming request, and respond as appropriate.\r\n  const name = req.query.name || (req.body && req.body.name) || 'World';\r\n  client.query(q.Concat([\"Hello \", name])).then((data) =&gt; {\r\n    context.done(null, { body: `FaunaDB response: ${JSON.stringify(data)}` });\r\n  })\r\n};</pre>\r\n<p>The query, in this case&nbsp;<code>q.Concat([\"Hello \", name])</code>, concatenates the&nbsp;<code>name</code>&nbsp;value (which may be passed in the request) with a “Hello” message, returning a string like “Hello World”. The client constructs an abstract representation of the query and sends it to the server. Query execution happens on the server as part of transaction processing.</p>\r\n<p><a href=\"https://fauna.com/documentation/queries\">Queries in FTL (Functional Transaction Language)</a>&nbsp;safely include complex expressions like loops and conditional logic. All processing happens on the server in the context of an isolated transaction.</p>\r\n<blockquote>Its global ACID properties make FaunaDB especially well-suited for use as an operational database.</blockquote>\r\n<p>In this hello world example, the query doesn’t read or write data so there is no transaction overhead. Stay tuned to this blog to learn more about consistency and isolation levels in FaunaDB.</p>\r\n<p>Once you have a globally consistent database that’s accessible from all your cloud providers, it opens up the possibility of remixing services from different clouds. When the database can coordinate services, why not combine Azure Functions with speech-to-text from Google and image recognition from AWS? All clouds are not created equal, so multi-cloud support lets you arbitrage capabilities as well as pricing across any combination of platforms.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-07-19T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 484,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "01ec5159-e424-44b6-9561-f0598e3acb19",
        "siteSettingsId": 484,
        "fieldLayoutId": 4,
        "contentId": 317,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB Developer Edition Preview Available Now",
        "slug": "faunadb-developer-edition",
        "uri": "blog/faunadb-developer-edition",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-developer-edition",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-developer-edition",
        "isCommunityPost": false,
        "blogBodyText": "<figure><img src=\"{asset:458:url}\"></figure>\r\n<p><br></p>\r\n<p>FaunaDB has been available as a managed cloud database since March. We’re seeing tremendous interest in and adoption of our cloud service, but we’ve also gotten many requests to make FaunaDB available for download. Sometimes it’s just easier to work against a local instance of your database.<br></p>\r\n<p>That’s why we’re excited to announce that you can begin developing against FaunaDB on your laptop today. We are releasing a preview version of&nbsp;<strong>FaunaDB Developer Edition</strong>&nbsp;that you can download simply by&nbsp;<a href=\"https://fauna.com/sign-up\">creating a FaunaDB account</a>. FaunaDB Developer Edition is a single-node, plug-and-play version of FaunaDB. It requires no installation or configuration, and is ideal for local development and testing.</p>\r\n<p>FaunaDB Developer Edition is free but limited&mdash;you can’t set up a distributed cluster with it, and it’s not tuned for high performance. But you can start developing against FaunaDB on your workstation today, and we look forward to incorporating your&nbsp;<a href=\"https://gitter.im/fauna/Lobby\">feedback</a>&nbsp;into the enterprise build.</p>\r\n<blockquote>It’s easy to install since it’s packaged as a single Java JAR and has no service or library dependencies aside from a recent JVM.</blockquote>\r\n<h2>Up and Running</h2>\r\n<p>You can download, install, and initialize FaunaDB in about 5 minutes. You will need to do the following:</p>\r\n<ol><li>Make sure Java 8 is installed on your machine.</li><li>Download&nbsp;<a href=\"https://fauna.com/releases/developer/2.2.0\">FaunaDB Developer Edition preview</a>.</li><li>Extract faunadb-developer.jar from the ZIP file.</li><li>In the directory where you extracted the JAR, run:&nbsp;<code>$ java -jar faunadb-developer.jar</code></li></ol>\r\n<p>You’ll see the following:</p>\r\n<pre class=\"language-bash\">   ____                    ___  ___\r\n  / __/__ ___ _____  ___ _/ _ \\/ _ )\r\n / _// _ `/ // / _ \\/ _ `/ // / _  |\r\n/_/  \\_,_/\\_,_/_//_/\\_,_/____/____/\r\n                        \r\nStarting FaunaDB Developer Edition 2.2.0-8d3631a\r\nData path: ./data\r\nAPI address: 0.0.0.0\r\nAPI port: 8443\r\nFaunaDB is ready.</pre>\r\n<p>Now you’re ready to start working with FaunaDB’s query semantics in your favorite programming language. Your application interacts with FaunaDB through a language-specific driver, so you’ll never need to learn a new query language to use all the features of FaunaDB. Download a native driver from our&nbsp;<a href=\"https://fauna.com/resources#drivers\">resources page</a>.</p>\r\n<p>Once you’ve downloaded your driver, use our&nbsp;<a href=\"https://fauna.com/tutorials/hello\">Hello World tutorial</a>&nbsp;to install your driver and try your very first query.</p>\r\n<p>Our tutorials are written to connect to FaunaDB Serverless Cloud, and all drivers are configured to communicate with FaunaDB Serverless Cloud by default. Configuring the driver to talk to your local installation of FaunaDB means overriding some defaults. Here is an example of what that looks like in JavaScript. You should make sure the address and port match what was printed when you launched the database.</p>\r\n<pre class=\"language-javascript\">var client = new faunadb.Client({\r\n  secret: \"secret\",\r\n  scheme: 'http',\r\n  domain: '127.0.0.1',\r\n  port: 8443\r\n})</pre>\r\n<p><a href=\"https://gist.github.com/jchris/6fe30093de29b9913875799eef3e9e15\">Here is the complete runnable example in JavaScript</a>, and configuring clients in the other languages works along the same lines. If you want to allow only local connections, you should launch the developer jar with a custom configuration file to bind it to a private interface and set a custom root secret.</p>\r\n<p>If you run into issues, we can help you on&nbsp;<a href=\"https://stackoverflow.com/questions/tagged/faunadb\">Stack Overflow</a>.</p>\r\n<h2>Onward</h2>\r\n<p>The full FaunaDB Developer Edition is planned for release in the fall. Between now and then, we’re focusing on query language improvements (always backwards compatible!), samples, documentation, and other tools, so that FaunaDB’s sweet spot of mixed document, relational, and graph use cases gets even easier for developers to use.</p>\r\n<p>We’ve built our communication and support tools directly into the dashboard so that you get help from our team with minimal friction. When your organization is ready to begin a strategic project, we’re happy to set up dedicated channels to support your work.</p>\r\n<p>In the meantime, happy coding!</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-07-10T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 485,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a4695c2f-b68a-45dc-a1f3-aa5c1f21f8ae",
        "siteSettingsId": 485,
        "fieldLayoutId": 4,
        "contentId": 318,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "FaunaDB Enters the Enterprise",
        "slug": "faunadb-enterprise-preview-release",
        "uri": "blog/faunadb-enterprise-preview-release",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-09-24T10:53:24-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-enterprise-preview-release",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-enterprise-preview-release",
        "isCommunityPost": false,
        "blogBodyText": "<p>Today we are excited to announce the preview release of FaunaDB Enterprise. With this release, FaunaDB’s modern query features and operational capabilities are available for deployment anywhere: developer workstations, on-premises datacenters, public clouds, private clouds, or hybrid configurations. This preview release represents&nbsp;<strong>years of design, development, and maturation of FaunaDB</strong>&nbsp;by a team with decades of collective experience building distributed systems.</p>\r\n<figure><img src=\"{asset:418:url}\" data-image=\"p2xz1nqob96q\"></figure>\r\n<figure><figcaption><br>FaunaDB Enterprise is designed to span any combination of public clouds, private clouds, or on-premises data centers.</figcaption></figure>\r\n<p>\r\n</p>\r\n<p>\r\n</p>\r\n<p>We launched&nbsp;<a href=\"https://fauna.com/blog/faunadb-serverless-cloud-launch-day\">FaunaDB Cloud in March</a>, and the response has been overwhelming. In less than 4 months, our database-as-a-service has seen tremendous adoption by individuals, consultancies, SaaS companies, and others. The emerging serverless movement needs serverless data, and we are honored to provide it.<br></p>\r\n<p>However, enterprises usually need to operate their own infrastructure for many different reasons, some technical, some business, beyond just the cutting edge. With this release, we are enabling IT organizations to do what we do: deploy and replicate FaunaDB clusters across any public clouds, any private clouds, and any physical infrastructure in any part of the world, while maintaining enterprise-grade strong consistency, availability, and security.</p>\r\n<h2>FaunaDB on Your Laptop</h2>\r\n<p>We can’t succeed without your feedback, so along with this announcement we are also releasing a preview version of&nbsp;<strong>FaunaDB Developer Edition</strong>. You can download the software package by&nbsp;<a href=\"https://fauna.com/sign-up\">creating a FaunaDB account</a>. The download can be found under&nbsp;<a href=\"https://fauna.com/releases/developer/2.2.0\">Releases</a>. Packaged as a single Java JAR, it has no service or library dependencies aside from a recent JVM.</p>\r\n<p>We’ve built our communication and support tools directly into the dashboard so that you get help from our team with minimal friction. When your organization is ready to begin a strategic project, we’re happy to set up dedicated channels to support your work.</p>\r\n<p>FaunaDB Developer Edition is free, but limited—you can’t set up a distributed cluster with it, and it’s not tuned for high performance. But you can start developing against FaunaDB on your workstation today, and we look forward to incorporating your feedback into the enterprise build.</p>\r\n<h2>Enterprise-First Development</h2>\r\n<p>Our team took an extremely conservative approach to developing and releasing FaunaDB. Foundational concerns like the query language, the performance profile, and consistency and operational capabilities must be co-developed. For example, it’s difficult to retrofit performance on a system that was never originally designed for it—<strong>early assumptions turn out to be invalid if not verified</strong>. So we took our time.</p>\r\n<blockquote>A significant portion of that time was spent architecting, developing, and maturing features under real-life customer production workloads.</blockquote>\r\n<p>FaunaDB has been running in production on commercial cloud platforms by our major customers for over a year. Last year NVIDIA launched to millions of end users, backed by FaunaDB in a global configuration that today serves tens of millions of users.</p>\r\n<h2>Cloud-First Launch</h2>\r\n<p>In contrast to the enterprise-first philosophy behind FaunaDB’s design and implementation, we took a cloud-first approach to our launch. FaunaDB’s unique&nbsp;<strong>multi-tenant</strong>,&nbsp;<strong>QoS-managed architecture</strong>&nbsp;makes it ideal for serverless delivery. FaunaDB Cloud makes FaunaDB instantly available to every developer for free. At the same time, it forces us to truly eat our own dogfood —by operating a global, multi-cloud FaunaDB cluster exactly like our enterprise customers do.</p>\r\n<p>FaunaDB Could is deployed to Amazon Web Services, Google Cloud Platform, and soon Microsoft Azure, supporting production applications at global scale.</p>\r\n<blockquote>While other databases can claim some level of feature parity with FaunaDB, they are either unproven in production use or they require a massive operational investment to productionize successfully.</blockquote>\r\n<p>We are confident that FaunaDB is the best general-purpose distributed database, and it will keep getting better.</p>\r\n<h2>The Road Ahead</h2>\r\n<p>FaunaDB Enterprise will reach general availability in the fall. Between now and then, we’re focusing on query language improvements (always backwards compatible!), the operational interface, documentation, and other tools, so that FaunaDB’s sweet spot of mixed document, relational, and graph use cases gets even easier to use.</p>\r\n<p>For production use and testing now,&nbsp;<a href=\"https://fauna.com/request-info\">contact us</a>&nbsp;and we’ll get you access to the full preview package.</p>",
        "blogCategory": [
            "1531"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Today we are excited to announce the preview release of FaunaDB Enterprise. With this release, FaunaDB’s modern query features and operational capabilities are available for deployment anywhere: developer workstations, on-premises datacenters, public clouds, private clouds, or hybrid configurations. This preview release represents <strong>years of design, development, and maturation of FaunaDB</strong> by a team with decades of collective experience building distributed systems.</p>\n<figure><img src=\"{asset:418:url||https://fauna.com/assets/site/blog-legacy/enterprise-graphic.png}\" alt=\"\" /></figure><figure><figcaption><br />FaunaDB Enterprise is designed to span any combination of public clouds, private clouds, or on-premises data centers.</figcaption></figure><p>\n</p>\n<p>\n</p>\n<p>We launched <a href=\"https://fauna.com/blog/faunadb-serverless-cloud-launch-day\">FaunaDB Cloud in March</a>, and the response has been overwhelming. In less than 4 months, our database-as-a-service has seen tremendous adoption by individuals, consultancies, SaaS companies, and others. The emerging serverless movement needs serverless data, and we are honored to provide it.<br /></p>\n<p>However, enterprises usually need to operate their own infrastructure for many different reasons, some technical, some business, beyond just the cutting edge. With this release, we are enabling IT organizations to do what we do: deploy and replicate FaunaDB clusters across any public clouds, any private clouds, and any physical infrastructure in any part of the world, while maintaining enterprise-grade strong consistency, availability, and security.</p>\n<h2>FaunaDB on Your Laptop</h2>\n<p>We can’t succeed without your feedback, so along with this announcement we are also releasing a preview version of <strong>FaunaDB Developer Edition</strong>. You can download the software package by <a href=\"https://fauna.com/sign-up\">creating a FaunaDB account</a>. The download can be found under <a href=\"https://fauna.com/releases/developer/2.2.0\">Releases</a>. Packaged as a single Java JAR, it has no service or library dependencies aside from a recent JVM.</p>\n<p>We’ve built our communication and support tools directly into the dashboard so that you get help from our team with minimal friction. When your organization is ready to begin a strategic project, we’re happy to set up dedicated channels to support your work.</p>\n<p>FaunaDB Developer Edition is free, but limited—you can’t set up a distributed cluster with it, and it’s not tuned for high performance. But you can start developing against FaunaDB on your workstation today, and we look forward to incorporating your feedback into the enterprise build.</p>\n<h2>Enterprise-First Development</h2>\n<p>Our team took an extremely conservative approach to developing and releasing FaunaDB. Foundational concerns like the query language, the performance profile, and consistency and operational capabilities must be co-developed. For example, it’s difficult to retrofit performance on a system that was never originally designed for it—<strong>early assumptions turn out to be invalid if not verified</strong>. So we took our time.</p>\n<blockquote>A significant portion of that time was spent architecting, developing, and maturing features under real-life customer production workloads.</blockquote>\n<p>FaunaDB has been running in production on commercial cloud platforms by our major customers for over a year. Last year NVIDIA launched to millions of end users, backed by FaunaDB in a global configuration that today serves tens of millions of users.</p>\n<h2>Cloud-First Launch</h2>\n<p>In contrast to the enterprise-first philosophy behind FaunaDB’s design and implementation, we took a cloud-first approach to our launch. FaunaDB’s unique <strong>multi-tenant</strong>, <strong>QoS-managed architecture</strong> makes it ideal for serverless delivery. FaunaDB Cloud makes FaunaDB instantly available to every developer for free. At the same time, it forces us to truly eat our own dogfood —by operating a global, multi-cloud FaunaDB cluster exactly like our enterprise customers do.</p>\n<p>FaunaDB Could is deployed to Amazon Web Services, Google Cloud Platform, and soon Microsoft Azure, supporting production applications at global scale.</p>\n<blockquote>While other databases can claim some level of feature parity with FaunaDB, they are either unproven in production use or they require a massive operational investment to productionize successfully.</blockquote>\n<p>We are confident that FaunaDB is the best general-purpose distributed database, and it will keep getting better.</p>\n<h2>The Road Ahead</h2>\n<p>FaunaDB Enterprise will reach general availability in the fall. Between now and then, we’re focusing on query language improvements (always backwards compatible!), the operational interface, documentation, and other tools, so that FaunaDB’s sweet spot of mixed document, relational, and graph use cases gets even easier to use.</p>\n<p>For production use and testing now, <a href=\"https://fauna.com/request-info\">contact us</a> and we’ll get you access to the full preview package.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-07-06T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 486,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "61c5e94a-4ea5-4273-9405-2331983b1e3b",
        "siteSettingsId": 486,
        "fieldLayoutId": 4,
        "contentId": 319,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How to Build Secure Database-Driven Applications on Neocities",
        "slug": "secure-database-driven-applications-on-neocities",
        "uri": "blog/secure-database-driven-applications-on-neocities",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/secure-database-driven-applications-on-neocities",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/secure-database-driven-applications-on-neocities",
        "isCommunityPost": false,
        "blogBodyText": "<p>Neocities’ mission is to “make the web fun again by giving you back control of how you express yourself online.” Their user-friendly platform for creating and hosting web content provides a crucial first step for aspiring designers and programmers. Neocities hosts millions of simple HTML sites, so we thought we would demonstrate how easy it is to use FaunaDB with a&nbsp;<a href=\"https://faunadb.neocities.org/\">single page application deployed to Neocities.</a></p>\n<p>With a secure database that scales from the smallest experiments on the free plan, to powering global brands on the web, the door is open for both beginning and experienced developers to build zero-maintenance apps that can seamlessly transition from “Hello, World!” to worldwide popularity.</p>\n<p>FaunaDB Cloud is a great companion for HTML app development, because it has a&nbsp;<a href=\"https://github.com/fauna/faunadb-js-release\">JavaScript driver that is also available via CDN for browser usage.</a>&nbsp;FaunaDB’s object-level security and user authentication allow you to connect to the cloud database directly from your browser app. Since mobile clients call the same API, no backend changes are required when you add native app support. With pay-as-you-go pricing, you’re only ever charged for actual usage with no upfront investment. Your small experiments can run for free and when one becomes popular, FaunaDB will scale up (and down) for you, remaining fast under pressure. You don’t even need to manually add nodes or other capacity. It’s automatic and continuous.</p>\n<h3>Getting Started with FaunaDB and Neocities</h3>\n<p>This example series will show you how a social collaboration app is built with FaunaDB and then deployed to Neocities. We’ll start by installing it locally and running it, then move on to putting it up on Neocities.&nbsp;<a href=\"https://faunadb.neocities.org/\">Or just skip the installation and use our copy running on Neocities here.</a></p>\n<figure><img src=\"{asset:451:url}\"></figure>\n<p><br></p>\n<p>In future blog posts, we’ll extend the security model by adding support for inviting others to collaborate on your lists. By doing so, we’ll demonstrate FaunaDB’s native fine-grained access control.</p>\n<p>To run your own copy of the app:</p>\n<p>1. Start a free trial of FaunaDB by&nbsp;<a href=\"https://fauna.com/sign-up\">providing your email</a>.<br>2. Create a database in the&nbsp;<a href=\"https://dashboard.fauna.com/db\">dashboard</a>:</p>\n<ol><li>Click&nbsp;<strong>Create a Database</strong>.</li><li>Enter the name todomvc-fauna-spa in the text box.</li><li>Click&nbsp;<strong>Create Database</strong>&nbsp;again.</li></ol>\n<p>3. Set up two database keys, one for with the&nbsp;<em>server</em>&nbsp;role which is used for setting up the schema, and one with the&nbsp;<em>client</em>&nbsp;role which is used by the browser code for creating new users:</p>\n<ol><li>Click ** / ** in the upper left side of the screen.</li><li>Click&nbsp;<strong>Manage Keys</strong>&nbsp;and&nbsp;<strong>Create a Key</strong>.</li><li>Name your key, assign it a&nbsp;<em>server</em>&nbsp;role, and choose the&nbsp;<em>todomvc-fauna-spa</em>&nbsp;database.</li><li>Click&nbsp;<strong>Create Key</strong>.</li><li>Your key’s secret will be displayed. Immediately copy it to schema.js</li><li>Then repeat the process, assigning this second key the&nbsp;<em>client</em>&nbsp;role and copying the key’s secret into Login.js.</li></ol>\n<p>4. Clone the project’s repo&nbsp;<a href=\"https://github.com/fauna/todomvc-fauna-spa\">here</a>:</p>\n<pre>\ngit clone https://github.com/fauna/todomvc-fauna-spa.git\ncd todomvc-fauna-spa\n</pre>\n<p>5. Then install the dependencies:</p>\n<pre>\nnpm install\n</pre>\n<p>6. Run schema.js locally:</p>\n<pre>\nnode schema.js\n</pre>\n<p>7. Build the app bundle for uploading:</p>\n<pre>\nnpm run build\n</pre>\n<p>8.&nbsp;<a href=\"https://neocities.org/\">Set up a Neocities site</a>.<br>9. Upload the project to your new Neocities site by using their Upload button or their&nbsp;<a href=\"https://neocities.org/cli\">command line tool</a>. With the command line tool, run:</p>\n<pre>\ncd build/\nneocities push .\n</pre>\n<p>The&nbsp;<code>neocities</code>&nbsp;command will prompt you to login to your Neocities account and then to upload your site.</p>\n<p>Then you’re ready to browse to your new app, sign up, and start tracking your to-dos, all of which are securely stored in a strongly consistent, globally replicated, multi-cloud database: FaunaDB!</p>\n<p>We encourage you to tinker with the app! Just make sure to run&nbsp;<code>npm run build</code>&nbsp;after any code changes before updating your Neocities site. If you want to preview your changes, just run&nbsp;<code>npm start</code>&nbsp;for a local server.</p>\n<h3>Conclusion</h3>\n<p>We’ll talk more about security in the next few blog posts. The FaunaDB security model is designed for connecting to the database directly from mobile apps and web browsers. All user authentication and fine-grained, object-level access control can be managed by FaunaDB. In some circumstances, this can actually be more secure than relying on middleware for authentication authorization. A native security model is especially useful for applications that directly use the cloud database, and it can also enhance existing security mechanisms through defense-in-depth, where access control can be enforced at all layers of the stack.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Neocities’ mission is to “make the web fun again by giving you back control of how you express yourself online.” Their user-friendly platform for creating and hosting web content provides a crucial first step for aspiring designers and programmers. Neocities hosts millions of simple HTML sites, so we thought we would demonstrate how easy it is to use FaunaDB with a <a href=\"https://faunadb.neocities.org/\">single page application deployed to Neocities.</a></p>\n<p>With a secure database that scales from the smallest experiments on the free plan, to powering global brands on the web, the door is open for both beginning and experienced developers to build zero-maintenance apps that can seamlessly transition from “Hello, World!” to worldwide popularity.</p>\n<p>FaunaDB Cloud is a great companion for HTML app development, because it has a <a href=\"https://github.com/fauna/faunadb-js-release\">JavaScript driver that is also available via CDN for browser usage.</a> FaunaDB’s object-level security and user authentication allow you to connect to the cloud database directly from your browser app. Since mobile clients call the same API, no backend changes are required when you add native app support. With pay-as-you-go pricing, you’re only ever charged for actual usage with no upfront investment. Your small experiments can run for free and when one becomes popular, FaunaDB will scale up (and down) for you, remaining fast under pressure. You don’t even need to manually add nodes or other capacity. It’s automatic and continuous.</p>\n<h3>Getting Started with FaunaDB and Neocities</h3>\n<p>This example series will show you how a social collaboration app is built with FaunaDB and then deployed to Neocities. We’ll start by installing it locally and running it, then move on to putting it up on Neocities. <a href=\"https://faunadb.neocities.org/\">Or just skip the installation and use our copy running on Neocities here.</a></p>\n<figure><img src=\"{asset:451:url||https://fauna.com/assets/site/blog-legacy/neocities.png}\" alt=\"\" /></figure><p><br /></p>\n<p>In future blog posts, we’ll extend the security model by adding support for inviting others to collaborate on your lists. By doing so, we’ll demonstrate FaunaDB’s native fine-grained access control.</p>\n<p>To run your own copy of the app:</p>\n<p>1. Start a free trial of FaunaDB by <a href=\"https://fauna.com/sign-up\">providing your email</a>.<br />2. Create a database in the <a href=\"https://dashboard.fauna.com/db\">dashboard</a>:</p>\n<ol><li>Click <strong>Create a Database</strong>.</li><li>Enter the name todomvc-fauna-spa in the text box.</li><li>Click <strong>Create Database</strong> again.</li></ol><p>3. Set up two database keys, one for with the <em>server</em> role which is used for setting up the schema, and one with the <em>client</em> role which is used by the browser code for creating new users:</p>\n<ol><li>Click ** / ** in the upper left side of the screen.</li><li>Click <strong>Manage Keys</strong> and <strong>Create a Key</strong>.</li><li>Name your key, assign it a <em>server</em> role, and choose the <em>todomvc-fauna-spa</em> database.</li><li>Click <strong>Create Key</strong>.</li><li>Your key’s secret will be displayed. Immediately copy it to schema.js</li><li>Then repeat the process, assigning this second key the <em>client</em> role and copying the key’s secret into Login.js.</li></ol><p>4. Clone the project’s repo <a href=\"https://github.com/fauna/todomvc-fauna-spa\">here</a>:</p>\n<pre>\ngit clone https://github.com/fauna/todomvc-fauna-spa.git\ncd todomvc-fauna-spa\n</pre>\n<p>5. Then install the dependencies:</p>\n<pre>\nnpm install\n</pre>\n<p>6. Run schema.js locally:</p>\n<pre>\nnode schema.js\n</pre>\n<p>7. Build the app bundle for uploading:</p>\n<pre>\nnpm run build\n</pre>\n<p>8. <a href=\"https://neocities.org/\">Set up a Neocities site</a>.<br />9. Upload the project to your new Neocities site by using their Upload button or their <a href=\"https://neocities.org/cli\">command line tool</a>. With the command line tool, run:</p>\n<pre>\ncd build/\nneocities push .\n</pre>\n<p>The <code>neocities</code> command will prompt you to login to your Neocities account and then to upload your site.</p>\n<p>Then you’re ready to browse to your new app, sign up, and start tracking your to-dos, all of which are securely stored in a strongly consistent, globally replicated, multi-cloud database: FaunaDB!</p>\n<p>We encourage you to tinker with the app! Just make sure to run <code>npm run build</code> after any code changes before updating your Neocities site. If you want to preview your changes, just run <code>npm start</code> for a local server.</p>\n<h3>Conclusion</h3>\n<p>We’ll talk more about security in the next few blog posts. The FaunaDB security model is designed for connecting to the database directly from mobile apps and web browsers. All user authentication and fine-grained, object-level access control can be managed by FaunaDB. In some circumstances, this can actually be more secure than relying on middleware for authentication authorization. A native security model is especially useful for applications that directly use the cloud database, and it can also enhance existing security mechanisms through defense-in-depth, where access control can be enforced at all layers of the stack.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-06-22T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 492,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e0be361d-f133-4162-bc9c-63625b0471e5",
        "siteSettingsId": 492,
        "fieldLayoutId": 4,
        "contentId": 325,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "How to Turn Your Data Into an API With FaunaDB and GraphQL",
        "slug": "graphql-faunadb",
        "uri": "blog/graphql-faunadb",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-08-19T20:59:58-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/graphql-faunadb",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/graphql-faunadb",
        "isCommunityPost": false,
        "blogBodyText": "<div class=\"admonitionblock\">\r\n<div class=\"icon\">Deprecated blog</div><br>\r\n<h3>FaunaDB now has native GraphQL!</h3>\r\n<p>FaunaDB now has native GraphQL, so this blog has been deprecated. We recommend following these tutorials instead:</p>\r\n<ol><li><a href=\"https://docs.fauna.com/fauna/current/graphql\">Getting started with GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/relations\">GraphQL Relations</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/unique\">Unique Constraints in GraphQL</a></li><li><a href=\"https://docs.fauna.com/fauna/current/howto/graphql/pagination\">GraphQL Pagination</a></li></ol>\r\n<p>You can also check out the <a href=\"https://docs.fauna.com/fauna/current/reference/graphql/\">GraphQL Reference</a> section in our docs to learn more about:</p>\r\n<ul>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/endpoints\" class=\"page\">Endpoints</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/directives/\" class=\"page\">Directives</a></li><li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/relations\" class=\"page\">Relations</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/functions\" class=\"page\">User-defined functions</a>\r\n</li>\r\n<li><a href=\"https://docs.fauna.com/fauna/current/reference/graphql/troubleshooting\" class=\"page\">Troubleshooting</a></li></ul>\r\n</div>\r\n<figure style=\"float: right; margin: 0px 0px 10px 10px;\"><img src=\"{asset:432:url}\" data-image=\"f7fn0loq4ll1\"></figure>\r\n<p><a href=\"http://graphql.org/\">GraphQL</a>&nbsp;is a query language for APIs that abstracts multiple data sources and enables&nbsp;app developers to request data in the format they need, without requiring backend API changes. Because GraphQL decouples front- and backend, it can consolidate multiple APIs into a single endpoint.</p>\r\n<p>GraphQL’s combination of expressiveness, performance, and flexibility has made it a runaway hit with developers, who find it more efficient than REST for many common use cases, and a natural fit for composing microservice calls.</p>\r\n<p>In this post, I’ll create a GraphQL resolver using FaunaDB for a schema of posts, authors, and comments to support a blog engine. It’s ported from an original example for the&nbsp;<a href=\"http://www.serverless.com/\">Serverless Framework</a>.</p>\r\n<p>GraphQL commonly accesses a database to resolve queries. A typical pattern is that each domain object (author, blog post, comment) is backed by a microservice which queries a database or other resource. GraphQL queries these microservices in parallel and composes the results for the client. FaunaDB is well suited for GraphQL’s parallel execution model, with strong consistency that can protect against some of the uncertainties that come with accessing data via microservices.</p>\r\n<p>Below you can see how FaunaDB’s query syntax compares with GraphQL. You’ll notice that GraphQL abstracts away concerns like pagination and indexes, presenting an app-focused surface area, while FaunaDB queries give more explicit control to allow for fine tuning and more complex queries.</p>\r\n<figure><img src=\"{asset:435:url}\" data-image=\"6u0ao89k3r8o\"></figure>\r\n<p>I based this example on Kevin Old’s&nbsp;<a href=\"http://kevinold.com/2016/02/01/serverless-graphql.html\">blog example</a>, which demonstrates the Serverless Framework with GraphQL. For this post, I ported Kevin’s example from DynamoDB to FaunaDB, reducing the code and complexity in the process. I also upgraded the example from version 0.5 of the Framework to the current version.<br></p>\r\n<p>One advantage of working with an existing project is that you can see&nbsp;<a href=\"https://github.com/fauna/serverless-graphql-blog/commit/2be3c4fad8891566b3612d5f866d6a615e1b98f5\">how much code DynamoDB required for its care and feeding</a>&nbsp;compared to the relative&nbsp;<a href=\"https://github.com/fauna/serverless-graphql-blog/blob/master/blog/lib/faunadb.js\">simplicity of using FaunaDB.</a>&nbsp;(Go ahead and click that last link there and read a handful of FaunaDB queries. You’ll be glad you did.)</p>\r\n<p>The service provides an API for a basic blog structure, including posts, authors and comments. The entire API consists of only 1 HTTP endpoint.</p>\r\n<h1>Getting Started</h1>\r\n<p>To run the demo below, you’ll need to do a few things first:</p>\r\n<p>1. Install&nbsp;<a href=\"https://serverless.com/\">Serverless Framework</a>&nbsp;with NodeV4+:</p>\r\n<pre class=\"language-sh\">npm install serverless -g</pre>\r\n<p>2. Get the example service code by cloning our demo repo:</p>\r\n<pre class=\"language-sh\">git clone https://github.com/fauna/serverless-graphql-blog\r\ncd serverless-graphql-blog</pre>\r\n<p>3. Inside the&nbsp;<code>serverless-graphql-blog</code>&nbsp;checkout, install dependencies:</p>\r\n<pre class=\"language-sh\">npm install\r\ncd blogs\r\nnpm install</pre>\r\n<h1>Create a Database on FaunaDB</h1>\r\n<p>Now that you have the tools you’ll need for this demo, create a database on FaunaDB. This database will hold the data returned by your GraphQL endpoint.</p>\r\n<p>1. If you don’t have a FaunaDB account,&nbsp;<a href=\"https://fauna.com/sign-up\">start a free trial</a>.&nbsp;<br>2. Create a database in the&nbsp;<a href=\"https://dashboard.fauna.com/db\">FaunaDB Dashboard</a>:</p>\r\n<ol><li>Click&nbsp;<strong>Create a Database</strong>.</li><li>Enter the name graphql-blog-demo in the text box.</li><li>Click&nbsp;<strong>Create Database</strong>&nbsp;again.</li></ol>\r\n<p>3. Get a key:</p>\r\n<ol><li>Click&nbsp;<strong>/</strong>&nbsp;in the upper left side of the screen.</li><li>Click&nbsp;<strong>Manage Keys</strong>&nbsp;and&nbsp;<strong>Create a Key</strong>.</li></ol>\r\n<figure><img src=\"{asset:424:url}\" data-image=\"movtznbn16md\"></figure>\r\n<p></p>\r\n<ol><li>Name your key, assign it a&nbsp;<em>server</em>&nbsp;role, and choose the&nbsp;<em>graphql-blog-demo</em>&nbsp;database.</li><li>Click&nbsp;<strong>Create Key</strong>.</li><li>Your key’s secret will be displayed. Copy it to your&nbsp;<code>serverless.yaml</code>&nbsp;file, replacing&nbsp;<code>SERVER_SECRET_FOR_YOUR_FAUNADB_DATABASE</code>.</li></ol>\r\n<h1>Deploy Your GraphQL Endpoint</h1>\r\n<p>Now that FaunaDB has some data to return via your GraphQL handler, it’s time to deploy that GraphQL endpoint so you can use it to query FaunaDB.</p>\r\n<p>When you run the following command,&nbsp;<strong>be sure to note the POST endpoint URL it’s assigned</strong>.</p>\r\n<pre class=\"language-sh\">serverless deploy</pre>\r\n<p>You’ll see a result like this, which contains your POST endpoint URL:</p>\r\n<pre class=\"language-bash\">Service Information\r\nservice: serverless-graphql-blog\r\nstage: dev\r\nregion: us-east-1\r\napi keys:\r\n  None\r\nendpoints:\r\n  POST - https://XYZ.execute-api.us-east-1.amazonaws.com/dev/blog/graphql\r\nfunctions:\r\n  setupFaunaDB: serverless-graphql-blog-dev-setupFaunaDB\r\n  sadiavas: serverless-graphql-blog-dev-sadiavas\r\nServerless: Removing old service versions...</pre>\r\n<p>Invoke the private endpoint for creating the classes and indexes in your FaunaDB database. This creates the posts, authors, and comments classes, and indexes for loading posts by author.</p>\r\n<pre class=\"language-sh\">serverless invoke --function setupFaunaDB</pre>\r\n<h3>Querying with GraphiQL</h3>\r\n<p>The&nbsp;<a href=\"https://github.com/graphql/graphql-js\">graphql-js</a>&nbsp;endpoint provided in this Serverless project is compatible with&nbsp;<a href=\"https://github.com/graphql/graphiql\">GraphiQL</a>, a query visualization tool.</p>\r\n<p>Usage with&nbsp;<a href=\"https://github.com/skevy/graphiql-app\">GraphiQL.app</a>&nbsp;(an Electron wrapper around GraphiQL) is recommended and shown below:</p>\r\n<figure><img src=\"{asset:434:url}\" data-image=\"tpvccp6jdiad\"></figure>\r\n<p></p>\r\n<h3>Sample GraphQL Queries</h3>\r\n<p>First, create an author and some of their posts.</p>\r\n<p>To create an author, visit the&nbsp;<code>authors</code>&nbsp;class in the FaunaDB dashboard. The URL is:&nbsp;<code>https://dashboard.fauna.com/db/graphql-blog-demo/classes/authors</code></p>\r\n<p>Click “Create Instance”, enter some JSON-formatted data. Then save your author instance.</p>\r\n<pre class=\"language-json\">{\r\n  \"name\": \"Chris\",\r\n  \"id\": \"123\"\r\n}</pre>\r\n<h4>Create a blog post</h4>\r\n<p>Now, you can switch to GraphiQL to run a mutation to create a blog post. Make sure to enter the endpoint URL that was returned by&nbsp;<code>serverless deploy</code>. Then you can enter a GraphQL query like this to create a blog post for your author:</p>\r\n<pre>mutation createNewPost {  post: createPost (id: \"5\",    title: \"Fifth post!\",    bodyContent: \"Test content\",    author: \"123\") { id, title } }\r\n</pre>\r\n<p>Now that you’ve created some data, you can run other queries.</p>\r\n<h3>List of author names</h3>\r\n<pre class=\"language-graphql\">{ authors { name } }\r\n</pre>\r\n<h4>Results</h4>\r\n<pre class=\"language-json\">{\r\n    \"data\": {\r\n        \"authors\":[\r\n            {\"name\":\"Chris\"}\r\n        ]\r\n    }\r\n}</pre>\r\n<h4>List of posts with id and title</h4>\r\n<pre class=\"language-graphql\">{ posts { id, title } }\r\n</pre>\r\n<h4>Results</h4>\r\n<pre class=\"language-json\">{  \r\n    \"data\": {    \r\n        \"posts\": [      \r\n            { \"id\":\"1\",\r\n              \"title\":\"First Post Title\"\r\n            }\r\n         ]  \r\n    }\r\n}\r\n</pre>\r\n<h3>List of posts with id, title and&nbsp;<em>nested</em>&nbsp;author name</h3>\r\n<pre class=\"language-graphql\">{ posts { id, title, author { name } } }\r\n</pre>\r\n<h4>Results</h4>\r\n<pre class=\"language-json\">{  \r\n    \"data\": {    \r\n        \"posts\": [\r\n            { \"id\":\"1\",\r\n              \"title\":\"First Post Title\",\r\n              \"author\": {\r\n                \"name\":\"Chris\"\r\n              }\r\n            }\r\n        ]\r\n    }\r\n}\r\n</pre>\r\n<h2>Conclusion</h2>\r\n<p>If you’re using FaunaDB as the backend of an existing app, standing up a microservice to allow GraphQL access to your database is an architecturally sound approach, which works especially well if you are already taking advantage of FaunaDB’s native object-level access control. Your FaunaDB/GraphQL endpoint can also compose data from other APIs, backend services, and IT systems.</p>\r\n<p>If you are already building microservices, FaunaDB can unify your data model. FaunaDB’s support for joins, constraints, and triggers lets you provide efficient access for each microservice, all while enforcing access control and validation logic.</p>\r\n<p>Of course, you don’t have to build microservices to benefit from the combination of GraphQL and FaunaDB. FaunaDB’s query language is flexible in a similar way to GraphQL, but processed by the backend database. A mature GraphQL/FaunaDB connector will be able to consolidate requests across multiple GraphQL schema resolvers into a single FaunaDB query. This approach will add an additional layer of optimization potential to your GraphQL queries. Watch this space for more GraphQL updates.</p>\r\n<p>Visit the&nbsp;<a href=\"https://github.com/fauna/serverless-graphql-blog\">FaunaDB Serverless GraphQL Blog README</a>&nbsp;for more query examples, and notes about how you can contribute to the example.</p>\r\n<style>\r\n.admonitionblock{padding: 1rem 1.5rem;border-left:3px solid #d32f2f; background: #fff;margin-bottom:2rem;\r\n    -webkit-box-shadow: 0 2px 8px rgba(0,0,0,.06);\r\n    box-shadow: 0 2px 8px rgba(0,0,0,.06);}\r\nbody.blog .description .admonitionblock h3{margin-top:0;}\r\n.admonitionblock .icon {\r\nbackground: #d32f2f;\r\n    font-size: .73rem;\r\n    padding: .3rem .5rem;\r\n    height: 1.35rem;\r\n    line-height: 1;\r\n    font-weight: 500;\r\n    text-transform: uppercase;\r\nmargin-bottom:1.5rem;\r\ndisplay:inline-block;\r\n    color: #fff;\r\n}\r\n</style>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<figure style=\"float:right;margin:0px 0px 10px 10px;\"><img src=\"{asset:432:url||https://fauna.com/assets/site/blog-legacy/gql-small.png}\" alt=\"\" /></figure><p><a href=\"http://graphql.org/\">GraphQL</a> is a query language for APIs that abstracts multiple data sources and enables app developers to request data in the format they need, without requiring backend API changes. Because GraphQL decouples front- and backend, it can consolidate multiple APIs into a single endpoint.</p>\n<p>GraphQL’s combination of expressiveness, performance, and flexibility has made it a runaway hit with developers, who find it more efficient than REST for many common use cases, and a natural fit for composing microservice calls.</p>\n<p>In this post, I’ll create a GraphQL resolver using FaunaDB for a schema of posts, authors, and comments to support a blog engine. It’s ported from an original example for the <a href=\"http://www.serverless.com/\">Serverless Framework</a>.</p>\n<p>GraphQL commonly accesses a database to resolve queries. A typical pattern is that each domain object (author, blog post, comment) is backed by a microservice which queries a database or other resource. GraphQL queries these microservices in parallel and composes the results for the client. FaunaDB is well suited for GraphQL’s parallel execution model, with strong consistency that can protect against some of the uncertainties that come with accessing data via microservices.</p>\n<p>Below you can see how FaunaDB’s query syntax compares with GraphQL. You’ll notice that GraphQL abstracts away concerns like pagination and indexes, presenting an app-focused surface area, while FaunaDB queries give more explicit control to allow for fine tuning and more complex queries.</p>\n<figure><img src=\"{asset:435:url||https://fauna.com/assets/site/blog-legacy/graphql-code.png}\" alt=\"\" /></figure><p>I based this example on Kevin Old’s <a href=\"http://kevinold.com/2016/02/01/serverless-graphql.html\">blog example</a>, which demonstrates the Serverless Framework with GraphQL. For this post, I ported Kevin’s example from DynamoDB to FaunaDB, reducing the code and complexity in the process. I also upgraded the example from version 0.5 of the Framework to the current version.<br /></p>\n<p>One advantage of working with an existing project is that you can see <a href=\"https://github.com/fauna/serverless-graphql-blog/commit/2be3c4fad8891566b3612d5f866d6a615e1b98f5\">how much code DynamoDB required for its care and feeding</a> compared to the relative <a href=\"https://github.com/fauna/serverless-graphql-blog/blob/master/blog/lib/faunadb.js\">simplicity of using FaunaDB.</a> (Go ahead and click that last link there and read a handful of FaunaDB queries. You’ll be glad you did.)</p>\n<p>The service provides an API for a basic blog structure, including posts, authors and comments. The entire API consists of only 1 HTTP endpoint.</p>\n<h1>Getting Started</h1>\n<p>To run the demo below, you’ll need to do a few things first:</p>\n<p>1. Install <a href=\"https://serverless.com/\">Serverless Framework</a> with NodeV4+:</p>\n<pre class=\"language-sh\">npm install serverless -g</pre>\n<p>2. Get the example service code by cloning our demo repo:</p>\n<pre class=\"language-sh\">git clone https://github.com/fauna/serverless-graphql-blog\ncd serverless-graphql-blog</pre>\n<p>3. Inside the <code>serverless-graphql-blog</code> checkout, install dependencies:</p>\n<pre class=\"language-sh\">npm install\ncd blogs\nnpm install</pre>\n<h1>Create a Database on FaunaDB</h1>\n<p>Now that you have the tools you’ll need for this demo, create a database on FaunaDB. This database will hold the data returned by your GraphQL endpoint.</p>\n<p>1. If you don’t have a FaunaDB account, <a href=\"https://fauna.com/sign-up\">start a free trial</a>. <br />2. Create a database in the <a href=\"https://dashboard.fauna.com/db\">FaunaDB Dashboard</a>:</p>\n<ol><li>Click <strong>Create a Database</strong>.</li><li>Enter the name graphql-blog-demo in the text box.</li><li>Click <strong>Create Database</strong> again.</li></ol><p>3. Get a key:</p>\n<ol><li>Click <strong>/</strong> in the upper left side of the screen.</li><li>Click <strong>Manage Keys</strong> and <strong>Create a Key</strong>.</li></ol><figure><img src=\"{asset:424:url||https://fauna.com/assets/site/blog-legacy/fauna-dashboard.png}\" alt=\"\" /></figure><ol><li>Name your key, assign it a <em>server</em> role, and choose the <em>graphql-blog-demo</em> database.</li><li>Click <strong>Create Key</strong>.</li><li>Your key’s secret will be displayed. Copy it to your <code>serverless.yaml</code> file, replacing <code>SERVER_SECRET_FOR_YOUR_FAUNADB_DATABASE</code>.</li></ol><h1>Deploy Your GraphQL Endpoint</h1>\n<p>Now that FaunaDB has some data to return via your GraphQL handler, it’s time to deploy that GraphQL endpoint so you can use it to query FaunaDB.</p>\n<p>When you run the following command, <strong>be sure to note the POST endpoint URL it’s assigned</strong>.</p>\n<pre class=\"language-sh\">serverless deploy</pre>\n<p>You’ll see a result like this, which contains your POST endpoint URL:</p>\n<pre class=\"language-bash\">Service Information\nservice: serverless-graphql-blog\nstage: dev\nregion: us-east-1\napi keys:\n None\nendpoints:\n POST - https://XYZ.execute-api.us-east-1.amazonaws.com/dev/blog/graphql\nfunctions:\n setupFaunaDB: serverless-graphql-blog-dev-setupFaunaDB\n sadiavas: serverless-graphql-blog-dev-sadiavas\nServerless: Removing old service versions...</pre>\n<p>Invoke the private endpoint for creating the classes and indexes in your FaunaDB database. This creates the posts, authors, and comments classes, and indexes for loading posts by author.</p>\n<pre class=\"language-sh\">serverless invoke --function setupFaunaDB</pre>\n<h3>Querying with GraphiQL</h3>\n<p>The <a href=\"https://github.com/graphql/graphql-js\">graphql-js</a> endpoint provided in this Serverless project is compatible with <a href=\"https://github.com/graphql/graphiql\">GraphiQL</a>, a query visualization tool.</p>\n<p>Usage with <a href=\"https://github.com/skevy/graphiql-app\">GraphiQL.app</a> (an Electron wrapper around GraphiQL) is recommended and shown below:</p>\n<figure><img src=\"{asset:434:url||https://fauna.com/assets/site/blog-legacy/graphql-app.png}\" alt=\"\" /></figure><h3>Sample GraphQL Queries</h3>\n<p>First, create an author and some of their posts.</p>\n<p>To create an author, visit the <code>authors</code> class in the FaunaDB dashboard. The URL is: <code>https://dashboard.fauna.com/db/graphql-blog-demo/classes/authors</code></p>\n<p>Click “Create Instance”, enter some JSON-formatted data. Then save your author instance.</p>\n<pre class=\"language-json\">{\n \"name\": \"Chris\",\n \"id\": \"123\"\n}</pre>\n<h4>Create a blog post</h4>\n<p>Now, you can switch to GraphiQL to run a mutation to create a blog post. Make sure to enter the endpoint URL that was returned by <code>serverless deploy</code>. Then you can enter a GraphQL query like this to create a blog post for your author:</p>\n<pre>mutation createNewPost { post: createPost (id: \"5\", title: \"Fifth post!\", bodyContent: \"Test content\", author: \"123\") { id, title } }\n</pre>\n<p>Now that you’ve created some data, you can run other queries.</p>\n<h3>List of author names</h3>\n<pre class=\"language-graphql\">{ authors { name } }\n</pre>\n<h4>Results</h4>\n<pre class=\"language-json\">{\n \"data\": {\n \"authors\":[\n {\"name\":\"Chris\"}\n ]\n }\n}</pre>\n<h4>List of posts with id and title</h4>\n<pre class=\"language-graphql\">{ posts { id, title } }\n</pre>\n<h4>Results</h4>\n<pre class=\"language-json\">{ \n \"data\": { \n \"posts\": [ \n { \"id\":\"1\",\n \"title\":\"First Post Title\"\n }\n ] \n }\n}\n</pre>\n<h3>List of posts with id, title and <em>nested</em> author name</h3>\n<pre class=\"language-graphql\">{ posts { id, title, author { name } } }\n</pre>\n<h4>Results</h4>\n<pre class=\"language-json\">{ \n \"data\": { \n \"posts\": [\n { \"id\":\"1\",\n \"title\":\"First Post Title\",\n \"author\": {\n \"name\":\"Chris\"\n }\n }\n ]\n }\n}\n</pre>\n<h2>Conclusion</h2>\n<p>If you’re using FaunaDB as the backend of an existing app, standing up a microservice to allow GraphQL access to your database is an architecturally sound approach, which works especially well if you are already taking advantage of FaunaDB’s native object-level access control. Your FaunaDB/GraphQL endpoint can also compose data from other APIs, backend services, and IT systems.</p>\n<p>If you are already building microservices, FaunaDB can unify your data model. FaunaDB’s support for joins, constraints, and triggers lets you provide efficient access for each microservice, all while enforcing access control and validation logic.</p>\n<p>Of course, you don’t have to build microservices to benefit from the combination of GraphQL and FaunaDB. FaunaDB’s query language is flexible in a similar way to GraphQL, but processed by the backend database. A mature GraphQL/FaunaDB connector will be able to consolidate requests across multiple GraphQL schema resolvers into a single FaunaDB query. This approach will add an additional layer of optimization potential to your GraphQL queries. Watch this space for more GraphQL updates.</p>\n<p>Visit the <a href=\"https://github.com/fauna/serverless-graphql-blog\">FaunaDB Serverless GraphQL Blog README</a> for more query examples, and notes about how you can contribute to the example.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-06-13T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 504,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "aee0e07e-dc1a-478d-8a91-370f5f209b00",
        "siteSettingsId": 504,
        "fieldLayoutId": 4,
        "contentId": 337,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "The Best Database for SpatialOS",
        "slug": "faunadb-the-best-database-for-spatialos",
        "uri": "blog/faunadb-the-best-database-for-spatialos",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-the-best-database-for-spatialos",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-the-best-database-for-spatialos",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’ve gotten some inquiries from customers about using FaunaDB with&nbsp;<a href=\"https://improbable.io/games\">SpatialOS</a>. FaunaDB’s global distribution, transactions, and temporal data model make it a great fit; I would like to explain why in detail.</p>\r\n<h2>What is SpatialOS?</h2>\r\n<p>SpatialOS is a cloud platform for building multi-agent systems: primarily online games, VR worlds, and physical simulations.</p>\r\n<figure><img src=\"{asset:431:url}\" data-image=\"b7mwwfdhf1h7\"></figure>\r\n<figure><figcaption><br>The game Worlds Adrift</figcaption></figure>\r\n<p>Data elements called&nbsp;<em>entities</em>&nbsp;are sharded and replicated in memory across machines that can host co-located&nbsp;<em>workers</em>. Workers are stateless logic processors that can be written in a variety of programming languages. They subscribe to change events on entities, or respond to user input, and react by executing logic, querying external resources, and updating other entities.<br></p>\r\n<p>In academia, they call this a&nbsp;<a href=\"http://lia.deis.unibo.it/courses/2007-2008/SMA-LS/papers/5/p80-gelernter.pdf\">tuple space</a>. In game development they call it a big problem, so it makes sense that SpatialOS has attracted so much attention.</p>\r\n<h2>The Spatial Sauce</h2>\r\n<p>The SpatialOS name comes from the fact that entities are sharded by their location in three-dimensional virtual space. This is nice for games because you get the benefits of batch processing nearby entities together, while letting the platform manage replication and partitioning at the boundaries.</p>\r\n<p>However, as Duco van Amstel, a SpatialOS engineer,&nbsp;<a href=\"https://forums.improbable.io/t/state-saving-how-with-spatialos/1706/2\">explains</a>:</p>\r\n<blockquote>[In SpatialOS], as soon as you have an element of data you care deeply about, you want something that is: written on disk…in multiple locations for redundancy; has access mechanisms that implement transactions correctly; [and] allows for rollback to a specific point in history. In other words, you want an on-disk, cloud-based transactional database.</blockquote>\r\n<p>FaunaDB is the only database that meets these requirements.</p>\r\n<h2>What is FaunaDB?</h2>\r\n<p>FaunaDB is a multi-cloud, globally distributed, temporal database. You get all your friendly database feature like transactions, indexes, constraints, and user-defined functions, you get a comprehensive cloud security model, and you never have to worry about provisioning or scaling your backend.</p>\r\n<p>Like SpatialOS, FaunaDB works hard to hide the infrastructure behind the system. If you don’t want to care about the underlying cloud providers, you don’t have to. Also like SpatialOS, FaunaDB Cloud is pay-as-you-go. As your system scales up and down in usage and complexity, you pay for only what you actually use.</p>\r\n<h2>Snapshots and Rollbacks</h2>\r\n<p>FaunaDB is also designed to store not just the state of your data, but its history. This solves a critical issue with SpatialOS. SpatialOS allows you to snapshot your entire world state, in order to duplicate it or survive catastrophes. But if you take a snapshot, you also need to have a snapshot of your database at the same point in time, or the two will be badly out of sync.</p>\r\n<p>Since FaunaDB is temporal, the entire database can be read at any point in the past for any query. In effect, it is “always snapshotted.” Any SpatialOS snapshot, entity state, or worker computation can be trivially correlated to the correct database state at any time. This also solves a related problem, partial rollback:</p>\r\n<blockquote>Snapshots are a costly operation and should not be counted on, for example, to roll back specific player transactions with respect to maintenance or similar incidents.</blockquote>\r\n<p>A temporal database is the right way to provide this discrete rollback capability.</p>\r\n<h2>Fun With History</h2>\r\n<p>If you can query any group of entities in your simulation from any point in time, what else can you accomplish? You could:</p>\r\n<ul><li>Render instant replays of anything, from any time and any perspective, by interpolating event data</li><li>Let entities use their “memory” to automatically create narratives about themselves</li><li>Use machine learning to detect unusual, fraudulent, or game-breaking behavior</li><li>Create activity feeds of world and player activity for second-screen apps</li></ul>\r\n<p>With FaunaDB, the virtual sky is the limit.</p>\r\n<h2>Latency and Locality</h2>\r\n<p>To top it all off, although you might not know where your SpatialOS nodes and application users are physically located, queries to FaunaDB from both workers and clients will automatically get routed to the closest physical datacenter, guaranteeing the lowest possible latency.</p>\r\n<p>Let us know if you build something with FaunaDB and SpatialOS!</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>We’ve gotten some inquiries from customers about using FaunaDB with <a href=\"https://improbable.io/games\">SpatialOS</a>. FaunaDB’s global distribution, transactions, and temporal data model make it a great fit; I would like to explain why in detail.</p>\n<h2>What is SpatialOS?</h2>\n<p>SpatialOS is a cloud platform for building multi-agent systems: primarily online games, VR worlds, and physical simulations.</p>\n<figure><img src=\"{asset:431:url||https://fauna.com/assets/site/blog-legacy/game-worlds-adrift.png}\" alt=\"\" /></figure><figure><figcaption><br />The game Worlds Adrift</figcaption></figure><p>Data elements called <em>entities</em> are sharded and replicated in memory across machines that can host co-located <em>workers</em>. Workers are stateless logic processors that can be written in a variety of programming languages. They subscribe to change events on entities, or respond to user input, and react by executing logic, querying external resources, and updating other entities.<br /></p>\n<p>In academia, they call this a <a href=\"http://lia.deis.unibo.it/courses/2007-2008/SMA-LS/papers/5/p80-gelernter.pdf\">tuple space</a>. In game development they call it a big problem, so it makes sense that SpatialOS has attracted so much attention.</p>\n<h2>The Spatial Sauce</h2>\n<p>The SpatialOS name comes from the fact that entities are sharded by their location in three-dimensional virtual space. This is nice for games because you get the benefits of batch processing nearby entities together, while letting the platform manage replication and partitioning at the boundaries.</p>\n<p>However, as Duco van Amstel, a SpatialOS engineer, <a href=\"https://forums.improbable.io/t/state-saving-how-with-spatialos/1706/2\">explains</a>:</p>\n<blockquote>[In SpatialOS], as soon as you have an element of data you care deeply about, you want something that is: written on disk…in multiple locations for redundancy; has access mechanisms that implement transactions correctly; [and] allows for rollback to a specific point in history. In other words, you want an on-disk, cloud-based transactional database.</blockquote>\n<p>FaunaDB is the only database that meets these requirements.</p>\n<h2>What is FaunaDB?</h2>\n<p>FaunaDB is a multi-cloud, globally distributed, temporal database. You get all your friendly database feature like transactions, indexes, constraints, and user-defined functions, you get a comprehensive cloud security model, and you never have to worry about provisioning or scaling your backend.</p>\n<p>Like SpatialOS, FaunaDB works hard to hide the infrastructure behind the system. If you don’t want to care about the underlying cloud providers, you don’t have to. Also like SpatialOS, FaunaDB Cloud is pay-as-you-go. As your system scales up and down in usage and complexity, you pay for only what you actually use.</p>\n<h2>Snapshots and Rollbacks</h2>\n<p>FaunaDB is also designed to store not just the state of your data, but its history. This solves a critical issue with SpatialOS. SpatialOS allows you to snapshot your entire world state, in order to duplicate it or survive catastrophes. But if you take a snapshot, you also need to have a snapshot of your database at the same point in time, or the two will be badly out of sync.</p>\n<p>Since FaunaDB is temporal, the entire database can be read at any point in the past for any query. In effect, it is “always snapshotted.” Any SpatialOS snapshot, entity state, or worker computation can be trivially correlated to the correct database state at any time. This also solves a related problem, partial rollback:</p>\n<blockquote>Snapshots are a costly operation and should not be counted on, for example, to roll back specific player transactions with respect to maintenance or similar incidents.</blockquote>\n<p>A temporal database is the right way to provide this discrete rollback capability.</p>\n<h2>Fun With History</h2>\n<p>If you can query any group of entities in your simulation from any point in time, what else can you accomplish? You could:</p>\n<ul><li>Render instant replays of anything, from any time and any perspective, by interpolating event data</li><li>Let entities use their “memory” to automatically create narratives about themselves</li><li>Use machine learning to detect unusual, fraudulent, or game-breaking behavior</li><li>Create activity feeds of world and player activity for second-screen apps</li></ul><p>With FaunaDB, the virtual sky is the limit.</p>\n<h2>Latency and Locality</h2>\n<p>To top it all off, although you might not know where your SpatialOS nodes and application users are physically located, queries to FaunaDB from both workers and clients will automatically get routed to the closest physical datacenter, guaranteeing the lowest possible latency.</p>\n<p>Let us know if you build something with FaunaDB and SpatialOS!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-06-08T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 505,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "25a7c436-77ae-4638-9422-38417e952827",
        "siteSettingsId": 505,
        "fieldLayoutId": 4,
        "contentId": 338,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Global Multi-Cloud Replication in FaunaDB",
        "slug": "global-multi-cloud-replication-in-faunadb-serverless-cloud",
        "uri": "blog/global-multi-cloud-replication-in-faunadb-serverless-cloud",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2020-02-04T11:13:57-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/global-multi-cloud-replication-in-faunadb-serverless-cloud",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/global-multi-cloud-replication-in-faunadb-serverless-cloud",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is a serverless&nbsp;globally distributed cloud database-as-a-service. Other cloud databases offer single-master configurations with global replication or multi-master configurations with continental replication, but only FaunaDB truly supports low latency read and write access from anywhere.</p>\n<p>But where does your FaunaDB data actually live? We’re very excited to show you the new&nbsp;<a href=\"https://status.fauna.com/\">cloud status page</a>.</p>\n\n\n<p>FaunaDB is multi-cloud, with a single cluster spanning AWS and Google Cloud Platform. After all, if you have to worry about your provider’s infrastructure availability, then you’re not really dealing with a&nbsp;<strong>truly serverless database</strong>. Part of our mission is to insulate applications from infrastructure dependencies. When your data spans public clouds, your applications can use best-of-breed cloud services wherever they are found.<br></p>\n<h2>Upcoming Regions</h2>\n<p>Soon, we’ll expand our cloud to GCP’s Singapore region. This will not affect the latency profile of existing applications: writes to FaunaDB only need to commit to the closest majority of datacenters to maintain consistency. Currently, all data in FaunaDB Cloud is replicated to every region, guaranteeing low latency reads.</p>\n<figure><img src=\"{asset:450:url}\" data-image=\"li324a82schi\"></figure>\n<figure><figcaption><br>Measured region latencies</figcaption></figure>\n<p>Later this year we will release&nbsp;<strong>dynamic site selection</strong>, letting you choose per-database where your data should live, and change it on the fly. Along with that release, we will expand to additional regions, including some in Microsoft Azure. The upcoming regions, subject to change, are:<br></p>\n<ul><li>Wenya, Singapore (GCP)</li><li>São Paulo, Brazil (GCP)</li><li>Frankfurt, Germany (Azure)</li><li>Toronto, Canada (Azure)</li><li>New South Wales, Australia (Azure)</li></ul>\n<h2>Cheaper, Faster, Better</h2>\n<p>A three, five, or even nine datacenter cluster would be prohibitively expensive with any other solution. But in our cloud, it’s not: it’s still paid<strong>&nbsp;as you go</strong>. You only ever pay for the queries you run and the actual data you choose to store in each region.</p>\n<p>FaunaDB is being used by individuals, small companies, and even the Fortune 500. With global availability at serverless pricing, why would you ever use anything else?</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB Cloud is a multi-master, globally distributed database-as-a-service. Other cloud databases offer single-master configurations with global replication, or multi-master configurations with continental replication, but only FaunaDB truly supports low latency read and write access from anywhere.</p>\n<p>But where does your FaunaDB Cloud data actually live? We’re very excited to show you with the new <a href=\"https://fauna.com/status\">cloud status page</a>.</p>\n<figure><img src=\"{asset:465:url||https://fauna.com/assets/site/blog-legacy/status-page.png}\" alt=\"\" /></figure><figure><figcaption><br />Locations and infrastructure providers</figcaption></figure><p>FaunaDB Cloud is multi-cloud, with a single cluster spanning AWS and Google Cloud Platform. After all, if you have to worry about your provider’s infrastructure availability, then you’re not really dealing with a <strong>truly serverless database</strong>. Part of our mission is to insulate applications from infrastructure dependencies. When your data spans public clouds, your applications can use best-of-breed cloud services wherever they are found.<br /></p>\n<h2>Upcoming Regions</h2>\n<p>Soon, we’ll expand our cloud to GCP’s Singapore region. This will not affect the latency profile of existing applications: writes to FaunaDB only need to commit to the closest majority of datacenters to maintain consistency. Currently all data in FaunaDB Cloud is replicated to every region, guaranteeing low latency reads.</p>\n<figure><img src=\"{asset:450:url||https://fauna.com/assets/site/blog-legacy/latency-table.png}\" alt=\"\" /></figure><figure><figcaption><br />Measured region latencies</figcaption></figure><p>Later this year we will release <strong>dynamic site selection</strong>, letting you choose per-database where your data should live, and change it on the fly. Along with that release we will expand to additional regions, including some in Microsoft Azure. The upcoming regions, subject to change, are:<br /></p>\n<ul><li>Wenya, Singapore (GCP)</li><li>São Paulo, Brazil (GCP)</li><li>Frankfurt, Germany (Azure)</li><li>Toronto, Canada (Azure)</li><li>New South Wales, Australia (Azure)</li></ul><h2>Cheaper, Faster, Better</h2>\n<p>A three, five, or even nine datacenter cluster would be prohibitively expensive with any other solution. But in our cloud, it’s not: it’s still <strong>pay as you go</strong>. You only ever pay for the queries you run and the actual data you choose to store in each region.</p>\n<p>FaunaDB Cloud is being used by individuals, small companies, and even the Fortune 500. With global availability at serverless pricing, why would you ever use anything else?</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-05-08T15:05:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 503,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b3432baa-80bf-4d7a-98cb-372218d26a5b",
        "siteSettingsId": 503,
        "fieldLayoutId": 4,
        "contentId": 336,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Why I Joined the Next Big Database Startup and You Should Too",
        "slug": "why-you-should-join-the-next-big-database-startup",
        "uri": "blog/why-you-should-join-the-next-big-database-startup",
        "dateCreated": "2018-08-27T07:12:43-07:00",
        "dateUpdated": "2019-08-21T14:31:03-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/why-you-should-join-the-next-big-database-startup",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/why-you-should-join-the-next-big-database-startup",
        "isCommunityPost": false,
        "blogBodyText": "<p>I’ve been with Fauna since November 2016 and in the database industry for a decade. I rode the first wave of NoSQL and built systems that power your everyday shopping, airline and hotel bookings, online gaming, viral media, medical records, wind farms and factory automation.</p>\r\n<p>Since those heady days, the NoSQL movement has matured. Everyone is onboard, from CIOs at the Fortune 100 to highschool hackers. The previous generation’s skepticism about leaving SQL behind is gone, replaced with real energy and a focus on practical solutions to business and operational problems. In the early days, even cutting edge adopters were full of questions. Today, people know what they need from a database.</p>\r\n<blockquote class=\"twitter-tweet\" data-lang=\"en\">In 2017, newly engineered multimodel DBMSs will disrupt old & new alike, built w assumptions of cloud, elasticity, microservices, “post-SQL\"<br>— Merv Adrian (@merv) <a href=\"https://twitter.com/merv/status/816777448947740673?ref_src=twsrc%5Etfw\">January 4, 201<br></a></blockquote>\r\n<p><br>FaunaDB is that database. I have never experienced the kind of interest and excitement that FaunaDB has created. Users of all kinds know that the Fauna team understands their problems, and critically–that we have the experience to solve them. The market is hungry to move on from legacy RDBMSs that don’t address modern development patterns, but also from first generation NoSQL systems with inadequate features and poor operational characteristics. The smiles on developers’ faces at conferences are a nice bonus.<br></p>\r\n<blockquote>“I have never been very comfortable with database programming, but FaunaDB has been extremely easy to integrate with my app, so much so that I am continually thinking of new uses for it! I absolutely love FaunaDB!”<br>— Chris Klugewicz</blockquote>\r\n<p>By any measure my previous startup is a huge success, running major components of the digital economy. But it’s different this time. Customers and users are educated about the options, and have the processes in place to evaluate and adopt new database technologies. New solutions are finding a ready market. For Fauna, that means favorable winds and following seas.</p>\r\n<p>FaunaDB Cloud signups are up and to the right, and even ahead of the FaunaDB On-Premises launch, our enterprise pipeline is overflowing as well, with huge brands you’ve heard of across all major market verticals. To be honest, we’re struggling to keep up.</p>\r\n<p>With that in mind, we’re hiring across all functions of the company: sales, marketing, and engineering. To prepare for this growth we’ve spent time building a culture to be proud of: innovative, diverse, supportive, and collaborative. Operational databases can be lifetime commitments for customers, and we, and our investors, are in it for the long haul. You can read about our employee friendly policies in&nbsp;<a href=\"https://www.washingtonpost.com/news/business/wp/2017/04/18/the-newest-silicon-valley-perk-paid-time-off-to-protest-trump/\">the Washington Post</a>, the&nbsp;<a href=\"http://www.csmonitor.com/USA/Politics/2017/0427/New-Silicon-Valley-perk-paid-time-off-to-protest\">Christian Science Monitor</a>, and&nbsp;<a href=\"https://fauna.com/blog/supporting-civic-engagement\">right here on our blog</a>.</p>\r\n<p>If you’re as excited as we are by this enormous opportunity,&nbsp;<a href=\"mailto:priority@fauna.com\">reach out and tell us</a>&nbsp;what your&nbsp;<a href=\"https://fauna.com/jobs\">dream job looks like.</a></p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>I’ve been with Fauna since November 2016 and in the database industry for a decade. I rode the first wave of NoSQL and built systems that power your everyday shopping, airline and hotel bookings, online gaming, viral media, medical records, wind farms and factory automation.</p>\n<p>Since those heady days, the NoSQL movement has matured. Everyone is onboard, from CIOs at the Fortune 100 to highschool hackers. The previous generation’s skepticism about leaving SQL behind is gone, replaced with real energy and a focus on practical solutions to business and operational problems. In the early days, even cutting edge adopters were full of questions. Today, people know what they need from a database.</p>\n<blockquote class=\"twitter-tweet\">In 2017, newly engineered multimodel DBMSs will disrupt old &amp; new alike, built w assumptions of cloud, elasticity, microservices, “post-SQL\"<br />— Merv Adrian (@merv) <a href=\"https://twitter.com/merv/status/816777448947740673?ref_src=twsrc%5Etfw\">January 4, 201<br /></a></blockquote>\n<p><br />FaunaDB is that database. I have never experienced the kind of interest and excitement that FaunaDB has created. Users of all kinds know that the Fauna team understands their problems, and critically–that we have the experience to solve them. The market is hungry to move on from legacy RDBMSs that don’t address modern development patterns, but also from first generation NoSQL systems with inadequate features and poor operational characteristics. The smiles on developers’ faces at conferences are a nice bonus.<br /></p>\n<blockquote>“I have never been very comfortable with database programming, but FaunaDB has been extremely easy to integrate with my app, so much so that I am continually thinking of new uses for it! I absolutely love FaunaDB!”<br />— Chris Klugewicz</blockquote>\n<p>By any measure my previous startup is a huge success, running major components of the digital economy. But it’s different this time. Customers and users are educated about the options, and have the processes in place to evaluate and adopt new database technologies. New solutions are finding a ready market. For Fauna, that means favorable winds and following seas.</p>\n<p>FaunaDB Cloud signups are up and to the right, and even ahead of the FaunaDB On-Premises launch, our enterprise pipeline is overflowing as well, with huge brands you’ve heard of across all major market verticals. To be honest, we’re struggling to keep up.</p>\n<p>With that in mind, we’re hiring across all functions of the company: sales, marketing, and engineering. To prepare for this growth we’ve spent time building a culture to be proud of: innovative, diverse, supportive, and collaborative. Operational databases can be lifetime commitments for customers, and we, and our investors, are in it for the long haul. You can read about our employee friendly policies in <a href=\"https://www.washingtonpost.com/news/business/wp/2017/04/18/the-newest-silicon-valley-perk-paid-time-off-to-protest-trump/\">the Washington Post</a>, the <a href=\"http://www.csmonitor.com/USA/Politics/2017/0427/New-Silicon-Valley-perk-paid-time-off-to-protest\">Christian Science Monitor</a>, and <a href=\"https://fauna.com/blog/supporting-civic-engagement\">right here on our blog</a>.</p>\n<p>If you’re as excited as we are by this enormous opportunity, <a href=\"mailto:priority@fauna.com\">reach out and tell us</a> what your <a href=\"https://fauna.com/jobs\">dream job looks like.</a></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "469",
        "postDate": "2017-04-21T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 502,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a9f8dfb4-94e4-49e2-96c2-b146b3fd3b1a",
        "siteSettingsId": 502,
        "fieldLayoutId": 4,
        "contentId": 335,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Reflecting on Three Months of Civic Engagement",
        "slug": "three-months-of-civic-engagement",
        "uri": "blog/three-months-of-civic-engagement",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-08-21T14:34:26-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/three-months-of-civic-engagement",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/three-months-of-civic-engagement",
        "isCommunityPost": false,
        "blogBodyText": "<p>In February, along with several other startups, we announced our new&nbsp;<a href=\"https://fauna.com/blog/supporting-civic-engagement\">civic engagement policy</a>.</p>\r\n<p>The policy was inspired by the outpouring of political action after the presidential inauguration. But it was a natural extension of our existing family, diversity, and community focus, and reflected staff engagement that was already happening. It’s been about three months now and we are excited to share how the policy has empowered our team.</p>\r\n<p>People at Fauna took time off to:</p>\r\n<ul><li>Vote in local elections</li><li>Write letters or call their state and federal representatives</li><li>Work with their municipal government to&nbsp;<a href=\"https://bikeportland.org/2017/04/21/how-i-worked-with-pbot-to-build-a-play-street-in-my-neighborhood-225869\">convert a street into a children’s play area</a></li><li>Talk to local college students about entrepreneurship</li><li>Attend rallies and protests with their families and friends</li></ul>\r\n<p>Personally, I took time to tutor adult learners and high school students in the East Bay and New York.</p>\r\n<p>Some of our international staff took time to attend events as well; the policy is not limited to the US. Nor is it limited to specific political views. We have never asked our team members to disclose their party affiliation, nor is there any approval required to take advantage of our policy.</p>\r\n<figure><figure><img src=\"{asset:411:url}\" data-image=\"mq6ujf01gktf\"></figure><figcaption><br><br>Building the Portland play street</figcaption></figure>\r\n<p>Although many of us, even at Fauna, disagree on the means and even the goals of civil society, ultimately only good things can come from encouraging everyone to be more informed and active in their communities.<br></p>\r\n<p>Chris Anderson, our Director of Developer Experience, said:</p>\r\n<blockquote>“With the policy, I’m more inclined to do simple things like let the neighbors drop their kids off with me while they run errands, or spend a half day volunteering. The policy changed my behavior to be more engaged.”</blockquote>\r\n<p>The response to our policy has been overwhelmingly positive both internally and externally. Please join us in doing the same thing at your company and help make America and the world a better place!</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>In February, along with several other startups, we announced our new <a href=\"https://fauna.com/blog/supporting-civic-engagement\">civic engagement policy</a>.</p>\n<p>The policy was inspired by the outpouring of political action after the presidential inauguration. But it was a natural extension of our existing family, diversity, and community focus, and reflected staff engagement that was already happening. It’s been about three months now and we are excited to share how the policy has empowered our team.</p>\n<p>People at Fauna took time off to:</p>\n<ul><li>Vote in local elections</li><li>Write letters or call their state and federal representatives</li><li>Work with their municipal government to <a href=\"https://bikeportland.org/2017/04/21/how-i-worked-with-pbot-to-build-a-play-street-in-my-neighborhood-225869\">convert a street into a children’s play area</a></li><li>Talk to local college students about entrepreneurship</li><li>Attend rallies and protests with their families and friends</li></ul><p>Personally, I took time to tutor adult learners and high school students in the East Bay and New York.</p>\n<p>Some of our international staff took time to attend events as well; the policy is not limited to the US. Nor is it limited to specific political views. We have never asked our team members to disclose their party affiliation, nor is there any approval required to take advantage of our policy.</p>\n<figure><figure><img src=\"{asset:411:url||https://fauna.com/assets/site/blog-legacy/civic-engagement-portland.jpg}\" alt=\"\" /></figure><figcaption><br /><br />Building the Portland play street</figcaption></figure><p>Although many of us, even at Fauna, disagree on the means and even the goals of civil society, ultimately only good things can come from encouraging everyone to be more informed and active in their communities.<br /></p>\n<p>Chris Anderson, our Director of Developer Experience, said:</p>\n<blockquote>“With the policy, I’m more inclined to do simple things like let the neighbors drop their kids off with me while they run errands, or spend a half day volunteering. The policy changed my behavior to be more engaged.”</blockquote>\n<p>The response to our policy has been overwhelmingly positive both internally and externally. Please join us in doing the same thing at your company and help make America and the world a better place!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "472",
        "postDate": "2017-04-06T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 501,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "56d24404-5eaf-4294-8548-6a520bb57ab1",
        "siteSettingsId": 501,
        "fieldLayoutId": 4,
        "contentId": 334,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Spanner vs. Calvin: Distributed Consistency at Scale",
        "slug": "distributed-consistency-at-scale-spanner-vs-calvin",
        "uri": "blog/distributed-consistency-at-scale-spanner-vs-calvin",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-08-19T21:01:27-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/distributed-consistency-at-scale-spanner-vs-calvin",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/distributed-consistency-at-scale-spanner-vs-calvin",
        "isCommunityPost": false,
        "blogBodyText": "<figure style=\"float: right; margin: 0px 0px 10px 10px;\"><img src=\"{asset:415:url}\" data-image=\"byl7qft3qr6r\"></figure>\r\n<p><em><a href=\"http://cs-www.cs.yale.edu/homes/dna/\">Daniel J. Abadi</a>&nbsp;is an Associate Professor at Yale University. He does research primarily in database system architecture and implementation. He received a Ph.D. from MIT and a M.Phil from Cambridge.&nbsp;</em></p>\r\n<h2>Introduction</h2>\r\n<p>In 2012, two research papers were published that described the design of geographically replicated, consistent, ACID compliant, transactional database systems. Both papers criticized the proliferation of NoSQL database systems that compromise replication consistency and transactional support, and argue that it is possible to build extremely scalable, geographically-replicated systems without giving up consistency and transactional support.</p>\r\n<blockquote>It is possible to build extremely scalable, geographically-replicated systems without giving up consistency and transactional support.</blockquote>\r\n<p>The first of these papers was the&nbsp;<a href=\"http://cs-www.cs.yale.edu/homes/dna/papers/calvin-sigmod12.pdf\">Calvin paper</a>, published in SIGMOD 2012. A few months later, Google published their&nbsp;<a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">Spanner paper</a>&nbsp;in OSDI 2012. Both of these papers have been cited many hundreds of times and have influenced the design of several modern “NewSQL” systems, including&nbsp;<a href=\"https://fauna.com/\">FaunaDB</a>.</p>\r\n<p>Recently, Google released a beta version of their Spanner implementation, available to customers who use Google Cloud Platform. This development has excited many users seeking to build on Google’s cloud, since they now have a reliably scalable and consistent transactional database system to use as a foundation.</p>\r\n<p>However, the availability of Spanner outside of Google has also brought it more scrutiny: what are its technical advantages, and what are its costs? Even though it has been five years since the Calvin paper was published, it is only now that the database community is asking me to directly compare and contrast the technical designs of Calvin and Spanner.</p>\r\n<blockquote>The availability of Spanner outside of Google has also brought it more scrutiny: what are its technical advantages, and what are its costs?</blockquote>\r\n<p>The goal of this post is to do exactly that—compare the architectural design decisions made in these two systems, and specifically focus on the advantages and disadvantages of these decisions against each other as they relate to performance and scalability.</p>\r\n<p>This post is focused on the protocols described in the original papers from 2012. Although the publicly available versions of these systems likely have deviated from the original papers, the core architectural distinctions remain the same.</p>\r\n<h2>The CAP Theorem in Context</h2>\r\n<p>Before we get started, allow me to suggest the following: Ignore the CAP theorem in the context of this discussion. Just forget about it. It’s not relevant for the type of modern architectural deployments discussed in this post where network partitions are rare.</p>\r\n<p>Both Spanner and Calvin replicate data across independent regions for high availability. And both Spanner and Calvin are technically CP systems from CAP: they guarantee 100% consistency (serializability, linearizability, etc.) across the entire system. Yes, when there is a network partition, both systems make slight compromises on availability, but partitions are rare enough in practice that developers on top of both systems can assume a fully-available system to many 9s of availability.</p>\r\n<blockquote>Both Spanner and Calvin are technically CP systems from CAP, but partitions are rare enough in practice that developers can assume a fully-available system to many 9s of availability.</blockquote>\r\n<p>If you didn’t believe me in 2010 when I explained the&nbsp;<a href=\"http://dbmsmusings.blogspot.co.il/2010/04/problems-with-cap-and-yahoos-little.html\">shortfalls of using CAP</a>&nbsp;to understand the practical consistency and availability properties of modern systems, maybe you will believe the author of the CAP theorem himself, Eric Brewer, who&nbsp;<a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45855.pdf\">recommends against</a>&nbsp;analyzing Spanner through the lens of the CAP theorem.</p>\r\n<h2>Ordering Transactions in Time</h2>\r\n<p>Let us start our comparison of Calvin and Spanner with the most obvious difference between them: Spanner’s use of “TrueTime” vs. Calvin’s use of “preprocessing” (or “sequencing” in the language of the original paper) for transaction ordering. In fact, most of the other differences between Spanner and Calvin stem from this fundamental choice.</p>\r\n<p>A serializable system provides a notion of transactional ordering. Even though many transactions may be executed in parallel across many CPUs and many servers in a large distributed system, the final state (and all observable intermediate states) must be as if each transaction was processed one-by-one. If no transactions touch the same data, it is trivial to process them in parallel and maintain this guarantee.</p>\r\n<blockquote>A serializable system provides a notion of transactional ordering. Even though many transactions may be executed in parallel, the final state must be as if each transaction was processed one-by-one.</blockquote>\r\n<p>However, if the transactions read or write each other’s data, then they must be ordered against each other, with one considered earlier than the other. The one considered “later” must be processed against a version of the database state that includes the writes of the earlier one. In addition, the one considered “earlier” must be processed against a version of the database state that excludes the writes of the later one.</p>\r\n<h2>Locking and Logging</h2>\r\n<p>Spanner uses TrueTime for this transaction ordering. Google famously uses a combination of GPS and atomic clocks in all of their regions to synchronize time to within a known uncertainty bound. If two transactions are processed during time periods that do not have overlapping uncertainty bounds, Spanner can be certain that the later transaction will see all the writes of the earlier transaction.</p>\r\n<blockquote>Google famously uses a combination of GPS and atomic clocks in all of their regions to synchronize time to within a known uncertainty bound.</blockquote>\r\n<p>Spanner obtains write locks within the data replicas on all the data it will write before performing any write. If it obtains all the locks it needs, it proceeds with all of its writes and then assigns the transaction a timestamp at the end of the uncertainty range of the coordinator server for that transaction. It then waits until this later timestamp has definitely passed for all servers in the system (which is the entire length of the uncertainty range) and then releases locks and commits the transaction. Future transactions will get later timestamps and see all the writes of this earlier transaction.</p>\r\n<p>Thus, in Spanner, every transaction receives a timestamp based on the actual time that it committed, and this timestamp is used to order transactions. Transactions with later timestamps see all the writes of transactions with earlier timestamps, with locking used to enforce this guarantee.</p>\r\n<blockquote>In Spanner, every transaction receives a timestamp based on the actual time that it committed, and this timestamp is used to order transactions.</blockquote>\r\n<p>In contrast, Calvin uses preprocessing to order transactions. All transactions are inserted into a distributed, replicated log before being processed. Clients submit transactions to the preprocessing layer of their local region, which then submits these transactions to the global log via a cross-region consensus process like Paxos. This is similar to a write-ahead log in a traditional, non-distributed database. The order that the transactions appear in this log is the official transaction ordering. Every replica reads from their local copy of this replicated log and processes transactions in a way that guarantees that their final state is equivalent to what it would have been had every transaction in the log been executed one-by-one.</p>\r\n<blockquote>Calvin uses preprocessing to order transactions. All transactions are inserted into a distributed, replicated log before being processed.</blockquote>\r\n<h2>Replication Overhead</h2>\r\n<p>The design difference between TrueTime vs. preprocessing directly leads to a difference in how the systems perform replication. In Calvin, the replication of transactional input during preprocessing is the only replication that is needed. Calvin uses a deterministic execution framework to avoid&nbsp;<em>all</em>&nbsp;cross-replication communication during normal (non-recovery mode) execution aside from preprocessing. Every replica sees the same log of transactions and guarantees not only a final state equivalent to executing the transactions in this log one-by-one, but also a final state equivalent to every other replica.</p>\r\n<blockquote>Calvin uses a deterministic execution framework to avoid all cross-replication communication during normal execution aside from preprocessing.</blockquote>\r\n<p>This requires the preprocessor to analyze the transaction code and “pre-execute” any nondeterministic code (e.g. calls to&nbsp;<code>sys.random()</code>&nbsp;or&nbsp;<code>time.now()</code>). Once all code within a transaction is deterministic, a replica can safely focus on just processing the transactions in the log in the correct order without concern for diverging with the other replicas. How this effects what types of transactions Calvin supports is discussed at the end of this post.</p>\r\n<p>In contrast, since Spanner does not do any transaction preprocessing, it can only perform replication after transaction execution. Spanner performs this replication via a cross-region Paxos process.</p>\r\n<blockquote>Since Spanner does not do any transaction preprocessing, it can only perform replication after transaction execution. Spanner performs this replication via a cross-region Paxos process.</blockquote>\r\n<h2>The Cost of Two-Phase Commit</h2>\r\n<p>Another key difference between Spanner and Calvin is how they commit multi-partitioned transactions. Both Calvin and Spanner partition data into separate shards that may be stored on separate machines that fail independently from each other. In order to guarantee transaction atomicity and durability, any transaction that accesses data in multiple partitions must go through a commit procedure that ensures that every partition successfully processed the part of the transaction that accessed data in that partition.</p>\r\n<p>Since machines may fail at any time, including during the commit procedure, this process generally takes two rounds of communication between the partitions involved in the transaction. This two-round commit protocol is called “two-phase commit” and is used in almost every ACID-compliant distributed database system, including Spanner. This two-phase commit protocol can often consume the majority of latency for short, simple transactions since the actual processing time of the transaction is much less than the delays involved in sending and receiving two rounds of messages over the network.</p>\r\n<p>The cost of two-phase commit is particularly high in Spanner because the protocol involves three forced writes to a log that cannot be overlapped with each other. In Spanner, every write to a log involves a cross-region Paxos agreement, so the latency of two-phase commit in Spanner is at least equal to three times the latency of cross-region Paxos.</p>\r\n<blockquote>In Spanner, every write to a log involves a cross-region Paxos agreement, so the latency of two-phase commit in Spanner is at least equal to three times the latency of cross-region Paxos.</blockquote>\r\n<h2>Determinism is Durability</h2>\r\n<p>In contrast to Spanner, Calvin leverages deterministic execution to avoid two-phase commit. Machine failures do not cause transactional aborts in Calvin.</p>\r\n<blockquote>Calvin leverages deterministic execution to avoid two-phase commit.</blockquote>\r\n<p>Instead, after a failure, the machine that failed in Calvin re-reads the input transaction log from a checkpoint, and deterministically replays it to recover its state at the time of the failure. It can then continue on from there as if nothing happened. As a result, the commit protocol does not need to worry about machine failures during the protocol, and can be performed in a single round of communication (and in some cases, zero rounds of communication—see the original paper for more details).</p>\r\n<h2>Performance Implications</h2>\r\n<p>At this point, I think I have provided enough details to make it possible to present a theoretical comparison of the bottom line performance of Calvin vs. Spanner for a variety of different types of requests. This comparison assumes a perfectly optimized and implemented version of each system.</p>\r\n<h3><em>Transactional write latency</em></h3>\r\n<p>A transaction that is “non-read-only” writes at least one value to the database state. In Calvin, such a transaction must pay the latency cost of preprocessing, which is roughly the cost of running cross-region Paxos to agree to append the transaction to the log. After this is complete, the remaining latency is the cost of processing the transaction itself, which includes the zero or one-phase commit protocol for distributed transactions.</p>\r\n<p>In Spanner, there is no preprocessing latency, but it still has to pay the cost of cross-region Paxos replication at commit time, which is roughly equivalent to the Calvin preprocessing latency. Spanner also has to pay the commit wait latency discussed above (which is the size of the time uncertainty window), but this can be overlapped with replication. It also pays the latency of two-phase commit for multi-partition transactions.</p>\r\n<p>Thus, Spanner and Calvin have roughly equivalent latency for single-partition transactions, but Spanner has worse latency than Calvin for multi-partition transactions due to the extra phases in the transaction commit protocol.</p>\r\n<blockquote>Spanner and Calvin have roughly equivalent latency for single-partition transactions, but Spanner has worse latency than Calvin for multi-partition transactions due to the extra phases in the transaction commit protocol.</blockquote>\r\n<h3><em>Snapshot read latency</em></h3>\r\n<p>Both Calvin and Spanner keep around older versions of data and read data at a requested earlier timestamp from a local replica without any Paxos-communication with the other replicas.</p>\r\n<p>Thus, both Calvin and Spanner can achieve very low snapshot-read latency.</p>\r\n<blockquote>Both Calvin and Spanner can achieve very low snapshot-read latency.</blockquote>\r\n<h3><em>Transactional read latency</em></h3>\r\n<p>Read-only transactions do not write any data, but they must be linearizable with respect to other transactions that write data. In practice, Calvin accomplishes this via placing the read-only transaction in the preprocessor log. This means that a read-only transaction in Calvin must pay the cross-region replication latency. In contrast, Spanner only needs to submit the read-only transaction to the leader replica(s) for the partition(s) that are accessed in order to get a global timestamp (and therefore be ordered relative to concurrent transactions). Therefore, there is no cross-region Paxos latency—only the commit time (uncertainty window) latency.</p>\r\n<p>Thus, Spanner has better latency than Calvin for read-only transactions submitted by clients that are physically close to the location of the leader servers for all partitions accessed by that transaction.</p>\r\n<blockquote>Spanner has better latency than Calvin for read-only transactions submitted by clients that are physically close to the location of the leader servers.</blockquote>\r\n<h2>Scalability</h2>\r\n<p>Both Spanner and Calvin are both (theoretically) roughly-linearly scalable for transactional workloads for which it is rare for concurrent transactions to be accessing the same data. However, major differences begin to present themselves as the conflict rate between concurrent transactions starts to increase.</p>\r\n<p>Both Spanner and Calvin, as presented in the paper, use locks to prevent concurrent transactions from interfering with each other in impermissible ways. However, the amount of time they hold locks for an identical transaction is substantially different. Both systems need to hold locks during the commit protocol. However, since Calvin’s commit protocol is shorter than Spanner’s, Calvin reduces the lock hold time at the end of the transaction. On the flip side, Calvin acquires all locks that it will need at the beginning of the transaction, whereas Spanner performs all reads for a transaction before acquiring write locks. Therefore, Spanner reduces lock time at the beginning of the transaction.</p>\r\n<p>However, this latter advantage for Spanner is generally outweighed by the former disadvantage, since, as discussed above, the latency of two-phase commit in Spanner involves at least three iterations of cross-region Paxos. Furthermore, Spanner has an additional major disadvantage relative to Calvin in lock-hold time: Spanner must also hold locks during replication (which, as mentioned above, is also a cross-region Paxos process). The farther apart the regions, the larger the latency of this replication, and therefore, the longer Spanner must hold locks.</p>\r\n<blockquote>Spanner must hold locks during replication. The farther apart the regions, the larger the latency of replication, and therefore, the longer Spanner must hold the locks.</blockquote>\r\n<p>In contrast, Calvin does its replication during preprocessing, and therefore does not need to hold locks during replication. This leads to Calvin holding locks for much shorter periods of time than Spanner, allowing it to process more conflicting concurrent transactions in parallel</p>\r\n<blockquote>Calvin can process more conflicting concurrent transactions in parallel.</blockquote>\r\n<p>A second difference that can affect scalability is the following: Calvin requires only a single Paxos group for replicating the input log. In contrast, Spanner requires one independent Paxos group per shard, with proportionally higher overhead.</p>\r\n<p>Calvin has higher throughput scalability than Spanner for transactional workloads where concurrent transactions access the same data. This advantage increases with the distance between datacenters.</p>\r\n<blockquote>Calvin has higher throughput scalability than Spanner for transactional workloads where concurrent transactions access the same data.</blockquote>\r\n<h2>Limitations on Transaction Types</h2>\r\n<p>In order to implement deterministic transaction processing, Calvin requires the preprocessor to analyze transactions and potentially “pre-execute” any non-deterministic code to ensure that replicas do not diverge. This implies that the preprocessor requires the entire transaction to be submitted at once. This highlights another difference between Calvin and Spanner—while Spanner theoretically allows arbitrary client-side interactive transactions (that may include external communication), Calvin supports a more limited transaction model.</p>\r\n<p>In particular, the client-side interactive transaction model of SQL, also known as session transactions, is a poor fit for Calvin. The public version of Spanner supports client-side interactive transactions, although mutations must explicitly reference the primary key of each row.</p>\r\n<blockquote>The client-side interactive transaction model of SQL is a poor fit for Calvin.</blockquote>\r\n<p>There are some subtle but interesting differences between Calvin and Spanner in rare situations where every single replica for a shard is unavailable, or if all but one are unavailable, but these differences are out of scope for this post.</p>\r\n<h2>Conclusion</h2>\r\n<p>I’m biased in favor of Calvin, but in going through this exercise, I found it very difficult to find cases where an ideal implementation of Spanner theoretically outperforms an ideal implementation of Calvin. The only place where I could find that Spanner has a clear performance advantage over Calvin is for latency of read-only transactions submitted by clients that are physically close to the location of the leader servers for the partitions accessed by that transaction. Since any complex transaction is likely to touch multiple partitions, this is almost impossible to guarantee in a real-world setting.</p>\r\n<blockquote>It is very difficult to find cases where an ideal implementation of Spanner would outperform an ideal implementation of Calvin.</blockquote>\r\n<p>Many real-world workloads do not require client-side interactive transactions, only need transactional support for writes, and are satisfied performing reads against snapshots (after all, this is the default isolation model of many SQL systems). Therefore, It seems to me that Calvin is the better fit for modern applications.</p>\r\n<p><em>FaunaDB is a globally replicated, strongly consistent operational database, inspired by Calvin. To learn more about how FaunaDB is built, request our&nbsp;<a href=\"https://fauna.com/whitepaper\">technical white paper</a>.</em></p>",
        "blogCategory": [
            "1462",
            "1465"
        ],
        "mainBlogImage": [],
        "bodyText": "<figure style=\"float:right;margin:0px 0px 10px 10px;\"><img src=\"{asset:415:url||https://fauna.com/assets/site/blog-legacy/daniel-abadi.png}\" alt=\"\" /></figure><p><em><a href=\"http://cs-www.cs.yale.edu/homes/dna/\">Daniel J. Abadi</a> is an Associate Professor at Yale University. He does research primarily in database system architecture and implementation. He received a Ph.D. from MIT and a M.Phil from Cambridge. </em></p>\n<h2>Introduction</h2>\n<p>In 2012, two research papers were published that described the design of geographically replicated, consistent, ACID compliant, transactional database systems. Both papers criticized the proliferation of NoSQL database systems that compromise replication consistency and transactional support, and argue that it is possible to build extremely scalable, geographically-replicated systems without giving up consistency and transactional support.</p>\n<blockquote>It is possible to build extremely scalable, geographically-replicated systems without giving up consistency and transactional support.</blockquote>\n<p>The first of these papers was the <a href=\"http://cs-www.cs.yale.edu/homes/dna/papers/calvin-sigmod12.pdf\">Calvin paper</a>, published in SIGMOD 2012. A few months later, Google published their <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">Spanner paper</a> in OSDI 2012. Both of these papers have been cited many hundreds of times and have influenced the design of several modern “NewSQL” systems, including <a href=\"https://fauna.com/\">FaunaDB</a>.</p>\n<p>Recently, Google released a beta version of their Spanner implementation, available to customers who use Google Cloud Platform. This development has excited many users seeking to build on Google’s cloud, since they now have a reliably scalable and consistent transactional database system to use as a foundation.</p>\n<p>However, the availability of Spanner outside of Google has also brought it more scrutiny: what are its technical advantages, and what are its costs? Even though it has been five years since the Calvin paper was published, it is only now that the database community is asking me to directly compare and contrast the technical designs of Calvin and Spanner.</p>\n<blockquote>The availability of Spanner outside of Google has also brought it more scrutiny: what are its technical advantages, and what are its costs?</blockquote>\n<p>The goal of this post is to do exactly that—compare the architectural design decisions made in these two systems, and specifically focus on the advantages and disadvantages of these decisions against each other as they relate to performance and scalability.</p>\n<p>This post is focused on the protocols described in the original papers from 2012. Although the publicly available versions of these systems likely have deviated from the original papers, the core architectural distinctions remain the same.</p>\n<h2>The CAP Theorem in Context</h2>\n<p>Before we get started, allow me to suggest the following: Ignore the CAP theorem in the context of this discussion. Just forget about it. It’s not relevant for the type of modern architectural deployments discussed in this post where network partitions are rare.</p>\n<p>Both Spanner and Calvin replicate data across independent regions for high availability. And both Spanner and Calvin are technically CP systems from CAP: they guarantee 100% consistency (serializability, linearizability, etc.) across the entire system. Yes, when there is a network partition, both systems make slight compromises on availability, but partitions are rare enough in practice that developers on top of both systems can assume a fully-available system to many 9s of availability.</p>\n<blockquote>Both Spanner and Calvin are technically CP systems from CAP, but partitions are rare enough in practice that developers can assume a fully-available system to many 9s of availability.</blockquote>\n<p>If you didn’t believe me in 2010 when I explained the <a href=\"http://dbmsmusings.blogspot.co.il/2010/04/problems-with-cap-and-yahoos-little.html\">shortfalls of using CAP</a> to understand the practical consistency and availability properties of modern systems, maybe you will believe the author of the CAP theorem himself, Eric Brewer, who <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45855.pdf\">recommends against</a> analyzing Spanner through the lens of the CAP theorem.</p>\n<h2>Ordering Transactions in Time</h2>\n<p>Let us start our comparison of Calvin and Spanner with the most obvious difference between them: Spanner’s use of “TrueTime” vs. Calvin’s use of “preprocessing” (or “sequencing” in the language of the original paper) for transaction ordering. In fact, most of the other differences between Spanner and Calvin stem from this fundamental choice.</p>\n<p>A serializable system provides a notion of transactional ordering. Even though many transactions may be executed in parallel across many CPUs and many servers in a large distributed system, the final state (and all observable intermediate states) must be as if each transaction was processed one-by-one. If no transactions touch the same data, it is trivial to process them in parallel and maintain this guarantee.</p>\n<blockquote>A serializable system provides a notion of transactional ordering. Even though many transactions may be executed in parallel, the final state must be as if each transaction was processed one-by-one.</blockquote>\n<p>However, if the transactions read or write each other’s data, then they must be ordered against each other, with one considered earlier than the other. The one considered “later” must be processed against a version of the database state that includes the writes of the earlier one. In addition, the one considered “earlier” must be processed against a version of the database state that excludes the writes of the later one.</p>\n<h2>Locking and Logging</h2>\n<p>Spanner uses TrueTime for this transaction ordering. Google famously uses a combination of GPS and atomic clocks in all of their regions to synchronize time to within a known uncertainty bound. If two transactions are processed during time periods that do not have overlapping uncertainty bounds, Spanner can be certain that the later transaction will see all the writes of the earlier transaction.</p>\n<blockquote>Google famously uses a combination of GPS and atomic clocks in all of their regions to synchronize time to within a known uncertainty bound.</blockquote>\n<p>Spanner obtains write locks within the data replicas on all the data it will write before performing any write. If it obtains all the locks it needs, it proceeds with all of its writes and then assigns the transaction a timestamp at the end of the uncertainty range of the coordinator server for that transaction. It then waits until this later timestamp has definitely passed for all servers in the system (which is the entire length of the uncertainty range) and then releases locks and commits the transaction. Future transactions will get later timestamps and see all the writes of this earlier transaction.</p>\n<p>Thus, in Spanner, every transaction receives a timestamp based on the actual time that it committed, and this timestamp is used to order transactions. Transactions with later timestamps see all the writes of transactions with earlier timestamps, with locking used to enforce this guarantee.</p>\n<blockquote>In Spanner, every transaction receives a timestamp based on the actual time that it committed, and this timestamp is used to order transactions.</blockquote>\n<p>In contrast, Calvin uses preprocessing to order transactions. All transactions are inserted into a distributed, replicated log before being processed. Clients submit transactions to the preprocessing layer of their local region, which then submits these transactions to the global log via a cross-region consensus process like Paxos. This is similar to a write-ahead log in a traditional, non-distributed database. The order that the transactions appear in this log is the official transaction ordering. Every replica reads from their local copy of this replicated log and processes transactions in a way that guarantees that their final state is equivalent to what it would have been had every transaction in the log been executed one-by-one.</p>\n<blockquote>Calvin uses preprocessing to order transactions. All transactions are inserted into a distributed, replicated log before being processed.</blockquote>\n<h2>Replication Overhead</h2>\n<p>The design difference between TrueTime vs. preprocessing directly leads to a difference in how the systems perform replication. In Calvin, the replication of transactional input during preprocessing is the only replication that is needed. Calvin uses a deterministic execution framework to avoid <em>all</em> cross-replication communication during normal (non-recovery mode) execution aside from preprocessing. Every replica sees the same log of transactions and guarantees not only a final state equivalent to executing the transactions in this log one-by-one, but also a final state equivalent to every other replica.</p>\n<blockquote>Calvin uses a deterministic execution framework to avoid all cross-replication communication during normal execution aside from preprocessing.</blockquote>\n<p>This requires the preprocessor to analyze the transaction code and “pre-execute” any nondeterministic code (e.g. calls to <code>sys.random()</code> or <code>time.now()</code>). Once all code within a transaction is deterministic, a replica can safely focus on just processing the transactions in the log in the correct order without concern for diverging with the other replicas. How this effects what types of transactions Calvin supports is discussed at the end of this post.</p>\n<p>In contrast, since Spanner does not do any transaction preprocessing, it can only perform replication after transaction execution. Spanner performs this replication via a cross-region Paxos process.</p>\n<blockquote>Since Spanner does not do any transaction preprocessing, it can only perform replication after transaction execution. Spanner performs this replication via a cross-region Paxos process.</blockquote>\n<h2>The Cost of Two-Phase Commit</h2>\n<p>Another key difference between Spanner and Calvin is how they commit multi-partitioned transactions. Both Calvin and Spanner partition data into separate shards that may be stored on separate machines that fail independently from each other. In order to guarantee transaction atomicity and durability, any transaction that accesses data in multiple partitions must go through a commit procedure that ensures that every partition successfully processed the part of the transaction that accessed data in that partition.</p>\n<p>Since machines may fail at any time, including during the commit procedure, this process generally takes two rounds of communication between the partitions involved in the transaction. This two-round commit protocol is called “two-phase commit” and is used in almost every ACID-compliant distributed database system, including Spanner. This two-phase commit protocol can often consume the majority of latency for short, simple transactions since the actual processing time of the transaction is much less than the delays involved in sending and receiving two rounds of messages over the network.</p>\n<p>The cost of two-phase commit is particularly high in Spanner because the protocol involves three forced writes to a log that cannot be overlapped with each other. In Spanner, every write to a log involves a cross-region Paxos agreement, so the latency of two-phase commit in Spanner is at least equal to three times the latency of cross-region Paxos.</p>\n<blockquote>In Spanner, every write to a log involves a cross-region Paxos agreement, so the latency of two-phase commit in Spanner is at least equal to three times the latency of cross-region Paxos.</blockquote>\n<h2>Determinism is Durability</h2>\n<p>In contrast to Spanner, Calvin leverages deterministic execution to avoid two-phase commit. Machine failures do not cause transactional aborts in Calvin.</p>\n<blockquote>Calvin leverages deterministic execution to avoid two-phase commit.</blockquote>\n<p>Instead, after a failure, the machine that failed in Calvin re-reads the input transaction log from a checkpoint, and deterministically replays it to recover its state at the time of the failure. It can then continue on from there as if nothing happened. As a result, the commit protocol does not need to worry about machine failures during the protocol, and can be performed in a single round of communication (and in some cases, zero rounds of communication—see the original paper for more details).</p>\n<h2>Performance Implications</h2>\n<p>At this point, I think I have provided enough details to make it possible to present a theoretical comparison of the bottom line performance of Calvin vs. Spanner for a variety of different types of requests. This comparison assumes a perfectly optimized and implemented version of each system.</p>\n<h3><em>Transactional write latency</em></h3>\n<p>A transaction that is “non-read-only” writes at least one value to the database state. In Calvin, such a transaction must pay the latency cost of preprocessing, which is roughly the cost of running cross-region Paxos to agree to append the transaction to the log. After this is complete, the remaining latency is the cost of processing the transaction itself, which includes the zero or one-phase commit protocol for distributed transactions.</p>\n<p>In Spanner, there is no preprocessing latency, but it still has to pay the cost of cross-region Paxos replication at commit time, which is roughly equivalent to the Calvin preprocessing latency. Spanner also has to pay the commit wait latency discussed above (which is the size of the time uncertainty window), but this can be overlapped with replication. It also pays the latency of two-phase commit for multi-partition transactions.</p>\n<p>Thus, Spanner and Calvin have roughly equivalent latency for single-partition transactions, but Spanner has worse latency than Calvin for multi-partition transactions due to the extra phases in the transaction commit protocol.</p>\n<blockquote>Spanner and Calvin have roughly equivalent latency for single-partition transactions, but Spanner has worse latency than Calvin for multi-partition transactions due to the extra phases in the transaction commit protocol.</blockquote>\n<h3><em>Snapshot read latency</em></h3>\n<p>Both Calvin and Spanner keep around older versions of data and read data at a requested earlier timestamp from a local replica without any Paxos-communication with the other replicas.</p>\n<p>Thus, both Calvin and Spanner can achieve very low snapshot-read latency.</p>\n<blockquote>Both Calvin and Spanner can achieve very low snapshot-read latency.</blockquote>\n<h3><em>Transactional read latency</em></h3>\n<p>Read-only transactions do not write any data, but they must be linearizable with respect to other transactions that write data. In practice, Calvin accomplishes this via placing the read-only transaction in the preprocessor log. This means that a read-only transaction in Calvin must pay the cross-region replication latency. In contrast, Spanner only needs to submit the read-only transaction to the leader replica(s) for the partition(s) that are accessed in order to get a global timestamp (and therefore be ordered relative to concurrent transactions). Therefore, there is no cross-region Paxos latency—only the commit time (uncertainty window) latency.</p>\n<p>Thus, Spanner has better latency than Calvin for read-only transactions submitted by clients that are physically close to the location of the leader servers for all partitions accessed by that transaction.</p>\n<blockquote>Spanner has better latency than Calvin for read-only transactions submitted by clients that are physically close to the location of the leader servers.</blockquote>\n<h2>Scalability</h2>\n<p>Both Spanner and Calvin are both (theoretically) roughly-linearly scalable for transactional workloads for which it is rare for concurrent transactions to be accessing the same data. However, major differences begin to present themselves as the conflict rate between concurrent transactions starts to increase.</p>\n<p>Both Spanner and Calvin, as presented in the paper, use locks to prevent concurrent transactions from interfering with each other in impermissible ways. However, the amount of time they hold locks for an identical transaction is substantially different. Both systems need to hold locks during the commit protocol. However, since Calvin’s commit protocol is shorter than Spanner’s, Calvin reduces the lock hold time at the end of the transaction. On the flip side, Calvin acquires all locks that it will need at the beginning of the transaction, whereas Spanner performs all reads for a transaction before acquiring write locks. Therefore, Spanner reduces lock time at the beginning of the transaction.</p>\n<p>However, this latter advantage for Spanner is generally outweighed by the former disadvantage, since, as discussed above, the latency of two-phase commit in Spanner involves at least three iterations of cross-region Paxos. Furthermore, Spanner has an additional major disadvantage relative to Calvin in lock-hold time: Spanner must also hold locks during replication (which, as mentioned above, is also a cross-region Paxos process). The farther apart the regions, the larger the latency of this replication, and therefore, the longer Spanner must hold locks.</p>\n<blockquote>Spanner must hold locks during replication. The farther apart the regions, the larger the latency of replication, and therefore, the longer Spanner must hold the locks.</blockquote>\n<p>In contrast, Calvin does its replication during preprocessing, and therefore does not need to hold locks during replication. This leads to Calvin holding locks for much shorter periods of time than Spanner, allowing it to process more conflicting concurrent transactions in parallel</p>\n<blockquote>Calvin can process more conflicting concurrent transactions in parallel.</blockquote>\n<p>A second difference that can affect scalability is the following: Calvin requires only a single Paxos group for replicating the input log. In contrast, Spanner requires one independent Paxos group per shard, with proportionally higher overhead.</p>\n<p>Calvin has higher throughput scalability than Spanner for transactional workloads where concurrent transactions access the same data. This advantage increases with the distance between datacenters.</p>\n<blockquote>Calvin has higher throughput scalability than Spanner for transactional workloads where concurrent transactions access the same data.</blockquote>\n<h2>Limitations on Transaction Types</h2>\n<p>In order to implement deterministic transaction processing, Calvin requires the preprocessor to analyze transactions and potentially “pre-execute” any non-deterministic code to ensure that replicas do not diverge. This implies that the preprocessor requires the entire transaction to be submitted at once. This highlights another difference between Calvin and Spanner—while Spanner theoretically allows arbitrary client-side interactive transactions (that may include external communication), Calvin supports a more limited transaction model.</p>\n<p>In particular, the client-side interactive transaction model of SQL, also known as session transactions, is a poor fit for Calvin. The public version of Spanner supports client-side interactive transactions, although mutations must explicitly reference the primary key of each row.</p>\n<blockquote>The client-side interactive transaction model of SQL is a poor fit for Calvin.</blockquote>\n<p>There are some subtle but interesting differences between Calvin and Spanner in rare situations where every single replica for a shard is unavailable, or if all but one are unavailable, but these differences are out of scope for this post.</p>\n<h2>Conclusion</h2>\n<p>I’m biased in favor of Calvin, but in going through this exercise, I found it very difficult to find cases where an ideal implementation of Spanner theoretically outperforms an ideal implementation of Calvin. The only place where I could find that Spanner has a clear performance advantage over Calvin is for latency of read-only transactions submitted by clients that are physically close to the location of the leader servers for the partitions accessed by that transaction. Since any complex transaction is likely to touch multiple partitions, this is almost impossible to guarantee in a real-world setting.</p>\n<blockquote>It is very difficult to find cases where an ideal implementation of Spanner would outperform an ideal implementation of Calvin.</blockquote>\n<p>Many real-world workloads do not require client-side interactive transactions, only need transactional support for writes, and are satisfied performing reads against snapshots (after all, this is the default isolation model of many SQL systems). Therefore, It seems to me that Calvin is the better fit for modern applications.</p>\n<p><em>FaunaDB is a globally replicated, strongly consistent operational database, inspired by Calvin. To learn more about how FaunaDB is built, request our <a href=\"https://fauna.com/whitepaper\">technical white paper</a>.</em></p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-03-31T14:58:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 500,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "43185e04-600c-4873-a245-9d019b36f1ae",
        "siteSettingsId": 500,
        "fieldLayoutId": 4,
        "contentId": 333,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Dive Into FaunaDB With Our Technical Whitepaper",
        "slug": "dive-into-faunadb-with-our-technical-white-paper",
        "uri": "blog/dive-into-faunadb-with-our-technical-white-paper",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/dive-into-faunadb-with-our-technical-white-paper",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/dive-into-faunadb-with-our-technical-white-paper",
        "isCommunityPost": false,
        "blogBodyText": "<p>How do you build a database like FaunaDB?</p>\r\n<p>Since our FaunaDB&nbsp;<a href=\"https://fauna.com/blog/faunadb-serverless-cloud-launch-day\">cloud launch</a>&nbsp;a few weeks ago, the community has been asking for deep technical insight into how FaunaDB is designed and implemented. A comment by mdasen on Hacker News summed it up:</p>\r\n<blockquote>“They seem to understand more than their marketing lets on.”</blockquote>\r\n<p>We are happy to release the first draft of our technical white paper in response. Download it&nbsp;<a href=\"http://www2.fauna.com/techwhitepaper\">here</a>.</p>\r\n<figure><img src=\"{asset:466:url}\" data-image=\"je2zjquvx7ic\"></figure>\r\n<p></p>\r\n<p>The paper explains the foundations of FaunaDB and the motivations for creating an adaptive operational database.</p>\r\n<h2>It’s an operating system, Jim, but not as we know it</h2>\r\n<p>Databases are some of the most complex systems ever created. A complete database, distributed or not, looks just like an operating system. This is not a new observation, and FaunaDB is no exception. It has:</p>\r\n<ol><li>A recursive, replicated, journaled, and rewindable storage engine</li><li>A priority-aware process scheduler, which effectively implements cooperative multi-threading for all machine resources</li><li>A query language, including saved queries—the equivalent of an executable format</li><li>Authentication, identity management, and data access control</li><li>Telemetry, logging, and tools for backups and integrity checks</li></ol>\r\n<p>Because it’s distributed, FaunaDB also implements:</p>\r\n<ol><li>A global request router</li><li>A consistent cluster management engine</li><li>An ACID-compliant transaction resolution engine</li></ol>\r\n<p>FaunaDB does not have a windowing system, but we are working on a query explorer and operational dashboard.</p>\r\n<h2>To boldly transact where no transaction has gone before</h2>\r\n<p>We’ve all suffered so long with existing database technology that anything new seems too good to be true. There has been healthy skepticism (as well as excitement) about FaunaDB so far. We want to begin putting those doubts to rest. We know we have more work ahead of us and are excited for your feedback.</p>\r\n<p>Grab the paper&nbsp;<a href=\"http://www2.fauna.com/techwhitepaper\">here</a>.</p>",
        "blogCategory": [
            "8",
            "1462"
        ],
        "mainBlogImage": [
            "1164"
        ],
        "bodyText": "<p>How do you build a database like FaunaDB?</p>\n<p>Since our FaunaDB <a href=\"https://fauna.com/blog/faunadb-serverless-cloud-launch-day\">cloud launch</a> a few weeks ago, the community has been asking for deep technical insight into how FaunaDB is designed and implemented. A comment by mdasen on Hacker News summed it up:</p>\n<blockquote>“They seem to understand more than their marketing lets on.”</blockquote>\n<p>We are happy to release the first draft of our technical white paper in response. Download it <a href=\"https://fauna.com/whitepaper\">here</a>.</p>\n<figure><img src=\"{asset:466:url||https://fauna.com/assets/site/blog-legacy/technical-white-paper.png}\" alt=\"\" /></figure><p>The paper explains the foundations of FaunaDB and the motivations for creating an adaptive operational database.</p>\n<h2>It’s an operating system, Jim, but not as we know it</h2>\n<p>Databases are some of the most complex systems ever created. A complete database, distributed or not, looks just like an operating system. This is not a new observation, and FaunaDB is no exception. It has:</p>\n<ol><li>A recursive, replicated, journaled, and rewindable storage engine</li><li>A priority-aware process scheduler, which effectively implements cooperative multi-threading for all machine resources</li><li>A query language, including saved queries—the equivalent of an executable format</li><li>Authentication, identity management, and data access control</li><li>Telemetry, logging, and tools for backups and integrity checks</li></ol><p>Because it’s distributed, FaunaDB also implements:</p>\n<ol><li>A global request router</li><li>A consistent cluster management engine</li><li>An ACID-compliant transaction resolution engine</li></ol><p>FaunaDB does not have a windowing system, but we are working on a query explorer and operational dashboard.</p>\n<h2>To boldly transact where no transaction has gone before</h2>\n<p>We’ve all suffered so long with existing database technology that anything new seems too good to be true. There has been healthy skepticism (as well as excitement) about FaunaDB so far. We want to begin putting those doubts to rest. We know we have more work ahead of us and are excited for your feedback.</p>\n<p>Grab the paper <a href=\"https://fauna.com/whitepaper\">here</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-03-28T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 491,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "0258438c-6cf7-4f95-95fb-70f66a04f21b",
        "siteSettingsId": 491,
        "fieldLayoutId": 4,
        "contentId": 324,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Video: Developing Provider-Independent Functions in the Serverless World",
        "slug": "developing-serverless-authentication-and-persistence",
        "uri": "blog/developing-serverless-authentication-and-persistence",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/developing-serverless-authentication-and-persistence",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/developing-serverless-authentication-and-persistence",
        "isCommunityPost": false,
        "blogBodyText": "<p>In March 2017, Fauna teamed up with&nbsp;<a href=\"https://serverless.com/\">Serverless</a>&nbsp;to host the&nbsp;<a href=\"https://www.meetup.com/Serverless/\">SF Serverless Meetup</a>.</p>\r<div class=\"video-container\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HpGt-6MboMw\" frameborder=\"0\" allowfullscreen=\"\"></iframe></div>\r<p><br>In this video from the Meetup&nbsp;we demonstrate a real world serverless application that incorporates both 3rd-party authentication and&nbsp;<a href=\"https://fauna.com/product\">FaunaDB Cloud</a>.<br></p>\r<p>This session builds on the app in my guest post over on the Serverless blog:&nbsp;<a href=\"https://serverless.com/blog/faunadb-serverless-authentication/\">Using Serverless Authentication Boilerplate with FaunaDB</a>. You can grab the code there to do it yourself.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>In March 2017, Fauna teamed up with <a href=\"https://serverless.com/\">Serverless</a> to host the <a href=\"https://www.meetup.com/Serverless/\">SF Serverless Meetup</a>.</p>\n<div class=\"video-container\"></div>\n<p><br />In this video from the Meetup we demonstrate a real world serverless application that incorporates both 3rd-party authentication and <a href=\"https://fauna.com/product\">FaunaDB Cloud</a>.<br /></p>\n<p>This session builds on the app in my guest post over on the Serverless blog: <a href=\"https://serverless.com/blog/faunadb-serverless-authentication/\">Using Serverless Authentication Boilerplate with FaunaDB</a>. You can grab the code there to do it yourself.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-03-15T14:36:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 494,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "14dd64a7-7a68-453a-8c80-d712fb28c17e",
        "siteSettingsId": 494,
        "fieldLayoutId": 4,
        "contentId": 327,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Launch Day",
        "slug": "faunadb-serverless-cloud-launch-day",
        "uri": "blog/faunadb-serverless-cloud-launch-day",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-09-23T16:50:06-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/faunadb-serverless-cloud-launch-day",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/faunadb-serverless-cloud-launch-day",
        "isCommunityPost": false,
        "blogBodyText": "<p>I’m excited to announce that today we are opening FaunaDB Cloud to the public.</p>\n<p>Even though we only&nbsp;<a href=\"https://fauna.com/blog/welcome-to-the-jungle\">started talking about ourselves</a>&nbsp;a few months ago, we have been blown away by the response, including a constant stream of inquiries from both the developer community and enterprises looking to escape legacy systems and defeat cloud lock-in. Early customers have launched their projects and now FaunaDB serves millions of end users every day.</p>\n<p>Our team has grown significantly in the last year, with key hires in engineering and marketing roles. Most recently we were thrilled to bring Chris Anderson onto the team. Chris is the former cofounder of Couchbase, and his deep industry experience has both validated our vision and accelerated our progress.</p>\n<figure data-gramm_id=\"da8657a4-4afd-d853-b708-a28fe3e88a62\" data-gramm=\"true\" spellcheck=\"false\" data-gramm_editor=\"true\"><img src=\"{asset:406:url}\" data-image=\"02qpmntf9oxo\"></figure>\n<p></p>\n<p>To understand why a serverless database is so important, please read Chris’s&nbsp;<a href=\"https://fauna.com/blog/escape-the-cloud-database-trap-with-serverless\">post</a>, and to hear from our early customers, please read our&nbsp;<a href=\"http://www.prnewswire.com/news-releases/fauna-launches-faunadb-serverless-cloud-the-first-serverless-database-300423874.html\">press release</a>.<br></p>\n<h2>Firsts</h2>\n<p>This launch is an industry milestone:</p>\n<ul><li>The first database built for serverless applications</li><li>The first globally replicated database as a service</li><li>The first transactional, strongly consistent, multi-region database available to the public</li></ul>\n<p>We’re also looking forward to the coming year, adding capabilities, improving performance and lowering customer costs.&nbsp;<br><br></p>\n<ul></ul>\n<h2>Thank you</h2>\n<p>I would like to thank everyone on our team, including my co-founder, Matt Freels, our staff, our investors, our advisors, and our family and friends for the dedication to get us to this point. And I would like to thank all our early customers for sharing our vision for modern data infrastructure.</p>\n<p>We have never done more challenging or more important work. I’m incredibly humbled and proud to be part of it.</p>",
        "blogCategory": [
            "1461",
            "1531"
        ],
        "mainBlogImage": [
            "1209"
        ],
        "bodyText": "<p>I’m excited to announce that today we are opening FaunaDB Cloud to the public.</p>\n<p>Even though we only <a href=\"https://fauna.com/blog/welcome-to-the-jungle\">started talking about ourselves</a> a few months ago, we have been blown away by the response, including a constant stream of inquiries from both the developer community and enterprises looking to escape legacy systems and defeat cloud lock-in. Early customers have launched their projects and now FaunaDB serves millions of end users every day.</p>\n<p>Our team has grown significantly in the last year, with key hires in engineering and marketing roles. Most recently we were thrilled to bring Chris Anderson onto the team. Chris is the former cofounder of Couchbase, and his deep industry experience has both validated our vision and accelerated our progress.</p>\n<figure><img src=\"{asset:406:url||https://fauna.com/assets/site/blog-legacy/bird.jpg}\" alt=\"\" /></figure><p>To understand why a serverless database is so important, please read Chris’s <a href=\"https://fauna.com/blog/escape-the-cloud-database-trap-with-serverless\">post</a>, and to hear from our early customers, please read our <a href=\"http://www.prnewswire.com/news-releases/fauna-launches-faunadb-serverless-cloud-the-first-serverless-database-300423874.html\">press release</a>.<br /></p>\n<h2>Firsts</h2>\n<p>This launch is an industry milestone:</p>\n<ul><li>The first database built for serverless applications</li><li>The first globally replicated database as a service</li><li>The first transactional, strongly consistent, multi-region database available to the public</li></ul><p>We’re also looking forward to the coming year:</p>\n<ul><li>Releasing FaunaDB On-Premises to everyone</li><li>Continually adding capabilities, improving performance, and lowering customer costs</li><li>Growing our <a href=\"http://labs.openviewpartners.com/founder-led-selling/\">repeatable sales process</a></li></ul><h2>Thank you</h2>\n<p>I would like to thank everyone on our team, including my cofounder, Matt Freels, our staff, our investors, our advisors, and our family and friends for the dedication to get us to this point. And I would like to thank all our early customers for sharing our vision for modern data infrastructure.</p>\n<p>We have never done more challenging or more important work. I’m incredibly humbled and proud to be part of it.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-03-15T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 498,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a2a1b0ff-1a62-4fd6-9697-f64b57053af8",
        "siteSettingsId": 498,
        "fieldLayoutId": 4,
        "contentId": 331,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Video: Fauna CEO Evan Weaver in Conversation With Alexy Khrabov",
        "slug": "fauna-ceo-interview",
        "uri": "blog/fauna-ceo-interview",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2020-05-26T09:07:17-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/fauna-ceo-interview",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/fauna-ceo-interview",
        "isCommunityPost": false,
        "blogBodyText": "<p>In January 2017, Fauna hosted the&nbsp;<a href=\"https://www.meetup.com/SF-Scala\">SF Scala</a>&nbsp;meetup. Alexy Khrabov, organizer of SF Scala, took a few minutes to catch up with our CEO and co-founder, Evan Weaver.</p>\r\n<div class=\"video-container\"><figure><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/M3YWfvYPdFQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe></figure></div>\r\n<p><strong><br>Alexy:</strong>&nbsp;Hello everybody! I am Alexy Khrabov. Organizer of SF Scala, here on location at Fauna and with us we have Evan Weaver, CEO and founder of Fauna. We actually met, I would say, about a year ago or more and talked about Fauna as a new kind of database. Now Fauna has opened their doors for the SF Scala meetup for the first time. So we’re really excited to be here and to catch up and see what’s new with Fauna.<br></p>\r\n<p><strong>Evan:</strong>&nbsp;Thanks Alexy. Yeah, we moved into this place about two months ago. Before that we were in the top floor of a converted Victorian near South Park. That’s where I first met you. Before that we were in my basement, a long time ago.</p>\r\n<p>We’re 14 people now, preparing for public launch of FaunaDB in March. So, we’re rolling out previews, especially previews of the Serverless Cloud product, and basically finishing up the business side of things with billing systems, improving tutorials, documentation…all that that kind of stuff that you need to use Fauna as a self-service adopter in particular.</p>\r\n<blockquote>We’re preparing for public launch of FaunaDB in March.</blockquote>\r\n<p>The database has been in production for a couple years now. In particular NVIDIA is our biggest early partner. They have an on-premises deployment which is in 3 data centers, and a couple dozen nodes, and they scale it up and down over time.</p>\r\n<p><strong>Alexy:</strong>&nbsp;So I remember we talked about Fauna and I thought, that’s really a cool way to use the database, right, because it’s more than a key value store. It’s an object store. Can you talk a little bit about Fauna as a core database? What does give you more than traditional databases give you?</p>\r\n<p><strong>Evan:</strong>&nbsp;Yes. Fauna is a temporal, object-relational, consistent, distributed database. Our goal is to basically marry all the kinds of different query patterns you would typically use from different domains like relational, document, graph, even search and analytics eventually, into a single coherent system. So you can scale that underlying piece of infrastructure and use it for all your different workloads, and share data across teams. Share data across data centers, because it’s a globally distributed system as well.</p>\r\n<blockquote>Fauna is a temporal, object-relational, consistent, distributed database.</blockquote>\r\n<p>Basically, get back to a world where you can integrate through the database and trust that the database will be the single lever you can move when you need to scale your application up and down.</p>\r\n<p><strong>Alexy:</strong>&nbsp;Right. And so and for this you need basically…you’re already everything inside the database…analytics queries…inside the database. So you have your own DSL for the queries, which you control, basically?</p>\r\n<p><strong>Evan:</strong>&nbsp;Yeah, the the interface is a functional interface, similar to LINQ in the C# world. You use embedded DSLs in your application languages. That means that your queries are type safe. You don’t have to learn a new syntax. You just have to learn Fauna semantics.</p>\r\n<blockquote>The interface is a functional interface, similar to LINQ in the C# world.</blockquote>\r\n<p>It’s pretty functional too, in a functional programming way. You do things like map and fold over the core database primitives, page through indexes, and you can do compute and you can do set arithmetic and that kind of thing. Ultimately you can write a very rich query, even more rich than SQL allows, remote it to the database, and trust that the database will execute in the the maximally optimal way against the underlying data.</p>\r\n<p><strong>Alexy:</strong>&nbsp;Interesting, I already like it. I like my query language to be more Fauna than SQL. If it’s functional, that’s even better. This is closer to the functional languages, right, because the problem with SQL is that it’s so declarative, right. So basically you can lose track and eventually create a bunch of temporary tables, and decompose it in weird ways.</p>\r\n<p>So how does it feel to write like a long complicated Fauna query program?</p>\r\n<p><strong>Evan:</strong>&nbsp;Yeah, it’s exactly like that. You’re dealing with immutable data structures and parallel computation. But the computation is explicit. You essentially compose the query plan in your in your query, so you know what the database is going to do. It’s not going to change its mind and start optimizing in a different way as your data set grows.</p>\r\n<p>You don’t have to run EXPLAIN to understand which indexes it’s going to use. You say these are my indexes, I’m going to start with them, and I want to do these operations, and I want to do this set algebra, get some results, cursor through it, that kinda thing. So it’s very explicit. That lets us guarantee a very consistent performance profile as you scale your systems up and down. You can trust that the execution pattern isn’t going to change.</p>\r\n<blockquote>You can trust that the execution pattern isn’t going to change .</blockquote>\r\n<p><strong>Alexy:</strong>&nbsp;Interesting. So how do you feed your data, like, I think there is a lot of activity in open source to put together data pipelines, right. So you have Kafka feeding Spark, feeding Cassandra, on top of Mesos. How do you compare to the SMACK stack, effectively?</p>\r\n<p><strong>Evan:</strong>&nbsp;Fauna…in a way it’s a traditional relational database. It’s just not SQL. So you can use it as a sink for data. You compute somewhere else. But you can also keep your core business objects in a fully normalized way, compose them with queries, push a lot of that, especially index computation that you might do in a second system like Spark or ElasticSearch, into Fauna. It works as a source, too, because the underlying data model is temporal.</p>\r\n<blockquote>In a way it’s a traditional relational database. It’s just not SQL.</blockquote>\r\n<p>You can get change feeds for any query out of the system. So you can say, like, oh here’s a distributed graph join, an activity feed. What happened since the last time I looked at it? And you get a bunch of, rather than just a different results, you get a bunch of change events that you can use to synchronize something downstream.</p>\r\n<blockquote>You can get change feeds for any query out of the system.</blockquote>\r\n<p>So, the goal eventually is to internalize all those concerns. Right now it works really well as a canonical store for fully normalized business objects, in particular social graphs, that kind of thing. Or as a low-latency distributed sink for stuff you might compute upstream. Like you want essentially a document database that you can index and scale, because you have all this offline computation from a null process. So where are you going to put it?</p>\r\n<p><strong>Alexy:</strong>&nbsp;Well, while this sounds really, really cool. It’s almost like, if I want to build a start-up, I can take this and solve problems. So I wonder what is the impression from customers? What are their early experiences? Where are they taking this, instead of a bunch of other things. What are they saying?</p>\r\n<p><strong>Evan:</strong>&nbsp;I mean, usually people don’t believe it can be true! If you get all your queries isolated you can dynamically provision where your data is, you can use our serverless cloud, and not even think about the backend at all.</p>\r\n<p>People have been so badly burned, in particular by the NoSQL movement, that they want us to show them. So we have to do a lot of work to prove that the database is sound and secure and performance is good and show that on their data sets we can import it. Here’s the query patterns. You can replicate what you already do. But now you can get all these scalability, isolation, and performance benefits out of the system.</p>\r\n<p>People like the interface. They’re tired of SQL. It’s unsafe. It’s hard to reason about the performance profile and the security model. It doesn’t work with the way people write modern applications. The proof of that is people put ORMs in front of their relational databases for operational workloads. Not having to deal with that, and being able to directly talk to your database again, it’s…what’s the word…refreshing.</p>\r\n<blockquote>People are tired of SQL. It’s unsafe. It’s hard to reason about the performance profile and the security model. It doesn’t work with the way people write modern applications.</blockquote>\r\n<p><strong>Alexy:</strong>&nbsp;How is your go-to-market stage. Are you out of stealth?</p>\r\n<p><strong>Evan:</strong>&nbsp;Yes, we’re out of stealth. We have beta customers both on-premises and in the serverless cloud. We’re looking for more. We’re preparing for general availability launch in the next couple months.</p>\r\n<p><strong>Alexy:</strong>&nbsp;So if somebody wants to try Fauna, how do they go about it?</p>\r\n<p><strong>Evan:</strong>&nbsp;Just go to the website click the request invite link, and we’ll hook you up.</p>\r\n<p><strong>Alexy:</strong>&nbsp;And so can you demo this at the meetup with a realistic application?</p>\r\n<p><strong>Evan:</strong>&nbsp;Yes, Chris Andersen, who is one of the Couchbase founders, joined our team recently. He’s working on a serverless presentation that will have AWS lambda executing the compute and Fauna as the backend. Then you have a fully serverless end-to-end stack where you never have to think about provisioning and you can even build a globally distributed dynamic application in a fully serverless model.</p>\r\n<blockquote>You can even build a globally distributed dynamic application in a fully serverless model.</blockquote>\r\n<p><strong>Alexy:</strong>&nbsp;I’ve got dibs on this presentation when it’s ready. It sounds really exciting. It makes a lot of sense. I can’t wait to see it. So thank you very much for enlightening us. Looking forward to playing more with this.</p>\r\n<p><strong>Evan:</strong>&nbsp;Thank you.</p>\r\n<p><strong>Alexy:</strong>&nbsp;Once we play some more with this we’ll come back with more questions.</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>In January 2017, Fauna hosted the <a href=\"https://www.meetup.com/SF-Scala\">SF Scala</a> meetup. Alexy Khrabov, organizer of SF Scala, took a few minutes to catch up with our CEO and co-founder, Evan Weaver.</p>\n<div class=\"video-container\"><figure></figure></div>\n<p><strong><br />Alexy:</strong> Hello everybody! I am Alexy Khrabov. Organizer of SF Scala, here on location at Fauna and with us we have Evan Weaver, CEO and founder of Fauna. We actually met, I would say, about a year ago or more and talked about Fauna as a new kind of database. Now Fauna has opened their doors for the SF Scala meetup for the first time. So we’re really excited to be here and to catch up and see what’s new with Fauna.<br /></p>\n<p><strong>Evan:</strong> Thanks Alexy. Yeah, we moved into this place about two months ago. Before that we were in the top floor of a converted Victorian near South Park. That’s where I first met you. Before that we were in my basement, a long time ago.</p>\n<p>We’re 14 people now, preparing for public launch of FaunaDB in March. So, we’re rolling out previews, especially previews of the Serverless Cloud product, and basically finishing up the business side of things with billing systems, improving tutorials, documentation…all that that kind of stuff that you need to use Fauna as a self-service adopter in particular.</p>\n<blockquote>We’re preparing for public launch of FaunaDB in March.</blockquote>\n<p>The database has been in production for a couple years now. In particular NVIDIA is our biggest early partner. They have an on-premises deployment which is in 3 data centers, and a couple dozen nodes, and they scale it up and down over time.</p>\n<p><strong>Alexy:</strong> So I remember we talked about Fauna and I thought, that’s really a cool way to use the database, right, because it’s more than a key value store. It’s an object store. Can you talk a little bit about Fauna as a core database? What does give you more than traditional databases give you?</p>\n<p><strong>Evan:</strong> Yes. Fauna is a temporal, object-relational, consistent, distributed database. Our goal is to basically marry all the kinds of different query patterns you would typically use from different domains like relational, document, graph, even search and analytics eventually, into a single coherent system. So you can scale that underlying piece of infrastructure and use it for all your different workloads, and share data across teams. Share data across data centers, because it’s a globally distributed system as well.</p>\n<blockquote>Fauna is a temporal, object-relational, consistent, distributed database.</blockquote>\n<p>Basically, get back to a world where you can integrate through the database and trust that the database will be the single lever you can move when you need to scale your application up and down.</p>\n<p><strong>Alexy:</strong> Right. And so and for this you need basically…you’re already everything inside the database…analytics queries…inside the database. So you have your own DSL for the queries, which you control, basically?</p>\n<p><strong>Evan:</strong> Yeah, the the interface is a functional interface, similar to LINQ in the C# world. You use embedded DSLs in your application languages. That means that your queries are type safe. You don’t have to learn a new syntax. You just have to learn Fauna semantics.</p>\n<blockquote>The interface is a functional interface, similar to LINQ in the C# world.</blockquote>\n<p>It’s pretty functional too, in a functional programming way. You do things like map and fold over the core database primitives, page through indexes, and you can do compute and you can do set arithmetic and that kind of thing. Ultimately you can write a very rich query, even more rich than SQL allows, remote it to the database, and trust that the database will execute in the the maximally optimal way against the underlying data.</p>\n<p><strong>Alexy:</strong> Interesting, I already like it. I like my query language to be more Fauna than SQL. If it’s functional, that’s even better. This is closer to the functional languages, right, because the problem with SQL is that it’s so declarative, right. So basically you can lose track and eventually create a bunch of temporary tables, and decompose it in weird ways.</p>\n<p>So how does it feel to write like a long complicated Fauna query program?</p>\n<p><strong>Evan:</strong> Yeah, it’s exactly like that. You’re dealing with immutable data structures and parallel computation. But the computation is explicit. You essentially compose the query plan in your in your query, so you know what the database is going to do. It’s not going to change its mind and start optimizing in a different way as your data set grows.</p>\n<p>You don’t have to run EXPLAIN to understand which indexes it’s going to use. You say these are my indexes, I’m going to start with them, and I want to do these operations, and I want to do this set algebra, get some results, cursor through it, that kinda thing. So it’s very explicit. That lets us guarantee a very consistent performance profile as you scale your systems up and down. You can trust that the execution pattern isn’t going to change.</p>\n<blockquote>You can trust that the execution pattern isn’t going to change .</blockquote>\n<p><strong>Alexy:</strong> Interesting. So how do you feed your data, like, I think there is a lot of activity in open source to put together data pipelines, right. So you have Kafka feeding Spark, feeding Cassandra, on top of Mesos. How do you compare to the SMACK stack, effectively?</p>\n<p><strong>Evan:</strong> Fauna…in a way it’s a traditional relational database. It’s just not SQL. So you can use it as a sink for data. You compute somewhere else. But you can also keep your core business objects in a fully normalized way, compose them with queries, push a lot of that, especially index computation that you might do in a second system like Spark or ElasticSearch, into Fauna. It works as a source, too, because the underlying data model is temporal.</p>\n<blockquote>In a way it’s a traditional relational database. It’s just not SQL.</blockquote>\n<p>You can get change feeds for any query out of the system. So you can say, like, oh here’s a distributed graph join, an activity feed. What happened since the last time I looked at it? And you get a bunch of, rather than just a different results, you get a bunch of change events that you can use to synchronize something downstream.</p>\n<blockquote>You can get change feeds for any query out of the system.</blockquote>\n<p>So, the goal eventually is to internalize all those concerns. Right now it works really well as a canonical store for fully normalized business objects, in particular social graphs, that kind of thing. Or as a low-latency distributed sink for stuff you might compute upstream. Like you want essentially a document database that you can index and scale, because you have all this offline computation from a null process. So where are you going to put it?</p>\n<p><strong>Alexy:</strong> Well, while this sounds really, really cool. It’s almost like, if I want to build a start-up, I can take this and solve problems. So I wonder what is the impression from customers? What are their early experiences? Where are they taking this, instead of a bunch of other things. What are they saying?</p>\n<p><strong>Evan:</strong> I mean, usually people don’t believe it can be true! If you get all your queries isolated you can dynamically provision where your data is, you can use our serverless cloud, and not even think about the backend at all.</p>\n<p>People have been so badly burned, in particular by the NoSQL movement, that they want us to show them. So we have to do a lot of work to prove that the database is sound and secure and performance is good and show that on their data sets we can import it. Here’s the query patterns. You can replicate what you already do. But now you can get all these scalability, isolation, and performance benefits out of the system.</p>\n<p>People like the interface. They’re tired of SQL. It’s unsafe. It’s hard to reason about the performance profile and the security model. It doesn’t work with the way people write modern applications. The proof of that is people put ORMs in front of their relational databases for operational workloads. Not having to deal with that, and being able to directly talk to your database again, it’s…what’s the word…refreshing.</p>\n<blockquote>People are tired of SQL. It’s unsafe. It’s hard to reason about the performance profile and the security model. It doesn’t work with the way people write modern applications.</blockquote>\n<p><strong>Alexy:</strong> How is your go-to-market stage. Are you out of stealth?</p>\n<p><strong>Evan:</strong> Yes, we’re out of stealth. We have beta customers both on-premises and in the serverless cloud. We’re looking for more. We’re preparing for general availability launch in the next couple months.</p>\n<p><strong>Alexy:</strong> So if somebody wants to try Fauna, how do they go about it?</p>\n<p><strong>Evan:</strong> Just go to the website click the request invite link, and we’ll hook you up.</p>\n<p><strong>Alexy:</strong> And so can you demo this at the meetup with a realistic application?</p>\n<p><strong>Evan:</strong> Yes, Chris Andersen, who is one of the Couchbase founders, joined our team recently. He’s working on a serverless presentation that will have AWS lambda executing the compute and Fauna as the backend. Then you have a fully serverless end-to-end stack where you never have to think about provisioning and you can even build a globally distributed dynamic application in a fully serverless model.</p>\n<blockquote>You can even build a globally distributed dynamic application in a fully serverless model.</blockquote>\n<p><strong>Alexy:</strong> I’ve got dibs on this presentation when it’s ready. It sounds really exciting. It makes a lot of sense. I can’t wait to see it. So thank you very much for enlightening us. Looking forward to playing more with this.</p>\n<p><strong>Evan:</strong> Thank you.</p>\n<p><strong>Alexy:</strong> Once we play some more with this we’ll come back with more questions.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "469",
        "postDate": "2017-02-25T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 495,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "c4df455e-5526-42bc-8a1c-9ceb022bb252",
        "siteSettingsId": 495,
        "fieldLayoutId": 4,
        "contentId": 328,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Join our Meetup Community",
        "slug": "bay-area-adaptive-technologies-meetup",
        "uri": "blog/bay-area-adaptive-technologies-meetup",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/bay-area-adaptive-technologies-meetup",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/bay-area-adaptive-technologies-meetup",
        "isCommunityPost": false,
        "blogBodyText": "<p>We’re happy to announce that we’ve started a meetup group,&nbsp;<a href=\"https://www.meetup.com/Bay-Area-Adaptive-Technologies/\">Bay Area Adaptive Technologies</a>. Join us to explore bleeding-edge distributed, cloud, reactive, and serverless technology in an open-minded and collaborative community.</p>\r\n<p>Our first event is this Tuesday, February 28, “<a href=\"https://www.meetup.com/Bay-Area-Adaptive-Technologies/events/237762292/\">Shaking up the Team</a>,” featuring Cliff Moon and Delyan Raychev from the data lake security company&nbsp;<a href=\"https://www.thinair.com/\">ThinAir</a>.</p>\r\n<p>The event will take place at our office in the historic Jackson Square neighborhood of San Francisco, and there will be snacks.</p>\r\n<p>Hope to see you there!</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>We’re happy to announce that we’ve started a meetup group, <a href=\"https://www.meetup.com/Bay-Area-Adaptive-Technologies/\">Bay Area Adaptive Technologies</a>. Join us to explore bleeding-edge distributed, cloud, reactive, and serverless technology in an open-minded and collaborative community.</p>\n<p>Our first event is this Tuesday, February 28, “<a href=\"https://www.meetup.com/Bay-Area-Adaptive-Technologies/events/237762292/\">Shaking up the Team</a>,” featuring Cliff Moon and Delyan Raychev from the data lake security company <a href=\"https://www.thinair.com/\">ThinAir</a>.</p>\n<p>The event will take place at our office in the historic Jackson Square neighborhood of San Francisco, and there will be snacks.</p>\n<p>Hope to see you there!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-02-25T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 496,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "a4eee5f2-1237-40e1-a96e-e40445eb78e4",
        "siteSettingsId": 496,
        "fieldLayoutId": 4,
        "contentId": 329,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "120,000 Distributed Consistent Writes per Second With Calvin",
        "slug": "distributed-acid-transaction-performance",
        "uri": "blog/distributed-acid-transaction-performance",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/distributed-acid-transaction-performance",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/distributed-acid-transaction-performance",
        "isCommunityPost": false,
        "blogBodyText": "<p>As we prepare for the general availability release of FaunaDB, we’re happy to begin sharing performance data. I’m a big fan of ACID-compliant distributed transactions, so we’ll start there.</p>\r\n<p>Our benchmarks show that FaunaDB can easily exceed 120,000 distributed, consistent writes per second, per logical database, on 15 machines.</p>\r\n<blockquote>FaunaDB can easily exceed 120,000 distributed, consistent writes per second, per logical database.</blockquote>\r\n<p>Unlike other distributed databases that rely on hardware clocks or multi-phase commits, FaunaDB’s transaction consistency algorithm is inspired by&nbsp;<a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>. Calvin is designed for high throughput regardless of network latency, and was the work of Alexander Thomson and others from Daniel Abadi’s lab at Yale.</p>\r\n<p>Calvin’s primary trade-off is that it doesn’t support session transactions, so it’s not well suited for SQL. Instead, transactions must be submitted atomically. Session transactions in SQL were designed for analytics, specifically human beings sitting at a workstation. They are pure overhead in a high-throughput operational context.</p>\r\n<blockquote>Calvin is designed for high throughput regardless of network latency.</blockquote>\r\n<p>FaunaDB’s functional, relational query language is more expressive than SQL in an operational data context, and works perfectly with Calvin. We’ll explore the transaction model in detail in the upcoming FaunaDB white paper, but for now, let’s check out the benchmarks.</p>\r\n<h2>Hardware</h2>\r\n<p>For the FaunaDB cluster, we have fifteen c3.4xlarge AWS EC2 instances configured as three logical datacenters of five instances each.</p>\r\n<p>These instances have 16 hardware threads, 30GB of RAM, and two ephemeral SSDs in RAID0. The entire cluster has 240 hardware threads, and each logical datacenter is in its own availability zone and replicates the entire dataset.</p>\r\n<p>For the load generators, we have two c3.large instances per availability zone, for a total of 12 hardware threads.</p>\r\n<h2>Software</h2>\r\n<p>We are using our most recent FaunaDB build on Linux/JDK 8 with the G1 garbage collector, which has shown good adaptability to changing workloads for us. Although the maximum pause time can still be somewhat high with G1, FaunaDB uses strategies like redundant dispatch to minimize the impact of collection pauses and other temporary partition events.</p>\r\n<blockquote>FaunaDB uses strategies like redundant dispatch to minimize the impact of temporary partition events.</blockquote>\r\n<p>The JVM heap size is set to half of physical RAM; unlike many other database systems, FaunaDB does not require any further tuning, and there are no service or hardware dependencies.</p>\r\n<h2>Dataset</h2>\r\n<p>The schema is a single logical database, with a single class (table) in it, with four indexes defined.</p>\r\n<p>Each request issues an authenticated query that inserts four instances (rows), with four random small strings in each instance. FaunaDB must issue each instance in the query a globally unique id, apply the four index functions to each instance, linearize and commit the transaction (maintaining the semantics of the entire batch), and replicate the effects to the appropriate replicas in every logical datacenter.</p>\r\n<p>Each instance insert generates 9 write effects–one for the instance itself, and two for each index–for a total of 36 writes per transaction. The instances themselves have no partition contention because they all have unique ids, but the associated index writes may contend if any instances in the same Calvin epoch happen to share a value.</p>\r\n<p>All transactions are fully replicated and persisted to disk before they are acknowledged to the client.</p>\r\n<h2>Results</h2>\r\n<p>Here are the results from running the benchmark for an hour:</p>\r\n<ul><li>Transactions per second: 3,330</li><li>Total transactions: 12,000,000</li><li>Writes per second: 120,000</li><li>Total writes: 432,000,000</li></ul>\r\n<p>Here are graphs of the various performance statistics over time:</p>\r\n<figure><img src=\"{asset:460:url}\" data-image=\"6jls5tlffgu2\"></figure>\r\n<p></p>\r\n<figure><img src=\"{asset:467:url}\" data-image=\"40z602de4p4q\"></figure>\r\n<p></p>\r\n<figure><img src=\"{asset:413:url}\" data-image=\"vv33k8c8mt9n\"></figure>\r\n<p></p>\r\n<p>This test shows great horizontal scalability across cores and machines. There is CPU to spare; CPU could be fully saturated if multiple logical databases were transacted at once. The theoretical throughput limit is 1 billion writes per second per logical database, so these results will only get better.<br></p>\r\n<blockquote>FaunaDB shows great horizontal scalability across cores and machines.</blockquote>\r\n<p>We have a lot more aspects of the system yet to demonstrate, including more reads, joins, and graph operations. Until then, happy transacting!</p>",
        "blogCategory": [
            "8"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>As we prepare for the general availability release of FaunaDB, we’re happy to begin sharing performance data. I’m a big fan of ACID-compliant distributed transactions, so we’ll start there.</p>\n<p>Our benchmarks show that FaunaDB can easily exceed 120,000 distributed, consistent writes per second, per logical database, on 15 machines.</p>\n<blockquote>FaunaDB can easily exceed 120,000 distributed, consistent writes per second, per logical database.</blockquote>\n<p>Unlike other distributed databases that rely on hardware clocks or multi-phase commits, FaunaDB’s transaction consistency algorithm is inspired by <a href=\"http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\">Calvin</a>. Calvin is designed for high throughput regardless of network latency, and was the work of Alexander Thomson and others from Daniel Abadi’s lab at Yale.</p>\n<p>Calvin’s primary trade-off is that it doesn’t support session transactions, so it’s not well suited for SQL. Instead, transactions must be submitted atomically. Session transactions in SQL were designed for analytics, specifically human beings sitting at a workstation. They are pure overhead in a high-throughput operational context.</p>\n<blockquote>Calvin is designed for high throughput regardless of network latency.</blockquote>\n<p>FaunaDB’s functional, relational query language is more expressive than SQL in an operational data context, and works perfectly with Calvin. We’ll explore the transaction model in detail in the upcoming FaunaDB white paper, but for now, let’s check out the benchmarks.</p>\n<h2>Hardware</h2>\n<p>For the FaunaDB cluster, we have fifteen c3.4xlarge AWS EC2 instances configured as three logical datacenters of five instances each.</p>\n<p>These instances have 16 hardware threads, 30GB of RAM, and two ephemeral SSDs in RAID0. The entire cluster has 240 hardware threads, and each logical datacenter is in its own availability zone and replicates the entire dataset.</p>\n<p>For the load generators, we have two c3.large instances per availability zone, for a total of 12 hardware threads.</p>\n<h2>Software</h2>\n<p>We are using our most recent FaunaDB build on Linux/JDK 8 with the G1 garbage collector, which has shown good adaptability to changing workloads for us. Although the maximum pause time can still be somewhat high with G1, FaunaDB uses strategies like redundant dispatch to minimize the impact of collection pauses and other temporary partition events.</p>\n<blockquote>FaunaDB uses strategies like redundant dispatch to minimize the impact of temporary partition events.</blockquote>\n<p>The JVM heap size is set to half of physical RAM; unlike many other database systems, FaunaDB does not require any further tuning, and there are no service or hardware dependencies.</p>\n<h2>Dataset</h2>\n<p>The schema is a single logical database, with a single class (table) in it, with four indexes defined.</p>\n<p>Each request issues an authenticated query that inserts four instances (rows), with four random small strings in each instance. FaunaDB must issue each instance in the query a globally unique id, apply the four index functions to each instance, linearize and commit the transaction (maintaining the semantics of the entire batch), and replicate the effects to the appropriate replicas in every logical datacenter.</p>\n<p>Each instance insert generates 9 write effects–one for the instance itself, and two for each index–for a total of 36 writes per transaction. The instances themselves have no partition contention because they all have unique ids, but the associated index writes may contend if any instances in the same Calvin epoch happen to share a value.</p>\n<p>All transactions are fully replicated and persisted to disk before they are acknowledged to the client.</p>\n<h2>Results</h2>\n<p>Here are the results from running the benchmark for an hour:</p>\n<ul><li>Transactions per second: 3,330</li><li>Total transactions: 12,000,000</li><li>Writes per second: 120,000</li><li>Total writes: 432,000,000</li></ul><p>Here are graphs of the various performance statistics over time:</p>\n<figure><img src=\"{asset:460:url||https://fauna.com/assets/site/blog-legacy/query-ops-graph.png}\" alt=\"\" /></figure><figure><img src=\"{asset:467:url||https://fauna.com/assets/site/blog-legacy/transactions-graph.png}\" alt=\"\" /></figure><figure><img src=\"{asset:413:url||https://fauna.com/assets/site/blog-legacy/cpu-graph.png}\" alt=\"\" /></figure><p>This test shows great horizontal scalability across cores and machines. There is CPU to spare; CPU could be fully saturated if multiple logical databases were transacted at once. The theoretical throughput limit is 1 billion writes per second per logical database, so these results will only get better.<br /></p>\n<blockquote>FaunaDB shows great horizontal scalability across cores and machines.</blockquote>\n<p>We have a lot more aspects of the system yet to demonstrate, including more reads, joins, and graph operations. Until then, happy transacting!</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-02-08T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 497,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "e55c7163-4076-4149-961f-3a22603b8288",
        "siteSettingsId": 497,
        "fieldLayoutId": 4,
        "contentId": 330,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Hello World: Build a Serverless App With the First Serverless Database",
        "slug": "serverless-cloud-database",
        "uri": "blog/serverless-cloud-database",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-09-04T16:13:27-07:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/serverless-cloud-database",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/serverless-cloud-database",
        "isCommunityPost": false,
        "blogBodyText": "<p>FaunaDB is the first truly serverless database. In this post, I’ll use the Serverless Framework to connect an AWS Lambda application with FaunaDB Serverless Cloud.</p>\n<p>When I say&nbsp;<em>serverless</em>, I’m referring to the function-as-a-service pattern. A serverless system must scale dynamically per request, and not require any capacity planning or provisioning. For instance, you can&nbsp;<a href=\"https://fauna.com/serverless-cloud-sign-up\">connect to FaunaDB in moments</a>, and scale seamlessly from prototype to runaway hit.</p>\n<blockquote>A serverless system must scale dynamically per request.</blockquote>\n<p>Current popular cloud databases do not support this level of elasticity—you have to pay for capacity you don’t use. Additionally, they often lack support for joins, indexes, authentication, and other capabilities necessary to build a rich application.</p>\n<h2>What is FaunaDB?</h2>\n<p>FaunaDB Serverless Cloud is a globally distributed database that requires no provisioning. Capacity is metered and available on-demand—you only pay for what you use.&nbsp;</p>\n<blockquote>FaunaDB is a globally distributed database that requires no provisioning—you only pay for what you use.</blockquote>\n<p>FaunaDB is a perfect fit for serverless development. Let’s code.</p>\n<h2>Getting Started with Serverless Framework</h2>\n<p><a href=\"https://serverless.com/\">Serverless</a>&nbsp;is leading the serverless framework market right now with a clean system for configuring, writing, and deploying serverless application code to different cloud infrastructure providers. I took an afternoon to port one of their storage examples from DynamoDB to FaunaDB. It was incredibly easy to accomplish. Looking through this code will show us how simple it is to set up a serverless environment.</p>\n<p>The&nbsp;<a href=\"https://github.com/faunadb/serverless-crud\">CRUD service</a>&nbsp;I am porting is a simple REST API that allows the creation, updating, and deletion of todo items, as well as listing all todo times. It’s a toy example, so after we look through the code, I’ll describe how you’d go about adding a multi-list multi-user data model where users can invite other members to read and update todo lists. It would be a lot more productive to push that logic to FaunaDB, but in this example I am limited by the capabilities DynamoDB also provides.</p>\n<p>The README contains&nbsp;<a href=\"https://github.com/faunadb/serverless-crud#installation\">installation and usage instructions</a>, and you can&nbsp;<a href=\"https://fauna.com/serverless-cloud-sign-up\">go here</a>&nbsp;to get instant access to FaunaDB.</p>\n<p><em>Once you have this set up running, you can play around with FaunaDB’s more interesting features, like implementing a social graph. Check out this&nbsp;<a href=\"https://fauna.com/tutorials/social\">social graph tutorial</a>.</em></p>\n<h2>Defining the Functions</h2>\n<p>The first file to start reading any app using the Serverless Framework is&nbsp;<a href=\"https://github.com/faunadb/serverless-crud/blob/master/serverless.yml\">serverless.yml</a>&nbsp;which defines the service and links functions to event handlers.</p>\n<h3>readAll Example</h3>\n<p>In ours we can see the function definitions; here’s one of them:</p>\n<pre class=\"language-yaml\">functions:\n  readAll:\n    handler: handler.readAll\n    events:\n      - http:\n          path: todos\n          method: get\n          cors: true</pre>\n<p>This configuration means the&nbsp;<code>readAll</code>&nbsp;function in&nbsp;<code>handler.js</code>&nbsp;will be called when an HTTP GET is received at the&nbsp;<code>todos</code>&nbsp;path. If you look through the configuration you’ll see all the functions are linked to&nbsp;<code>handler.js</code>, so that’s where I’ll look next.</p>\n<pre class=\"language-javascript\">module.exports.readAll = (event, context, callback) =&gt; {\n  todosReadAll(event, (error, result) =&gt; {\n    const response = {\n      statusCode: 200,\n      headers: {\n        \"Access-Control-Allow-Origin\" : \"*\"\n      },\n      body: JSON.stringify(result),\n    };\n    context.succeed(response);\n  });\n};</pre>\n<p>Everything in&nbsp;<code>handler.js</code>&nbsp;is concerned with managing HTTP, so the actual logic is imported from a module for each function. In this case,&nbsp;<code>todos-read-all.js</code>. We’ll include the entire file here because this is the place FaunaDB comes into play.</p>\n<pre class=\"language-javascript\">'use strict';\nconst faunadb = require('faunadb');\nconst q = faunadb.query;\nconst client = new faunadb.Client({\n  secret: process.env.FAUNADB_SECRET\n});\nmodule.exports = (event, callback) =&gt; {\n  return client.query(q.Paginate(q.Match(q.Ref(\"indexes/all_todos\"))))\n  .then((response) =&gt; {\n    callback(false, response);\n  }).catch((error) =&gt; {\n    callback(error)\n  })\n};</pre>\n<p>In this case I’ll run a query for all todos, using a FaunaDB secret passed via configuration in&nbsp;<code>serverless.yml</code>. FaunaDB uses HTTP for queries so you don’t need to worry about sharing connections between modules or invocations.</p>\n<h3>Differences Between FaunaDB and DynamoDB Interfaces</h3>\n<p>The main difference between the DynamoDB and the FaunaDB versions is that this:</p>\n<pre class=\"language-javascript\">return dynamoDb.scan({TableName: 'todos'}, (error, data) =&gt; { … })</pre>\n<p>becomes:</p>\n<pre class=\"language-javascript\">return client.query(q.Paginate(q.Match(q.Ref(\"indexes/all_todos\"))))</pre>\n<h2>Try It Out</h2>\n<p>Follow the README instructions to launch and run the service, then create a few todo items.</p>\n<p>Now you’re ready to explore your data and experiment with queries in the FaunaDB dashboard. Open the dashboard&nbsp;<a href=\"https://fauna.com/serverless-cloud-sign-up\">via this sign up form</a>. It will look something like this:</p>\n<figure><img src=\"{asset:464:url}\" data-image=\"5xnap0owkjvk\"></figure>\n<p></p>\n<p>If you look closely at the screenshot you get a hint at FaunaDB’s temporal capabilities which can power everything from social activity feeds, to auditing, to mobile sync.</p>\n<h2>Moving Beyond a Demo</h2>\n<p>In a production-worthy version of this application, the request would contain a list ID, and our query would validate that the list is visible to the user, before returning just the matching items. This security model is similar to collaboration apps you may be familiar with, and it’s supported natively by FaunaDB.</p>\n<p>The source DynamoDB example lists all todos using a&nbsp;<code>scan</code>&nbsp;operation. To move to a secure list sharing model in DynamoDB, you would have to add an index and then run multiple queries from your serverless handler to validate that the list is visible to the requesting user.</p>\n<p>To build a real application with FaunaDB, you’d also create an index on list ID, but you’d be able to pass authentication credentials into the database with your query. Instead of requiring multiple calls, FaunaDB will return data you need in a single query, including list items and metadata. Watch this blog for an updated serverless application with a more mature multi-user data model using FaunaDB’s security features.</p>\n<h2>Conclusion</h2>\n<p>As I build more examples using a serverless architecture, I’ll choose the Serverless.com framework again. You can look forward to an example of serverless code dynamically provisioning resource using FauanDB’s multi-tenant QoS features, and how to integrate FaunaDB with other serverless components.</p>\n<p>Over time, which do you think provides better agility: a database that requires rethinking your data layout when requirements change, and implementing big pieces of policy in application code—or a database with flexible queries and security awareness? I hope you agree that the answer is FaunaDB.</p>",
        "blogCategory": [
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>FaunaDB is the first truly serverless database. In this post, I’ll use the Serverless Framework to connect an AWS Lambda application with FaunaDB Serverless Cloud.</p>\n<p>When I say <em>serverless</em>, I’m referring to the function-as-a-service pattern. A serverless system must scale dynamically per request, and not require any capacity planning or provisioning. For instance, you can <a href=\"https://fauna.com/serverless-cloud-sign-up\">connect to FaunaDB in moments</a>, and scale seamlessly from prototype to runaway hit.</p>\n<blockquote>A serverless system must scale dynamically per request.</blockquote>\n<p>Current popular cloud databases do not support this level of elasticity—you have to pay for capacity you don’t use. Additionally, they often lack support for joins, indexes, authentication, and other capabilities necessary to build a rich application.</p>\n<h2>What is FaunaDB?</h2>\n<p>FaunaDB Serverless Cloud is a globally distributed database that requires no provisioning. Capacity is metered and available on demand—you only pay for what you use. In addition, you can always port your application to FaunaDB On-Premises in your own datacenter or private cloud, eliminating cloud infrastructure lock-in.</p>\n<blockquote>FaunaDB is a globally distributed database that requires no provisioning—you only pay for what you use.</blockquote>\n<p>FaunaDB is a perfect fit for serverless development. Let’s code.</p>\n<h2>Getting Started with Serverless Framework</h2>\n<p><a href=\"https://serverless.com/\">Serverless</a> is leading the serverless framework market right now with a clean system for configuring, writing, and deploying serverless application code to different cloud infrastructure providers. I took an afternoon to port one of their storage examples from DynamoDB to FaunaDB. It was incredibly easy to accomplish. Looking through this code will show us how simple it is to set up a serverless environment.</p>\n<p>The <a href=\"https://github.com/faunadb/serverless-crud\">CRUD service</a> I am porting is a simple REST API that allows the creation, updating, and deletion of todo items, as well as listing all todo times. It’s a toy example, so after we look through the code, I’ll describe how you’d go about adding a multi-list multi-user data model where users can invite other members to read and update todo lists. It would be a lot more productive to push that logic to FaunaDB, but in this example I am limited by the capabilities DynamoDB also provides.</p>\n<p>The README contains <a href=\"https://github.com/faunadb/serverless-crud#installation\">installation and usage instructions</a>, and you can <a href=\"https://fauna.com/serverless-cloud-sign-up\">go here</a> to get instant access to FaunaDB.</p>\n<p><em>Once you have this set up running, you can play around with FaunaDB’s more interesting features, like implementing a social graph. Check out this <a href=\"https://fauna.com/tutorials/social\">social graph tutorial</a>.</em></p>\n<h2>Defining the Functions</h2>\n<p>The first file to start reading any app using the Serverless Framework is <a href=\"https://github.com/faunadb/serverless-crud/blob/master/serverless.yml\">serverless.yml</a> which defines the service and links functions to event handlers.</p>\n<h3>readAll Example</h3>\n<p>In ours we can see the function definitions; here’s one of them:</p>\n<pre class=\"language-yaml\">functions:\n readAll:\n handler: handler.readAll\n events:\n - http:\n path: todos\n method: get\n cors: true</pre>\n<p>This configuration means the <code>readAll</code> function in <code>handler.js</code> will be called when an HTTP GET is received at the <code>todos</code> path. If you look through the configuration you’ll see all the functions are linked to <code>handler.js</code>, so that’s where I’ll look next.</p>\n<pre class=\"language-javascript\">module.exports.readAll = (event, context, callback) =&gt; {\n todosReadAll(event, (error, result) =&gt; {\n const response = {\n statusCode: 200,\n headers: {\n \"Access-Control-Allow-Origin\" : \"*\"\n },\n body: JSON.stringify(result),\n };\n context.succeed(response);\n });\n};</pre>\n<p>Everything in <code>handler.js</code> is concerned with managing HTTP, so the actual logic is imported from a module for each function. In this case, <code>todos-read-all.js</code>. We’ll include the entire file here because this is the place FaunaDB comes into play.</p>\n<pre class=\"language-javascript\">'use strict';\nconst faunadb = require('faunadb');\nconst q = faunadb.query;\nconst client = new faunadb.Client({\n secret: process.env.FAUNADB_SECRET\n});\nmodule.exports = (event, callback) =&gt; {\n return client.query(q.Paginate(q.Match(q.Ref(\"indexes/all_todos\"))))\n .then((response) =&gt; {\n callback(false, response);\n }).catch((error) =&gt; {\n callback(error)\n })\n};</pre>\n<p>In this case I’ll run a query for all todos, using a FaunaDB secret passed via configuration in <code>serverless.yml</code>. FaunaDB uses HTTP for queries so you don’t need to worry about sharing connections between modules or invocations.</p>\n<h3>Differences Between FaunaDB and DynamoDB Interfaces</h3>\n<p>The main difference between the DynamoDB and the FaunaDB versions is that this:</p>\n<pre class=\"language-javascript\">return dynamoDb.scan({TableName: 'todos'}, (error, data) =&gt; { … })</pre>\n<p>becomes:</p>\n<pre class=\"language-javascript\">return client.query(q.Paginate(q.Match(q.Ref(\"indexes/all_todos\"))))</pre>\n<h2>Try It Out</h2>\n<p>Follow the README instructions to launch and run the service, then create a few todo items.</p>\n<p>Now you’re ready to explore your data and experiment with queries in the FaunaDB dashboard. Open the dashboard <a href=\"https://fauna.com/serverless-cloud-sign-up\">via this sign up form</a>. It will look something like this:</p>\n<figure><img src=\"{asset:464:url||https://fauna.com/assets/site/blog-legacy/serverless-dashboard.png}\" alt=\"\" /></figure><p>If you look closely at the screenshot you get a hint at FaunaDB’s temporal capabilities which can power everything from social activity feeds, to auditing, to mobile sync.</p>\n<h2>Moving Beyond a Demo</h2>\n<p>In a production-worthy version of this application, the request would contain a list ID, and our query would validate that the list is visible to the user, before returning just the matching items. This security model is similar to collaboration apps you may be familiar with, and it’s supported natively by FaunaDB.</p>\n<p>The source DynamoDB example lists all todos using a <code>scan</code> operation. To move to a secure list sharing model in DynamoDB, you would have to add an index and then run multiple queries from your serverless handler to validate that the list is visible to the requesting user.</p>\n<p>To build a real application with FaunaDB, you’d also create an index on list ID, but you’d be able to pass authentication credentials into the database with your query. Instead of requiring multiple calls, FaunaDB will return data you need in a single query, including list items and metadata. Watch this blog for an updated serverless application with a more mature multi-user data model using FaunaDB’s security features.</p>\n<h2>Conclusion</h2>\n<p>As I build more examples using a serverless architecture, I’ll choose the Serverless.com framework again. You can look forward to an example of serverless code dynamically provisioning resource using FauanDB’s multi-tenant QoS features, and how to integrate FaunaDB with other serverless components.</p>\n<p>Over time, which do you think provides better agility: a database that requires rethinking your data layout when requirements change, and implementing big pieces of policy in application code—or a database with flexible queries and security awareness? I hope you agree that the answer is FaunaDB.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2017-02-03T14:34:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 493,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "fde2bfed-0582-4551-a825-5d661f139021",
        "siteSettingsId": 493,
        "fieldLayoutId": 4,
        "contentId": 326,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Escape the Cloud Database Trap With Serverless",
        "slug": "escape-the-cloud-database-trap-with-serverless",
        "uri": "blog/escape-the-cloud-database-trap-with-serverless",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/escape-the-cloud-database-trap-with-serverless",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/escape-the-cloud-database-trap-with-serverless",
        "isCommunityPost": false,
        "blogBodyText": "<p>If you rely on any cloud infrastructure, you know it is complex. It promises to free you from hardware—but you still have to worry about regions, zones, volumes, memory, software versions, and CPUs. Migrating from one service to another, or even simply changing capacity, is often a manual, error-prone process.</p>\r\n<p>When I cofounded the company that became Couchbase, there was no cloud. Databases were built for physical infrastructure.</p>\r\n<blockquote>Databases were built for physical infrastructure.</blockquote>\r\n<p>Now the world has moved on, but databases have not. Cloud databases are still constrained by provisioning choices. But when demand spikes, shouldn’t your database have your back? Isn’t that what the cloud is for?</p>\r\n<h2>The Earthy History of the Cloud</h2>\r\n<p>The cloud’s abstractions are rooted in the physical infrastructure of the past. A decade ago, public clouds were designed to mimic physical datacenters because developers were only comfortable with physical hardware. Today, that model is pure overhead.</p>\r\n<figure><img src=\"{asset:448:url}\" data-image=\"lqq4l8yxr68h\"></figure>\r\n<p></p>\r\n<figure><figcaption>Buy the ticket, take the ride. </figcaption></figure>\r\n<p>Using the cloud forces you to become an operations expert. “Even after building for the cloud, my team constantly spends time thinking about capacity planning, provisioning, sharding, backups, performance tuning, and monitoring,” said Rudy Winnacker, Head of Operations at Medium. “Cloud databases still incur significant operational overhead. We’re happy to see Fauna reducing this burden.”<br></p>\r\n<blockquote>“My team constantly spends time thinking about capacity planning, provisioning, sharding, backups, performance tuning, and monitoring.”</blockquote>\r\n<p>At Fauna, I regularly hear prospective customers say: “we were told everything would be scalable, available, and cheap in the cloud, but the operational work to meet those goals never ends.”</p>\r\n<h2>Dangerous Data Games</h2>\r\n<p>Stateless applications (like web servers) are difficult to configure, but once you figure out the concurrency model, hardware profile, and packaging strategy, you can scale them up and down on demand. Stateful services are far more challenging, and operational databases are the most critical stateful services. That’s why databases are at the core of the cloud operations trap.</p>\r\n<p>Adding more database nodes accomplishes nothing without repartitioning the data. Some systems, like MySQL, simply don’t support partitioning, and only offer simplistic replication strategies. Other systems, like DynamoDB, can partition and scale up, but not down. Still more, like Cassandra, can scale both up and down, but are extremely difficult to operate and have subtle performance limitations at scale.</p>\r\n<p>You shouldn’t have to turn away customers because your product is too popular, but an unexpected traffic surge is also the worst time to mess with your database infrastructure—this is the cloud database trap.</p>\r\n<blockquote>An unexpected traffic surge is the worst time to mess with your database infrastructure. This is the cloud database trap.</blockquote>\r\n<p>Instead, databases should be adaptive—fitting themselves to apps and workloads, not the other way around—and serverless—only charging for queries actually run, and data actually stored.</p>\r\n<h2>Serverless Can Save Us</h2>\r\n<p>When I heard about Fauna’s vision for a serverless database future, I was hooked. FaunaDB Cloud is an adaptive, serverless database, launching to the public today. With pay-as-you-go pricing, your database costs nothing when no one is using it. Combine it with a function-as-a-service provider like AWS Lambda or Google Cloud Functions, and your entire cost structure scales dynamically with usage.</p>\r\n<blockquote>A serverless database costs nothing when no one is using it.</blockquote>\r\n<p>You can also run FaunaDB in your private cloud or own datacenter, eliminating infrastructure lock-in. The query language, data model (including graphs and change feeds), security features, strong consistency, scalability and performance are best in class.</p>\r\n<figure><img src=\"{asset:461:url}\" data-image=\"4vvtx85qxhom\"></figure>\r\n<p></p>\r\n<figure><figcaption>An activity feed query in FaunaDB.</figcaption></figure>\r\n<p>Move your database to the cloud, and you will find its operational burden still falls on your shoulders. But a serverless database vanishes into the network. Now your entire stack transparently scales up, down, and sideways—even across datacenters and different infrastructure vendors.<br></p>\r\n<p>Serverless frees you from the cloud database trap.&nbsp;<a href=\"https://app.fauna.com/sign-up\">Sign up and build</a>.</p>",
        "blogCategory": [
            "1530"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>If you rely on any cloud infrastructure, you know it is complex. It promises to free you from hardware—but you still have to worry about regions, zones, volumes, memory, software versions, and CPUs. Migrating from one service to another, or even simply changing capacity, is often a manual, error-prone process.</p>\n<p>When I cofounded the company that became Couchbase, there was no cloud. Databases were built for physical infrastructure.</p>\n<blockquote>Databases were built for physical infrastructure.</blockquote>\n<p>Now the world has moved on, but databases have not. Cloud databases are still constrained by provisioning choices. But when demand spikes, shouldn’t your database have your back? Isn’t that what the cloud is for?</p>\n<h2>The Earthy History of the Cloud</h2>\n<p>The cloud’s abstractions are rooted in the physical infrastructure of the past. A decade ago, public clouds were designed to mimic physical datacenters because developers were only comfortable with physical hardware. Today, that model is pure overhead.</p>\n<figure><img src=\"{asset:448:url||https://fauna.com/assets/site/blog-legacy/instance-specifications.png}\" alt=\"\" /></figure><figure><figcaption>Buy the ticket, take the ride. </figcaption></figure><p>Using the cloud forces you to become an operations expert. “Even after building for the cloud, my team constantly spends time thinking about capacity planning, provisioning, sharding, backups, performance tuning, and monitoring,” said Rudy Winnacker, Head of Operations at Medium. “Cloud databases still incur significant operational overhead. We’re happy to see Fauna reducing this burden.”<br /></p>\n<blockquote>“My team constantly spends time thinking about capacity planning, provisioning, sharding, backups, performance tuning, and monitoring.”</blockquote>\n<p>At Fauna, I regularly hear prospective customers say: “we were told everything would be scalable, available, and cheap in the cloud, but the operational work to meet those goals never ends.”</p>\n<h2>Dangerous Data Games</h2>\n<p>Stateless applications (like web servers) are difficult to configure, but once you figure out the concurrency model, hardware profile, and packaging strategy, you can scale them up and down on demand. Stateful services are far more challenging, and operational databases are the most critical stateful services. That’s why databases are at the core of the cloud operations trap.</p>\n<p>Adding more database nodes accomplishes nothing without repartitioning the data. Some systems, like MySQL, simply don’t support partitioning, and only offer simplistic replication strategies. Other systems, like DynamoDB, can partition and scale up, but not down. Still more, like Cassandra, can scale both up and down, but are extremely difficult to operate and have subtle performance limitations at scale.</p>\n<p>You shouldn’t have to turn away customers because your product is too popular, but an unexpected traffic surge is also the worst time to mess with your database infrastructure—this is the cloud database trap.</p>\n<blockquote>An unexpected traffic surge is the worst time to mess with your database infrastructure. This is the cloud database trap.</blockquote>\n<p>Instead, databases should be adaptive—fitting themselves to apps and workloads, not the other way around—and serverless—only charging for queries actually run, and data actually stored.</p>\n<h2>Serverless Can Save Us</h2>\n<p>When I heard about Fauna’s vision for a serverless database future, I was hooked. FaunaDB Cloud is an adaptive, serverless database, launching to the public today. With pay-as-you-go pricing, your database costs nothing when no one is using it. Combine it with a function-as-a-service provider like AWS Lambda or Google Cloud Functions, and your entire cost structure scales dynamically with usage.</p>\n<blockquote>A serverless database costs nothing when no one is using it.</blockquote>\n<p>You can also run FaunaDB in your private cloud or own datacenter, eliminating infrastructure lock-in. The query language, data model (including graphs and change feeds), security features, strong consistency, scalability and performance are best in class.</p>\n<figure><img src=\"{asset:461:url||https://fauna.com/assets/site/blog-legacy/query.png}\" alt=\"\" /></figure><figure><figcaption>An activity feed query in FaunaDB.</figcaption></figure><p>Move your database to the cloud, and you will find its operational burden still falls on your shoulders. But a serverless database vanishes into the network. Now your entire stack transparently scales up, down, and sideways—even across datacenters and different infrastructure vendors.<br /></p>\n<p>Serverless frees you from the cloud database trap. <a href=\"https://fauna.com/serverless-cloud-sign-up\">Sign up and build</a>.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2017-02-01T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 499,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "3d0c270a-b832-4bfd-8b80-4293d2488806",
        "siteSettingsId": 499,
        "fieldLayoutId": 4,
        "contentId": 332,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Supporting Civic Engagement",
        "slug": "supporting-civic-engagement",
        "uri": "blog/supporting-civic-engagement",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:55-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/supporting-civic-engagement",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/supporting-civic-engagement",
        "isCommunityPost": false,
        "blogBodyText": "<p>Two years ago, on a cold Monday in Washington, D.C., I took the&nbsp;oath of office&nbsp;as a temporary Federal employee of the United States.</p>\r\n<p>The oath begins:</p>\r\n<blockquote>I do solemnly swear that I will support and defend the Constitution of the United States</blockquote>\r\n<p>The duty to “secure the Blessings of Liberty to ourselves and our Posterity,” as&nbsp;<a href=\"https://www.constituteproject.org/constitution/United_States_of_America_1992\">the Constitution describes</a>, is one that every American has the right and obligation to uphold. Today our obligation weighs on us more heavily than ever, especially those of us in Silicon Valley who have benefited so much from the laws and freedoms of our nation.</p>\r\n<p>In order to more effectively encourage our employees to participate in civic life, Fauna is adopting a civic engagement policy. An additional amount of paid time off—equal to the amount of time we already provide for vacation—is now available to our staff to make America a better nation through participation in political life, community building, and other acts of civic service.</p>\r\n<p>Whether this means voting, writing a letter to an elected official, attending a protest or rally, volunteering at a library or school, or spending the time to talk to your neighbor about the issues facing all of us, we want our staff to know that Fauna supports them in every possible way.</p>\r\n<p>We are joined in adopting this policy by the following companies:</p>\r\n<ul><li><a href=\"https://buoyant.io/\">Buoyant</a></li><li><a href=\"https://medium.com/turbine-labs/https-medium-com-turbine-labs-take-what-you-need-civic-engagement-edition-c2de92d3b9ec\">Turbine Labs</a></li><li><a href=\"https://www.atipica.co/\">Atipica</a></li><li><a href=\"http://www.vicarious.com/\">Vicarious</a></li><li><a href=\"https://blog.jelly.co/jellys-new-civic-engagement-policy-886559f0813c\">Jelly</a></li></ul>\r\n<p>We call on every company with a presence in the San Francisco Bay Area to join us. If you would like your company to be listed here, please email us at priority@fauna.com.</p>",
        "blogCategory": [
            "1461"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>Two years ago, on a cold Monday in Washington, D.C., I took the oath of office as a temporary Federal employee of the United States.</p>\n<p>The oath begins:</p>\n<blockquote>I do solemnly swear that I will support and defend the Constitution of the United States</blockquote>\n<p>The duty to “secure the Blessings of Liberty to ourselves and our Posterity,” as <a href=\"https://www.constituteproject.org/constitution/United_States_of_America_1992\">the Constitution describes</a>, is one that every American has the right and obligation to uphold. Today our obligation weighs on us more heavily than ever, especially those of us in Silicon Valley who have benefited so much from the laws and freedoms of our nation.</p>\n<p>In order to more effectively encourage our employees to participate in civic life, Fauna is adopting a civic engagement policy. An additional amount of paid time off—equal to the amount of time we already provide for vacation—is now available to our staff to make America a better nation through participation in political life, community building, and other acts of civic service.</p>\n<p>Whether this means voting, writing a letter to an elected official, attending a protest or rally, volunteering at a library or school, or spending the time to talk to your neighbor about the issues facing all of us, we want our staff to know that Fauna supports them in every possible way.</p>\n<p>We are joined in adopting this policy by the following companies:</p>\n<ul><li><a href=\"https://buoyant.io/\">Buoyant</a></li><li><a href=\"https://medium.com/turbine-labs/https-medium-com-turbine-labs-take-what-you-need-civic-engagement-edition-c2de92d3b9ec\">Turbine Labs</a></li><li><a href=\"https://www.atipica.co/\">Atipica</a></li><li><a href=\"http://www.vicarious.com/\">Vicarious</a></li><li><a href=\"https://blog.jelly.co/jellys-new-civic-engagement-policy-886559f0813c\">Jelly</a></li></ul><p>We call on every company with a presence in the San Francisco Bay Area to join us. If you would like your company to be listed here, please email us at priority@fauna.com.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "471",
        "postDate": "2016-11-18T00:00:00-08:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 490,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "b4cb4fa6-ae22-435b-9fc0-484dbf4b8d90",
        "siteSettingsId": 490,
        "fieldLayoutId": 4,
        "contentId": 323,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Introducing Support for the Go Programming Language",
        "slug": "introducing-support-for-the-go-programming-language",
        "uri": "blog/introducing-support-for-the-go-programming-language",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/introducing-support-for-the-go-programming-language",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/introducing-support-for-the-go-programming-language",
        "isCommunityPost": false,
        "blogBodyText": "<p>A high priority for the team at Fauna is making developers’ lives easier. Each job requires the right programming language, so we develop and fully support drivers in a variety of languages, including Java, Javascript, Ruby, Scala, Python, and C#. Today we are proud to announce the addition of the Go programming language to our arsenal.</p>\r\n<p>Our new driver makes it easy for developers working in the Go programming language to leverage all of the features of FaunaDB. If you are already familiar with using FaunaDB in another language, the Go driver will be immediately familiar and you will feel right at home coding against FaunaDB in Go.</p>\r\n<p>A quick note: To use these drivers with FaunaDB Cloud, you will need to join the&nbsp;<a href=\"https://fauna.com/join-beta\">managed beta program</a>.</p>\r\n<p>The rest of this blog post is a tour of essential FaunaDB operations exposed by the Go driver. The fun ones are presented first, with client setup at the end, so&nbsp;<a href=\"https://gist.github.com/jchris/6a6756f97b94e9cdac963ffa617150fb\">look here for an executable version of this code</a>.</p>\r\n<h2>Basic Queries</h2>\r\n<p>Here is an example of building and executing a FaunaDB query in Go, using a domain specific language designed to be friendly to Go users and also similar to FaunaDB APIs in other languages. Note that we alias our package name to f, so that queries are readable while still being expressed as standard Go.</p>\r\n<pre class=\"language-go\">import (\r\n    f \"github.com/faunadb/faunadb-go/faunadb\"\r\n)\r\n// skipping setup client code, see the last code block of this article...\r\ninstance, _ := client.Query(\r\n    f.Get(\r\n        f.MatchTerm(\r\n            f.Index(\"pet_by_name\"),\r\n            \"bob the cat\",\r\n        ),\r\n    ),\r\n)</pre>\r\n<h2>Tagged Structs in Go</h2>\r\n<p>The Go driver supports custom data structures via struct tags to control how your data is saved and loaded from FaunaDB. This makes it easy to populate objects for use by your application. You can mark the fields and let the driver decode from Fauna values to your own type. Here is the application data structure for pets in our example.</p>\r\n<pre class=\"language-go\">type Pet struct {\r\n    Name string `fauna:\"name\"`\r\n    Age  int    `fauna:\"age\"`\r\n}</pre>\r\n<p>You can let the driver decode the result of the query above into an instance of your Pet struct right away.</p>\r\n<pre class=\"language-go\">var myPet Pet\r\ninstance.Get(&myPet)</pre>\r\n<p>Encoding data structures to FaunaDB is also very simple, below you can see how a custom data struct is used in the create query.</p>\r\n<pre class=\"language-go\">bob := Pet {\r\n    Name: \"bob the cat\",\r\n    Age:  5,\r\n}\r\nclient.Query(\r\n    f.Create(\r\n        f.Class(\"pets\"),\r\n        f.Obj{\"data\": bob},\r\n    ),\r\n)</pre>\r\n<h2>Setting Up the Client</h2>\r\n<p>Before we can do any of the fun stuff we just read about, we have to set up a connection and create the schema. Here the admin key creates a database and then a server access key for data operations on that database. The server key creates a class for pets, and a database index on pet names.</p>\r\n<pre class=\"language-go\">adminClient := f.NewFaunaClient(\"your-secret-here\")\r\nadminClient.Query(\r\n    f.CreateDatabase(f.Obj{\"name\": \"db-test\"}),\r\n)\r\nserverKey, _ := adminClient.Query(\r\n    f.CreateKey(f.Obj{\r\n        \"role\":      \"server\", \r\n        \"database\":  f.Database(\"db-test\"),\r\n    }),\r\n)\r\nvar secret string\r\nserverKey.At(f.ObjKey(\"secret\")).Get(&secret)\r\nclient := adminClient.NewSessionClient(secret)\r\nclient.Query(\r\n    f.CreateClass(f.Obj{\"name\": \"pets\"}),\r\n)\r\nclient.Query(\r\n    f.CreateIndex(f.Obj{\r\n       \"name\":   \"pet_by_name\",\r\n       \"source\":  f.Class(\"pets\"),\r\n       \"terms\":   f.Arr{f.Obj{\"field\": f.Arr{\"data\", \"name\"}}},\r\n       \"unique\": true,\r\n    }),\r\n)</pre>\r\n<p>We hope you enjoy our Go support, and we look forward to your pull requests.</p>\r\n<p>To see our full documentation join our managed beta program, which will also get an account on Fauna’s cloud installation. When you are logged into the Fauna site, view our full documentation, and click the tab on the upper right of the page to set your language to Go.</p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>A high priority for the team at Fauna is making developers’ lives easier. Each job requires the right programming language, so we develop and fully support drivers in a variety of languages, including Java, Javascript, Ruby, Scala, Python, and C#. Today we are proud to announce the addition of the Go programming language to our arsenal.</p>\n<p>Our new driver makes it easy for developers working in the Go programming language to leverage all of the features of FaunaDB. If you are already familiar with using FaunaDB in another language, the Go driver will be immediately familiar and you will feel right at home coding against FaunaDB in Go.</p>\n<p>A quick note: To use these drivers with FaunaDB Cloud, you will need to join the <a href=\"https://fauna.com/join-beta\">managed beta program</a>.</p>\n<p>The rest of this blog post is a tour of essential FaunaDB operations exposed by the Go driver. The fun ones are presented first, with client setup at the end, so <a href=\"https://gist.github.com/jchris/6a6756f97b94e9cdac963ffa617150fb\">look here for an executable version of this code</a>.</p>\n<h2>Basic Queries</h2>\n<p>Here is an example of building and executing a FaunaDB query in Go, using a domain specific language designed to be friendly to Go users and also similar to FaunaDB APIs in other languages. Note that we alias our package name to f, so that queries are readable while still being expressed as standard Go.</p>\n<pre class=\"language-go\">import (\n f \"github.com/faunadb/faunadb-go/faunadb\"\n)\n// skipping setup client code, see the last code block of this article...\ninstance, _ := client.Query(\n f.Get(\n f.MatchTerm(\n f.Index(\"pet_by_name\"),\n \"bob the cat\",\n ),\n ),\n)</pre>\n<h2>Tagged Structs in Go</h2>\n<p>The Go driver supports custom data structures via struct tags to control how your data is saved and loaded from FaunaDB. This makes it easy to populate objects for use by your application. You can mark the fields and let the driver decode from Fauna values to your own type. Here is the application data structure for pets in our example.</p>\n<pre class=\"language-go\">type Pet struct {\n Name string `fauna:\"name\"`\n Age int `fauna:\"age\"`\n}</pre>\n<p>You can let the driver decode the result of the query above into an instance of your Pet struct right away.</p>\n<pre class=\"language-go\">var myPet Pet\ninstance.Get(&amp;myPet)</pre>\n<p>Encoding data structures to FaunaDB is also very simple, below you can see how a custom data struct is used in the create query.</p>\n<pre class=\"language-go\">bob := Pet {\n Name: \"bob the cat\",\n Age: 5,\n}\nclient.Query(\n f.Create(\n f.Class(\"pets\"),\n f.Obj{\"data\": bob},\n ),\n)</pre>\n<h2>Setting Up the Client</h2>\n<p>Before we can do any of the fun stuff we just read about, we have to set up a connection and create the schema. Here the admin key creates a database and then a server access key for data operations on that database. The server key creates a class for pets, and a database index on pet names.</p>\n<pre class=\"language-go\">adminClient := f.NewFaunaClient(\"your-secret-here\")\nadminClient.Query(\n f.CreateDatabase(f.Obj{\"name\": \"db-test\"}),\n)\nserverKey, _ := adminClient.Query(\n f.CreateKey(f.Obj{\n \"role\": \"server\", \n \"database\": f.Database(\"db-test\"),\n }),\n)\nvar secret string\nserverKey.At(f.ObjKey(\"secret\")).Get(&amp;secret)\nclient := adminClient.NewSessionClient(secret)\nclient.Query(\n f.CreateClass(f.Obj{\"name\": \"pets\"}),\n)\nclient.Query(\n f.CreateIndex(f.Obj{\n \"name\": \"pet_by_name\",\n \"source\": f.Class(\"pets\"),\n \"terms\": f.Arr{f.Obj{\"field\": f.Arr{\"data\", \"name\"}}},\n \"unique\": true,\n }),\n)</pre>\n<p>We hope you enjoy our Go support, and we look forward to your pull requests.</p>\n<p>To see our full documentation join our managed beta program, which will also get an account on Fauna’s cloud installation. When you are logged into the Fauna site, view our full documentation, and click the tab on the upper right of the page to set your language to Go.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "477",
        "postDate": "2016-10-17T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 489,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "72aa0d6a-ed77-4084-81fc-f756d95d4f7a",
        "siteSettingsId": 489,
        "fieldLayoutId": 4,
        "contentId": 322,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Time-Traveling Databases: Exploring Temporality in FaunaDB",
        "slug": "time-traveling-databases",
        "uri": "blog/time-traveling-databases",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/time-traveling-databases",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/time-traveling-databases",
        "isCommunityPost": false,
        "blogBodyText": "<p>Is a temporal database the same as a time-series database? This is a frequent point of confusion.</p>\r\n<h2>What is a time series?</h2>\r\n<p>Let’s start by discussing the more popular concept of the time-series database.</p>\r\n<p>Time-series databases are optimized for low-latency recording of values that change over time: often sampled data like temperature, stock price, or fuel consumption. They are also useful for counting and aggregating events: number of cars that pass over a road, number of votes cast, number of likes on a social media post. As such they are optimized heavily for writing numeric data.</p>\r\n<p>To achieve this, time-series databases support only very simple transaction patterns that do not involve multiple keys, types other than numbers, large data sizes, or indexes. This is fine. They make it easy to study aggregated trends across time periods. Complex business transactions remain the domain of operational databases, and complex analysis remains the domain of columnar or map/reduce analytics systems.</p>\r\n<blockquote>Time-series databases support only very simple transaction patterns.</blockquote>\r\n<h2>What is temporality?</h2>\r\n<p>The concept behind a temporal database is more subtle. Rather than recording sampled numeric data ordered by time, a temporal database tracks every change to business data within a retention period. In other words, it is a historical database. As transactions are processed, they append, rather than overwrite, previous state. The previous state of the world can be viewed by running a complex query with a timestamp in the past.</p>\r\n<h2>Temporality in FaunaDB</h2>\r\n<p>FaunaDB is a temporal database, not a time-series database. That said, FaunaDB can be a great solution for many time-series use cases involving values that change over time, for example, data center operational metrics.</p>\r\n<p>In FaunaDB, all records (including schema) are temporal and support configurable retention policies. When records are updated or deleted, their prior contents are not overwritten; instead, a new immutable version at the current transaction timestamp is inserted into the instance history, either as a create, update, or delete event. All transactions, including transactions involving indexes, can be executed at a point in the past (or the future!), or transformed into a change feed of the events between any two points in time.</p>\r\n<blockquote>In FaunaDB, all transactions can be executed at any point in the past or transformed into a change feed.</blockquote>\r\n<p>This is extremely useful for auditing business transactions, undoing developer mistakes or security breaches–even deleting an entire database can be reversed–, syncing partially-connected clients like mobile phones, constructing activity feeds, keeping analytics systems up-to-date and is a fundamental part of FaunaDB’s isolation model.</p>\r\n<h3>Instance History</h3>\r\n<p>One way temporality in FaunaDB makes developers’ lives easier is through ‘snapshots’. Imagine that you need to ask questions about the state of an entity at a specific time, or within a date range. For example, you are building a ‘Friend Locator’ app. Users check in to update their current location, which results in the database setting a field on the user instance:</p>\r\n<pre class=\"language-ruby\">update(ref(class('users'), 123), params: { data: { location: 'Sydney' } })\r\n</pre>\r\n<pre class=\"language-json\">{\r\n  \"ref\": { \"@ref\": \"classes/users/123\" },\r\n  \"ts\": &lt;clock_time&gt;,\r\n  \"data\": {\r\n    \"location\": \"Sydney\"\r\n  }\r\n}\r\n</pre>\r\n<p>Want to show the user where the user was at the same time last week? Simply retrieve the user record using a timestamp in the past:<br></p>\r\n<pre class=\"language-ruby\">get(ref(class('users'), 123), ts: &lt;week ago=\"\"&gt;)&lt;/week&gt;</pre>\r\n<pre class=\"language-json\">{\r\n  \"ref\": { \"@ref\": \"classes/users/123\" },\r\n  \"ts\": &lt;week_ago&gt;,\r\n  \"data\": {\r\n    \"location\": \"San Francisco\"\r\n  }\r\n}\r\n</pre>\r\n<h3>Index History</h3>\r\n<p>Or, since FaunaDB maintains temporality even in indexes, you can query an index for where all of a user’s friends were in the past:</p>\r\n<pre class=\"language-ruby\">paginate(match(index('friends_by_location'), ref(class('users'), 123)))</pre>\r\n<pre class=\"language-json\">{\r\n  \"data\": [\r\n    [\"Austin\", { \"@ref\": \"classes/users/789\" }],\r\n    [\"Los Angeles\", { \"@ref\": \"classes/users/234\" }],\r\n    [\"New York\", { \"@ref\": \"classes/users/456\" }],\r\n    [\"Oakland\", { \"@ref\": \"classes/users/567\" }]\r\n  ]\r\n}\r\n</pre>\r\n<pre class=\"language-ruby\">paginate(match(index('friends_by_location'), ref(class('users'), 123)), ts: &lt;week ago=\"\"&gt;)&lt;/week&gt;\r\n</pre>\r\n<pre class=\"language-json\">{\r\n  \"data\": [\r\n    [\"Chicago\", { \"@ref\": \"classes/users/456\" }],\r\n    [\"Fremont\", { \"@ref\": \"classes/users/789\" }],\r\n    [\"Houston\", { \"@ref\": \"classes/users/567\" }],\r\n    [\"San Diego\", { \"@ref\": \"classes/users/234\" }]\r\n  ]\r\n}\r\n</pre>\r\n<h3>Change Feeds</h3>\r\n<p>If you want to provide the user a journal view of where the user has been recently, FaunaDB’s events view takes temporality beyond snapshots. The events view returns a change feed of how data the result set changed over time:</p>\r\n<pre class=\"language-ruby\">map(paginate(ref(class('users'), 123), after: &lt;week_ago&gt;, events: true)) do |event|\r\n  get(select('resource', event), select('ts', event))\r\nend</pre>\r\n<pre class=\"language-json\">  \"data\": [\r\n    {\r\n      \"ref\": { \"@ref\": \"classes/users/123\" },\r\n      \"ts\": &lt;week_ago&gt;,\r\n      \"data\": {\r\n        \"location\": \"San Francisco\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": { \"@ref\": \"classes/users/123\" },\r\n      \"ts\": &lt;day_ago&gt;,\r\n      \"data\": {\r\n        \"location\": \"Melbourne\"\r\n      }\r\n    },\r\n    {\r\n      \"ref\": { \"@ref\": \"classes/users/123\" },\r\n      \"ts\": &lt;minute_ago&gt;,\r\n      \"data\": {\r\n        \"location\": \"Sydney\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n</pre>\r\n<h2>Conclusion</h2>\r\n<p>Time-series databases only store a sequence of numeric values. They cannot respond to queries more sophisticated than a simple list or aggregation of the numeric values they store.</p>\r\n<p>Temporal databases encode temporality into transactional query engines. For this reason, they are vastly more powerful and general purpose than time-series databases. In fact, you can easily create a time-series database within a temporal database by doing rollup aggregations. (Although FaunaDB does not currently support aggregations natively, rollup aggregations are very simple to implement on the application side.)</p>\r\n<blockquote>Temporal databases are vastly more powerful and general purpose than time-series databases.</blockquote>\r\n<p>This may bring to mind multi-version concurrency control (MVCC) in SQL systems, like PostgreSQL. It should! MVCC originally began as a specific application of general temporality. But hardware constraints at the time discouraged users from retaining more data than the bare minimum and the feature fell by the wayside.</p>\r\n<p>Those constraints don’t exist in the cloud era. So rather than garbage-collecting old data immediately, FaunaDB leaves retention policy up to you and your business. Further, neither Postgres nor MySQL database enables you to get a view of the minimal history it does maintain. FaunaDB not only exposes the history, it integrates it with the full power of the query engine.</p>\r\n<h2>Upcoming Posts</h2>\r\n<p>We look forward to providing more sophisticated examples of temporality in the future, as well as discussing in detail how the consistency model also takes advantage of FaunaDB’s temporal architecture. We will talk about these subjects in detail in forthcoming blog posts.</p>",
        "blogCategory": [
            "8",
            "10"
        ],
        "mainBlogImage": [],
        "bodyText": null
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "469",
        "postDate": "2016-10-03T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 488,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "d84adcd2-91cd-4313-b44b-e9c44cfa09d7",
        "siteSettingsId": 488,
        "fieldLayoutId": 4,
        "contentId": 321,
        "enabled": false,
        "archived": false,
        "siteId": 1,
        "title": "Not Your Typical Office Space",
        "slug": "not-your-regular-office-space",
        "uri": "blog/not-your-regular-office-space",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/not-your-regular-office-space",
        "status": "disabled",
        "structureId": null,
        "url": "https://fauna.com/blog/not-your-regular-office-space",
        "isCommunityPost": false,
        "blogBodyText": "<p>Diversity is an interesting contemplation for me. As the Director of Recruiting, it is always a pressing concern: how can I bring a variety of different ideas, perspectives, histories, and cultures together to draw out the best in the team? As a woman in a male-dominated industry, an immigrant, and a member of an ethnic and religious minority, the question has always been: will I fit in?</p>\r<p>Both of these thoughts swirled in my head when I first started at Fauna. I was transitioning from a cross-country move to a city I had never lived in. I was familiar with Fauna’s commitment to diversity; it was part of the reason I joined the team. But “diversity” has so many meanings to different organizations. For some, it is fulfilling a quota of “x” people” in “y” positions, or “z” years. For others, it is ensuring that every voice is heard, every view is seen, and every food is tasted (Thank you, San Francisco!). As you may be able to tell, Fauna is most certainly the latter.</p>\r<p>One insight into the diverse culture comes from my own experience. Moving to San Francisco, and subsequently leaving behind friends and family in New York City, was a risky decision. As with any new employment, I had to worry about understanding the jokes, the local events, the great spots for lunch – only with the added fear of being in a brand new city. Of course, it wasn’t long before I showed my lack of knowledge. I stared blankly when someone quoted a movie that everyone else in the office clearly knew.</p>\r<p>“It’s from Office Space.”</p>\r<p>“Oh&hellip; I haven’t seen it.”</p>\r<p>“What?!”</p>\r<p>Different companies I have worked for, and people I have worked with proceed from here in various ways. At Fauna, the entire team rallied around to organize a screening of the film. What could have easily turned into a moment of exclusion became one of inclusion. This moment, and the many others that followed, solidified that I had come to the right place. Where others may create walls, Fauna builds bridges. It fosters what so many other places try to: a team.</p>\r<p>At Fauna, diversity not only matters, it is celebrated. You can find more information about how Fauna celebrates diversity in the diversity statement on&nbsp;<a href=\"https://fauna.com/jobs\">our jobs page</a>.</p>\r<p>“We know that diverse teams are strong teams, and we welcome those with alternative identities, backgrounds, and experiences. Our team includes women, men, mothers, fathers, the self-taught, the college-educated, and people of various races, nationalities, ages, and socio-economic backgrounds.”</p>\r<p>Fauna sets a new standard for inclusion by setting a precedent that focuses on diversity early in the company’s development. Doing so has resulted in a socially aware and&nbsp;<a href=\"https://fauna.com/blog/supporting-civic-engagement\">conscious organization</a>. Our core values extend from our work life, to our home life, by providing time for parents and caregivers to tend to familial obligations, and time to recharge. At Fauna, we are a team – a team that can sit together after work and watch some 90’s movies that will forever be culturally relevant, minus the TPS reports.</p>\r<p>By the way, Office Space? It was a great movie.</p>",
        "blogCategory": [],
        "mainBlogImage": [],
        "bodyText": "<p>Diversity is an interesting contemplation for me. As the Director of Recruiting, it is always a pressing concern: how can I bring a variety of different ideas, perspectives, histories, and cultures together to draw out the best in the team? As a woman in a male-dominated industry, an immigrant, and a member of an ethnic and religious minority, the question has always been: will I fit in?</p>\n<p>Both of these thoughts swirled in my head when I first started at Fauna. I was transitioning from a cross-country move to a city I had never lived in. I was familiar with Fauna’s commitment to diversity; it was part of the reason I joined the team. But “diversity” has so many meanings to different organizations. For some, it is fulfilling a quota of “x” people” in “y” positions, or “z” years. For others, it is ensuring that every voice is heard, every view is seen, and every food is tasted (Thank you, San Francisco!). As you may be able to tell, Fauna is most certainly the latter.</p>\n<p>One insight into the diverse culture comes from my own experience. Moving to San Francisco, and subsequently leaving behind friends and family in New York City, was a risky decision. As with any new employment, I had to worry about understanding the jokes, the local events, the great spots for lunch – only with the added fear of being in a brand new city. Of course, it wasn’t long before I showed my lack of knowledge. I stared blankly when someone quoted a movie that everyone else in the office clearly knew.</p>\n<p>“It’s from Office Space.”</p>\n<p>“Oh… I haven’t seen it.”</p>\n<p>“What?!”</p>\n<p>Different companies I have worked for, and people I have worked with proceed from here in various ways. At Fauna, the entire team rallied around to organize a screening of the film. What could have easily turned into a moment of exclusion became one of inclusion. This moment, and the many others that followed, solidified that I had come to the right place. Where others may create walls, Fauna builds bridges. It fosters what so many other places try to: a team.</p>\n<p>At Fauna, diversity not only matters, it is celebrated. You can find more information about how Fauna celebrates diversity in the diversity statement on <a href=\"https://fauna.com/jobs\">our jobs page</a>.</p>\n<p>“We know that diverse teams are strong teams, and we welcome those with alternative identities, backgrounds, and experiences. Our team includes women, men, mothers, fathers, the self-taught, the college-educated, and people of various races, nationalities, ages, and socio-economic backgrounds.”</p>\n<p>Fauna sets a new standard for inclusion by setting a precedent that focuses on diversity early in the company’s development. Doing so has resulted in a socially aware and <a href=\"https://fauna.com/blog/supporting-civic-engagement\">conscious organization</a>. Our core values extend from our work life, to our home life, by providing time for parents and caregivers to tend to familial obligations, and time to recharge. At Fauna, we are a team – a team that can sit together after work and watch some 90’s movies that will forever be culturally relevant, minus the TPS reports.</p>\n<p>By the way, Office Space? It was a great movie.</p>"
    },
    {
        "sectionId": "2",
        "typeId": "2",
        "authorId": "474",
        "postDate": "2016-09-26T00:00:00-07:00",
        "expiryDate": null,
        "newParentId": null,
        "deletedWithEntryType": false,
        "id": 487,
        "tempId": null,
        "draftId": null,
        "revisionId": null,
        "uid": "cc0cad55-61ed-45f0-b3a3-c1cd361fe04c",
        "siteSettingsId": 487,
        "fieldLayoutId": 4,
        "contentId": 320,
        "enabled": true,
        "archived": false,
        "siteId": 1,
        "title": "Welcome to the Jungle",
        "slug": "welcome-to-the-jungle",
        "uri": "blog/welcome-to-the-jungle",
        "dateCreated": "2018-08-27T07:12:42-07:00",
        "dateUpdated": "2019-02-26T09:52:54-08:00",
        "dateDeleted": null,
        "trashed": false,
        "propagateAll": false,
        "newSiteIds": [],
        "resaving": false,
        "duplicateOf": null,
        "previewing": false,
        "hardDelete": false,
        "ref": "blog/welcome-to-the-jungle",
        "status": "live",
        "structureId": null,
        "url": "https://fauna.com/blog/welcome-to-the-jungle",
        "isCommunityPost": false,
        "blogBodyText": "<p>In early 2008, I joined Twitter as a backend engineer. First order of business: adding a server timeout. Second, something to display after that timeout: the famous fail whale page. At the time, MySQL scalability was our most critical bottleneck. Unable to find any off-the-shelf databases that would meet our needs, we did it ourselves and created the distributed data services that made tweeting reliable: the social graph, the timeline service, the image repository, the users database, and the tweet database.</p>\r\n<p>These databases shared an Achilles heel. Due to extreme hardware and time constraints, each system, though incredibly efficient, was dedicated to doing only one thing–serving millions of very specific requests per second. The rocket only went one way, limiting the product and the company from adapting to new business opportunities.</p>\r\n<blockquote>We were unable find any off-the-shelf databases that would meet our needs at Twitter.</blockquote>\r\n<p>Software infrastructure often inhibits innovation. Business people propose product improvements, but are blocked by the cost of modifying applications deployed at scale. Operators struggle to maintain reliability and efficiency as workloads change. Developers take on increasing amounts of technical debt to deliver features on time.</p>\r\n<p>When we left Twitter, we were surprised and disappointed to find that this fundamental computer science problem remained unsolved. So we set out to fix it–with the same team, the same experience, and the same proven ability to deliver. Our goal was to create a new type of data system: the adaptive database. Any size, any workload, any operating environment.</p>\r\n<blockquote>Our goal was to create a new type of data system: the adaptive database. Any size, any workload, any operating environment.</blockquote>\r\n<p>We have spent four years methodically building this solution. Although we have more work ahead of us, FaunaDB is production-ready and available now in managed beta in the cloud, on-premises, and in hybrid deployments. Although we are just emerging from stealth, FaunaDB is battle-hardened, running in dozens of data centers around the world, serving tens of millions of end-users in business-critical transactions.</p>\r\n<p>For the technically-minded, FaunaDB is a transactional, temporal, geographically distributed, strongly consistent, secure, multi-tenant, QoS-managed operational database. It’s implemented on the JVM for portability, and it’s relational, but not SQL. Instead, it’s queried via type-safe embedded DSLs, like LINQ. We have put a lot thought into FaunaDB and can’t wait to explain the reasoning behind each of our design decisions to the community.</p>\r\n<blockquote>Teams, products, and applications should coexist in one data infrastructure–just like a rainforest.</blockquote>\r\n<p>We named the company and the product Fauna because it reflects our vision of a dynamic, unified ecosystem. Teams, products, and applications should coexist, unconstrained by siloed and expensive legacy technology–just like a rainforest, where the environment supports a diverse, ever-evolving array of species, no resource is wasted, and constant adaptation is the key to success.</p>\r\n<p>To this end, I am thrilled to announce our&nbsp;<a href=\"http://www.prnewswire.com/news-releases/fauna-inc-raises-45m-in-funding-to-bring-industrys-first-adaptive-operational-database-to-market-300330693.html\">seed round</a>&nbsp;and our emergence from stealth. We’re at the beginning of a great migration in data. The journey will be long, but exciting. We hope you will join us on our trek.</p>",
        "blogCategory": [
            "1531"
        ],
        "mainBlogImage": [],
        "bodyText": "<p>In early 2008, I joined Twitter as a backend engineer. First order of business: adding a server timeout. Second, something to display after that timeout: the famous fail whale page. At the time, MySQL scalability was our most critical bottleneck. Unable to find any off-the-shelf databases that would meet our needs, we did it ourselves and created the distributed data services that made tweeting reliable: the social graph, the timeline service, the image repository, the users database, and the tweet database.</p>\n<p>These databases shared an Achilles heel. Due to extreme hardware and time constraints, each system, though incredibly efficient, was dedicated to doing only one thing–serving millions of very specific requests per second. The rocket only went one way, limiting the product and the company from adapting to new business opportunities.</p>\n<blockquote>We were unable find any off-the-shelf databases that would meet our needs at Twitter.</blockquote>\n<p>Software infrastructure often inhibits innovation. Business people propose product improvements, but are blocked by the cost of modifying applications deployed at scale. Operators struggle to maintain reliability and efficiency as workloads change. Developers take on increasing amounts of technical debt to deliver features on time.</p>\n<p>When we left Twitter, we were surprised and disappointed to find that this fundamental computer science problem remained unsolved. So we set out to fix it–with the same team, the same experience, and the same proven ability to deliver. Our goal was to create a new type of data system: the adaptive database. Any size, any workload, any operating environment.</p>\n<blockquote>Our goal was to create a new type of data system: the adaptive database. Any size, any workload, any operating environment.</blockquote>\n<p>We have spent four years methodically building this solution. Although we have more work ahead of us, FaunaDB is production-ready and available now in managed beta in the cloud, on-premises, and in hybrid deployments. Although we are just emerging from stealth, FaunaDB is battle-hardened, running in dozens of data centers around the world, serving tens of millions of end-users in business-critical transactions.</p>\n<p>For the technically-minded, FaunaDB is a transactional, temporal, geographically distributed, strongly consistent, secure, multi-tenant, QoS-managed operational database. It’s implemented on the JVM for portability, and it’s relational, but not SQL. Instead, it’s queried via type-safe embedded DSLs, like LINQ. We have put a lot thought into FaunaDB and can’t wait to explain the reasoning behind each of our design decisions to the community.</p>\n<blockquote>Teams, products, and applications should coexist in one data infrastructure–just like a rainforest.</blockquote>\n<p>We named the company and the product Fauna because it reflects our vision of a dynamic, unified ecosystem. Teams, products, and applications should coexist, unconstrained by siloed and expensive legacy technology–just like a rainforest, where the environment supports a diverse, ever-evolving array of species, no resource is wasted, and constant adaptation is the key to success.</p>\n<p>To this end, I am thrilled to announce our <a href=\"http://www.prnewswire.com/news-releases/fauna-inc-raises-45m-in-funding-to-bring-industrys-first-adaptive-operational-database-to-market-300330693.html\">seed round</a> and our emergence from stealth. We’re at the beginning of a great migration in data. The journey will be long, but exciting. We hope you will join us on our trek.</p>"
    }
]